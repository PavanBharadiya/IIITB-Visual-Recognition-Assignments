{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['iiitbmixed', 'face-combined-dataset', 'indian-movie-face-database-imfdb', 'iiitbcroppeddata', 'iiitbdata', 'celeba']\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Reshape, Dropout, Dense, Flatten, BatchNormalization, Activation, ZeroPadding2D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.optimizers import Adam,SGD,Nadam, RMSprop\n",
    "\n",
    "#matplotlib will help with displaying the results\n",
    "import matplotlib.pyplot as plt\n",
    "#numpy for some mathematical operations\n",
    "import numpy as np\n",
    "#PIL for opening,resizing and saving images\n",
    "from PIL import Image\n",
    "#tqdm for a progress bar when loading the dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "#os library is needed for extracting filenames from the dataset folder.\n",
    "import os\n",
    "print(os.listdir(\"../input\"))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "class FaceGenerator:\n",
    "    #RGB-images: 3-channels, grayscale: 1-channel, RGBA-images: 4-channels\n",
    "    def __init__(self,image_width,image_height,channels):\n",
    "        self.image_width = image_width\n",
    "        self.image_height = image_height\n",
    "\n",
    "        self.channels = channels\n",
    "\n",
    "        self.image_shape = (self.image_width,self.image_height,self.channels)\n",
    "\n",
    "        #Amount of randomly generated numbers for the first layer of the generator.\n",
    "        self.random_noise_dimension = 100\n",
    "\n",
    "        #Just 10 times higher learning rate would result in generator loss being stuck at 0.\n",
    "        optimizer = Adam(lr = 0.0002,beta_1 = 0.5)\n",
    "\n",
    "        self.discriminator = self.build_discriminator()\n",
    "        self.discriminator.compile(loss=\"binary_crossentropy\",optimizer=optimizer,metrics=[\"accuracy\"])\n",
    "        self.generator = self.build_generator()\n",
    "\n",
    "        #A placeholder for the generator input.\n",
    "        random_input = Input(shape=(self.random_noise_dimension,))\n",
    "\n",
    "        #Generator generates images from random noise.\n",
    "        generated_image = self.generator(random_input)\n",
    "\n",
    "        # For the combined model we will only train the generator\n",
    "        self.discriminator.trainable = False\n",
    "\n",
    "        #Discriminator attempts to determine if image is real or generated\n",
    "        validity = self.discriminator(generated_image)\n",
    "\n",
    "        #Combined model = generator and discriminator combined.\n",
    "        #1. Takes random noise as an input.\n",
    "        #2. Generates an image.\n",
    "        #3. Attempts to determine if image is real or generated.\n",
    "        self.combined = Model(random_input,validity)\n",
    "        self.combined.compile(loss=\"binary_crossentropy\",optimizer=optimizer)\n",
    "\n",
    "    def get_training_data(self,datafolder):\n",
    "        print(\"Loading training data...\")\n",
    "\n",
    "        training_data = []\n",
    "        count  = 0\n",
    "        #Finds all files in datafolder\n",
    "        filenames = os.listdir(datafolder)\n",
    "        for filename in tqdm(filenames):\n",
    "            #Combines folder name and file name.\n",
    "            path = os.path.join(datafolder,filename)\n",
    "            #Opens an image as an Image object.\n",
    "            image = Image.open(path)\n",
    "            #Resizes to a desired size.\n",
    "            image = image.resize((self.image_width,self.image_height),Image.ANTIALIAS)\n",
    "            #Creates an array of pixel values from the image.\n",
    "            pixel_array = np.asarray(image)\n",
    "\n",
    "            training_data.append(pixel_array)\n",
    "            count  = count + 1\n",
    "            \n",
    "            if(count  == 10000):\n",
    "                break\n",
    "                \n",
    "\n",
    "        #training_data is converted to a numpy array\n",
    "        training_data = np.reshape(training_data,(-1,self.image_width,self.image_height,self.channels))\n",
    "        return training_data\n",
    "\n",
    "\n",
    "    def build_generator(self):\n",
    "        #Generator attempts to fool discriminator by generating new images.\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Dense(256*4*4,activation=\"relu\",input_dim=self.random_noise_dimension))\n",
    "        model.add(Reshape((4,4,256)))\n",
    "\n",
    "        #Four layers of upsampling, convolution, batch normalization and activation.\n",
    "        # 1. Upsampling: Input data is repeated. Default is (2,2). In that case a 4x4x256 array becomes an 8x8x256 array.\n",
    "        # 2. Convolution: If you are not familiar, you should watch this video: https://www.youtube.com/watch?v=FTr3n7uBIuE\n",
    "        # 3. Normalization normalizes outputs from convolution.\n",
    "        # 4. Relu activation:  f(x) = max(0,x). If x < 0, then f(x) = 0.\n",
    "\n",
    "\n",
    "        model.add(UpSampling2D())\n",
    "        model.add(Conv2D(256,kernel_size=3,padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Activation(\"relu\"))\n",
    "\n",
    "        model.add(UpSampling2D())\n",
    "        model.add(Conv2D(256,kernel_size=3,padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Activation(\"relu\"))\n",
    "\n",
    "        model.add(UpSampling2D())\n",
    "        model.add(Conv2D(128,kernel_size=3,padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Activation(\"relu\"))\n",
    "\n",
    "        model.add(UpSampling2D())\n",
    "        model.add(Conv2D(128,kernel_size=3,padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Activation(\"relu\"))\n",
    "\n",
    "\n",
    "        # Last convolutional layer outputs as many featuremaps as channels in the final image.\n",
    "        model.add(Conv2D(self.channels,kernel_size=3,padding=\"same\"))\n",
    "        # tanh maps everything to a range between -1 and 1.\n",
    "        model.add(Activation(\"tanh\"))\n",
    "\n",
    "        # show the summary of the model architecture\n",
    "        model.summary()\n",
    "\n",
    "        # Placeholder for the random noise input\n",
    "        input = Input(shape=(self.random_noise_dimension,))\n",
    "        #Model output\n",
    "        generated_image = model(input)\n",
    "\n",
    "        #Change the model type from Sequential to Model (functional API) More at: https://keras.io/models/model/.\n",
    "        return Model(input,generated_image)\n",
    "\n",
    "\n",
    "    def build_discriminator(self):\n",
    "        #Discriminator attempts to classify real and generated images\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Conv2D(32, kernel_size=3, strides=2, input_shape=self.image_shape, padding=\"same\"))\n",
    "        #Leaky relu is similar to usual relu. If x < 0 then f(x) = x * alpha, otherwise f(x) = x.\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "        #Dropout blocks some connections randomly. This help the model to generalize better.\n",
    "        #0.25 means that every connection has a 25% chance of being blocked.\n",
    "        \n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Conv2D(64, kernel_size=3, strides=2, padding=\"same\"))\n",
    "        #Zero padding adds additional rows and columns to the image. Those rows and columns are made of zeros.\n",
    "        model.add(ZeroPadding2D(padding=((0,1),(0,1))))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Conv2D(128, kernel_size=3, strides=2, padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Conv2D(256, kernel_size=3, strides=1, padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Conv2D(512, kernel_size=3, strides=1, padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "        model.add(Dropout(0.25))\n",
    "        #Flatten layer flattens the output of the previous layer to a single dimension.\n",
    "        model.add(Flatten())\n",
    "        #Outputs a value between 0 and 1 that predicts whether image is real or generated. 0 = generated, 1 = real.\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "        input_image = Input(shape=self.image_shape)\n",
    "\n",
    "        #Model output given an image.\n",
    "        validity = model(input_image)\n",
    "\n",
    "        return Model(input_image, validity)\n",
    "\n",
    "    def train(self, datafolder ,epochs,batch_size,save_images_interval):\n",
    "        #Get the real images\n",
    "        training_data = self.get_training_data(datafolder)\n",
    "\n",
    "        #Map all values to a range between -1 and 1.\n",
    "        training_data = training_data / 127.5 - 1.\n",
    "\n",
    "        #Two arrays of labels. Labels for real images: [1,1,1 ... 1,1,1], labels for generated images: [0,0,0 ... 0,0,0]\n",
    "        labels_for_real_images = np.ones((batch_size,1))\n",
    "        labels_for_generated_images = np.zeros((batch_size,1))\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            # Select a random half of images\n",
    "            indices = np.random.randint(0,training_data.shape[0],batch_size)\n",
    "            real_images = training_data[indices]\n",
    "\n",
    "            #Generate random noise for a whole batch.\n",
    "            random_noise = np.random.normal(0,1,(batch_size,self.random_noise_dimension))\n",
    "            #Generate a batch of new images.\n",
    "            generated_images = self.generator.predict(random_noise)\n",
    "\n",
    "            #Train the discriminator on real images.\n",
    "            discriminator_loss_real = self.discriminator.train_on_batch(real_images,labels_for_real_images)\n",
    "            #Train the discriminator on generated images.\n",
    "            discriminator_loss_generated = self.discriminator.train_on_batch(generated_images,labels_for_generated_images)\n",
    "            #Calculate the average discriminator loss.\n",
    "            discriminator_loss = 0.5 * np.add(discriminator_loss_real,discriminator_loss_generated)\n",
    "\n",
    "            #Train the generator using the combined model. Generator tries to trick discriminator into mistaking generated images as real.\n",
    "            generator_loss = self.combined.train_on_batch(random_noise,labels_for_real_images)\n",
    "            print (\"%d [Discriminator loss: %f, acc.: %.2f%%] [Generator loss: %f]\" % (epoch, discriminator_loss[0], 100*discriminator_loss[1], generator_loss))\n",
    "\n",
    "            if epoch % save_images_interval == 0:\n",
    "                self.save_images(epoch)\n",
    "\n",
    "        #Save the model for a later use\n",
    "        #self.generator.save(\"saved_models/facegenerator.h5\")\n",
    "\n",
    "\n",
    "    def save_images(self,epoch):\n",
    "        #Save 25 generated images for demonstration purposes using matplotlib.pyplot.\n",
    "        rows, columns = 3, 3\n",
    "        noise = np.random.normal(0, 1, (rows * columns, self.random_noise_dimension))\n",
    "        generated_images = self.generator.predict(noise)\n",
    "\n",
    "        generated_images = 0.5 * generated_images + 0.5\n",
    "\n",
    "        figure, axis = plt.subplots(rows, columns)\n",
    "        image_count = 0\n",
    "        for row in range(rows):\n",
    "            for column in range(columns):\n",
    "                axis[row,column].imshow(generated_images[image_count, :], cmap='spring')\n",
    "                axis[row,column].axis('off')\n",
    "                image_count += 1\n",
    "        figure.savefig(\"new_generated_%d.png\" % epoch)\n",
    "        plt.close()\n",
    "\n",
    "    def generate_single_image(self,model_path,image_save_path):\n",
    "        noise = np.random.normal(0,1,(1,self.random_noise_dimension))\n",
    "        model = load_model(model_path)\n",
    "        generated_image = model.predict(noise)\n",
    "        #Normalized (-1 to 1) pixel values to the real (0 to 256) pixel values.\n",
    "        generated_image = (generated_image+1)*127.5\n",
    "        print(generated_image)\n",
    "        #Drop the batch dimension. From (1,w,h,c) to (w,h,c)\n",
    "        generated_image = np.reshape(generated_image,self.image_shape)\n",
    "\n",
    "        image = Image.fromarray(generated_image,\"RGB\")\n",
    "        image.save(image_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 32, 32, 32)        896       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 16, 16, 64)        18496     \n",
      "_________________________________________________________________\n",
      "zero_padding2d_1 (ZeroPaddin (None, 17, 17, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 17, 17, 64)        256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 17, 17, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 17, 17, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 9, 9, 128)         73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 9, 9, 128)         512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 9, 9, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 9, 9, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 9, 9, 256)         295168    \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 9, 9, 256)         1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 9, 9, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 9, 9, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 9, 9, 512)         1180160   \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 9, 9, 512)         2048      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 9, 9, 512)         0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 9, 9, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 41472)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 41473     \n",
      "=================================================================\n",
      "Total params: 1,613,889\n",
      "Trainable params: 1,611,969\n",
      "Non-trainable params: 1,920\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_2 (Dense)              (None, 4096)              413696    \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2 (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 8, 8, 256)         590080    \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 8, 8, 256)         1024      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_2 (UpSampling2 (None, 16, 16, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 16, 16, 256)       590080    \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 16, 16, 256)       1024      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 16, 16, 256)       0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_3 (UpSampling2 (None, 32, 32, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 32, 32, 128)       295040    \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 32, 32, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 32, 32, 128)       0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_4 (UpSampling2 (None, 64, 64, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 64, 64, 128)       147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 64, 64, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 64, 64, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 64, 64, 3)         3459      \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 64, 64, 3)         0         \n",
      "=================================================================\n",
      "Total params: 2,043,011\n",
      "Trainable params: 2,041,475\n",
      "Non-trainable params: 1,536\n",
      "_________________________________________________________________\n",
      "Loading training data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 9989/202599 [00:42<13:11, 243.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/keras/engine/training.py:490: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [Discriminator loss: 2.494061, acc.: 28.91%] [Generator loss: 0.304888]\n",
      "1 [Discriminator loss: 0.723092, acc.: 63.28%] [Generator loss: 2.666610]\n",
      "2 [Discriminator loss: 0.739049, acc.: 63.28%] [Generator loss: 2.121581]\n",
      "3 [Discriminator loss: 0.486298, acc.: 78.12%] [Generator loss: 1.280195]\n",
      "4 [Discriminator loss: 0.228636, acc.: 92.97%] [Generator loss: 0.809170]\n",
      "5 [Discriminator loss: 0.300765, acc.: 88.28%] [Generator loss: 1.677244]\n",
      "6 [Discriminator loss: 0.663156, acc.: 64.06%] [Generator loss: 4.768089]\n",
      "7 [Discriminator loss: 0.782762, acc.: 67.19%] [Generator loss: 2.760571]\n",
      "8 [Discriminator loss: 0.353271, acc.: 85.94%] [Generator loss: 2.264981]\n",
      "9 [Discriminator loss: 0.160199, acc.: 93.75%] [Generator loss: 1.159399]\n",
      "10 [Discriminator loss: 0.354208, acc.: 86.72%] [Generator loss: 1.199779]\n",
      "11 [Discriminator loss: 0.322933, acc.: 84.38%] [Generator loss: 2.140587]\n",
      "12 [Discriminator loss: 0.519498, acc.: 76.56%] [Generator loss: 4.952358]\n",
      "13 [Discriminator loss: 0.273106, acc.: 85.16%] [Generator loss: 5.680761]\n",
      "14 [Discriminator loss: 0.328831, acc.: 91.41%] [Generator loss: 5.191242]\n",
      "15 [Discriminator loss: 0.195833, acc.: 95.31%] [Generator loss: 5.190987]\n",
      "16 [Discriminator loss: 0.136745, acc.: 96.09%] [Generator loss: 6.069309]\n",
      "17 [Discriminator loss: 0.171564, acc.: 92.19%] [Generator loss: 4.542400]\n",
      "18 [Discriminator loss: 0.259205, acc.: 93.75%] [Generator loss: 2.949788]\n",
      "19 [Discriminator loss: 0.202440, acc.: 92.97%] [Generator loss: 1.055432]\n",
      "20 [Discriminator loss: 0.280684, acc.: 88.28%] [Generator loss: 2.760528]\n",
      "21 [Discriminator loss: 0.327330, acc.: 89.06%] [Generator loss: 3.363450]\n",
      "22 [Discriminator loss: 0.334367, acc.: 84.38%] [Generator loss: 7.279242]\n",
      "23 [Discriminator loss: 0.233482, acc.: 91.41%] [Generator loss: 5.874098]\n",
      "24 [Discriminator loss: 0.200139, acc.: 93.75%] [Generator loss: 5.363489]\n",
      "25 [Discriminator loss: 0.996905, acc.: 60.16%] [Generator loss: 12.247080]\n",
      "26 [Discriminator loss: 0.810644, acc.: 75.78%] [Generator loss: 8.563216]\n",
      "27 [Discriminator loss: 0.353305, acc.: 85.16%] [Generator loss: 5.757852]\n",
      "28 [Discriminator loss: 0.222204, acc.: 95.31%] [Generator loss: 3.233040]\n",
      "29 [Discriminator loss: 1.438995, acc.: 56.25%] [Generator loss: 10.832293]\n",
      "30 [Discriminator loss: 0.526695, acc.: 82.81%] [Generator loss: 7.334291]\n",
      "31 [Discriminator loss: 0.684514, acc.: 68.75%] [Generator loss: 10.095242]\n",
      "32 [Discriminator loss: 0.367298, acc.: 88.28%] [Generator loss: 8.674801]\n",
      "33 [Discriminator loss: 0.201542, acc.: 95.31%] [Generator loss: 3.216085]\n",
      "34 [Discriminator loss: 0.287708, acc.: 88.28%] [Generator loss: 3.203299]\n",
      "35 [Discriminator loss: 0.115430, acc.: 97.66%] [Generator loss: 2.452413]\n",
      "36 [Discriminator loss: 0.190799, acc.: 91.41%] [Generator loss: 2.497496]\n",
      "37 [Discriminator loss: 0.176875, acc.: 96.09%] [Generator loss: 2.705713]\n",
      "38 [Discriminator loss: 0.310066, acc.: 85.94%] [Generator loss: 6.328273]\n",
      "39 [Discriminator loss: 0.203096, acc.: 92.19%] [Generator loss: 3.521467]\n",
      "40 [Discriminator loss: 0.365260, acc.: 85.16%] [Generator loss: 4.311913]\n",
      "41 [Discriminator loss: 0.627067, acc.: 73.44%] [Generator loss: 8.867732]\n",
      "42 [Discriminator loss: 1.133174, acc.: 65.62%] [Generator loss: 4.991661]\n",
      "43 [Discriminator loss: 0.635843, acc.: 74.22%] [Generator loss: 4.327256]\n",
      "44 [Discriminator loss: 0.317022, acc.: 89.06%] [Generator loss: 1.022898]\n",
      "45 [Discriminator loss: 0.466386, acc.: 81.25%] [Generator loss: 4.222636]\n",
      "46 [Discriminator loss: 0.271564, acc.: 91.41%] [Generator loss: 3.233727]\n",
      "47 [Discriminator loss: 0.176916, acc.: 96.09%] [Generator loss: 0.424336]\n",
      "48 [Discriminator loss: 0.348182, acc.: 89.84%] [Generator loss: 1.823418]\n",
      "49 [Discriminator loss: 0.169156, acc.: 93.75%] [Generator loss: 1.174569]\n",
      "50 [Discriminator loss: 0.229824, acc.: 92.19%] [Generator loss: 1.353513]\n",
      "51 [Discriminator loss: 0.231815, acc.: 92.97%] [Generator loss: 1.293540]\n",
      "52 [Discriminator loss: 0.281285, acc.: 89.84%] [Generator loss: 2.563769]\n",
      "53 [Discriminator loss: 0.170020, acc.: 95.31%] [Generator loss: 3.446629]\n",
      "54 [Discriminator loss: 0.379041, acc.: 82.03%] [Generator loss: 4.945512]\n",
      "55 [Discriminator loss: 0.152409, acc.: 92.19%] [Generator loss: 2.054381]\n",
      "56 [Discriminator loss: 0.138756, acc.: 92.97%] [Generator loss: 0.479337]\n",
      "57 [Discriminator loss: 0.052264, acc.: 100.00%] [Generator loss: 0.330407]\n",
      "58 [Discriminator loss: 0.056482, acc.: 98.44%] [Generator loss: 0.560090]\n",
      "59 [Discriminator loss: 0.035480, acc.: 99.22%] [Generator loss: 0.622899]\n",
      "60 [Discriminator loss: 0.037016, acc.: 99.22%] [Generator loss: 0.441008]\n",
      "61 [Discriminator loss: 0.116307, acc.: 96.88%] [Generator loss: 0.894962]\n",
      "62 [Discriminator loss: 0.123931, acc.: 96.09%] [Generator loss: 0.953269]\n",
      "63 [Discriminator loss: 0.079988, acc.: 98.44%] [Generator loss: 1.937109]\n",
      "64 [Discriminator loss: 0.120245, acc.: 95.31%] [Generator loss: 1.683618]\n",
      "65 [Discriminator loss: 0.200361, acc.: 90.62%] [Generator loss: 3.645896]\n",
      "66 [Discriminator loss: 0.383633, acc.: 85.94%] [Generator loss: 2.778148]\n",
      "67 [Discriminator loss: 0.122881, acc.: 96.88%] [Generator loss: 0.661189]\n",
      "68 [Discriminator loss: 0.463990, acc.: 83.59%] [Generator loss: 6.379888]\n",
      "69 [Discriminator loss: 0.663007, acc.: 76.56%] [Generator loss: 6.593247]\n",
      "70 [Discriminator loss: 0.378638, acc.: 87.50%] [Generator loss: 0.643923]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 9989/202599 [00:59<13:11, 243.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71 [Discriminator loss: 0.169130, acc.: 93.75%] [Generator loss: 1.107778]\n",
      "72 [Discriminator loss: 0.388493, acc.: 86.72%] [Generator loss: 0.500643]\n",
      "73 [Discriminator loss: 0.490003, acc.: 82.03%] [Generator loss: 6.641741]\n",
      "74 [Discriminator loss: 1.304078, acc.: 59.38%] [Generator loss: 10.082106]\n",
      "75 [Discriminator loss: 1.554600, acc.: 69.53%] [Generator loss: 6.116855]\n",
      "76 [Discriminator loss: 1.089485, acc.: 53.91%] [Generator loss: 12.857650]\n",
      "77 [Discriminator loss: 0.138578, acc.: 93.75%] [Generator loss: 14.629406]\n",
      "78 [Discriminator loss: 0.254597, acc.: 95.31%] [Generator loss: 7.216669]\n",
      "79 [Discriminator loss: 1.783230, acc.: 53.12%] [Generator loss: 15.755770]\n",
      "80 [Discriminator loss: 0.854284, acc.: 84.38%] [Generator loss: 16.049721]\n",
      "81 [Discriminator loss: 0.954074, acc.: 81.25%] [Generator loss: 12.400682]\n",
      "82 [Discriminator loss: 0.909343, acc.: 70.31%] [Generator loss: 14.840240]\n",
      "83 [Discriminator loss: 0.245635, acc.: 92.19%] [Generator loss: 14.091536]\n",
      "84 [Discriminator loss: 0.340284, acc.: 92.97%] [Generator loss: 5.382803]\n",
      "85 [Discriminator loss: 1.734315, acc.: 50.00%] [Generator loss: 12.443850]\n",
      "86 [Discriminator loss: 1.204109, acc.: 70.31%] [Generator loss: 4.097641]\n",
      "87 [Discriminator loss: 0.626794, acc.: 84.38%] [Generator loss: 0.139522]\n",
      "88 [Discriminator loss: 0.504992, acc.: 80.47%] [Generator loss: 4.140119]\n",
      "89 [Discriminator loss: 0.138761, acc.: 93.75%] [Generator loss: 0.036852]\n",
      "90 [Discriminator loss: 0.281028, acc.: 87.50%] [Generator loss: 0.205930]\n",
      "91 [Discriminator loss: 0.247750, acc.: 93.75%] [Generator loss: 0.966686]\n",
      "92 [Discriminator loss: 0.540384, acc.: 78.12%] [Generator loss: 10.926440]\n",
      "93 [Discriminator loss: 0.218630, acc.: 91.41%] [Generator loss: 10.230787]\n",
      "94 [Discriminator loss: 0.504432, acc.: 85.94%] [Generator loss: 0.769793]\n",
      "95 [Discriminator loss: 0.598389, acc.: 79.69%] [Generator loss: 8.125210]\n",
      "96 [Discriminator loss: 0.360795, acc.: 91.41%] [Generator loss: 4.453581]\n",
      "97 [Discriminator loss: 4.640494, acc.: 37.50%] [Generator loss: 9.411304]\n",
      "98 [Discriminator loss: 1.451457, acc.: 74.22%] [Generator loss: 0.580237]\n",
      "99 [Discriminator loss: 0.706532, acc.: 74.22%] [Generator loss: 0.006401]\n",
      "100 [Discriminator loss: 0.105208, acc.: 96.09%] [Generator loss: 0.001260]\n",
      "101 [Discriminator loss: 0.024390, acc.: 99.22%] [Generator loss: 0.004157]\n",
      "102 [Discriminator loss: 0.051061, acc.: 97.66%] [Generator loss: 0.007336]\n",
      "103 [Discriminator loss: 0.102478, acc.: 96.09%] [Generator loss: 0.071080]\n",
      "104 [Discriminator loss: 0.016377, acc.: 99.22%] [Generator loss: 0.163611]\n",
      "105 [Discriminator loss: 0.101776, acc.: 96.09%] [Generator loss: 0.277325]\n",
      "106 [Discriminator loss: 0.065349, acc.: 96.88%] [Generator loss: 1.023275]\n",
      "107 [Discriminator loss: 0.344266, acc.: 86.72%] [Generator loss: 1.539725]\n",
      "108 [Discriminator loss: 0.367123, acc.: 87.50%] [Generator loss: 1.414353]\n",
      "109 [Discriminator loss: 0.351700, acc.: 88.28%] [Generator loss: 1.138256]\n",
      "110 [Discriminator loss: 0.850070, acc.: 67.97%] [Generator loss: 1.634518]\n",
      "111 [Discriminator loss: 0.810258, acc.: 75.00%] [Generator loss: 0.350317]\n",
      "112 [Discriminator loss: 0.569042, acc.: 82.81%] [Generator loss: 0.007232]\n",
      "113 [Discriminator loss: 0.381201, acc.: 84.38%] [Generator loss: 0.167746]\n",
      "114 [Discriminator loss: 0.203685, acc.: 91.41%] [Generator loss: 0.088790]\n",
      "115 [Discriminator loss: 0.121643, acc.: 96.88%] [Generator loss: 0.001628]\n",
      "116 [Discriminator loss: 0.292210, acc.: 87.50%] [Generator loss: 0.286293]\n",
      "117 [Discriminator loss: 0.185671, acc.: 89.84%] [Generator loss: 0.212947]\n",
      "118 [Discriminator loss: 0.207570, acc.: 92.19%] [Generator loss: 0.000846]\n",
      "119 [Discriminator loss: 0.236873, acc.: 88.28%] [Generator loss: 0.458628]\n",
      "120 [Discriminator loss: 0.094356, acc.: 96.88%] [Generator loss: 0.736424]\n",
      "121 [Discriminator loss: 0.361921, acc.: 89.84%] [Generator loss: 0.015744]\n",
      "122 [Discriminator loss: 0.124934, acc.: 95.31%] [Generator loss: 1.309139]\n",
      "123 [Discriminator loss: 0.579416, acc.: 73.44%] [Generator loss: 4.828495]\n",
      "124 [Discriminator loss: 1.039780, acc.: 62.50%] [Generator loss: 8.235968]\n",
      "125 [Discriminator loss: 0.485706, acc.: 85.94%] [Generator loss: 6.371454]\n",
      "126 [Discriminator loss: 0.740688, acc.: 71.09%] [Generator loss: 14.161959]\n",
      "127 [Discriminator loss: 0.317584, acc.: 90.62%] [Generator loss: 12.883902]\n",
      "128 [Discriminator loss: 0.214440, acc.: 91.41%] [Generator loss: 0.005689]\n",
      "129 [Discriminator loss: 0.767124, acc.: 66.41%] [Generator loss: 7.909839]\n",
      "130 [Discriminator loss: 0.373632, acc.: 87.50%] [Generator loss: 0.653616]\n",
      "131 [Discriminator loss: 0.401214, acc.: 83.59%] [Generator loss: 2.862910]\n",
      "132 [Discriminator loss: 0.575688, acc.: 86.72%] [Generator loss: 0.319380]\n",
      "133 [Discriminator loss: 0.286548, acc.: 88.28%] [Generator loss: 1.512724]\n",
      "134 [Discriminator loss: 0.107662, acc.: 96.09%] [Generator loss: 0.756759]\n",
      "135 [Discriminator loss: 0.245481, acc.: 92.97%] [Generator loss: 1.285846]\n",
      "136 [Discriminator loss: 0.322626, acc.: 82.81%] [Generator loss: 3.951252]\n",
      "137 [Discriminator loss: 0.526569, acc.: 79.69%] [Generator loss: 6.944009]\n",
      "138 [Discriminator loss: 0.359397, acc.: 87.50%] [Generator loss: 4.005968]\n",
      "139 [Discriminator loss: 0.817043, acc.: 68.75%] [Generator loss: 16.029297]\n",
      "140 [Discriminator loss: 1.250190, acc.: 71.09%] [Generator loss: 15.526056]\n",
      "141 [Discriminator loss: 0.196749, acc.: 92.19%] [Generator loss: 2.992299]\n",
      "142 [Discriminator loss: 3.423172, acc.: 47.66%] [Generator loss: 14.108111]\n",
      "143 [Discriminator loss: 0.342288, acc.: 88.28%] [Generator loss: 13.469138]\n",
      "144 [Discriminator loss: 0.813900, acc.: 82.03%] [Generator loss: 0.037873]\n",
      "145 [Discriminator loss: 0.511444, acc.: 78.12%] [Generator loss: 2.239787]\n",
      "146 [Discriminator loss: 0.171219, acc.: 96.09%] [Generator loss: 1.882299]\n",
      "147 [Discriminator loss: 0.198556, acc.: 92.97%] [Generator loss: 0.213820]\n",
      "148 [Discriminator loss: 0.941598, acc.: 67.97%] [Generator loss: 9.796133]\n",
      "149 [Discriminator loss: 0.195440, acc.: 92.97%] [Generator loss: 9.601227]\n",
      "150 [Discriminator loss: 0.640598, acc.: 77.34%] [Generator loss: 1.649974]\n",
      "151 [Discriminator loss: 0.332345, acc.: 89.84%] [Generator loss: 0.801666]\n",
      "152 [Discriminator loss: 0.208648, acc.: 91.41%] [Generator loss: 0.014589]\n",
      "153 [Discriminator loss: 0.329534, acc.: 85.94%] [Generator loss: 0.332912]\n",
      "154 [Discriminator loss: 0.161163, acc.: 97.66%] [Generator loss: 0.836116]\n",
      "155 [Discriminator loss: 0.168524, acc.: 94.53%] [Generator loss: 0.007529]\n",
      "156 [Discriminator loss: 0.465493, acc.: 82.03%] [Generator loss: 3.506988]\n",
      "157 [Discriminator loss: 0.307398, acc.: 92.19%] [Generator loss: 2.927450]\n",
      "158 [Discriminator loss: 0.083919, acc.: 94.53%] [Generator loss: 0.145054]\n",
      "159 [Discriminator loss: 0.287668, acc.: 89.06%] [Generator loss: 4.996624]\n",
      "160 [Discriminator loss: 0.301908, acc.: 90.62%] [Generator loss: 2.077493]\n",
      "161 [Discriminator loss: 0.649446, acc.: 71.09%] [Generator loss: 14.374523]\n",
      "162 [Discriminator loss: 1.337785, acc.: 72.66%] [Generator loss: 3.201713]\n",
      "163 [Discriminator loss: 4.569569, acc.: 47.66%] [Generator loss: 16.088198]\n",
      "164 [Discriminator loss: 1.533530, acc.: 66.41%] [Generator loss: 16.109015]\n",
      "165 [Discriminator loss: 0.900433, acc.: 82.03%] [Generator loss: 14.947026]\n",
      "166 [Discriminator loss: 0.071694, acc.: 96.88%] [Generator loss: 6.111787]\n",
      "167 [Discriminator loss: 0.744804, acc.: 71.88%] [Generator loss: 4.047625]\n",
      "168 [Discriminator loss: 0.253932, acc.: 91.41%] [Generator loss: 0.456528]\n",
      "169 [Discriminator loss: 0.355888, acc.: 92.19%] [Generator loss: 0.012633]\n",
      "170 [Discriminator loss: 0.778149, acc.: 69.53%] [Generator loss: 3.888742]\n",
      "171 [Discriminator loss: 0.210287, acc.: 91.41%] [Generator loss: 6.280615]\n",
      "172 [Discriminator loss: 0.243086, acc.: 91.41%] [Generator loss: 1.021713]\n",
      "173 [Discriminator loss: 1.642292, acc.: 60.16%] [Generator loss: 15.599932]\n",
      "174 [Discriminator loss: 1.204790, acc.: 78.12%] [Generator loss: 16.111862]\n",
      "175 [Discriminator loss: 0.502539, acc.: 87.50%] [Generator loss: 14.891310]\n",
      "176 [Discriminator loss: 0.029617, acc.: 99.22%] [Generator loss: 10.584780]\n",
      "177 [Discriminator loss: 0.069555, acc.: 97.66%] [Generator loss: 1.953866]\n",
      "178 [Discriminator loss: 1.607124, acc.: 51.56%] [Generator loss: 15.193288]\n",
      "179 [Discriminator loss: 1.075583, acc.: 78.91%] [Generator loss: 15.643076]\n",
      "180 [Discriminator loss: 0.771531, acc.: 81.25%] [Generator loss: 11.789170]\n",
      "181 [Discriminator loss: 0.394528, acc.: 89.84%] [Generator loss: 3.978787]\n",
      "182 [Discriminator loss: 2.571242, acc.: 47.66%] [Generator loss: 16.050240]\n",
      "183 [Discriminator loss: 1.569025, acc.: 72.66%] [Generator loss: 16.115349]\n",
      "184 [Discriminator loss: 0.871557, acc.: 82.03%] [Generator loss: 16.040249]\n",
      "185 [Discriminator loss: 0.135566, acc.: 95.31%] [Generator loss: 14.447939]\n",
      "186 [Discriminator loss: 2.137122, acc.: 60.94%] [Generator loss: 15.185108]\n",
      "187 [Discriminator loss: 0.408708, acc.: 92.19%] [Generator loss: 16.041481]\n",
      "188 [Discriminator loss: 0.775031, acc.: 82.03%] [Generator loss: 14.517543]\n",
      "189 [Discriminator loss: 0.510014, acc.: 82.81%] [Generator loss: 7.524702]\n",
      "190 [Discriminator loss: 4.210279, acc.: 50.78%] [Generator loss: 14.204997]\n",
      "191 [Discriminator loss: 1.454749, acc.: 74.22%] [Generator loss: 14.095015]\n",
      "192 [Discriminator loss: 0.851859, acc.: 81.25%] [Generator loss: 5.760481]\n",
      "193 [Discriminator loss: 1.121018, acc.: 64.84%] [Generator loss: 11.929913]\n",
      "194 [Discriminator loss: 0.512423, acc.: 89.84%] [Generator loss: 11.457710]\n",
      "195 [Discriminator loss: 0.318612, acc.: 92.19%] [Generator loss: 2.409973]\n",
      "196 [Discriminator loss: 1.350461, acc.: 60.16%] [Generator loss: 9.793890]\n",
      "197 [Discriminator loss: 0.402579, acc.: 86.72%] [Generator loss: 8.733751]\n",
      "198 [Discriminator loss: 0.392237, acc.: 88.28%] [Generator loss: 3.301518]\n",
      "199 [Discriminator loss: 0.968213, acc.: 69.53%] [Generator loss: 8.603007]\n",
      "200 [Discriminator loss: 0.710431, acc.: 85.16%] [Generator loss: 5.217375]\n",
      "201 [Discriminator loss: 0.903238, acc.: 63.28%] [Generator loss: 2.556455]\n",
      "202 [Discriminator loss: 0.918815, acc.: 77.34%] [Generator loss: 1.434657]\n",
      "203 [Discriminator loss: 1.088506, acc.: 61.72%] [Generator loss: 4.625702]\n",
      "204 [Discriminator loss: 1.592061, acc.: 59.38%] [Generator loss: 1.722804]\n",
      "205 [Discriminator loss: 1.277582, acc.: 58.59%] [Generator loss: 4.056198]\n",
      "206 [Discriminator loss: 0.310405, acc.: 90.62%] [Generator loss: 3.906473]\n",
      "207 [Discriminator loss: 0.722638, acc.: 79.69%] [Generator loss: 3.601069]\n",
      "208 [Discriminator loss: 0.158991, acc.: 96.09%] [Generator loss: 2.872508]\n",
      "209 [Discriminator loss: 0.564690, acc.: 76.56%] [Generator loss: 5.904033]\n",
      "210 [Discriminator loss: 0.309047, acc.: 89.06%] [Generator loss: 4.980515]\n",
      "211 [Discriminator loss: 0.371262, acc.: 84.38%] [Generator loss: 3.953336]\n",
      "212 [Discriminator loss: 0.323434, acc.: 83.59%] [Generator loss: 3.211826]\n",
      "213 [Discriminator loss: 0.118713, acc.: 95.31%] [Generator loss: 2.086710]\n",
      "214 [Discriminator loss: 0.423280, acc.: 85.16%] [Generator loss: 0.615208]\n",
      "215 [Discriminator loss: 0.365245, acc.: 84.38%] [Generator loss: 1.157546]\n",
      "216 [Discriminator loss: 0.265274, acc.: 88.28%] [Generator loss: 1.064916]\n",
      "217 [Discriminator loss: 0.225492, acc.: 90.62%] [Generator loss: 2.504168]\n",
      "218 [Discriminator loss: 0.459425, acc.: 79.69%] [Generator loss: 5.294301]\n",
      "219 [Discriminator loss: 0.510498, acc.: 85.16%] [Generator loss: 2.567280]\n",
      "220 [Discriminator loss: 1.020974, acc.: 64.06%] [Generator loss: 5.422475]\n",
      "221 [Discriminator loss: 0.540429, acc.: 83.59%] [Generator loss: 2.341014]\n",
      "222 [Discriminator loss: 2.077674, acc.: 43.75%] [Generator loss: 4.471844]\n",
      "223 [Discriminator loss: 0.661705, acc.: 82.03%] [Generator loss: 4.182298]\n",
      "224 [Discriminator loss: 2.140586, acc.: 39.06%] [Generator loss: 8.362706]\n",
      "225 [Discriminator loss: 1.333605, acc.: 68.75%] [Generator loss: 5.159660]\n",
      "226 [Discriminator loss: 4.526975, acc.: 27.34%] [Generator loss: 2.280871]\n",
      "227 [Discriminator loss: 1.021351, acc.: 78.12%] [Generator loss: 2.409192]\n",
      "228 [Discriminator loss: 1.019431, acc.: 68.75%] [Generator loss: 0.927551]\n",
      "229 [Discriminator loss: 0.541653, acc.: 87.50%] [Generator loss: 1.130629]\n",
      "230 [Discriminator loss: 0.143489, acc.: 94.53%] [Generator loss: 0.949188]\n",
      "231 [Discriminator loss: 1.085665, acc.: 60.94%] [Generator loss: 2.497167]\n",
      "232 [Discriminator loss: 0.287391, acc.: 86.72%] [Generator loss: 1.370209]\n",
      "233 [Discriminator loss: 1.526490, acc.: 53.12%] [Generator loss: 3.663073]\n",
      "234 [Discriminator loss: 0.595489, acc.: 80.47%] [Generator loss: 3.161896]\n",
      "235 [Discriminator loss: 1.200971, acc.: 43.75%] [Generator loss: 4.219749]\n",
      "236 [Discriminator loss: 1.462055, acc.: 53.12%] [Generator loss: 4.014381]\n",
      "237 [Discriminator loss: 0.797443, acc.: 70.31%] [Generator loss: 3.303693]\n",
      "238 [Discriminator loss: 0.522791, acc.: 75.78%] [Generator loss: 2.201712]\n",
      "239 [Discriminator loss: 1.289859, acc.: 57.81%] [Generator loss: 2.781307]\n",
      "240 [Discriminator loss: 1.609755, acc.: 36.72%] [Generator loss: 3.733125]\n",
      "241 [Discriminator loss: 1.499883, acc.: 46.09%] [Generator loss: 4.469518]\n",
      "242 [Discriminator loss: 1.121230, acc.: 55.47%] [Generator loss: 4.111562]\n",
      "243 [Discriminator loss: 1.274521, acc.: 47.66%] [Generator loss: 3.454216]\n",
      "244 [Discriminator loss: 0.767024, acc.: 69.53%] [Generator loss: 3.417869]\n",
      "245 [Discriminator loss: 1.125881, acc.: 54.69%] [Generator loss: 2.879388]\n",
      "246 [Discriminator loss: 0.391449, acc.: 82.81%] [Generator loss: 2.740782]\n",
      "247 [Discriminator loss: 0.639919, acc.: 74.22%] [Generator loss: 2.831743]\n",
      "248 [Discriminator loss: 0.442070, acc.: 78.12%] [Generator loss: 2.821517]\n",
      "249 [Discriminator loss: 0.767401, acc.: 64.06%] [Generator loss: 2.677132]\n",
      "250 [Discriminator loss: 0.573075, acc.: 68.75%] [Generator loss: 3.770331]\n",
      "251 [Discriminator loss: 1.136865, acc.: 54.69%] [Generator loss: 3.046018]\n",
      "252 [Discriminator loss: 1.341225, acc.: 42.97%] [Generator loss: 3.035465]\n",
      "253 [Discriminator loss: 0.999163, acc.: 60.16%] [Generator loss: 2.352155]\n",
      "254 [Discriminator loss: 1.418470, acc.: 46.09%] [Generator loss: 2.880163]\n",
      "255 [Discriminator loss: 1.226127, acc.: 54.69%] [Generator loss: 2.559256]\n",
      "256 [Discriminator loss: 1.171852, acc.: 47.66%] [Generator loss: 2.316995]\n",
      "257 [Discriminator loss: 0.974326, acc.: 57.81%] [Generator loss: 2.881321]\n",
      "258 [Discriminator loss: 0.885529, acc.: 61.72%] [Generator loss: 2.611550]\n",
      "259 [Discriminator loss: 1.042115, acc.: 52.34%] [Generator loss: 2.993401]\n",
      "260 [Discriminator loss: 1.309198, acc.: 44.53%] [Generator loss: 3.437091]\n",
      "261 [Discriminator loss: 0.832019, acc.: 55.47%] [Generator loss: 2.660489]\n",
      "262 [Discriminator loss: 0.713746, acc.: 64.84%] [Generator loss: 2.770961]\n",
      "263 [Discriminator loss: 0.956801, acc.: 51.56%] [Generator loss: 3.029503]\n",
      "264 [Discriminator loss: 0.744567, acc.: 67.19%] [Generator loss: 3.035033]\n",
      "265 [Discriminator loss: 0.506234, acc.: 76.56%] [Generator loss: 3.603779]\n",
      "266 [Discriminator loss: 0.603665, acc.: 75.78%] [Generator loss: 3.164606]\n",
      "267 [Discriminator loss: 0.941776, acc.: 60.94%] [Generator loss: 2.702558]\n",
      "268 [Discriminator loss: 1.114942, acc.: 54.69%] [Generator loss: 3.357013]\n",
      "269 [Discriminator loss: 1.152834, acc.: 50.00%] [Generator loss: 2.300750]\n",
      "270 [Discriminator loss: 1.263247, acc.: 46.09%] [Generator loss: 2.281439]\n",
      "271 [Discriminator loss: 0.578822, acc.: 70.31%] [Generator loss: 2.782445]\n",
      "272 [Discriminator loss: 0.509094, acc.: 73.44%] [Generator loss: 2.189407]\n",
      "273 [Discriminator loss: 1.237152, acc.: 43.75%] [Generator loss: 3.012719]\n",
      "274 [Discriminator loss: 1.140012, acc.: 48.44%] [Generator loss: 2.482060]\n",
      "275 [Discriminator loss: 0.879680, acc.: 55.47%] [Generator loss: 2.242426]\n",
      "276 [Discriminator loss: 0.912676, acc.: 53.12%] [Generator loss: 2.550792]\n",
      "277 [Discriminator loss: 0.505540, acc.: 71.88%] [Generator loss: 2.101114]\n",
      "278 [Discriminator loss: 0.778243, acc.: 60.94%] [Generator loss: 1.657141]\n",
      "279 [Discriminator loss: 0.520100, acc.: 77.34%] [Generator loss: 1.366568]\n",
      "280 [Discriminator loss: 0.353604, acc.: 87.50%] [Generator loss: 1.662211]\n",
      "281 [Discriminator loss: 0.296301, acc.: 87.50%] [Generator loss: 1.856802]\n",
      "282 [Discriminator loss: 0.583322, acc.: 71.09%] [Generator loss: 2.617258]\n",
      "283 [Discriminator loss: 0.418701, acc.: 85.16%] [Generator loss: 2.797126]\n",
      "284 [Discriminator loss: 0.925764, acc.: 60.94%] [Generator loss: 2.377331]\n",
      "285 [Discriminator loss: 0.730300, acc.: 64.06%] [Generator loss: 2.713578]\n",
      "286 [Discriminator loss: 0.629221, acc.: 69.53%] [Generator loss: 3.070153]\n",
      "287 [Discriminator loss: 0.561327, acc.: 75.78%] [Generator loss: 1.975739]\n",
      "288 [Discriminator loss: 0.325911, acc.: 85.16%] [Generator loss: 2.058695]\n",
      "289 [Discriminator loss: 0.268746, acc.: 89.84%] [Generator loss: 2.194733]\n",
      "290 [Discriminator loss: 0.474485, acc.: 78.91%] [Generator loss: 2.457003]\n",
      "291 [Discriminator loss: 0.592790, acc.: 74.22%] [Generator loss: 3.098351]\n",
      "292 [Discriminator loss: 0.544913, acc.: 73.44%] [Generator loss: 2.800149]\n",
      "293 [Discriminator loss: 0.696432, acc.: 67.97%] [Generator loss: 2.200025]\n",
      "294 [Discriminator loss: 0.429401, acc.: 80.47%] [Generator loss: 1.921029]\n",
      "295 [Discriminator loss: 0.489394, acc.: 77.34%] [Generator loss: 0.720543]\n",
      "296 [Discriminator loss: 0.437767, acc.: 82.03%] [Generator loss: 0.243379]\n",
      "297 [Discriminator loss: 0.418295, acc.: 80.47%] [Generator loss: 0.462233]\n",
      "298 [Discriminator loss: 0.262239, acc.: 92.19%] [Generator loss: 0.229154]\n",
      "299 [Discriminator loss: 0.531636, acc.: 68.75%] [Generator loss: 1.158920]\n",
      "300 [Discriminator loss: 0.561478, acc.: 76.56%] [Generator loss: 0.575904]\n",
      "301 [Discriminator loss: 0.538891, acc.: 76.56%] [Generator loss: 1.240003]\n",
      "302 [Discriminator loss: 0.768095, acc.: 61.72%] [Generator loss: 4.470638]\n",
      "303 [Discriminator loss: 1.808308, acc.: 27.34%] [Generator loss: 3.797123]\n",
      "304 [Discriminator loss: 1.703144, acc.: 34.38%] [Generator loss: 4.065773]\n",
      "305 [Discriminator loss: 0.856409, acc.: 65.62%] [Generator loss: 2.598458]\n",
      "306 [Discriminator loss: 0.770917, acc.: 70.31%] [Generator loss: 2.107967]\n",
      "307 [Discriminator loss: 0.722944, acc.: 70.31%] [Generator loss: 2.672974]\n",
      "308 [Discriminator loss: 1.335394, acc.: 46.09%] [Generator loss: 2.463464]\n",
      "309 [Discriminator loss: 0.920150, acc.: 63.28%] [Generator loss: 1.403929]\n",
      "310 [Discriminator loss: 0.713777, acc.: 66.41%] [Generator loss: 2.318528]\n",
      "311 [Discriminator loss: 0.359748, acc.: 86.72%] [Generator loss: 1.239672]\n",
      "312 [Discriminator loss: 0.540196, acc.: 76.56%] [Generator loss: 1.100779]\n",
      "313 [Discriminator loss: 0.290763, acc.: 89.06%] [Generator loss: 1.691208]\n",
      "314 [Discriminator loss: 0.344012, acc.: 85.94%] [Generator loss: 1.882496]\n",
      "315 [Discriminator loss: 0.374071, acc.: 83.59%] [Generator loss: 0.636552]\n",
      "316 [Discriminator loss: 0.399411, acc.: 79.69%] [Generator loss: 2.524744]\n",
      "317 [Discriminator loss: 0.302716, acc.: 87.50%] [Generator loss: 1.952646]\n",
      "318 [Discriminator loss: 0.869599, acc.: 60.94%] [Generator loss: 3.474646]\n",
      "319 [Discriminator loss: 0.618799, acc.: 75.78%] [Generator loss: 3.593167]\n",
      "320 [Discriminator loss: 0.847654, acc.: 53.12%] [Generator loss: 2.060669]\n",
      "321 [Discriminator loss: 1.166388, acc.: 46.09%] [Generator loss: 2.175720]\n",
      "322 [Discriminator loss: 0.557037, acc.: 76.56%] [Generator loss: 1.248847]\n",
      "323 [Discriminator loss: 0.475547, acc.: 77.34%] [Generator loss: 0.933697]\n",
      "324 [Discriminator loss: 0.459771, acc.: 75.78%] [Generator loss: 0.497058]\n",
      "325 [Discriminator loss: 0.465907, acc.: 78.12%] [Generator loss: 1.219920]\n",
      "326 [Discriminator loss: 0.410903, acc.: 83.59%] [Generator loss: 1.388907]\n",
      "327 [Discriminator loss: 0.664659, acc.: 64.06%] [Generator loss: 1.608994]\n",
      "328 [Discriminator loss: 0.881727, acc.: 56.25%] [Generator loss: 2.346197]\n",
      "329 [Discriminator loss: 1.193428, acc.: 42.97%] [Generator loss: 2.289649]\n",
      "330 [Discriminator loss: 1.118040, acc.: 51.56%] [Generator loss: 2.327646]\n",
      "331 [Discriminator loss: 1.171214, acc.: 53.12%] [Generator loss: 1.580713]\n",
      "332 [Discriminator loss: 0.784456, acc.: 58.59%] [Generator loss: 2.570101]\n",
      "333 [Discriminator loss: 0.968977, acc.: 57.03%] [Generator loss: 1.578927]\n",
      "334 [Discriminator loss: 0.838737, acc.: 53.91%] [Generator loss: 1.919735]\n",
      "335 [Discriminator loss: 0.761323, acc.: 65.62%] [Generator loss: 1.749968]\n",
      "336 [Discriminator loss: 0.651703, acc.: 69.53%] [Generator loss: 1.448301]\n",
      "337 [Discriminator loss: 0.780851, acc.: 61.72%] [Generator loss: 2.428227]\n",
      "338 [Discriminator loss: 0.659758, acc.: 72.66%] [Generator loss: 1.245902]\n",
      "339 [Discriminator loss: 1.064764, acc.: 40.62%] [Generator loss: 2.025052]\n",
      "340 [Discriminator loss: 1.173025, acc.: 41.41%] [Generator loss: 1.685856]\n",
      "341 [Discriminator loss: 1.020974, acc.: 48.44%] [Generator loss: 1.867466]\n",
      "342 [Discriminator loss: 1.100575, acc.: 36.72%] [Generator loss: 1.629505]\n",
      "343 [Discriminator loss: 0.929539, acc.: 56.25%] [Generator loss: 0.820327]\n",
      "344 [Discriminator loss: 0.764386, acc.: 58.59%] [Generator loss: 1.230785]\n",
      "345 [Discriminator loss: 0.379026, acc.: 84.38%] [Generator loss: 1.279676]\n",
      "346 [Discriminator loss: 0.594262, acc.: 67.97%] [Generator loss: 1.109061]\n",
      "347 [Discriminator loss: 0.539079, acc.: 71.88%] [Generator loss: 1.613815]\n",
      "348 [Discriminator loss: 0.400426, acc.: 84.38%] [Generator loss: 1.304982]\n",
      "349 [Discriminator loss: 0.617588, acc.: 69.53%] [Generator loss: 2.507666]\n",
      "350 [Discriminator loss: 0.621222, acc.: 67.97%] [Generator loss: 2.427938]\n",
      "351 [Discriminator loss: 0.871865, acc.: 54.69%] [Generator loss: 2.194586]\n",
      "352 [Discriminator loss: 0.464892, acc.: 82.81%] [Generator loss: 1.303992]\n",
      "353 [Discriminator loss: 0.213818, acc.: 91.41%] [Generator loss: 1.099243]\n",
      "354 [Discriminator loss: 0.407852, acc.: 87.50%] [Generator loss: 0.153174]\n",
      "355 [Discriminator loss: 0.308414, acc.: 85.16%] [Generator loss: 0.377323]\n",
      "356 [Discriminator loss: 0.141530, acc.: 96.88%] [Generator loss: 0.387243]\n",
      "357 [Discriminator loss: 0.175410, acc.: 92.97%] [Generator loss: 0.345822]\n",
      "358 [Discriminator loss: 0.258466, acc.: 86.72%] [Generator loss: 0.585178]\n",
      "359 [Discriminator loss: 0.205474, acc.: 95.31%] [Generator loss: 0.708698]\n",
      "360 [Discriminator loss: 0.831074, acc.: 61.72%] [Generator loss: 2.479529]\n",
      "361 [Discriminator loss: 0.518467, acc.: 78.91%] [Generator loss: 1.152540]\n",
      "362 [Discriminator loss: 0.918340, acc.: 57.81%] [Generator loss: 3.627783]\n",
      "363 [Discriminator loss: 0.758209, acc.: 65.62%] [Generator loss: 2.287670]\n",
      "364 [Discriminator loss: 0.766054, acc.: 68.75%] [Generator loss: 1.299242]\n",
      "365 [Discriminator loss: 0.559369, acc.: 69.53%] [Generator loss: 1.284906]\n",
      "366 [Discriminator loss: 0.787067, acc.: 61.72%] [Generator loss: 0.737849]\n",
      "367 [Discriminator loss: 0.478416, acc.: 80.47%] [Generator loss: 0.873161]\n",
      "368 [Discriminator loss: 0.570148, acc.: 78.91%] [Generator loss: 0.264409]\n",
      "369 [Discriminator loss: 0.417340, acc.: 80.47%] [Generator loss: 1.237565]\n",
      "370 [Discriminator loss: 0.225745, acc.: 92.19%] [Generator loss: 2.075880]\n",
      "371 [Discriminator loss: 0.215593, acc.: 91.41%] [Generator loss: 0.976353]\n",
      "372 [Discriminator loss: 0.286260, acc.: 85.94%] [Generator loss: 1.694434]\n",
      "373 [Discriminator loss: 0.204732, acc.: 89.06%] [Generator loss: 1.725700]\n",
      "374 [Discriminator loss: 0.650268, acc.: 75.00%] [Generator loss: 0.299926]\n",
      "375 [Discriminator loss: 0.151857, acc.: 95.31%] [Generator loss: 0.549198]\n",
      "376 [Discriminator loss: 0.964546, acc.: 58.59%] [Generator loss: 4.116158]\n",
      "377 [Discriminator loss: 1.307945, acc.: 57.03%] [Generator loss: 1.053495]\n",
      "378 [Discriminator loss: 1.476137, acc.: 43.75%] [Generator loss: 3.603784]\n",
      "379 [Discriminator loss: 0.870975, acc.: 57.81%] [Generator loss: 3.609882]\n",
      "380 [Discriminator loss: 1.547986, acc.: 33.59%] [Generator loss: 2.932067]\n",
      "381 [Discriminator loss: 0.780568, acc.: 65.62%] [Generator loss: 2.465574]\n",
      "382 [Discriminator loss: 0.299354, acc.: 88.28%] [Generator loss: 0.697400]\n",
      "383 [Discriminator loss: 0.366023, acc.: 82.03%] [Generator loss: 0.326088]\n",
      "384 [Discriminator loss: 0.239466, acc.: 85.16%] [Generator loss: 0.286041]\n",
      "385 [Discriminator loss: 0.191637, acc.: 95.31%] [Generator loss: 0.304575]\n",
      "386 [Discriminator loss: 0.243613, acc.: 92.19%] [Generator loss: 0.480153]\n",
      "387 [Discriminator loss: 0.299500, acc.: 86.72%] [Generator loss: 0.347932]\n",
      "388 [Discriminator loss: 0.520517, acc.: 80.47%] [Generator loss: 2.368404]\n",
      "389 [Discriminator loss: 0.535648, acc.: 78.12%] [Generator loss: 1.102464]\n",
      "390 [Discriminator loss: 2.442788, acc.: 30.47%] [Generator loss: 4.351030]\n",
      "391 [Discriminator loss: 1.243600, acc.: 56.25%] [Generator loss: 2.143457]\n",
      "392 [Discriminator loss: 1.329813, acc.: 47.66%] [Generator loss: 1.329319]\n",
      "393 [Discriminator loss: 1.000224, acc.: 53.91%] [Generator loss: 1.627167]\n",
      "394 [Discriminator loss: 1.010376, acc.: 55.47%] [Generator loss: 1.541291]\n",
      "395 [Discriminator loss: 0.733954, acc.: 60.94%] [Generator loss: 1.442010]\n",
      "396 [Discriminator loss: 0.439719, acc.: 76.56%] [Generator loss: 0.627899]\n",
      "397 [Discriminator loss: 1.005214, acc.: 50.78%] [Generator loss: 0.902262]\n",
      "398 [Discriminator loss: 0.611434, acc.: 76.56%] [Generator loss: 0.702563]\n",
      "399 [Discriminator loss: 1.330658, acc.: 36.72%] [Generator loss: 1.839335]\n",
      "400 [Discriminator loss: 0.401713, acc.: 81.25%] [Generator loss: 1.647880]\n",
      "401 [Discriminator loss: 1.147499, acc.: 48.44%] [Generator loss: 2.606415]\n",
      "402 [Discriminator loss: 0.502663, acc.: 76.56%] [Generator loss: 2.114861]\n",
      "403 [Discriminator loss: 1.341312, acc.: 39.06%] [Generator loss: 1.215617]\n",
      "404 [Discriminator loss: 0.766592, acc.: 60.94%] [Generator loss: 2.068904]\n",
      "405 [Discriminator loss: 0.534011, acc.: 71.88%] [Generator loss: 1.736809]\n",
      "406 [Discriminator loss: 0.603642, acc.: 67.19%] [Generator loss: 1.363999]\n",
      "407 [Discriminator loss: 0.812574, acc.: 56.25%] [Generator loss: 1.805669]\n",
      "408 [Discriminator loss: 0.384826, acc.: 81.25%] [Generator loss: 2.803084]\n",
      "409 [Discriminator loss: 0.731779, acc.: 65.62%] [Generator loss: 2.772692]\n",
      "410 [Discriminator loss: 0.862098, acc.: 59.38%] [Generator loss: 2.190768]\n",
      "411 [Discriminator loss: 0.718408, acc.: 67.19%] [Generator loss: 2.272757]\n",
      "412 [Discriminator loss: 0.879091, acc.: 53.12%] [Generator loss: 2.099258]\n",
      "413 [Discriminator loss: 1.053035, acc.: 53.12%] [Generator loss: 1.755038]\n",
      "414 [Discriminator loss: 0.476638, acc.: 85.16%] [Generator loss: 1.628892]\n",
      "415 [Discriminator loss: 0.723262, acc.: 58.59%] [Generator loss: 1.050081]\n",
      "416 [Discriminator loss: 0.611761, acc.: 71.09%] [Generator loss: 1.200881]\n",
      "417 [Discriminator loss: 0.588733, acc.: 66.41%] [Generator loss: 1.527181]\n",
      "418 [Discriminator loss: 0.430989, acc.: 84.38%] [Generator loss: 1.723343]\n",
      "419 [Discriminator loss: 0.564831, acc.: 67.97%] [Generator loss: 1.707794]\n",
      "420 [Discriminator loss: 0.694419, acc.: 61.72%] [Generator loss: 1.542549]\n",
      "421 [Discriminator loss: 0.634918, acc.: 64.84%] [Generator loss: 1.159770]\n",
      "422 [Discriminator loss: 0.807893, acc.: 60.16%] [Generator loss: 1.118798]\n",
      "423 [Discriminator loss: 0.487922, acc.: 75.78%] [Generator loss: 1.145729]\n",
      "424 [Discriminator loss: 0.597497, acc.: 75.00%] [Generator loss: 0.814836]\n",
      "425 [Discriminator loss: 0.886782, acc.: 51.56%] [Generator loss: 1.380500]\n",
      "426 [Discriminator loss: 0.998900, acc.: 46.88%] [Generator loss: 1.281924]\n",
      "427 [Discriminator loss: 0.868673, acc.: 51.56%] [Generator loss: 1.925802]\n",
      "428 [Discriminator loss: 0.980092, acc.: 54.69%] [Generator loss: 2.396021]\n",
      "429 [Discriminator loss: 1.053156, acc.: 45.31%] [Generator loss: 1.697425]\n",
      "430 [Discriminator loss: 0.842416, acc.: 59.38%] [Generator loss: 3.723211]\n",
      "431 [Discriminator loss: 1.646044, acc.: 34.38%] [Generator loss: 2.653753]\n",
      "432 [Discriminator loss: 1.919174, acc.: 21.88%] [Generator loss: 3.168771]\n",
      "433 [Discriminator loss: 1.814623, acc.: 28.91%] [Generator loss: 3.108631]\n",
      "434 [Discriminator loss: 1.209517, acc.: 37.50%] [Generator loss: 2.053390]\n",
      "435 [Discriminator loss: 0.981430, acc.: 54.69%] [Generator loss: 2.282303]\n",
      "436 [Discriminator loss: 0.880573, acc.: 57.81%] [Generator loss: 2.979192]\n",
      "437 [Discriminator loss: 1.148632, acc.: 45.31%] [Generator loss: 2.328804]\n",
      "438 [Discriminator loss: 0.786560, acc.: 64.84%] [Generator loss: 2.130662]\n",
      "439 [Discriminator loss: 0.893403, acc.: 58.59%] [Generator loss: 2.749740]\n",
      "440 [Discriminator loss: 0.691651, acc.: 71.09%] [Generator loss: 3.347015]\n",
      "441 [Discriminator loss: 0.959094, acc.: 51.56%] [Generator loss: 2.816125]\n",
      "442 [Discriminator loss: 0.700675, acc.: 64.84%] [Generator loss: 3.004982]\n",
      "443 [Discriminator loss: 0.987246, acc.: 50.78%] [Generator loss: 2.782803]\n",
      "444 [Discriminator loss: 1.130205, acc.: 47.66%] [Generator loss: 3.032816]\n",
      "445 [Discriminator loss: 1.393760, acc.: 39.84%] [Generator loss: 2.757625]\n",
      "446 [Discriminator loss: 1.526665, acc.: 40.62%] [Generator loss: 2.480814]\n",
      "447 [Discriminator loss: 1.618612, acc.: 34.38%] [Generator loss: 2.177779]\n",
      "448 [Discriminator loss: 1.107241, acc.: 42.19%] [Generator loss: 1.821613]\n",
      "449 [Discriminator loss: 0.714782, acc.: 61.72%] [Generator loss: 1.275681]\n",
      "450 [Discriminator loss: 1.155466, acc.: 49.22%] [Generator loss: 1.048875]\n",
      "451 [Discriminator loss: 0.585763, acc.: 75.00%] [Generator loss: 1.312902]\n",
      "452 [Discriminator loss: 0.786854, acc.: 60.16%] [Generator loss: 1.231396]\n",
      "453 [Discriminator loss: 0.626329, acc.: 65.62%] [Generator loss: 1.435514]\n",
      "454 [Discriminator loss: 0.917396, acc.: 59.38%] [Generator loss: 1.217947]\n",
      "455 [Discriminator loss: 0.937437, acc.: 51.56%] [Generator loss: 0.929182]\n",
      "456 [Discriminator loss: 0.892622, acc.: 51.56%] [Generator loss: 1.137587]\n",
      "457 [Discriminator loss: 0.361155, acc.: 80.47%] [Generator loss: 1.532054]\n",
      "458 [Discriminator loss: 0.580552, acc.: 71.88%] [Generator loss: 1.485669]\n",
      "459 [Discriminator loss: 0.673153, acc.: 60.16%] [Generator loss: 1.784611]\n",
      "460 [Discriminator loss: 0.570918, acc.: 72.66%] [Generator loss: 1.894216]\n",
      "461 [Discriminator loss: 0.733979, acc.: 63.28%] [Generator loss: 2.262405]\n",
      "462 [Discriminator loss: 0.560682, acc.: 73.44%] [Generator loss: 2.370062]\n",
      "463 [Discriminator loss: 0.962732, acc.: 47.66%] [Generator loss: 2.110315]\n",
      "464 [Discriminator loss: 0.938809, acc.: 53.91%] [Generator loss: 1.727608]\n",
      "465 [Discriminator loss: 0.847514, acc.: 47.66%] [Generator loss: 1.588888]\n",
      "466 [Discriminator loss: 0.726170, acc.: 65.62%] [Generator loss: 1.724833]\n",
      "467 [Discriminator loss: 0.785076, acc.: 60.16%] [Generator loss: 1.016548]\n",
      "468 [Discriminator loss: 0.838851, acc.: 55.47%] [Generator loss: 2.368644]\n",
      "469 [Discriminator loss: 0.644993, acc.: 70.31%] [Generator loss: 1.928664]\n",
      "470 [Discriminator loss: 0.840522, acc.: 53.91%] [Generator loss: 1.313749]\n",
      "471 [Discriminator loss: 0.718384, acc.: 67.19%] [Generator loss: 2.223723]\n",
      "472 [Discriminator loss: 0.761770, acc.: 69.53%] [Generator loss: 1.695989]\n",
      "473 [Discriminator loss: 0.825194, acc.: 57.81%] [Generator loss: 1.610324]\n",
      "474 [Discriminator loss: 0.542601, acc.: 77.34%] [Generator loss: 1.628193]\n",
      "475 [Discriminator loss: 1.077872, acc.: 42.97%] [Generator loss: 2.063070]\n",
      "476 [Discriminator loss: 1.060164, acc.: 39.84%] [Generator loss: 2.139701]\n",
      "477 [Discriminator loss: 1.372166, acc.: 33.59%] [Generator loss: 1.821609]\n",
      "478 [Discriminator loss: 1.196955, acc.: 35.94%] [Generator loss: 1.369458]\n",
      "479 [Discriminator loss: 1.020005, acc.: 35.94%] [Generator loss: 1.574280]\n",
      "480 [Discriminator loss: 0.712217, acc.: 52.34%] [Generator loss: 1.632753]\n",
      "481 [Discriminator loss: 0.721059, acc.: 64.84%] [Generator loss: 1.901677]\n",
      "482 [Discriminator loss: 1.155801, acc.: 39.06%] [Generator loss: 1.768789]\n",
      "483 [Discriminator loss: 0.684499, acc.: 61.72%] [Generator loss: 2.179696]\n",
      "484 [Discriminator loss: 1.083623, acc.: 41.41%] [Generator loss: 1.778920]\n",
      "485 [Discriminator loss: 0.871815, acc.: 53.12%] [Generator loss: 2.394347]\n",
      "486 [Discriminator loss: 1.251269, acc.: 38.28%] [Generator loss: 1.725383]\n",
      "487 [Discriminator loss: 1.205178, acc.: 36.72%] [Generator loss: 2.512018]\n",
      "488 [Discriminator loss: 0.920668, acc.: 53.12%] [Generator loss: 1.479016]\n",
      "489 [Discriminator loss: 1.269459, acc.: 37.50%] [Generator loss: 2.367238]\n",
      "490 [Discriminator loss: 1.042960, acc.: 52.34%] [Generator loss: 2.719101]\n",
      "491 [Discriminator loss: 1.062067, acc.: 42.19%] [Generator loss: 2.179297]\n",
      "492 [Discriminator loss: 1.081018, acc.: 45.31%] [Generator loss: 1.834939]\n",
      "493 [Discriminator loss: 0.895645, acc.: 53.91%] [Generator loss: 2.158765]\n",
      "494 [Discriminator loss: 1.084294, acc.: 41.41%] [Generator loss: 1.942811]\n",
      "495 [Discriminator loss: 0.956272, acc.: 54.69%] [Generator loss: 2.203152]\n",
      "496 [Discriminator loss: 1.332950, acc.: 37.50%] [Generator loss: 1.598051]\n",
      "497 [Discriminator loss: 0.738640, acc.: 61.72%] [Generator loss: 2.289037]\n",
      "498 [Discriminator loss: 0.903280, acc.: 50.78%] [Generator loss: 1.848349]\n",
      "499 [Discriminator loss: 0.947488, acc.: 51.56%] [Generator loss: 2.213420]\n",
      "500 [Discriminator loss: 1.022800, acc.: 43.75%] [Generator loss: 1.952345]\n",
      "501 [Discriminator loss: 1.104798, acc.: 42.19%] [Generator loss: 1.251276]\n",
      "502 [Discriminator loss: 0.834323, acc.: 52.34%] [Generator loss: 1.315508]\n",
      "503 [Discriminator loss: 0.560862, acc.: 76.56%] [Generator loss: 2.252666]\n",
      "504 [Discriminator loss: 0.582193, acc.: 70.31%] [Generator loss: 1.242387]\n",
      "505 [Discriminator loss: 0.602400, acc.: 69.53%] [Generator loss: 1.116232]\n",
      "506 [Discriminator loss: 0.540530, acc.: 72.66%] [Generator loss: 1.335526]\n",
      "507 [Discriminator loss: 0.667396, acc.: 65.62%] [Generator loss: 1.784056]\n",
      "508 [Discriminator loss: 0.597033, acc.: 70.31%] [Generator loss: 1.712664]\n",
      "509 [Discriminator loss: 0.836732, acc.: 51.56%] [Generator loss: 1.250680]\n",
      "510 [Discriminator loss: 0.794907, acc.: 57.81%] [Generator loss: 1.580724]\n",
      "511 [Discriminator loss: 0.967621, acc.: 48.44%] [Generator loss: 1.864629]\n",
      "512 [Discriminator loss: 1.045776, acc.: 44.53%] [Generator loss: 1.814012]\n",
      "513 [Discriminator loss: 0.966221, acc.: 47.66%] [Generator loss: 1.569082]\n",
      "514 [Discriminator loss: 0.845850, acc.: 53.91%] [Generator loss: 1.815723]\n",
      "515 [Discriminator loss: 1.050072, acc.: 44.53%] [Generator loss: 1.673250]\n",
      "516 [Discriminator loss: 0.772196, acc.: 54.69%] [Generator loss: 1.774194]\n",
      "517 [Discriminator loss: 0.790903, acc.: 60.94%] [Generator loss: 1.894143]\n",
      "518 [Discriminator loss: 0.737187, acc.: 62.50%] [Generator loss: 2.659341]\n",
      "519 [Discriminator loss: 0.776497, acc.: 64.06%] [Generator loss: 2.324071]\n",
      "520 [Discriminator loss: 0.840020, acc.: 51.56%] [Generator loss: 2.278266]\n",
      "521 [Discriminator loss: 0.633253, acc.: 64.84%] [Generator loss: 1.849811]\n",
      "522 [Discriminator loss: 1.023346, acc.: 44.53%] [Generator loss: 1.803400]\n",
      "523 [Discriminator loss: 1.224183, acc.: 32.03%] [Generator loss: 2.175611]\n",
      "524 [Discriminator loss: 0.778162, acc.: 54.69%] [Generator loss: 1.698231]\n",
      "525 [Discriminator loss: 1.146921, acc.: 35.94%] [Generator loss: 2.130973]\n",
      "526 [Discriminator loss: 0.884535, acc.: 50.00%] [Generator loss: 1.885418]\n",
      "527 [Discriminator loss: 0.815061, acc.: 50.78%] [Generator loss: 1.947922]\n",
      "528 [Discriminator loss: 0.972361, acc.: 50.78%] [Generator loss: 1.990279]\n",
      "529 [Discriminator loss: 0.729932, acc.: 67.97%] [Generator loss: 1.198251]\n",
      "530 [Discriminator loss: 0.406991, acc.: 82.03%] [Generator loss: 1.267161]\n",
      "531 [Discriminator loss: 0.376214, acc.: 85.94%] [Generator loss: 1.157223]\n",
      "532 [Discriminator loss: 0.440873, acc.: 80.47%] [Generator loss: 0.719874]\n",
      "533 [Discriminator loss: 0.573951, acc.: 71.09%] [Generator loss: 1.675951]\n",
      "534 [Discriminator loss: 0.411723, acc.: 84.38%] [Generator loss: 2.149208]\n",
      "535 [Discriminator loss: 0.684321, acc.: 70.31%] [Generator loss: 0.351103]\n",
      "536 [Discriminator loss: 0.570747, acc.: 71.09%] [Generator loss: 1.506332]\n",
      "537 [Discriminator loss: 0.207418, acc.: 89.84%] [Generator loss: 1.074266]\n",
      "538 [Discriminator loss: 0.371309, acc.: 85.16%] [Generator loss: 0.460387]\n",
      "539 [Discriminator loss: 1.977012, acc.: 22.66%] [Generator loss: 1.711185]\n",
      "540 [Discriminator loss: 0.588512, acc.: 75.00%] [Generator loss: 1.119259]\n",
      "541 [Discriminator loss: 0.375475, acc.: 82.81%] [Generator loss: 0.573755]\n",
      "542 [Discriminator loss: 0.569207, acc.: 70.31%] [Generator loss: 0.315007]\n",
      "543 [Discriminator loss: 0.350383, acc.: 87.50%] [Generator loss: 0.294494]\n",
      "544 [Discriminator loss: 0.737529, acc.: 64.06%] [Generator loss: 1.964658]\n",
      "545 [Discriminator loss: 2.387981, acc.: 17.19%] [Generator loss: 1.443207]\n",
      "546 [Discriminator loss: 2.114475, acc.: 14.06%] [Generator loss: 2.863414]\n",
      "547 [Discriminator loss: 1.097430, acc.: 42.97%] [Generator loss: 2.774687]\n",
      "548 [Discriminator loss: 1.023551, acc.: 55.47%] [Generator loss: 1.507097]\n",
      "549 [Discriminator loss: 0.474882, acc.: 78.12%] [Generator loss: 1.101614]\n",
      "550 [Discriminator loss: 0.581982, acc.: 69.53%] [Generator loss: 0.622434]\n",
      "551 [Discriminator loss: 0.329976, acc.: 85.16%] [Generator loss: 0.478291]\n",
      "552 [Discriminator loss: 0.264972, acc.: 89.84%] [Generator loss: 0.596154]\n",
      "553 [Discriminator loss: 0.374000, acc.: 85.94%] [Generator loss: 1.405952]\n",
      "554 [Discriminator loss: 0.222326, acc.: 92.19%] [Generator loss: 0.960615]\n",
      "555 [Discriminator loss: 0.278929, acc.: 88.28%] [Generator loss: 0.962847]\n",
      "556 [Discriminator loss: 0.392729, acc.: 85.16%] [Generator loss: 1.513500]\n",
      "557 [Discriminator loss: 0.523645, acc.: 77.34%] [Generator loss: 1.928128]\n",
      "558 [Discriminator loss: 0.818388, acc.: 54.69%] [Generator loss: 2.278634]\n",
      "559 [Discriminator loss: 1.389311, acc.: 39.06%] [Generator loss: 0.898147]\n",
      "560 [Discriminator loss: 0.384838, acc.: 81.25%] [Generator loss: 0.897238]\n",
      "561 [Discriminator loss: 0.281283, acc.: 90.62%] [Generator loss: 0.818539]\n",
      "562 [Discriminator loss: 0.682395, acc.: 58.59%] [Generator loss: 1.460466]\n",
      "563 [Discriminator loss: 0.259284, acc.: 90.62%] [Generator loss: 1.308611]\n",
      "564 [Discriminator loss: 0.563422, acc.: 72.66%] [Generator loss: 1.176554]\n",
      "565 [Discriminator loss: 1.127739, acc.: 41.41%] [Generator loss: 1.460083]\n",
      "566 [Discriminator loss: 0.563350, acc.: 70.31%] [Generator loss: 1.157880]\n",
      "567 [Discriminator loss: 1.171998, acc.: 42.97%] [Generator loss: 2.371136]\n",
      "568 [Discriminator loss: 0.450271, acc.: 78.91%] [Generator loss: 2.235144]\n",
      "569 [Discriminator loss: 0.933883, acc.: 51.56%] [Generator loss: 1.650768]\n",
      "570 [Discriminator loss: 0.867477, acc.: 56.25%] [Generator loss: 2.275705]\n",
      "571 [Discriminator loss: 0.975454, acc.: 49.22%] [Generator loss: 2.100593]\n",
      "572 [Discriminator loss: 0.703785, acc.: 64.84%] [Generator loss: 2.209516]\n",
      "573 [Discriminator loss: 0.587280, acc.: 74.22%] [Generator loss: 1.963256]\n",
      "574 [Discriminator loss: 0.664746, acc.: 68.75%] [Generator loss: 2.266994]\n",
      "575 [Discriminator loss: 0.755013, acc.: 65.62%] [Generator loss: 2.545032]\n",
      "576 [Discriminator loss: 0.811829, acc.: 57.03%] [Generator loss: 1.515405]\n",
      "577 [Discriminator loss: 0.525858, acc.: 76.56%] [Generator loss: 1.509594]\n",
      "578 [Discriminator loss: 1.290441, acc.: 39.06%] [Generator loss: 2.462402]\n",
      "579 [Discriminator loss: 1.872699, acc.: 20.31%] [Generator loss: 1.339042]\n",
      "580 [Discriminator loss: 0.858487, acc.: 57.03%] [Generator loss: 1.212861]\n",
      "581 [Discriminator loss: 1.140298, acc.: 40.62%] [Generator loss: 2.312262]\n",
      "582 [Discriminator loss: 0.855204, acc.: 64.06%] [Generator loss: 1.204732]\n",
      "583 [Discriminator loss: 0.577685, acc.: 69.53%] [Generator loss: 1.205924]\n",
      "584 [Discriminator loss: 0.452441, acc.: 81.25%] [Generator loss: 1.497190]\n",
      "585 [Discriminator loss: 0.571211, acc.: 71.88%] [Generator loss: 1.156641]\n",
      "586 [Discriminator loss: 0.370339, acc.: 83.59%] [Generator loss: 0.318288]\n",
      "587 [Discriminator loss: 0.466837, acc.: 76.56%] [Generator loss: 1.679851]\n",
      "588 [Discriminator loss: 1.406683, acc.: 28.12%] [Generator loss: 2.728834]\n",
      "589 [Discriminator loss: 0.803369, acc.: 58.59%] [Generator loss: 3.135443]\n",
      "590 [Discriminator loss: 1.190308, acc.: 39.06%] [Generator loss: 1.658208]\n",
      "591 [Discriminator loss: 1.147911, acc.: 39.84%] [Generator loss: 1.911345]\n",
      "592 [Discriminator loss: 0.826364, acc.: 58.59%] [Generator loss: 1.209047]\n",
      "593 [Discriminator loss: 0.606584, acc.: 72.66%] [Generator loss: 0.805086]\n",
      "594 [Discriminator loss: 0.431814, acc.: 77.34%] [Generator loss: 0.733732]\n",
      "595 [Discriminator loss: 0.393653, acc.: 80.47%] [Generator loss: 1.204545]\n",
      "596 [Discriminator loss: 0.522994, acc.: 69.53%] [Generator loss: 2.339766]\n",
      "597 [Discriminator loss: 0.259016, acc.: 87.50%] [Generator loss: 1.674948]\n",
      "598 [Discriminator loss: 0.491671, acc.: 78.12%] [Generator loss: 1.530151]\n",
      "599 [Discriminator loss: 0.329398, acc.: 85.94%] [Generator loss: 2.724365]\n",
      "600 [Discriminator loss: 0.837394, acc.: 57.03%] [Generator loss: 2.334522]\n",
      "601 [Discriminator loss: 0.847494, acc.: 60.94%] [Generator loss: 2.435761]\n",
      "602 [Discriminator loss: 0.636745, acc.: 67.97%] [Generator loss: 2.349940]\n",
      "603 [Discriminator loss: 0.894888, acc.: 57.81%] [Generator loss: 1.572508]\n",
      "604 [Discriminator loss: 0.850365, acc.: 57.81%] [Generator loss: 0.870066]\n",
      "605 [Discriminator loss: 0.419250, acc.: 78.91%] [Generator loss: 0.218968]\n",
      "606 [Discriminator loss: 0.909643, acc.: 59.38%] [Generator loss: 2.914175]\n",
      "607 [Discriminator loss: 0.809372, acc.: 59.38%] [Generator loss: 3.569722]\n",
      "608 [Discriminator loss: 1.363094, acc.: 35.94%] [Generator loss: 2.216664]\n",
      "609 [Discriminator loss: 0.700676, acc.: 67.97%] [Generator loss: 0.937734]\n",
      "610 [Discriminator loss: 0.712509, acc.: 69.53%] [Generator loss: 0.933187]\n",
      "611 [Discriminator loss: 0.589179, acc.: 70.31%] [Generator loss: 1.799071]\n",
      "612 [Discriminator loss: 0.438622, acc.: 79.69%] [Generator loss: 1.175560]\n",
      "613 [Discriminator loss: 1.214715, acc.: 46.09%] [Generator loss: 3.520880]\n",
      "614 [Discriminator loss: 0.902831, acc.: 67.19%] [Generator loss: 2.013348]\n",
      "615 [Discriminator loss: 1.006526, acc.: 56.25%] [Generator loss: 1.868192]\n",
      "616 [Discriminator loss: 0.598915, acc.: 69.53%] [Generator loss: 2.125310]\n",
      "617 [Discriminator loss: 0.672426, acc.: 67.19%] [Generator loss: 2.167092]\n",
      "618 [Discriminator loss: 0.770247, acc.: 57.03%] [Generator loss: 2.007275]\n",
      "619 [Discriminator loss: 1.258665, acc.: 31.25%] [Generator loss: 1.395840]\n",
      "620 [Discriminator loss: 0.521065, acc.: 72.66%] [Generator loss: 1.057633]\n",
      "621 [Discriminator loss: 0.457479, acc.: 82.81%] [Generator loss: 0.716480]\n",
      "622 [Discriminator loss: 0.726100, acc.: 67.97%] [Generator loss: 0.771505]\n",
      "623 [Discriminator loss: 0.489116, acc.: 74.22%] [Generator loss: 1.807260]\n",
      "624 [Discriminator loss: 0.480721, acc.: 71.88%] [Generator loss: 0.856694]\n",
      "625 [Discriminator loss: 0.259096, acc.: 91.41%] [Generator loss: 0.195284]\n",
      "626 [Discriminator loss: 0.760197, acc.: 60.16%] [Generator loss: 1.669952]\n",
      "627 [Discriminator loss: 0.591873, acc.: 74.22%] [Generator loss: 2.511662]\n",
      "628 [Discriminator loss: 0.893301, acc.: 46.88%] [Generator loss: 2.448411]\n",
      "629 [Discriminator loss: 1.313352, acc.: 29.69%] [Generator loss: 2.516330]\n",
      "630 [Discriminator loss: 0.631046, acc.: 66.41%] [Generator loss: 1.714760]\n",
      "631 [Discriminator loss: 1.171014, acc.: 38.28%] [Generator loss: 1.715123]\n",
      "632 [Discriminator loss: 0.778107, acc.: 57.03%] [Generator loss: 2.233500]\n",
      "633 [Discriminator loss: 1.361084, acc.: 30.47%] [Generator loss: 1.841279]\n",
      "634 [Discriminator loss: 0.536085, acc.: 73.44%] [Generator loss: 1.705762]\n",
      "635 [Discriminator loss: 0.891611, acc.: 53.12%] [Generator loss: 1.867621]\n",
      "636 [Discriminator loss: 0.641435, acc.: 66.41%] [Generator loss: 1.374841]\n",
      "637 [Discriminator loss: 0.535238, acc.: 75.78%] [Generator loss: 1.091497]\n",
      "638 [Discriminator loss: 0.317222, acc.: 85.94%] [Generator loss: 0.815696]\n",
      "639 [Discriminator loss: 0.278292, acc.: 87.50%] [Generator loss: 0.824495]\n",
      "640 [Discriminator loss: 0.512275, acc.: 75.00%] [Generator loss: 0.056960]\n",
      "641 [Discriminator loss: 0.746021, acc.: 60.94%] [Generator loss: 0.508223]\n",
      "642 [Discriminator loss: 0.248247, acc.: 89.84%] [Generator loss: 0.356448]\n",
      "643 [Discriminator loss: 2.019286, acc.: 35.16%] [Generator loss: 2.737447]\n",
      "644 [Discriminator loss: 1.723544, acc.: 45.31%] [Generator loss: 0.914975]\n",
      "645 [Discriminator loss: 0.739352, acc.: 60.16%] [Generator loss: 1.131152]\n",
      "646 [Discriminator loss: 0.960795, acc.: 52.34%] [Generator loss: 4.026315]\n",
      "647 [Discriminator loss: 0.833656, acc.: 66.41%] [Generator loss: 1.485850]\n",
      "648 [Discriminator loss: 1.454881, acc.: 42.97%] [Generator loss: 1.404524]\n",
      "649 [Discriminator loss: 0.645815, acc.: 71.88%] [Generator loss: 2.257206]\n",
      "650 [Discriminator loss: 0.458354, acc.: 78.12%] [Generator loss: 0.983188]\n",
      "651 [Discriminator loss: 0.608883, acc.: 65.62%] [Generator loss: 0.845947]\n",
      "652 [Discriminator loss: 0.391502, acc.: 81.25%] [Generator loss: 1.153403]\n",
      "653 [Discriminator loss: 0.500936, acc.: 78.91%] [Generator loss: 0.915614]\n",
      "654 [Discriminator loss: 0.416933, acc.: 82.03%] [Generator loss: 0.571189]\n",
      "655 [Discriminator loss: 0.538815, acc.: 75.00%] [Generator loss: 0.436356]\n",
      "656 [Discriminator loss: 1.221907, acc.: 40.62%] [Generator loss: 2.392498]\n",
      "657 [Discriminator loss: 0.730679, acc.: 68.75%] [Generator loss: 2.916924]\n",
      "658 [Discriminator loss: 1.204008, acc.: 43.75%] [Generator loss: 1.778440]\n",
      "659 [Discriminator loss: 0.420305, acc.: 80.47%] [Generator loss: 2.245024]\n",
      "660 [Discriminator loss: 0.940826, acc.: 58.59%] [Generator loss: 1.833930]\n",
      "661 [Discriminator loss: 0.483801, acc.: 75.78%] [Generator loss: 0.883473]\n",
      "662 [Discriminator loss: 1.085953, acc.: 46.09%] [Generator loss: 1.091877]\n",
      "663 [Discriminator loss: 1.037537, acc.: 54.69%] [Generator loss: 1.887471]\n",
      "664 [Discriminator loss: 0.942466, acc.: 50.78%] [Generator loss: 2.155527]\n",
      "665 [Discriminator loss: 0.659184, acc.: 69.53%] [Generator loss: 1.954542]\n",
      "666 [Discriminator loss: 0.970323, acc.: 50.00%] [Generator loss: 1.839238]\n",
      "667 [Discriminator loss: 1.123076, acc.: 42.97%] [Generator loss: 1.136752]\n",
      "668 [Discriminator loss: 0.732729, acc.: 58.59%] [Generator loss: 1.546029]\n",
      "669 [Discriminator loss: 1.568075, acc.: 25.00%] [Generator loss: 0.963135]\n",
      "670 [Discriminator loss: 0.656625, acc.: 58.59%] [Generator loss: 1.170199]\n",
      "671 [Discriminator loss: 0.932024, acc.: 50.00%] [Generator loss: 1.410468]\n",
      "672 [Discriminator loss: 0.508446, acc.: 76.56%] [Generator loss: 1.326489]\n",
      "673 [Discriminator loss: 0.937055, acc.: 46.88%] [Generator loss: 0.877792]\n",
      "674 [Discriminator loss: 0.782090, acc.: 55.47%] [Generator loss: 1.914971]\n",
      "675 [Discriminator loss: 0.634748, acc.: 64.84%] [Generator loss: 2.054395]\n",
      "676 [Discriminator loss: 0.952148, acc.: 50.00%] [Generator loss: 1.122890]\n",
      "677 [Discriminator loss: 0.893414, acc.: 50.78%] [Generator loss: 1.364913]\n",
      "678 [Discriminator loss: 1.085348, acc.: 39.84%] [Generator loss: 1.553669]\n",
      "679 [Discriminator loss: 0.662500, acc.: 68.75%] [Generator loss: 1.421070]\n",
      "680 [Discriminator loss: 0.783382, acc.: 53.91%] [Generator loss: 1.341639]\n",
      "681 [Discriminator loss: 0.941694, acc.: 43.75%] [Generator loss: 1.208132]\n",
      "682 [Discriminator loss: 1.463349, acc.: 24.22%] [Generator loss: 1.728034]\n",
      "683 [Discriminator loss: 1.332700, acc.: 28.91%] [Generator loss: 2.322547]\n",
      "684 [Discriminator loss: 0.702902, acc.: 59.38%] [Generator loss: 1.954701]\n",
      "685 [Discriminator loss: 0.790504, acc.: 55.47%] [Generator loss: 1.825275]\n",
      "686 [Discriminator loss: 0.576512, acc.: 71.88%] [Generator loss: 2.407613]\n",
      "687 [Discriminator loss: 0.621019, acc.: 71.88%] [Generator loss: 2.214489]\n",
      "688 [Discriminator loss: 0.874760, acc.: 52.34%] [Generator loss: 1.511476]\n",
      "689 [Discriminator loss: 0.909789, acc.: 47.66%] [Generator loss: 1.736301]\n",
      "690 [Discriminator loss: 1.188560, acc.: 35.94%] [Generator loss: 1.719705]\n",
      "691 [Discriminator loss: 1.127963, acc.: 33.59%] [Generator loss: 1.681183]\n",
      "692 [Discriminator loss: 1.212947, acc.: 29.69%] [Generator loss: 1.567145]\n",
      "693 [Discriminator loss: 0.785764, acc.: 60.16%] [Generator loss: 1.792275]\n",
      "694 [Discriminator loss: 0.892233, acc.: 49.22%] [Generator loss: 1.624920]\n",
      "695 [Discriminator loss: 0.800973, acc.: 53.91%] [Generator loss: 2.063941]\n",
      "696 [Discriminator loss: 1.048033, acc.: 41.41%] [Generator loss: 2.040918]\n",
      "697 [Discriminator loss: 0.975177, acc.: 53.12%] [Generator loss: 1.088597]\n",
      "698 [Discriminator loss: 1.188475, acc.: 36.72%] [Generator loss: 1.959902]\n",
      "699 [Discriminator loss: 0.592890, acc.: 68.75%] [Generator loss: 2.359439]\n",
      "700 [Discriminator loss: 1.049302, acc.: 43.75%] [Generator loss: 2.246808]\n",
      "701 [Discriminator loss: 0.741612, acc.: 57.03%] [Generator loss: 2.235469]\n",
      "702 [Discriminator loss: 1.216955, acc.: 32.81%] [Generator loss: 1.830683]\n",
      "703 [Discriminator loss: 0.814610, acc.: 57.81%] [Generator loss: 2.198675]\n",
      "704 [Discriminator loss: 1.072801, acc.: 41.41%] [Generator loss: 1.542783]\n",
      "705 [Discriminator loss: 0.893279, acc.: 48.44%] [Generator loss: 1.733409]\n",
      "706 [Discriminator loss: 0.673486, acc.: 68.75%] [Generator loss: 1.758286]\n",
      "707 [Discriminator loss: 0.786269, acc.: 56.25%] [Generator loss: 2.436208]\n",
      "708 [Discriminator loss: 0.639001, acc.: 68.75%] [Generator loss: 1.653058]\n",
      "709 [Discriminator loss: 0.897990, acc.: 54.69%] [Generator loss: 1.910238]\n",
      "710 [Discriminator loss: 0.806461, acc.: 57.81%] [Generator loss: 1.220226]\n",
      "711 [Discriminator loss: 1.026736, acc.: 43.75%] [Generator loss: 1.272378]\n",
      "712 [Discriminator loss: 1.023540, acc.: 47.66%] [Generator loss: 2.873001]\n",
      "713 [Discriminator loss: 1.002707, acc.: 51.56%] [Generator loss: 2.142906]\n",
      "714 [Discriminator loss: 0.805273, acc.: 59.38%] [Generator loss: 3.232715]\n",
      "715 [Discriminator loss: 0.491436, acc.: 78.91%] [Generator loss: 1.350001]\n",
      "716 [Discriminator loss: 0.998868, acc.: 57.03%] [Generator loss: 1.624568]\n",
      "717 [Discriminator loss: 0.676786, acc.: 70.31%] [Generator loss: 1.236038]\n",
      "718 [Discriminator loss: 0.888117, acc.: 49.22%] [Generator loss: 2.101637]\n",
      "719 [Discriminator loss: 0.469322, acc.: 77.34%] [Generator loss: 0.360603]\n",
      "720 [Discriminator loss: 0.206412, acc.: 95.31%] [Generator loss: 0.293491]\n",
      "721 [Discriminator loss: 0.261328, acc.: 87.50%] [Generator loss: 0.740431]\n",
      "722 [Discriminator loss: 0.079414, acc.: 98.44%] [Generator loss: 1.230541]\n",
      "723 [Discriminator loss: 0.156680, acc.: 95.31%] [Generator loss: 0.820452]\n",
      "724 [Discriminator loss: 0.469280, acc.: 71.88%] [Generator loss: 1.949912]\n",
      "725 [Discriminator loss: 0.353004, acc.: 82.81%] [Generator loss: 1.450240]\n",
      "726 [Discriminator loss: 2.915432, acc.: 12.50%] [Generator loss: 4.363945]\n",
      "727 [Discriminator loss: 0.524324, acc.: 76.56%] [Generator loss: 2.748853]\n",
      "728 [Discriminator loss: 0.893082, acc.: 54.69%] [Generator loss: 2.377134]\n",
      "729 [Discriminator loss: 0.927528, acc.: 59.38%] [Generator loss: 3.304513]\n",
      "730 [Discriminator loss: 0.849355, acc.: 57.81%] [Generator loss: 2.774945]\n",
      "731 [Discriminator loss: 1.026153, acc.: 53.12%] [Generator loss: 2.688686]\n",
      "732 [Discriminator loss: 0.445884, acc.: 81.25%] [Generator loss: 2.376592]\n",
      "733 [Discriminator loss: 0.480266, acc.: 71.88%] [Generator loss: 2.277432]\n",
      "734 [Discriminator loss: 0.556245, acc.: 75.00%] [Generator loss: 1.965576]\n",
      "735 [Discriminator loss: 1.007263, acc.: 55.47%] [Generator loss: 1.332790]\n",
      "736 [Discriminator loss: 0.568195, acc.: 71.09%] [Generator loss: 1.720081]\n",
      "737 [Discriminator loss: 0.957194, acc.: 54.69%] [Generator loss: 2.800138]\n",
      "738 [Discriminator loss: 0.490072, acc.: 77.34%] [Generator loss: 2.788937]\n",
      "739 [Discriminator loss: 0.750213, acc.: 65.62%] [Generator loss: 2.290665]\n",
      "740 [Discriminator loss: 0.704598, acc.: 66.41%] [Generator loss: 2.694076]\n",
      "741 [Discriminator loss: 0.632489, acc.: 75.00%] [Generator loss: 1.131376]\n",
      "742 [Discriminator loss: 0.473729, acc.: 75.78%] [Generator loss: 0.662986]\n",
      "743 [Discriminator loss: 0.209358, acc.: 92.97%] [Generator loss: 0.446712]\n",
      "744 [Discriminator loss: 0.192927, acc.: 92.97%] [Generator loss: 0.396084]\n",
      "745 [Discriminator loss: 0.180189, acc.: 92.97%] [Generator loss: 0.542039]\n",
      "746 [Discriminator loss: 0.099601, acc.: 98.44%] [Generator loss: 0.670153]\n",
      "747 [Discriminator loss: 0.133970, acc.: 96.88%] [Generator loss: 0.606091]\n",
      "748 [Discriminator loss: 0.155643, acc.: 96.88%] [Generator loss: 0.647429]\n",
      "749 [Discriminator loss: 0.146431, acc.: 95.31%] [Generator loss: 0.965136]\n",
      "750 [Discriminator loss: 0.204216, acc.: 89.84%] [Generator loss: 0.737759]\n",
      "751 [Discriminator loss: 0.093708, acc.: 99.22%] [Generator loss: 0.749475]\n",
      "752 [Discriminator loss: 0.478193, acc.: 76.56%] [Generator loss: 0.923481]\n",
      "753 [Discriminator loss: 0.210869, acc.: 92.19%] [Generator loss: 0.876461]\n",
      "754 [Discriminator loss: 0.552318, acc.: 78.91%] [Generator loss: 1.218124]\n",
      "755 [Discriminator loss: 0.545406, acc.: 76.56%] [Generator loss: 1.384601]\n",
      "756 [Discriminator loss: 0.590370, acc.: 71.09%] [Generator loss: 1.944581]\n",
      "757 [Discriminator loss: 2.266963, acc.: 14.84%] [Generator loss: 1.374728]\n",
      "758 [Discriminator loss: 0.933389, acc.: 51.56%] [Generator loss: 2.560245]\n",
      "759 [Discriminator loss: 0.683882, acc.: 65.62%] [Generator loss: 2.158364]\n",
      "760 [Discriminator loss: 1.077224, acc.: 48.44%] [Generator loss: 1.339337]\n",
      "761 [Discriminator loss: 0.785636, acc.: 61.72%] [Generator loss: 0.775502]\n",
      "762 [Discriminator loss: 0.906986, acc.: 52.34%] [Generator loss: 1.690623]\n",
      "763 [Discriminator loss: 0.732998, acc.: 64.84%] [Generator loss: 1.732631]\n",
      "764 [Discriminator loss: 0.923245, acc.: 53.91%] [Generator loss: 1.400252]\n",
      "765 [Discriminator loss: 0.798350, acc.: 49.22%] [Generator loss: 2.106001]\n",
      "766 [Discriminator loss: 0.871831, acc.: 54.69%] [Generator loss: 2.013019]\n",
      "767 [Discriminator loss: 0.736869, acc.: 59.38%] [Generator loss: 2.059166]\n",
      "768 [Discriminator loss: 0.683710, acc.: 65.62%] [Generator loss: 1.279428]\n",
      "769 [Discriminator loss: 0.717163, acc.: 62.50%] [Generator loss: 1.208188]\n",
      "770 [Discriminator loss: 0.708386, acc.: 58.59%] [Generator loss: 1.582127]\n",
      "771 [Discriminator loss: 0.541435, acc.: 71.09%] [Generator loss: 1.469057]\n",
      "772 [Discriminator loss: 0.429394, acc.: 79.69%] [Generator loss: 0.621047]\n",
      "773 [Discriminator loss: 0.786559, acc.: 59.38%] [Generator loss: 1.698426]\n",
      "774 [Discriminator loss: 0.494034, acc.: 78.91%] [Generator loss: 1.468225]\n",
      "775 [Discriminator loss: 0.919515, acc.: 49.22%] [Generator loss: 1.415863]\n",
      "776 [Discriminator loss: 0.646248, acc.: 64.84%] [Generator loss: 2.157086]\n",
      "777 [Discriminator loss: 0.674784, acc.: 64.06%] [Generator loss: 1.952359]\n",
      "778 [Discriminator loss: 0.715469, acc.: 63.28%] [Generator loss: 1.596676]\n",
      "779 [Discriminator loss: 1.136040, acc.: 46.09%] [Generator loss: 2.074035]\n",
      "780 [Discriminator loss: 1.192921, acc.: 42.97%] [Generator loss: 1.019615]\n",
      "781 [Discriminator loss: 1.251406, acc.: 42.97%] [Generator loss: 1.459327]\n",
      "782 [Discriminator loss: 0.737810, acc.: 58.59%] [Generator loss: 1.250864]\n",
      "783 [Discriminator loss: 0.934623, acc.: 49.22%] [Generator loss: 1.437187]\n",
      "784 [Discriminator loss: 0.878386, acc.: 50.00%] [Generator loss: 1.573439]\n",
      "785 [Discriminator loss: 0.776170, acc.: 65.62%] [Generator loss: 0.797981]\n",
      "786 [Discriminator loss: 1.200979, acc.: 42.19%] [Generator loss: 2.481989]\n",
      "787 [Discriminator loss: 0.645970, acc.: 67.97%] [Generator loss: 2.842143]\n",
      "788 [Discriminator loss: 0.764792, acc.: 65.62%] [Generator loss: 1.638932]\n",
      "789 [Discriminator loss: 0.921785, acc.: 54.69%] [Generator loss: 2.452541]\n",
      "790 [Discriminator loss: 0.725910, acc.: 61.72%] [Generator loss: 1.960829]\n",
      "791 [Discriminator loss: 0.967012, acc.: 47.66%] [Generator loss: 1.857087]\n",
      "792 [Discriminator loss: 0.595878, acc.: 68.75%] [Generator loss: 1.232176]\n",
      "793 [Discriminator loss: 0.561145, acc.: 71.09%] [Generator loss: 0.917252]\n",
      "794 [Discriminator loss: 0.289821, acc.: 87.50%] [Generator loss: 0.851211]\n",
      "795 [Discriminator loss: 0.608766, acc.: 65.62%] [Generator loss: 2.462434]\n",
      "796 [Discriminator loss: 0.666554, acc.: 67.97%] [Generator loss: 2.145442]\n",
      "797 [Discriminator loss: 0.628227, acc.: 67.19%] [Generator loss: 1.509444]\n",
      "798 [Discriminator loss: 0.573244, acc.: 64.06%] [Generator loss: 2.057265]\n",
      "799 [Discriminator loss: 0.357135, acc.: 88.28%] [Generator loss: 1.722444]\n",
      "800 [Discriminator loss: 0.441603, acc.: 80.47%] [Generator loss: 1.341193]\n",
      "801 [Discriminator loss: 0.664867, acc.: 60.16%] [Generator loss: 0.932228]\n",
      "802 [Discriminator loss: 0.527980, acc.: 75.78%] [Generator loss: 2.897761]\n",
      "803 [Discriminator loss: 0.874413, acc.: 58.59%] [Generator loss: 1.458411]\n",
      "804 [Discriminator loss: 0.949346, acc.: 50.78%] [Generator loss: 2.235838]\n",
      "805 [Discriminator loss: 1.045705, acc.: 48.44%] [Generator loss: 1.858073]\n",
      "806 [Discriminator loss: 0.895818, acc.: 53.12%] [Generator loss: 1.378707]\n",
      "807 [Discriminator loss: 0.958642, acc.: 47.66%] [Generator loss: 1.977821]\n",
      "808 [Discriminator loss: 1.193423, acc.: 34.38%] [Generator loss: 2.760748]\n",
      "809 [Discriminator loss: 0.671254, acc.: 64.84%] [Generator loss: 1.972347]\n",
      "810 [Discriminator loss: 0.889447, acc.: 59.38%] [Generator loss: 1.682565]\n",
      "811 [Discriminator loss: 0.556154, acc.: 73.44%] [Generator loss: 1.968319]\n",
      "812 [Discriminator loss: 0.848540, acc.: 49.22%] [Generator loss: 1.631929]\n",
      "813 [Discriminator loss: 0.691899, acc.: 64.06%] [Generator loss: 1.796965]\n",
      "814 [Discriminator loss: 0.856510, acc.: 52.34%] [Generator loss: 1.833656]\n",
      "815 [Discriminator loss: 0.682578, acc.: 60.94%] [Generator loss: 1.328661]\n",
      "816 [Discriminator loss: 0.401378, acc.: 79.69%] [Generator loss: 0.663659]\n",
      "817 [Discriminator loss: 0.887596, acc.: 49.22%] [Generator loss: 1.777481]\n",
      "818 [Discriminator loss: 0.404519, acc.: 80.47%] [Generator loss: 1.322302]\n",
      "819 [Discriminator loss: 0.810841, acc.: 51.56%] [Generator loss: 1.357201]\n",
      "820 [Discriminator loss: 0.634817, acc.: 70.31%] [Generator loss: 0.507150]\n",
      "821 [Discriminator loss: 0.592238, acc.: 66.41%] [Generator loss: 0.859915]\n",
      "822 [Discriminator loss: 0.864522, acc.: 52.34%] [Generator loss: 2.049945]\n",
      "823 [Discriminator loss: 0.392905, acc.: 79.69%] [Generator loss: 1.669983]\n",
      "824 [Discriminator loss: 0.852383, acc.: 49.22%] [Generator loss: 1.591178]\n",
      "825 [Discriminator loss: 0.524910, acc.: 75.00%] [Generator loss: 0.365559]\n",
      "826 [Discriminator loss: 1.438303, acc.: 39.84%] [Generator loss: 1.790580]\n",
      "827 [Discriminator loss: 0.843145, acc.: 62.50%] [Generator loss: 1.760002]\n",
      "828 [Discriminator loss: 0.909645, acc.: 52.34%] [Generator loss: 0.552625]\n",
      "829 [Discriminator loss: 0.362140, acc.: 83.59%] [Generator loss: 0.192667]\n",
      "830 [Discriminator loss: 0.783406, acc.: 58.59%] [Generator loss: 1.529040]\n",
      "831 [Discriminator loss: 1.375240, acc.: 32.03%] [Generator loss: 2.579165]\n",
      "832 [Discriminator loss: 0.459074, acc.: 79.69%] [Generator loss: 1.626422]\n",
      "833 [Discriminator loss: 0.756523, acc.: 57.81%] [Generator loss: 1.576132]\n",
      "834 [Discriminator loss: 1.090354, acc.: 48.44%] [Generator loss: 3.592719]\n",
      "835 [Discriminator loss: 1.687099, acc.: 24.22%] [Generator loss: 3.010807]\n",
      "836 [Discriminator loss: 0.877860, acc.: 55.47%] [Generator loss: 2.321387]\n",
      "837 [Discriminator loss: 1.340183, acc.: 31.25%] [Generator loss: 1.849997]\n",
      "838 [Discriminator loss: 0.271420, acc.: 89.84%] [Generator loss: 1.612002]\n",
      "839 [Discriminator loss: 1.357062, acc.: 33.59%] [Generator loss: 1.223462]\n",
      "840 [Discriminator loss: 0.548425, acc.: 72.66%] [Generator loss: 2.056068]\n",
      "841 [Discriminator loss: 0.866889, acc.: 56.25%] [Generator loss: 1.164042]\n",
      "842 [Discriminator loss: 0.754935, acc.: 66.41%] [Generator loss: 1.425255]\n",
      "843 [Discriminator loss: 0.794054, acc.: 61.72%] [Generator loss: 1.807935]\n",
      "844 [Discriminator loss: 0.818022, acc.: 59.38%] [Generator loss: 0.835876]\n",
      "845 [Discriminator loss: 0.714107, acc.: 63.28%] [Generator loss: 1.231640]\n",
      "846 [Discriminator loss: 0.544778, acc.: 71.88%] [Generator loss: 1.099004]\n",
      "847 [Discriminator loss: 0.461060, acc.: 77.34%] [Generator loss: 0.276328]\n",
      "848 [Discriminator loss: 0.512516, acc.: 72.66%] [Generator loss: 1.151179]\n",
      "849 [Discriminator loss: 0.351330, acc.: 81.25%] [Generator loss: 0.552888]\n",
      "850 [Discriminator loss: 0.411259, acc.: 85.94%] [Generator loss: 0.120635]\n",
      "851 [Discriminator loss: 0.425588, acc.: 76.56%] [Generator loss: 0.811652]\n",
      "852 [Discriminator loss: 0.431001, acc.: 79.69%] [Generator loss: 2.191375]\n",
      "853 [Discriminator loss: 0.576141, acc.: 71.09%] [Generator loss: 0.649800]\n",
      "854 [Discriminator loss: 1.011812, acc.: 52.34%] [Generator loss: 2.719275]\n",
      "855 [Discriminator loss: 1.017228, acc.: 60.16%] [Generator loss: 1.579646]\n",
      "856 [Discriminator loss: 1.026090, acc.: 46.88%] [Generator loss: 2.872608]\n",
      "857 [Discriminator loss: 0.728520, acc.: 68.75%] [Generator loss: 2.313411]\n",
      "858 [Discriminator loss: 1.337617, acc.: 30.47%] [Generator loss: 1.919996]\n",
      "859 [Discriminator loss: 0.788530, acc.: 61.72%] [Generator loss: 2.723106]\n",
      "860 [Discriminator loss: 0.937976, acc.: 55.47%] [Generator loss: 2.175523]\n",
      "861 [Discriminator loss: 0.918995, acc.: 50.78%] [Generator loss: 2.017355]\n",
      "862 [Discriminator loss: 0.906465, acc.: 54.69%] [Generator loss: 2.207128]\n",
      "863 [Discriminator loss: 0.590857, acc.: 70.31%] [Generator loss: 2.388032]\n",
      "864 [Discriminator loss: 0.758030, acc.: 61.72%] [Generator loss: 2.340808]\n",
      "865 [Discriminator loss: 0.658360, acc.: 63.28%] [Generator loss: 1.970103]\n",
      "866 [Discriminator loss: 1.290403, acc.: 36.72%] [Generator loss: 1.944776]\n",
      "867 [Discriminator loss: 0.833138, acc.: 57.81%] [Generator loss: 1.752518]\n",
      "868 [Discriminator loss: 1.364259, acc.: 39.84%] [Generator loss: 1.893447]\n",
      "869 [Discriminator loss: 0.833766, acc.: 53.91%] [Generator loss: 1.602199]\n",
      "870 [Discriminator loss: 1.187785, acc.: 34.38%] [Generator loss: 1.222947]\n",
      "871 [Discriminator loss: 0.815694, acc.: 60.16%] [Generator loss: 0.591842]\n",
      "872 [Discriminator loss: 0.521517, acc.: 75.00%] [Generator loss: 0.680790]\n",
      "873 [Discriminator loss: 0.411464, acc.: 82.03%] [Generator loss: 0.793943]\n",
      "874 [Discriminator loss: 0.704814, acc.: 67.97%] [Generator loss: 2.222672]\n",
      "875 [Discriminator loss: 1.030591, acc.: 50.78%] [Generator loss: 2.617144]\n",
      "876 [Discriminator loss: 1.119580, acc.: 55.47%] [Generator loss: 1.871486]\n",
      "877 [Discriminator loss: 0.839689, acc.: 62.50%] [Generator loss: 1.241743]\n",
      "878 [Discriminator loss: 1.037439, acc.: 42.97%] [Generator loss: 2.118738]\n",
      "879 [Discriminator loss: 0.411698, acc.: 75.00%] [Generator loss: 1.481957]\n",
      "880 [Discriminator loss: 0.599255, acc.: 67.97%] [Generator loss: 1.316180]\n",
      "881 [Discriminator loss: 0.348727, acc.: 84.38%] [Generator loss: 2.579305]\n",
      "882 [Discriminator loss: 0.694663, acc.: 63.28%] [Generator loss: 1.611567]\n",
      "883 [Discriminator loss: 1.024598, acc.: 50.00%] [Generator loss: 1.556430]\n",
      "884 [Discriminator loss: 0.875285, acc.: 58.59%] [Generator loss: 1.662405]\n",
      "885 [Discriminator loss: 0.959729, acc.: 50.78%] [Generator loss: 1.832189]\n",
      "886 [Discriminator loss: 0.963731, acc.: 45.31%] [Generator loss: 1.754405]\n",
      "887 [Discriminator loss: 0.919109, acc.: 48.44%] [Generator loss: 0.831078]\n",
      "888 [Discriminator loss: 0.840620, acc.: 53.12%] [Generator loss: 1.776592]\n",
      "889 [Discriminator loss: 0.410658, acc.: 80.47%] [Generator loss: 1.435735]\n",
      "890 [Discriminator loss: 0.840320, acc.: 56.25%] [Generator loss: 1.092042]\n",
      "891 [Discriminator loss: 0.724863, acc.: 60.94%] [Generator loss: 2.369169]\n",
      "892 [Discriminator loss: 0.568935, acc.: 74.22%] [Generator loss: 1.521002]\n",
      "893 [Discriminator loss: 0.674512, acc.: 64.06%] [Generator loss: 1.011736]\n",
      "894 [Discriminator loss: 0.581877, acc.: 71.88%] [Generator loss: 2.100095]\n",
      "895 [Discriminator loss: 0.809940, acc.: 59.38%] [Generator loss: 2.019554]\n",
      "896 [Discriminator loss: 1.387716, acc.: 32.03%] [Generator loss: 1.689152]\n",
      "897 [Discriminator loss: 0.847161, acc.: 58.59%] [Generator loss: 2.313785]\n",
      "898 [Discriminator loss: 1.230705, acc.: 36.72%] [Generator loss: 1.618613]\n",
      "899 [Discriminator loss: 0.748963, acc.: 61.72%] [Generator loss: 2.232929]\n",
      "900 [Discriminator loss: 0.597881, acc.: 68.75%] [Generator loss: 1.941507]\n",
      "901 [Discriminator loss: 0.890320, acc.: 50.00%] [Generator loss: 1.788638]\n",
      "902 [Discriminator loss: 0.908534, acc.: 50.78%] [Generator loss: 1.686247]\n",
      "903 [Discriminator loss: 0.744103, acc.: 57.81%] [Generator loss: 1.504782]\n",
      "904 [Discriminator loss: 0.632570, acc.: 68.75%] [Generator loss: 1.402538]\n",
      "905 [Discriminator loss: 0.694180, acc.: 59.38%] [Generator loss: 1.109526]\n",
      "906 [Discriminator loss: 0.577500, acc.: 73.44%] [Generator loss: 1.073339]\n",
      "907 [Discriminator loss: 1.126069, acc.: 37.50%] [Generator loss: 1.334311]\n",
      "908 [Discriminator loss: 0.627368, acc.: 68.75%] [Generator loss: 0.866575]\n",
      "909 [Discriminator loss: 0.702812, acc.: 62.50%] [Generator loss: 1.618721]\n",
      "910 [Discriminator loss: 0.689068, acc.: 62.50%] [Generator loss: 1.419452]\n",
      "911 [Discriminator loss: 1.169832, acc.: 36.72%] [Generator loss: 1.665797]\n",
      "912 [Discriminator loss: 0.835513, acc.: 53.12%] [Generator loss: 1.396831]\n",
      "913 [Discriminator loss: 1.323132, acc.: 39.84%] [Generator loss: 2.284027]\n",
      "914 [Discriminator loss: 0.615148, acc.: 65.62%] [Generator loss: 2.234540]\n",
      "915 [Discriminator loss: 0.921243, acc.: 53.12%] [Generator loss: 1.579451]\n",
      "916 [Discriminator loss: 0.860196, acc.: 56.25%] [Generator loss: 0.725342]\n",
      "917 [Discriminator loss: 0.623760, acc.: 64.84%] [Generator loss: 1.290690]\n",
      "918 [Discriminator loss: 0.765211, acc.: 62.50%] [Generator loss: 1.132777]\n",
      "919 [Discriminator loss: 0.781057, acc.: 56.25%] [Generator loss: 1.740852]\n",
      "920 [Discriminator loss: 1.427966, acc.: 28.12%] [Generator loss: 1.150466]\n",
      "921 [Discriminator loss: 0.855870, acc.: 46.09%] [Generator loss: 2.262315]\n",
      "922 [Discriminator loss: 0.973934, acc.: 46.88%] [Generator loss: 2.027760]\n",
      "923 [Discriminator loss: 1.036887, acc.: 46.88%] [Generator loss: 1.488458]\n",
      "924 [Discriminator loss: 0.773517, acc.: 54.69%] [Generator loss: 2.011642]\n",
      "925 [Discriminator loss: 1.008311, acc.: 44.53%] [Generator loss: 1.758835]\n",
      "926 [Discriminator loss: 1.044715, acc.: 39.84%] [Generator loss: 1.705853]\n",
      "927 [Discriminator loss: 0.981135, acc.: 51.56%] [Generator loss: 1.654901]\n",
      "928 [Discriminator loss: 1.041210, acc.: 42.19%] [Generator loss: 1.597217]\n",
      "929 [Discriminator loss: 0.897627, acc.: 58.59%] [Generator loss: 1.113537]\n",
      "930 [Discriminator loss: 0.974503, acc.: 40.62%] [Generator loss: 2.108731]\n",
      "931 [Discriminator loss: 0.476894, acc.: 77.34%] [Generator loss: 1.576229]\n",
      "932 [Discriminator loss: 0.822285, acc.: 53.12%] [Generator loss: 1.384403]\n",
      "933 [Discriminator loss: 0.651115, acc.: 65.62%] [Generator loss: 1.979100]\n",
      "934 [Discriminator loss: 1.123755, acc.: 30.47%] [Generator loss: 1.592550]\n",
      "935 [Discriminator loss: 0.852333, acc.: 50.78%] [Generator loss: 1.431762]\n",
      "936 [Discriminator loss: 0.837976, acc.: 53.12%] [Generator loss: 1.817362]\n",
      "937 [Discriminator loss: 0.752933, acc.: 59.38%] [Generator loss: 1.895766]\n",
      "938 [Discriminator loss: 0.820816, acc.: 57.81%] [Generator loss: 1.254610]\n",
      "939 [Discriminator loss: 1.011450, acc.: 43.75%] [Generator loss: 1.639085]\n",
      "940 [Discriminator loss: 0.795674, acc.: 56.25%] [Generator loss: 1.684054]\n",
      "941 [Discriminator loss: 0.736932, acc.: 58.59%] [Generator loss: 1.488800]\n",
      "942 [Discriminator loss: 0.669059, acc.: 64.84%] [Generator loss: 1.502191]\n",
      "943 [Discriminator loss: 0.838732, acc.: 48.44%] [Generator loss: 1.598685]\n",
      "944 [Discriminator loss: 0.894890, acc.: 46.09%] [Generator loss: 0.910993]\n",
      "945 [Discriminator loss: 0.833827, acc.: 50.78%] [Generator loss: 1.268499]\n",
      "946 [Discriminator loss: 0.616379, acc.: 65.62%] [Generator loss: 1.182265]\n",
      "947 [Discriminator loss: 1.044120, acc.: 39.06%] [Generator loss: 1.292006]\n",
      "948 [Discriminator loss: 0.857745, acc.: 56.25%] [Generator loss: 1.054530]\n",
      "949 [Discriminator loss: 0.842333, acc.: 53.12%] [Generator loss: 1.566259]\n",
      "950 [Discriminator loss: 0.851243, acc.: 53.91%] [Generator loss: 1.779471]\n",
      "951 [Discriminator loss: 1.102842, acc.: 35.16%] [Generator loss: 1.452383]\n",
      "952 [Discriminator loss: 1.086563, acc.: 42.19%] [Generator loss: 1.565662]\n",
      "953 [Discriminator loss: 0.969432, acc.: 42.19%] [Generator loss: 2.149146]\n",
      "954 [Discriminator loss: 0.850620, acc.: 59.38%] [Generator loss: 1.176188]\n",
      "955 [Discriminator loss: 0.484932, acc.: 77.34%] [Generator loss: 0.708538]\n",
      "956 [Discriminator loss: 0.292651, acc.: 92.19%] [Generator loss: 0.615604]\n",
      "957 [Discriminator loss: 0.281284, acc.: 92.19%] [Generator loss: 0.620897]\n",
      "958 [Discriminator loss: 0.148171, acc.: 94.53%] [Generator loss: 0.408169]\n",
      "959 [Discriminator loss: 0.634359, acc.: 63.28%] [Generator loss: 0.386114]\n",
      "960 [Discriminator loss: 0.250799, acc.: 89.84%] [Generator loss: 0.903148]\n",
      "961 [Discriminator loss: 0.552025, acc.: 78.91%] [Generator loss: 1.052698]\n",
      "962 [Discriminator loss: 0.656094, acc.: 66.41%] [Generator loss: 2.064779]\n",
      "963 [Discriminator loss: 0.677506, acc.: 65.62%] [Generator loss: 1.090935]\n",
      "964 [Discriminator loss: 1.002008, acc.: 50.00%] [Generator loss: 0.953832]\n",
      "965 [Discriminator loss: 0.941860, acc.: 55.47%] [Generator loss: 1.005764]\n",
      "966 [Discriminator loss: 0.901357, acc.: 53.91%] [Generator loss: 1.741391]\n",
      "967 [Discriminator loss: 0.801608, acc.: 62.50%] [Generator loss: 0.862964]\n",
      "968 [Discriminator loss: 1.493340, acc.: 39.84%] [Generator loss: 2.164874]\n",
      "969 [Discriminator loss: 1.028521, acc.: 50.00%] [Generator loss: 2.121770]\n",
      "970 [Discriminator loss: 1.002662, acc.: 48.44%] [Generator loss: 1.853057]\n",
      "971 [Discriminator loss: 1.035160, acc.: 41.41%] [Generator loss: 1.522456]\n",
      "972 [Discriminator loss: 0.867584, acc.: 54.69%] [Generator loss: 0.801136]\n",
      "973 [Discriminator loss: 0.769915, acc.: 59.38%] [Generator loss: 1.211898]\n",
      "974 [Discriminator loss: 0.365195, acc.: 82.03%] [Generator loss: 0.927440]\n",
      "975 [Discriminator loss: 0.571663, acc.: 72.66%] [Generator loss: 0.942424]\n",
      "976 [Discriminator loss: 0.761668, acc.: 57.81%] [Generator loss: 1.260407]\n",
      "977 [Discriminator loss: 0.737091, acc.: 62.50%] [Generator loss: 0.564432]\n",
      "978 [Discriminator loss: 0.843020, acc.: 57.03%] [Generator loss: 1.432462]\n",
      "979 [Discriminator loss: 0.781554, acc.: 56.25%] [Generator loss: 1.266256]\n",
      "980 [Discriminator loss: 1.067331, acc.: 42.97%] [Generator loss: 1.795978]\n",
      "981 [Discriminator loss: 0.564620, acc.: 71.88%] [Generator loss: 1.440249]\n",
      "982 [Discriminator loss: 0.503800, acc.: 75.00%] [Generator loss: 0.590668]\n",
      "983 [Discriminator loss: 0.926433, acc.: 56.25%] [Generator loss: 1.577420]\n",
      "984 [Discriminator loss: 1.059387, acc.: 39.84%] [Generator loss: 1.485890]\n",
      "985 [Discriminator loss: 0.913196, acc.: 48.44%] [Generator loss: 1.284466]\n",
      "986 [Discriminator loss: 0.904575, acc.: 49.22%] [Generator loss: 1.275070]\n",
      "987 [Discriminator loss: 0.866842, acc.: 55.47%] [Generator loss: 1.010618]\n",
      "988 [Discriminator loss: 0.588647, acc.: 64.84%] [Generator loss: 1.470260]\n",
      "989 [Discriminator loss: 0.441752, acc.: 78.91%] [Generator loss: 1.693717]\n",
      "990 [Discriminator loss: 0.392516, acc.: 84.38%] [Generator loss: 0.929653]\n",
      "991 [Discriminator loss: 0.512404, acc.: 74.22%] [Generator loss: 1.340908]\n",
      "992 [Discriminator loss: 0.275770, acc.: 85.94%] [Generator loss: 1.212938]\n",
      "993 [Discriminator loss: 0.845899, acc.: 54.69%] [Generator loss: 2.133509]\n",
      "994 [Discriminator loss: 0.626541, acc.: 68.75%] [Generator loss: 3.303203]\n",
      "995 [Discriminator loss: 0.376550, acc.: 88.28%] [Generator loss: 1.867846]\n",
      "996 [Discriminator loss: 0.554238, acc.: 71.09%] [Generator loss: 1.244633]\n",
      "997 [Discriminator loss: 1.500704, acc.: 23.44%] [Generator loss: 1.148730]\n",
      "998 [Discriminator loss: 0.423327, acc.: 85.16%] [Generator loss: 1.231287]\n",
      "999 [Discriminator loss: 1.229447, acc.: 31.25%] [Generator loss: 1.109374]\n",
      "1000 [Discriminator loss: 0.871377, acc.: 54.69%] [Generator loss: 1.460073]\n",
      "1001 [Discriminator loss: 0.678781, acc.: 64.84%] [Generator loss: 1.461420]\n",
      "1002 [Discriminator loss: 0.967447, acc.: 44.53%] [Generator loss: 1.109488]\n",
      "1003 [Discriminator loss: 0.638137, acc.: 67.19%] [Generator loss: 1.219241]\n",
      "1004 [Discriminator loss: 0.372974, acc.: 83.59%] [Generator loss: 0.611189]\n",
      "1005 [Discriminator loss: 0.832696, acc.: 60.16%] [Generator loss: 1.754438]\n",
      "1006 [Discriminator loss: 0.418642, acc.: 80.47%] [Generator loss: 1.708856]\n",
      "1007 [Discriminator loss: 1.317373, acc.: 31.25%] [Generator loss: 2.049281]\n",
      "1008 [Discriminator loss: 0.252243, acc.: 89.06%] [Generator loss: 1.533244]\n",
      "1009 [Discriminator loss: 0.861597, acc.: 54.69%] [Generator loss: 0.781038]\n",
      "1010 [Discriminator loss: 0.513865, acc.: 75.78%] [Generator loss: 0.425127]\n",
      "1011 [Discriminator loss: 0.688597, acc.: 60.16%] [Generator loss: 1.357017]\n",
      "1012 [Discriminator loss: 0.557580, acc.: 74.22%] [Generator loss: 0.627832]\n",
      "1013 [Discriminator loss: 0.665875, acc.: 66.41%] [Generator loss: 1.791468]\n",
      "1014 [Discriminator loss: 0.750056, acc.: 63.28%] [Generator loss: 2.385489]\n",
      "1015 [Discriminator loss: 1.149692, acc.: 40.62%] [Generator loss: 1.487015]\n",
      "1016 [Discriminator loss: 0.846232, acc.: 59.38%] [Generator loss: 1.232690]\n",
      "1017 [Discriminator loss: 0.390217, acc.: 82.81%] [Generator loss: 0.299376]\n",
      "1018 [Discriminator loss: 0.916398, acc.: 53.12%] [Generator loss: 2.288209]\n",
      "1019 [Discriminator loss: 1.192531, acc.: 44.53%] [Generator loss: 0.904072]\n",
      "1020 [Discriminator loss: 0.680661, acc.: 67.97%] [Generator loss: 1.923764]\n",
      "1021 [Discriminator loss: 1.153717, acc.: 35.16%] [Generator loss: 2.450907]\n",
      "1022 [Discriminator loss: 1.933898, acc.: 17.97%] [Generator loss: 2.064135]\n",
      "1023 [Discriminator loss: 0.801699, acc.: 55.47%] [Generator loss: 2.942859]\n",
      "1024 [Discriminator loss: 1.007364, acc.: 46.09%] [Generator loss: 1.767281]\n",
      "1025 [Discriminator loss: 1.421285, acc.: 25.78%] [Generator loss: 1.443677]\n",
      "1026 [Discriminator loss: 0.695206, acc.: 62.50%] [Generator loss: 1.855107]\n",
      "1027 [Discriminator loss: 1.235315, acc.: 45.31%] [Generator loss: 1.425494]\n",
      "1028 [Discriminator loss: 0.625155, acc.: 67.19%] [Generator loss: 1.572334]\n",
      "1029 [Discriminator loss: 0.520725, acc.: 74.22%] [Generator loss: 1.401560]\n",
      "1030 [Discriminator loss: 0.865090, acc.: 59.38%] [Generator loss: 1.499748]\n",
      "1031 [Discriminator loss: 0.734502, acc.: 67.97%] [Generator loss: 0.296881]\n",
      "1032 [Discriminator loss: 0.460571, acc.: 77.34%] [Generator loss: 0.476662]\n",
      "1033 [Discriminator loss: 0.535424, acc.: 72.66%] [Generator loss: 1.920681]\n",
      "1034 [Discriminator loss: 0.630816, acc.: 69.53%] [Generator loss: 1.322773]\n",
      "1035 [Discriminator loss: 0.388107, acc.: 83.59%] [Generator loss: 0.919309]\n",
      "1036 [Discriminator loss: 0.278268, acc.: 85.94%] [Generator loss: 0.342560]\n",
      "1037 [Discriminator loss: 0.453326, acc.: 78.91%] [Generator loss: 1.141430]\n",
      "1038 [Discriminator loss: 0.199140, acc.: 92.19%] [Generator loss: 0.829393]\n",
      "1039 [Discriminator loss: 0.373365, acc.: 88.28%] [Generator loss: 1.563201]\n",
      "1040 [Discriminator loss: 0.290652, acc.: 88.28%] [Generator loss: 0.737870]\n",
      "1041 [Discriminator loss: 0.825389, acc.: 60.16%] [Generator loss: 2.340868]\n",
      "1042 [Discriminator loss: 0.808194, acc.: 60.94%] [Generator loss: 2.416221]\n",
      "1043 [Discriminator loss: 2.276345, acc.: 20.31%] [Generator loss: 2.467911]\n",
      "1044 [Discriminator loss: 0.578347, acc.: 72.66%] [Generator loss: 2.238667]\n",
      "1045 [Discriminator loss: 1.334903, acc.: 32.03%] [Generator loss: 1.053866]\n",
      "1046 [Discriminator loss: 0.515770, acc.: 76.56%] [Generator loss: 1.496453]\n",
      "1047 [Discriminator loss: 0.639049, acc.: 64.06%] [Generator loss: 0.929343]\n",
      "1048 [Discriminator loss: 0.558505, acc.: 72.66%] [Generator loss: 1.315941]\n",
      "1049 [Discriminator loss: 0.700265, acc.: 63.28%] [Generator loss: 2.573345]\n",
      "1050 [Discriminator loss: 0.537149, acc.: 72.66%] [Generator loss: 1.618396]\n",
      "1051 [Discriminator loss: 0.637813, acc.: 67.19%] [Generator loss: 2.369822]\n",
      "1052 [Discriminator loss: 0.539860, acc.: 77.34%] [Generator loss: 2.576844]\n",
      "1053 [Discriminator loss: 0.749252, acc.: 66.41%] [Generator loss: 2.049329]\n",
      "1054 [Discriminator loss: 0.798370, acc.: 62.50%] [Generator loss: 2.964243]\n",
      "1055 [Discriminator loss: 0.871352, acc.: 49.22%] [Generator loss: 2.359522]\n",
      "1056 [Discriminator loss: 1.158544, acc.: 43.75%] [Generator loss: 1.924959]\n",
      "1057 [Discriminator loss: 1.129244, acc.: 40.62%] [Generator loss: 1.743679]\n",
      "1058 [Discriminator loss: 0.935204, acc.: 53.12%] [Generator loss: 1.518305]\n",
      "1059 [Discriminator loss: 1.174715, acc.: 37.50%] [Generator loss: 1.474596]\n",
      "1060 [Discriminator loss: 0.552414, acc.: 76.56%] [Generator loss: 1.854016]\n",
      "1061 [Discriminator loss: 1.020425, acc.: 46.09%] [Generator loss: 1.653620]\n",
      "1062 [Discriminator loss: 0.763640, acc.: 54.69%] [Generator loss: 2.165510]\n",
      "1063 [Discriminator loss: 0.717517, acc.: 67.97%] [Generator loss: 1.523944]\n",
      "1064 [Discriminator loss: 1.010036, acc.: 49.22%] [Generator loss: 2.066360]\n",
      "1065 [Discriminator loss: 0.530216, acc.: 73.44%] [Generator loss: 2.121247]\n",
      "1066 [Discriminator loss: 0.874677, acc.: 44.53%] [Generator loss: 1.518271]\n",
      "1067 [Discriminator loss: 0.705144, acc.: 64.06%] [Generator loss: 1.229952]\n",
      "1068 [Discriminator loss: 0.732905, acc.: 60.94%] [Generator loss: 1.880251]\n",
      "1069 [Discriminator loss: 1.236109, acc.: 39.84%] [Generator loss: 0.817756]\n",
      "1070 [Discriminator loss: 1.075331, acc.: 41.41%] [Generator loss: 1.421895]\n",
      "1071 [Discriminator loss: 0.550056, acc.: 72.66%] [Generator loss: 1.920564]\n",
      "1072 [Discriminator loss: 1.004070, acc.: 44.53%] [Generator loss: 1.611109]\n",
      "1073 [Discriminator loss: 0.649879, acc.: 67.97%] [Generator loss: 1.605493]\n",
      "1074 [Discriminator loss: 0.906230, acc.: 45.31%] [Generator loss: 1.543618]\n",
      "1075 [Discriminator loss: 0.640121, acc.: 64.06%] [Generator loss: 1.965571]\n",
      "1076 [Discriminator loss: 0.996154, acc.: 42.19%] [Generator loss: 1.215604]\n",
      "1077 [Discriminator loss: 0.634063, acc.: 66.41%] [Generator loss: 1.400651]\n",
      "1078 [Discriminator loss: 0.851150, acc.: 48.44%] [Generator loss: 1.315626]\n",
      "1079 [Discriminator loss: 0.752267, acc.: 57.81%] [Generator loss: 1.262365]\n",
      "1080 [Discriminator loss: 0.768698, acc.: 54.69%] [Generator loss: 1.845885]\n",
      "1081 [Discriminator loss: 0.652670, acc.: 64.06%] [Generator loss: 1.696760]\n",
      "1082 [Discriminator loss: 0.606790, acc.: 68.75%] [Generator loss: 1.165123]\n",
      "1083 [Discriminator loss: 0.445364, acc.: 79.69%] [Generator loss: 1.335977]\n",
      "1084 [Discriminator loss: 0.542273, acc.: 71.88%] [Generator loss: 1.296266]\n",
      "1085 [Discriminator loss: 1.116400, acc.: 36.72%] [Generator loss: 1.598729]\n",
      "1086 [Discriminator loss: 0.959780, acc.: 43.75%] [Generator loss: 1.691983]\n",
      "1087 [Discriminator loss: 1.239619, acc.: 25.78%] [Generator loss: 1.613160]\n",
      "1088 [Discriminator loss: 0.858675, acc.: 47.66%] [Generator loss: 1.897046]\n",
      "1089 [Discriminator loss: 1.133839, acc.: 35.94%] [Generator loss: 1.549625]\n",
      "1090 [Discriminator loss: 0.930766, acc.: 49.22%] [Generator loss: 2.027394]\n",
      "1091 [Discriminator loss: 0.727039, acc.: 55.47%] [Generator loss: 2.233698]\n",
      "1092 [Discriminator loss: 0.741245, acc.: 60.16%] [Generator loss: 2.116800]\n",
      "1093 [Discriminator loss: 0.983091, acc.: 42.19%] [Generator loss: 1.501179]\n",
      "1094 [Discriminator loss: 0.901533, acc.: 47.66%] [Generator loss: 1.766227]\n",
      "1095 [Discriminator loss: 0.778400, acc.: 60.94%] [Generator loss: 2.004469]\n",
      "1096 [Discriminator loss: 1.098890, acc.: 38.28%] [Generator loss: 1.531855]\n",
      "1097 [Discriminator loss: 0.993434, acc.: 42.97%] [Generator loss: 2.026990]\n",
      "1098 [Discriminator loss: 1.312492, acc.: 27.34%] [Generator loss: 1.936677]\n",
      "1099 [Discriminator loss: 0.815884, acc.: 50.78%] [Generator loss: 2.397160]\n",
      "1100 [Discriminator loss: 0.717814, acc.: 65.62%] [Generator loss: 2.527593]\n",
      "1101 [Discriminator loss: 0.903866, acc.: 53.91%] [Generator loss: 1.719950]\n",
      "1102 [Discriminator loss: 0.854887, acc.: 57.03%] [Generator loss: 1.751425]\n",
      "1103 [Discriminator loss: 0.733398, acc.: 60.94%] [Generator loss: 1.200392]\n",
      "1104 [Discriminator loss: 0.503622, acc.: 77.34%] [Generator loss: 0.985772]\n",
      "1105 [Discriminator loss: 0.645321, acc.: 63.28%] [Generator loss: 1.632217]\n",
      "1106 [Discriminator loss: 0.777831, acc.: 60.94%] [Generator loss: 1.670248]\n",
      "1107 [Discriminator loss: 0.931032, acc.: 42.97%] [Generator loss: 1.984398]\n",
      "1108 [Discriminator loss: 0.894958, acc.: 46.09%] [Generator loss: 2.522789]\n",
      "1109 [Discriminator loss: 0.810889, acc.: 55.47%] [Generator loss: 1.157006]\n",
      "1110 [Discriminator loss: 0.983499, acc.: 44.53%] [Generator loss: 2.270505]\n",
      "1111 [Discriminator loss: 0.969732, acc.: 51.56%] [Generator loss: 1.673293]\n",
      "1112 [Discriminator loss: 0.718866, acc.: 56.25%] [Generator loss: 1.285959]\n",
      "1113 [Discriminator loss: 0.844118, acc.: 50.78%] [Generator loss: 1.099106]\n",
      "1114 [Discriminator loss: 0.438455, acc.: 75.00%] [Generator loss: 0.372096]\n",
      "1115 [Discriminator loss: 1.126480, acc.: 46.88%] [Generator loss: 1.423076]\n",
      "1116 [Discriminator loss: 0.606888, acc.: 67.19%] [Generator loss: 0.884332]\n",
      "1117 [Discriminator loss: 0.504265, acc.: 71.88%] [Generator loss: 0.965164]\n",
      "1118 [Discriminator loss: 0.502191, acc.: 75.78%] [Generator loss: 1.490549]\n",
      "1119 [Discriminator loss: 0.513987, acc.: 71.88%] [Generator loss: 1.404393]\n",
      "1120 [Discriminator loss: 0.410756, acc.: 82.03%] [Generator loss: 1.340345]\n",
      "1121 [Discriminator loss: 1.031217, acc.: 39.84%] [Generator loss: 2.209965]\n",
      "1122 [Discriminator loss: 0.686229, acc.: 62.50%] [Generator loss: 2.104115]\n",
      "1123 [Discriminator loss: 0.689689, acc.: 61.72%] [Generator loss: 2.262357]\n",
      "1124 [Discriminator loss: 0.808101, acc.: 58.59%] [Generator loss: 2.658497]\n",
      "1125 [Discriminator loss: 0.576921, acc.: 66.41%] [Generator loss: 2.347337]\n",
      "1126 [Discriminator loss: 0.767924, acc.: 55.47%] [Generator loss: 2.227980]\n",
      "1127 [Discriminator loss: 0.967926, acc.: 38.28%] [Generator loss: 1.708537]\n",
      "1128 [Discriminator loss: 0.755638, acc.: 60.16%] [Generator loss: 1.401415]\n",
      "1129 [Discriminator loss: 0.631222, acc.: 61.72%] [Generator loss: 1.111065]\n",
      "1130 [Discriminator loss: 0.669513, acc.: 64.84%] [Generator loss: 2.357971]\n",
      "1131 [Discriminator loss: 0.800742, acc.: 54.69%] [Generator loss: 1.665500]\n",
      "1132 [Discriminator loss: 0.514596, acc.: 72.66%] [Generator loss: 1.515537]\n",
      "1133 [Discriminator loss: 1.008005, acc.: 47.66%] [Generator loss: 1.145799]\n",
      "1134 [Discriminator loss: 0.498172, acc.: 75.78%] [Generator loss: 1.406331]\n",
      "1135 [Discriminator loss: 1.183454, acc.: 38.28%] [Generator loss: 2.218238]\n",
      "1136 [Discriminator loss: 0.782538, acc.: 60.94%] [Generator loss: 2.351736]\n",
      "1137 [Discriminator loss: 0.823732, acc.: 59.38%] [Generator loss: 1.359869]\n",
      "1138 [Discriminator loss: 0.839299, acc.: 56.25%] [Generator loss: 1.801493]\n",
      "1139 [Discriminator loss: 0.561976, acc.: 73.44%] [Generator loss: 1.517706]\n",
      "1140 [Discriminator loss: 0.594988, acc.: 68.75%] [Generator loss: 0.895446]\n",
      "1141 [Discriminator loss: 0.498078, acc.: 78.91%] [Generator loss: 1.423332]\n",
      "1142 [Discriminator loss: 0.859407, acc.: 45.31%] [Generator loss: 2.258309]\n",
      "1143 [Discriminator loss: 0.594125, acc.: 71.88%] [Generator loss: 1.277435]\n",
      "1144 [Discriminator loss: 0.523401, acc.: 71.09%] [Generator loss: 0.350687]\n",
      "1145 [Discriminator loss: 0.530841, acc.: 71.88%] [Generator loss: 1.593661]\n",
      "1146 [Discriminator loss: 0.565927, acc.: 71.09%] [Generator loss: 0.744701]\n",
      "1147 [Discriminator loss: 0.258666, acc.: 89.84%] [Generator loss: 0.379448]\n",
      "1148 [Discriminator loss: 0.361669, acc.: 85.94%] [Generator loss: 0.826351]\n",
      "1149 [Discriminator loss: 1.345534, acc.: 39.06%] [Generator loss: 2.282288]\n",
      "1150 [Discriminator loss: 1.123363, acc.: 37.50%] [Generator loss: 2.187428]\n",
      "1151 [Discriminator loss: 1.341656, acc.: 32.81%] [Generator loss: 1.706962]\n",
      "1152 [Discriminator loss: 1.193540, acc.: 32.03%] [Generator loss: 2.088100]\n",
      "1153 [Discriminator loss: 0.901510, acc.: 48.44%] [Generator loss: 1.973209]\n",
      "1154 [Discriminator loss: 0.913600, acc.: 50.00%] [Generator loss: 1.920762]\n",
      "1155 [Discriminator loss: 0.994460, acc.: 47.66%] [Generator loss: 1.697944]\n",
      "1156 [Discriminator loss: 1.007413, acc.: 39.84%] [Generator loss: 2.373678]\n",
      "1157 [Discriminator loss: 0.931386, acc.: 49.22%] [Generator loss: 2.183037]\n",
      "1158 [Discriminator loss: 0.636650, acc.: 67.97%] [Generator loss: 2.252105]\n",
      "1159 [Discriminator loss: 0.567215, acc.: 67.97%] [Generator loss: 2.328055]\n",
      "1160 [Discriminator loss: 0.410004, acc.: 75.78%] [Generator loss: 2.116486]\n",
      "1161 [Discriminator loss: 0.513842, acc.: 75.78%] [Generator loss: 1.466453]\n",
      "1162 [Discriminator loss: 0.349376, acc.: 84.38%] [Generator loss: 1.244607]\n",
      "1163 [Discriminator loss: 0.902009, acc.: 53.12%] [Generator loss: 1.049008]\n",
      "1164 [Discriminator loss: 0.583140, acc.: 73.44%] [Generator loss: 1.566623]\n",
      "1165 [Discriminator loss: 1.634199, acc.: 20.31%] [Generator loss: 1.705558]\n",
      "1166 [Discriminator loss: 0.764696, acc.: 57.81%] [Generator loss: 2.320698]\n",
      "1167 [Discriminator loss: 0.864438, acc.: 55.47%] [Generator loss: 1.845692]\n",
      "1168 [Discriminator loss: 0.468740, acc.: 79.69%] [Generator loss: 1.243415]\n",
      "1169 [Discriminator loss: 0.643693, acc.: 64.84%] [Generator loss: 1.754015]\n",
      "1170 [Discriminator loss: 0.435831, acc.: 82.81%] [Generator loss: 0.906535]\n",
      "1171 [Discriminator loss: 0.526685, acc.: 72.66%] [Generator loss: 1.192970]\n",
      "1172 [Discriminator loss: 0.188519, acc.: 96.09%] [Generator loss: 1.262187]\n",
      "1173 [Discriminator loss: 0.499452, acc.: 77.34%] [Generator loss: 1.711337]\n",
      "1174 [Discriminator loss: 0.716587, acc.: 56.25%] [Generator loss: 2.289047]\n",
      "1175 [Discriminator loss: 0.946977, acc.: 49.22%] [Generator loss: 2.148811]\n",
      "1176 [Discriminator loss: 0.667682, acc.: 64.06%] [Generator loss: 1.996908]\n",
      "1177 [Discriminator loss: 0.391853, acc.: 86.72%] [Generator loss: 1.276616]\n",
      "1178 [Discriminator loss: 0.696368, acc.: 64.06%] [Generator loss: 0.736269]\n",
      "1179 [Discriminator loss: 0.501149, acc.: 72.66%] [Generator loss: 1.282246]\n",
      "1180 [Discriminator loss: 0.299185, acc.: 89.84%] [Generator loss: 0.906267]\n",
      "1181 [Discriminator loss: 0.442167, acc.: 77.34%] [Generator loss: 1.142556]\n",
      "1182 [Discriminator loss: 1.253928, acc.: 37.50%] [Generator loss: 1.770530]\n",
      "1183 [Discriminator loss: 0.480480, acc.: 79.69%] [Generator loss: 1.860996]\n",
      "1184 [Discriminator loss: 0.765152, acc.: 59.38%] [Generator loss: 2.864344]\n",
      "1185 [Discriminator loss: 0.516375, acc.: 75.78%] [Generator loss: 2.083616]\n",
      "1186 [Discriminator loss: 1.135418, acc.: 39.06%] [Generator loss: 1.969717]\n",
      "1187 [Discriminator loss: 0.670655, acc.: 60.16%] [Generator loss: 2.215614]\n",
      "1188 [Discriminator loss: 0.661740, acc.: 64.06%] [Generator loss: 1.730642]\n",
      "1189 [Discriminator loss: 0.896035, acc.: 47.66%] [Generator loss: 1.263942]\n",
      "1190 [Discriminator loss: 0.723446, acc.: 57.03%] [Generator loss: 1.988405]\n",
      "1191 [Discriminator loss: 0.697491, acc.: 62.50%] [Generator loss: 1.227207]\n",
      "1192 [Discriminator loss: 1.211478, acc.: 34.38%] [Generator loss: 1.458336]\n",
      "1193 [Discriminator loss: 0.884108, acc.: 59.38%] [Generator loss: 2.225279]\n",
      "1194 [Discriminator loss: 1.106958, acc.: 49.22%] [Generator loss: 0.764757]\n",
      "1195 [Discriminator loss: 1.023619, acc.: 48.44%] [Generator loss: 1.708296]\n",
      "1196 [Discriminator loss: 0.736962, acc.: 65.62%] [Generator loss: 2.082636]\n",
      "1197 [Discriminator loss: 0.856751, acc.: 58.59%] [Generator loss: 1.599874]\n",
      "1198 [Discriminator loss: 0.975163, acc.: 44.53%] [Generator loss: 1.123198]\n",
      "1199 [Discriminator loss: 1.003212, acc.: 45.31%] [Generator loss: 1.572175]\n",
      "1200 [Discriminator loss: 0.595691, acc.: 75.00%] [Generator loss: 1.702764]\n",
      "1201 [Discriminator loss: 0.950180, acc.: 53.91%] [Generator loss: 0.943371]\n",
      "1202 [Discriminator loss: 0.539176, acc.: 75.00%] [Generator loss: 1.522858]\n",
      "1203 [Discriminator loss: 0.336249, acc.: 85.16%] [Generator loss: 1.561190]\n",
      "1204 [Discriminator loss: 0.365804, acc.: 86.72%] [Generator loss: 0.368563]\n",
      "1205 [Discriminator loss: 0.555319, acc.: 71.88%] [Generator loss: 1.012003]\n",
      "1206 [Discriminator loss: 0.352733, acc.: 85.94%] [Generator loss: 0.826630]\n",
      "1207 [Discriminator loss: 0.421010, acc.: 81.25%] [Generator loss: 0.161346]\n",
      "1208 [Discriminator loss: 1.416074, acc.: 50.00%] [Generator loss: 2.949218]\n",
      "1209 [Discriminator loss: 1.068902, acc.: 53.91%] [Generator loss: 2.630391]\n",
      "1210 [Discriminator loss: 0.904700, acc.: 49.22%] [Generator loss: 1.868498]\n",
      "1211 [Discriminator loss: 1.168524, acc.: 38.28%] [Generator loss: 2.232770]\n",
      "1212 [Discriminator loss: 0.377714, acc.: 85.16%] [Generator loss: 2.852659]\n",
      "1213 [Discriminator loss: 0.659886, acc.: 67.97%] [Generator loss: 2.286265]\n",
      "1214 [Discriminator loss: 0.660974, acc.: 63.28%] [Generator loss: 1.238891]\n",
      "1215 [Discriminator loss: 0.340684, acc.: 87.50%] [Generator loss: 0.767028]\n",
      "1216 [Discriminator loss: 0.744255, acc.: 59.38%] [Generator loss: 0.840289]\n",
      "1217 [Discriminator loss: 0.364541, acc.: 85.94%] [Generator loss: 1.084169]\n",
      "1218 [Discriminator loss: 0.671426, acc.: 62.50%] [Generator loss: 1.056529]\n",
      "1219 [Discriminator loss: 0.546515, acc.: 71.09%] [Generator loss: 1.810981]\n",
      "1220 [Discriminator loss: 1.292477, acc.: 27.34%] [Generator loss: 1.891333]\n",
      "1221 [Discriminator loss: 0.422520, acc.: 80.47%] [Generator loss: 2.041583]\n",
      "1222 [Discriminator loss: 1.126731, acc.: 38.28%] [Generator loss: 1.602752]\n",
      "1223 [Discriminator loss: 1.774394, acc.: 17.97%] [Generator loss: 1.145785]\n",
      "1224 [Discriminator loss: 0.731925, acc.: 61.72%] [Generator loss: 2.100958]\n",
      "1225 [Discriminator loss: 0.618064, acc.: 68.75%] [Generator loss: 2.332016]\n",
      "1226 [Discriminator loss: 0.742667, acc.: 58.59%] [Generator loss: 1.562628]\n",
      "1227 [Discriminator loss: 0.939505, acc.: 41.41%] [Generator loss: 1.856517]\n",
      "1228 [Discriminator loss: 0.845999, acc.: 53.12%] [Generator loss: 1.165295]\n",
      "1229 [Discriminator loss: 0.885165, acc.: 53.12%] [Generator loss: 0.865509]\n",
      "1230 [Discriminator loss: 0.390041, acc.: 83.59%] [Generator loss: 0.980176]\n",
      "1231 [Discriminator loss: 1.065608, acc.: 46.88%] [Generator loss: 1.737797]\n",
      "1232 [Discriminator loss: 1.116163, acc.: 39.84%] [Generator loss: 1.101456]\n",
      "1233 [Discriminator loss: 0.882300, acc.: 49.22%] [Generator loss: 1.047176]\n",
      "1234 [Discriminator loss: 1.105006, acc.: 41.41%] [Generator loss: 1.733679]\n",
      "1235 [Discriminator loss: 0.742766, acc.: 59.38%] [Generator loss: 2.073598]\n",
      "1236 [Discriminator loss: 1.137541, acc.: 36.72%] [Generator loss: 1.506847]\n",
      "1237 [Discriminator loss: 1.204379, acc.: 39.06%] [Generator loss: 1.487103]\n",
      "1238 [Discriminator loss: 0.681541, acc.: 62.50%] [Generator loss: 1.550116]\n",
      "1239 [Discriminator loss: 0.920506, acc.: 52.34%] [Generator loss: 1.855681]\n",
      "1240 [Discriminator loss: 0.877075, acc.: 49.22%] [Generator loss: 1.901713]\n",
      "1241 [Discriminator loss: 0.863431, acc.: 53.12%] [Generator loss: 1.850185]\n",
      "1242 [Discriminator loss: 0.808167, acc.: 57.81%] [Generator loss: 1.582304]\n",
      "1243 [Discriminator loss: 1.029698, acc.: 42.19%] [Generator loss: 1.499557]\n",
      "1244 [Discriminator loss: 0.862222, acc.: 50.00%] [Generator loss: 1.885852]\n",
      "1245 [Discriminator loss: 0.789367, acc.: 55.47%] [Generator loss: 1.293093]\n",
      "1246 [Discriminator loss: 0.525823, acc.: 75.00%] [Generator loss: 1.223667]\n",
      "1247 [Discriminator loss: 0.881064, acc.: 47.66%] [Generator loss: 1.311826]\n",
      "1248 [Discriminator loss: 0.585676, acc.: 67.19%] [Generator loss: 0.720851]\n",
      "1249 [Discriminator loss: 0.678375, acc.: 64.84%] [Generator loss: 0.805101]\n",
      "1250 [Discriminator loss: 0.732755, acc.: 60.94%] [Generator loss: 0.521491]\n",
      "1251 [Discriminator loss: 0.771293, acc.: 58.59%] [Generator loss: 1.292023]\n",
      "1252 [Discriminator loss: 0.948216, acc.: 42.19%] [Generator loss: 2.005535]\n",
      "1253 [Discriminator loss: 0.622090, acc.: 68.75%] [Generator loss: 0.809841]\n",
      "1254 [Discriminator loss: 0.316569, acc.: 87.50%] [Generator loss: 0.321898]\n",
      "1255 [Discriminator loss: 0.802330, acc.: 57.03%] [Generator loss: 1.743689]\n",
      "1256 [Discriminator loss: 1.129220, acc.: 36.72%] [Generator loss: 1.992114]\n",
      "1257 [Discriminator loss: 1.014277, acc.: 42.19%] [Generator loss: 1.591327]\n",
      "1258 [Discriminator loss: 0.914597, acc.: 43.75%] [Generator loss: 1.489739]\n",
      "1259 [Discriminator loss: 0.951641, acc.: 46.88%] [Generator loss: 1.650488]\n",
      "1260 [Discriminator loss: 1.073271, acc.: 44.53%] [Generator loss: 1.321852]\n",
      "1261 [Discriminator loss: 1.025877, acc.: 37.50%] [Generator loss: 0.913203]\n",
      "1262 [Discriminator loss: 0.873477, acc.: 52.34%] [Generator loss: 1.536292]\n",
      "1263 [Discriminator loss: 0.699139, acc.: 60.94%] [Generator loss: 1.670623]\n",
      "1264 [Discriminator loss: 0.696939, acc.: 57.03%] [Generator loss: 1.137062]\n",
      "1265 [Discriminator loss: 0.753788, acc.: 53.91%] [Generator loss: 1.760248]\n",
      "1266 [Discriminator loss: 0.774404, acc.: 50.78%] [Generator loss: 1.647732]\n",
      "1267 [Discriminator loss: 0.819744, acc.: 56.25%] [Generator loss: 1.492145]\n",
      "1268 [Discriminator loss: 0.776112, acc.: 53.12%] [Generator loss: 1.577471]\n",
      "1269 [Discriminator loss: 0.918060, acc.: 45.31%] [Generator loss: 1.476085]\n",
      "1270 [Discriminator loss: 1.107276, acc.: 39.84%] [Generator loss: 1.437625]\n",
      "1271 [Discriminator loss: 1.035455, acc.: 41.41%] [Generator loss: 1.473535]\n",
      "1272 [Discriminator loss: 1.024214, acc.: 42.97%] [Generator loss: 1.591980]\n",
      "1273 [Discriminator loss: 1.037344, acc.: 39.06%] [Generator loss: 1.380956]\n",
      "1274 [Discriminator loss: 1.147993, acc.: 34.38%] [Generator loss: 1.485568]\n",
      "1275 [Discriminator loss: 0.856668, acc.: 47.66%] [Generator loss: 1.743553]\n",
      "1276 [Discriminator loss: 0.775082, acc.: 54.69%] [Generator loss: 1.982028]\n",
      "1277 [Discriminator loss: 0.903347, acc.: 51.56%] [Generator loss: 1.644439]\n",
      "1278 [Discriminator loss: 0.827080, acc.: 53.12%] [Generator loss: 1.464301]\n",
      "1279 [Discriminator loss: 0.725999, acc.: 60.16%] [Generator loss: 1.613913]\n",
      "1280 [Discriminator loss: 1.030798, acc.: 34.38%] [Generator loss: 1.291617]\n",
      "1281 [Discriminator loss: 0.871722, acc.: 47.66%] [Generator loss: 1.489303]\n",
      "1282 [Discriminator loss: 0.738198, acc.: 56.25%] [Generator loss: 1.417382]\n",
      "1283 [Discriminator loss: 0.666742, acc.: 66.41%] [Generator loss: 1.457197]\n",
      "1284 [Discriminator loss: 0.826084, acc.: 50.00%] [Generator loss: 1.390878]\n",
      "1285 [Discriminator loss: 0.638738, acc.: 69.53%] [Generator loss: 1.466097]\n",
      "1286 [Discriminator loss: 0.679707, acc.: 69.53%] [Generator loss: 1.340552]\n",
      "1287 [Discriminator loss: 0.684347, acc.: 59.38%] [Generator loss: 1.478349]\n",
      "1288 [Discriminator loss: 0.604653, acc.: 67.19%] [Generator loss: 1.337709]\n",
      "1289 [Discriminator loss: 0.842118, acc.: 55.47%] [Generator loss: 1.918235]\n",
      "1290 [Discriminator loss: 0.623302, acc.: 69.53%] [Generator loss: 1.483210]\n",
      "1291 [Discriminator loss: 0.803822, acc.: 56.25%] [Generator loss: 1.505948]\n",
      "1292 [Discriminator loss: 0.603099, acc.: 75.00%] [Generator loss: 1.385826]\n",
      "1293 [Discriminator loss: 0.651175, acc.: 64.84%] [Generator loss: 1.249568]\n",
      "1294 [Discriminator loss: 0.646081, acc.: 64.84%] [Generator loss: 1.764921]\n",
      "1295 [Discriminator loss: 1.329676, acc.: 32.81%] [Generator loss: 0.994650]\n",
      "1296 [Discriminator loss: 0.718549, acc.: 60.16%] [Generator loss: 1.671721]\n",
      "1297 [Discriminator loss: 0.831892, acc.: 51.56%] [Generator loss: 1.730522]\n",
      "1298 [Discriminator loss: 0.628487, acc.: 61.72%] [Generator loss: 1.201978]\n",
      "1299 [Discriminator loss: 0.823112, acc.: 59.38%] [Generator loss: 1.227922]\n",
      "1300 [Discriminator loss: 0.859787, acc.: 48.44%] [Generator loss: 1.458135]\n",
      "1301 [Discriminator loss: 0.834294, acc.: 47.66%] [Generator loss: 1.522509]\n",
      "1302 [Discriminator loss: 0.688913, acc.: 57.81%] [Generator loss: 1.805930]\n",
      "1303 [Discriminator loss: 0.732591, acc.: 57.03%] [Generator loss: 1.675182]\n",
      "1304 [Discriminator loss: 0.842038, acc.: 48.44%] [Generator loss: 1.590067]\n",
      "1305 [Discriminator loss: 0.765315, acc.: 55.47%] [Generator loss: 1.271971]\n",
      "1306 [Discriminator loss: 0.727098, acc.: 54.69%] [Generator loss: 1.453763]\n",
      "1307 [Discriminator loss: 0.682374, acc.: 57.03%] [Generator loss: 1.465183]\n",
      "1308 [Discriminator loss: 0.683230, acc.: 62.50%] [Generator loss: 1.462338]\n",
      "1309 [Discriminator loss: 0.943773, acc.: 42.19%] [Generator loss: 1.368423]\n",
      "1310 [Discriminator loss: 0.624272, acc.: 68.75%] [Generator loss: 1.124837]\n",
      "1311 [Discriminator loss: 0.626309, acc.: 62.50%] [Generator loss: 1.489491]\n",
      "1312 [Discriminator loss: 0.795294, acc.: 53.12%] [Generator loss: 1.379442]\n",
      "1313 [Discriminator loss: 0.700985, acc.: 60.16%] [Generator loss: 1.939152]\n",
      "1314 [Discriminator loss: 0.816918, acc.: 56.25%] [Generator loss: 1.687586]\n",
      "1315 [Discriminator loss: 0.690701, acc.: 57.81%] [Generator loss: 1.696991]\n",
      "1316 [Discriminator loss: 0.788884, acc.: 55.47%] [Generator loss: 1.716145]\n",
      "1317 [Discriminator loss: 0.793805, acc.: 51.56%] [Generator loss: 1.311635]\n",
      "1318 [Discriminator loss: 1.138372, acc.: 35.16%] [Generator loss: 1.449831]\n",
      "1319 [Discriminator loss: 0.737393, acc.: 57.81%] [Generator loss: 1.603754]\n",
      "1320 [Discriminator loss: 0.824694, acc.: 52.34%] [Generator loss: 1.508349]\n",
      "1321 [Discriminator loss: 1.031680, acc.: 38.28%] [Generator loss: 1.388071]\n",
      "1322 [Discriminator loss: 0.582843, acc.: 71.09%] [Generator loss: 1.127941]\n",
      "1323 [Discriminator loss: 0.548146, acc.: 73.44%] [Generator loss: 0.953799]\n",
      "1324 [Discriminator loss: 0.503335, acc.: 76.56%] [Generator loss: 0.944536]\n",
      "1325 [Discriminator loss: 0.376501, acc.: 85.16%] [Generator loss: 0.850324]\n",
      "1326 [Discriminator loss: 0.550945, acc.: 74.22%] [Generator loss: 2.064646]\n",
      "1327 [Discriminator loss: 0.954132, acc.: 50.00%] [Generator loss: 1.028995]\n",
      "1328 [Discriminator loss: 0.545011, acc.: 78.91%] [Generator loss: 2.098292]\n",
      "1329 [Discriminator loss: 0.724532, acc.: 64.84%] [Generator loss: 1.608393]\n",
      "1330 [Discriminator loss: 0.682397, acc.: 61.72%] [Generator loss: 1.579823]\n",
      "1331 [Discriminator loss: 0.810716, acc.: 55.47%] [Generator loss: 0.870497]\n",
      "1332 [Discriminator loss: 0.947069, acc.: 50.00%] [Generator loss: 1.329211]\n",
      "1333 [Discriminator loss: 0.622016, acc.: 70.31%] [Generator loss: 1.372286]\n",
      "1334 [Discriminator loss: 1.115971, acc.: 35.94%] [Generator loss: 1.060986]\n",
      "1335 [Discriminator loss: 0.658777, acc.: 64.84%] [Generator loss: 0.795615]\n",
      "1336 [Discriminator loss: 0.456823, acc.: 78.91%] [Generator loss: 0.689618]\n",
      "1337 [Discriminator loss: 0.704823, acc.: 54.69%] [Generator loss: 1.153668]\n",
      "1338 [Discriminator loss: 0.630709, acc.: 64.06%] [Generator loss: 0.886316]\n",
      "1339 [Discriminator loss: 0.545155, acc.: 70.31%] [Generator loss: 1.206643]\n",
      "1340 [Discriminator loss: 0.494637, acc.: 75.78%] [Generator loss: 0.959086]\n",
      "1341 [Discriminator loss: 0.822026, acc.: 50.00%] [Generator loss: 1.204711]\n",
      "1342 [Discriminator loss: 0.446405, acc.: 80.47%] [Generator loss: 1.701029]\n",
      "1343 [Discriminator loss: 0.691318, acc.: 66.41%] [Generator loss: 1.144383]\n",
      "1344 [Discriminator loss: 0.596837, acc.: 71.09%] [Generator loss: 1.060506]\n",
      "1345 [Discriminator loss: 0.433826, acc.: 81.25%] [Generator loss: 0.904547]\n",
      "1346 [Discriminator loss: 0.530893, acc.: 76.56%] [Generator loss: 0.341735]\n",
      "1347 [Discriminator loss: 0.518763, acc.: 75.00%] [Generator loss: 0.660775]\n",
      "1348 [Discriminator loss: 0.384823, acc.: 81.25%] [Generator loss: 0.970186]\n",
      "1349 [Discriminator loss: 0.327057, acc.: 84.38%] [Generator loss: 0.258669]\n",
      "1350 [Discriminator loss: 0.321274, acc.: 88.28%] [Generator loss: 0.295651]\n",
      "1351 [Discriminator loss: 0.346571, acc.: 83.59%] [Generator loss: 0.577963]\n",
      "1352 [Discriminator loss: 0.157955, acc.: 95.31%] [Generator loss: 0.190391]\n",
      "1353 [Discriminator loss: 0.313689, acc.: 89.06%] [Generator loss: 0.603257]\n",
      "1354 [Discriminator loss: 0.454779, acc.: 77.34%] [Generator loss: 0.633735]\n",
      "1355 [Discriminator loss: 0.897374, acc.: 53.91%] [Generator loss: 5.798350]\n",
      "1356 [Discriminator loss: 1.152013, acc.: 56.25%] [Generator loss: 5.438052]\n",
      "1357 [Discriminator loss: 0.260027, acc.: 91.41%] [Generator loss: 0.242064]\n",
      "1358 [Discriminator loss: 1.536555, acc.: 48.44%] [Generator loss: 3.092108]\n",
      "1359 [Discriminator loss: 0.460471, acc.: 77.34%] [Generator loss: 3.048390]\n",
      "1360 [Discriminator loss: 0.608131, acc.: 69.53%] [Generator loss: 0.671918]\n",
      "1361 [Discriminator loss: 0.814622, acc.: 52.34%] [Generator loss: 1.226249]\n",
      "1362 [Discriminator loss: 0.431794, acc.: 78.91%] [Generator loss: 1.397943]\n",
      "1363 [Discriminator loss: 0.587134, acc.: 71.88%] [Generator loss: 1.010271]\n",
      "1364 [Discriminator loss: 0.464001, acc.: 75.78%] [Generator loss: 1.138278]\n",
      "1365 [Discriminator loss: 0.292245, acc.: 89.06%] [Generator loss: 0.986134]\n",
      "1366 [Discriminator loss: 0.497796, acc.: 73.44%] [Generator loss: 1.076422]\n",
      "1367 [Discriminator loss: 1.317719, acc.: 33.59%] [Generator loss: 1.747385]\n",
      "1368 [Discriminator loss: 0.372145, acc.: 84.38%] [Generator loss: 1.759803]\n",
      "1369 [Discriminator loss: 0.871120, acc.: 50.78%] [Generator loss: 1.351012]\n",
      "1370 [Discriminator loss: 0.754960, acc.: 56.25%] [Generator loss: 0.962360]\n",
      "1371 [Discriminator loss: 0.434038, acc.: 78.12%] [Generator loss: 0.953349]\n",
      "1372 [Discriminator loss: 0.263377, acc.: 91.41%] [Generator loss: 0.547224]\n",
      "1373 [Discriminator loss: 0.462216, acc.: 78.12%] [Generator loss: 0.845243]\n",
      "1374 [Discriminator loss: 0.439015, acc.: 79.69%] [Generator loss: 1.071893]\n",
      "1375 [Discriminator loss: 0.855487, acc.: 53.91%] [Generator loss: 1.076205]\n",
      "1376 [Discriminator loss: 0.375408, acc.: 82.03%] [Generator loss: 3.187524]\n",
      "1377 [Discriminator loss: 1.190680, acc.: 48.44%] [Generator loss: 0.694057]\n",
      "1378 [Discriminator loss: 1.231043, acc.: 42.97%] [Generator loss: 1.733077]\n",
      "1379 [Discriminator loss: 0.272371, acc.: 89.84%] [Generator loss: 2.082865]\n",
      "1380 [Discriminator loss: 0.907733, acc.: 60.94%] [Generator loss: 1.741897]\n",
      "1381 [Discriminator loss: 0.847493, acc.: 53.91%] [Generator loss: 2.064083]\n",
      "1382 [Discriminator loss: 0.947340, acc.: 49.22%] [Generator loss: 1.877136]\n",
      "1383 [Discriminator loss: 0.519951, acc.: 76.56%] [Generator loss: 1.598175]\n",
      "1384 [Discriminator loss: 0.581981, acc.: 70.31%] [Generator loss: 1.011728]\n",
      "1385 [Discriminator loss: 0.301803, acc.: 86.72%] [Generator loss: 1.137297]\n",
      "1386 [Discriminator loss: 0.448789, acc.: 77.34%] [Generator loss: 1.285867]\n",
      "1387 [Discriminator loss: 0.710641, acc.: 62.50%] [Generator loss: 1.960379]\n",
      "1388 [Discriminator loss: 0.702481, acc.: 64.84%] [Generator loss: 2.150924]\n",
      "1389 [Discriminator loss: 1.051623, acc.: 39.06%] [Generator loss: 1.522478]\n",
      "1390 [Discriminator loss: 0.319922, acc.: 83.59%] [Generator loss: 1.241209]\n",
      "1391 [Discriminator loss: 0.522471, acc.: 75.00%] [Generator loss: 1.468946]\n",
      "1392 [Discriminator loss: 0.461411, acc.: 79.69%] [Generator loss: 1.007704]\n",
      "1393 [Discriminator loss: 0.517132, acc.: 70.31%] [Generator loss: 0.601449]\n",
      "1394 [Discriminator loss: 0.225828, acc.: 93.75%] [Generator loss: 0.247178]\n",
      "1395 [Discriminator loss: 0.654620, acc.: 64.06%] [Generator loss: 1.138053]\n",
      "1396 [Discriminator loss: 0.588954, acc.: 66.41%] [Generator loss: 0.447097]\n",
      "1397 [Discriminator loss: 0.492625, acc.: 72.66%] [Generator loss: 1.764308]\n",
      "1398 [Discriminator loss: 0.879779, acc.: 54.69%] [Generator loss: 3.021125]\n",
      "1399 [Discriminator loss: 0.830329, acc.: 61.72%] [Generator loss: 1.923753]\n",
      "1400 [Discriminator loss: 1.416904, acc.: 36.72%] [Generator loss: 2.333696]\n",
      "1401 [Discriminator loss: 0.944247, acc.: 47.66%] [Generator loss: 1.819609]\n",
      "1402 [Discriminator loss: 0.855537, acc.: 55.47%] [Generator loss: 1.400622]\n",
      "1403 [Discriminator loss: 0.897566, acc.: 54.69%] [Generator loss: 2.376543]\n",
      "1404 [Discriminator loss: 0.838106, acc.: 55.47%] [Generator loss: 1.738808]\n",
      "1405 [Discriminator loss: 0.967064, acc.: 47.66%] [Generator loss: 1.589242]\n",
      "1406 [Discriminator loss: 0.909103, acc.: 53.91%] [Generator loss: 1.667235]\n",
      "1407 [Discriminator loss: 0.915511, acc.: 48.44%] [Generator loss: 2.033924]\n",
      "1408 [Discriminator loss: 0.833628, acc.: 53.12%] [Generator loss: 2.088909]\n",
      "1409 [Discriminator loss: 1.296081, acc.: 37.50%] [Generator loss: 1.209121]\n",
      "1410 [Discriminator loss: 1.213842, acc.: 34.38%] [Generator loss: 1.694229]\n",
      "1411 [Discriminator loss: 0.943726, acc.: 39.06%] [Generator loss: 1.866186]\n",
      "1412 [Discriminator loss: 0.881107, acc.: 46.09%] [Generator loss: 1.230934]\n",
      "1413 [Discriminator loss: 0.567849, acc.: 70.31%] [Generator loss: 1.392770]\n",
      "1414 [Discriminator loss: 0.861034, acc.: 53.12%] [Generator loss: 1.463071]\n",
      "1415 [Discriminator loss: 0.602788, acc.: 67.19%] [Generator loss: 1.861708]\n",
      "1416 [Discriminator loss: 0.874357, acc.: 50.00%] [Generator loss: 1.485722]\n",
      "1417 [Discriminator loss: 0.932292, acc.: 48.44%] [Generator loss: 1.866069]\n",
      "1418 [Discriminator loss: 0.588689, acc.: 68.75%] [Generator loss: 1.916800]\n",
      "1419 [Discriminator loss: 0.822050, acc.: 56.25%] [Generator loss: 1.592668]\n",
      "1420 [Discriminator loss: 0.654882, acc.: 64.06%] [Generator loss: 1.908659]\n",
      "1421 [Discriminator loss: 0.447701, acc.: 79.69%] [Generator loss: 1.597250]\n",
      "1422 [Discriminator loss: 0.708778, acc.: 64.84%] [Generator loss: 1.069175]\n",
      "1423 [Discriminator loss: 0.613246, acc.: 71.09%] [Generator loss: 0.643638]\n",
      "1424 [Discriminator loss: 1.136312, acc.: 47.66%] [Generator loss: 1.790441]\n",
      "1425 [Discriminator loss: 0.665152, acc.: 67.97%] [Generator loss: 0.845607]\n",
      "1426 [Discriminator loss: 1.372949, acc.: 34.38%] [Generator loss: 1.205695]\n",
      "1427 [Discriminator loss: 0.560856, acc.: 73.44%] [Generator loss: 1.385975]\n",
      "1428 [Discriminator loss: 0.906249, acc.: 53.12%] [Generator loss: 1.964058]\n",
      "1429 [Discriminator loss: 0.867319, acc.: 49.22%] [Generator loss: 1.805977]\n",
      "1430 [Discriminator loss: 0.727960, acc.: 61.72%] [Generator loss: 0.532603]\n",
      "1431 [Discriminator loss: 1.466011, acc.: 29.69%] [Generator loss: 1.176361]\n",
      "1432 [Discriminator loss: 0.427854, acc.: 82.03%] [Generator loss: 1.832905]\n",
      "1433 [Discriminator loss: 0.665635, acc.: 66.41%] [Generator loss: 1.660779]\n",
      "1434 [Discriminator loss: 1.102529, acc.: 37.50%] [Generator loss: 1.397529]\n",
      "1435 [Discriminator loss: 0.781144, acc.: 59.38%] [Generator loss: 1.335337]\n",
      "1436 [Discriminator loss: 0.754852, acc.: 55.47%] [Generator loss: 1.466418]\n",
      "1437 [Discriminator loss: 0.768642, acc.: 53.91%] [Generator loss: 1.776889]\n",
      "1438 [Discriminator loss: 0.816481, acc.: 60.16%] [Generator loss: 0.863584]\n",
      "1439 [Discriminator loss: 1.050080, acc.: 46.88%] [Generator loss: 1.408297]\n",
      "1440 [Discriminator loss: 0.474526, acc.: 75.78%] [Generator loss: 1.540749]\n",
      "1441 [Discriminator loss: 0.688258, acc.: 63.28%] [Generator loss: 1.556615]\n",
      "1442 [Discriminator loss: 0.597193, acc.: 68.75%] [Generator loss: 1.604988]\n",
      "1443 [Discriminator loss: 0.673683, acc.: 65.62%] [Generator loss: 1.281514]\n",
      "1444 [Discriminator loss: 0.723499, acc.: 59.38%] [Generator loss: 1.549518]\n",
      "1445 [Discriminator loss: 0.662846, acc.: 64.06%] [Generator loss: 1.337327]\n",
      "1446 [Discriminator loss: 0.714509, acc.: 60.16%] [Generator loss: 1.076885]\n",
      "1447 [Discriminator loss: 0.599372, acc.: 66.41%] [Generator loss: 1.358837]\n",
      "1448 [Discriminator loss: 0.368201, acc.: 85.16%] [Generator loss: 1.315349]\n",
      "1449 [Discriminator loss: 0.528454, acc.: 67.97%] [Generator loss: 0.797329]\n",
      "1450 [Discriminator loss: 0.742066, acc.: 61.72%] [Generator loss: 0.943519]\n",
      "1451 [Discriminator loss: 0.534623, acc.: 72.66%] [Generator loss: 1.103222]\n",
      "1452 [Discriminator loss: 0.372910, acc.: 85.94%] [Generator loss: 0.814061]\n",
      "1453 [Discriminator loss: 1.033634, acc.: 45.31%] [Generator loss: 1.000313]\n",
      "1454 [Discriminator loss: 0.519117, acc.: 80.47%] [Generator loss: 0.779819]\n",
      "1455 [Discriminator loss: 0.858241, acc.: 53.91%] [Generator loss: 1.687999]\n",
      "1456 [Discriminator loss: 0.424415, acc.: 82.03%] [Generator loss: 1.773428]\n",
      "1457 [Discriminator loss: 0.521578, acc.: 71.88%] [Generator loss: 0.965342]\n",
      "1458 [Discriminator loss: 0.921580, acc.: 50.00%] [Generator loss: 1.342697]\n",
      "1459 [Discriminator loss: 0.632192, acc.: 70.31%] [Generator loss: 2.034140]\n",
      "1460 [Discriminator loss: 0.788256, acc.: 56.25%] [Generator loss: 1.574742]\n",
      "1461 [Discriminator loss: 0.658035, acc.: 63.28%] [Generator loss: 1.724293]\n",
      "1462 [Discriminator loss: 0.710725, acc.: 60.94%] [Generator loss: 1.300362]\n",
      "1463 [Discriminator loss: 0.817218, acc.: 54.69%] [Generator loss: 1.265526]\n",
      "1464 [Discriminator loss: 0.829471, acc.: 56.25%] [Generator loss: 0.751334]\n",
      "1465 [Discriminator loss: 0.410273, acc.: 78.12%] [Generator loss: 0.831542]\n",
      "1466 [Discriminator loss: 0.367566, acc.: 84.38%] [Generator loss: 0.448878]\n",
      "1467 [Discriminator loss: 0.443554, acc.: 81.25%] [Generator loss: 0.687307]\n",
      "1468 [Discriminator loss: 0.438163, acc.: 82.03%] [Generator loss: 0.708265]\n",
      "1469 [Discriminator loss: 0.329483, acc.: 87.50%] [Generator loss: 0.702507]\n",
      "1470 [Discriminator loss: 0.474587, acc.: 75.78%] [Generator loss: 0.789385]\n",
      "1471 [Discriminator loss: 0.895413, acc.: 51.56%] [Generator loss: 1.617996]\n",
      "1472 [Discriminator loss: 1.561933, acc.: 27.34%] [Generator loss: 1.293798]\n",
      "1473 [Discriminator loss: 1.265759, acc.: 31.25%] [Generator loss: 0.996013]\n",
      "1474 [Discriminator loss: 0.761865, acc.: 58.59%] [Generator loss: 1.486186]\n",
      "1475 [Discriminator loss: 1.158422, acc.: 32.03%] [Generator loss: 1.431056]\n",
      "1476 [Discriminator loss: 0.875917, acc.: 50.78%] [Generator loss: 1.548140]\n",
      "1477 [Discriminator loss: 1.048492, acc.: 39.84%] [Generator loss: 1.365359]\n",
      "1478 [Discriminator loss: 1.058172, acc.: 33.59%] [Generator loss: 1.197706]\n",
      "1479 [Discriminator loss: 0.834787, acc.: 53.12%] [Generator loss: 1.792735]\n",
      "1480 [Discriminator loss: 1.130525, acc.: 32.81%] [Generator loss: 1.345107]\n",
      "1481 [Discriminator loss: 1.354849, acc.: 21.09%] [Generator loss: 1.331031]\n",
      "1482 [Discriminator loss: 1.242010, acc.: 21.09%] [Generator loss: 1.968104]\n",
      "1483 [Discriminator loss: 1.081840, acc.: 39.06%] [Generator loss: 1.534786]\n",
      "1484 [Discriminator loss: 1.316086, acc.: 27.34%] [Generator loss: 1.466022]\n",
      "1485 [Discriminator loss: 0.800352, acc.: 50.78%] [Generator loss: 1.603066]\n",
      "1486 [Discriminator loss: 0.849775, acc.: 46.88%] [Generator loss: 1.360994]\n",
      "1487 [Discriminator loss: 0.807387, acc.: 50.78%] [Generator loss: 1.344713]\n",
      "1488 [Discriminator loss: 0.595393, acc.: 70.31%] [Generator loss: 1.270314]\n",
      "1489 [Discriminator loss: 0.702159, acc.: 54.69%] [Generator loss: 1.192738]\n",
      "1490 [Discriminator loss: 0.721881, acc.: 54.69%] [Generator loss: 1.591602]\n",
      "1491 [Discriminator loss: 0.816403, acc.: 49.22%] [Generator loss: 1.613372]\n",
      "1492 [Discriminator loss: 0.620563, acc.: 67.19%] [Generator loss: 1.528552]\n",
      "1493 [Discriminator loss: 0.718973, acc.: 57.03%] [Generator loss: 1.291025]\n",
      "1494 [Discriminator loss: 0.832679, acc.: 44.53%] [Generator loss: 1.619583]\n",
      "1495 [Discriminator loss: 0.933076, acc.: 45.31%] [Generator loss: 1.527442]\n",
      "1496 [Discriminator loss: 0.964980, acc.: 40.62%] [Generator loss: 1.499641]\n",
      "1497 [Discriminator loss: 0.982264, acc.: 42.19%] [Generator loss: 1.382371]\n",
      "1498 [Discriminator loss: 0.611848, acc.: 64.84%] [Generator loss: 1.080302]\n",
      "1499 [Discriminator loss: 0.691540, acc.: 61.72%] [Generator loss: 1.240154]\n",
      "1500 [Discriminator loss: 0.757451, acc.: 58.59%] [Generator loss: 1.796561]\n",
      "1501 [Discriminator loss: 0.861559, acc.: 43.75%] [Generator loss: 1.561469]\n",
      "1502 [Discriminator loss: 0.693260, acc.: 60.94%] [Generator loss: 1.860543]\n",
      "1503 [Discriminator loss: 0.693961, acc.: 61.72%] [Generator loss: 1.505144]\n",
      "1504 [Discriminator loss: 0.924577, acc.: 49.22%] [Generator loss: 1.514297]\n",
      "1505 [Discriminator loss: 0.764360, acc.: 58.59%] [Generator loss: 1.500149]\n",
      "1506 [Discriminator loss: 0.801684, acc.: 51.56%] [Generator loss: 1.581761]\n",
      "1507 [Discriminator loss: 0.822184, acc.: 59.38%] [Generator loss: 1.133043]\n",
      "1508 [Discriminator loss: 0.942724, acc.: 42.97%] [Generator loss: 1.678818]\n",
      "1509 [Discriminator loss: 0.695492, acc.: 61.72%] [Generator loss: 1.504452]\n",
      "1510 [Discriminator loss: 0.921195, acc.: 44.53%] [Generator loss: 1.362959]\n",
      "1511 [Discriminator loss: 0.892165, acc.: 46.88%] [Generator loss: 1.500542]\n",
      "1512 [Discriminator loss: 0.844773, acc.: 50.00%] [Generator loss: 1.358965]\n",
      "1513 [Discriminator loss: 0.829157, acc.: 49.22%] [Generator loss: 1.540354]\n",
      "1514 [Discriminator loss: 0.833490, acc.: 55.47%] [Generator loss: 1.345350]\n",
      "1515 [Discriminator loss: 0.800048, acc.: 50.00%] [Generator loss: 1.407650]\n",
      "1516 [Discriminator loss: 0.946058, acc.: 46.88%] [Generator loss: 1.411049]\n",
      "1517 [Discriminator loss: 0.734241, acc.: 50.78%] [Generator loss: 1.626721]\n",
      "1518 [Discriminator loss: 0.908292, acc.: 44.53%] [Generator loss: 1.359719]\n",
      "1519 [Discriminator loss: 0.714810, acc.: 63.28%] [Generator loss: 1.443347]\n",
      "1520 [Discriminator loss: 0.782438, acc.: 56.25%] [Generator loss: 1.505127]\n",
      "1521 [Discriminator loss: 0.758790, acc.: 58.59%] [Generator loss: 1.295581]\n",
      "1522 [Discriminator loss: 0.595389, acc.: 67.97%] [Generator loss: 0.939335]\n",
      "1523 [Discriminator loss: 0.554057, acc.: 67.97%] [Generator loss: 1.505820]\n",
      "1524 [Discriminator loss: 0.479483, acc.: 78.91%] [Generator loss: 0.937340]\n",
      "1525 [Discriminator loss: 0.767459, acc.: 59.38%] [Generator loss: 1.266868]\n",
      "1526 [Discriminator loss: 0.655651, acc.: 64.84%] [Generator loss: 1.596606]\n",
      "1527 [Discriminator loss: 1.018602, acc.: 42.97%] [Generator loss: 1.263622]\n",
      "1528 [Discriminator loss: 0.603155, acc.: 72.66%] [Generator loss: 1.228098]\n",
      "1529 [Discriminator loss: 0.371888, acc.: 83.59%] [Generator loss: 0.690362]\n",
      "1530 [Discriminator loss: 1.183838, acc.: 40.62%] [Generator loss: 1.705817]\n",
      "1531 [Discriminator loss: 0.808088, acc.: 55.47%] [Generator loss: 1.961297]\n",
      "1532 [Discriminator loss: 0.856995, acc.: 55.47%] [Generator loss: 0.902684]\n",
      "1533 [Discriminator loss: 0.711712, acc.: 58.59%] [Generator loss: 0.821633]\n",
      "1534 [Discriminator loss: 1.284934, acc.: 31.25%] [Generator loss: 1.109746]\n",
      "1535 [Discriminator loss: 0.545214, acc.: 71.88%] [Generator loss: 1.887881]\n",
      "1536 [Discriminator loss: 0.751151, acc.: 59.38%] [Generator loss: 1.859005]\n",
      "1537 [Discriminator loss: 0.888009, acc.: 50.00%] [Generator loss: 1.935947]\n",
      "1538 [Discriminator loss: 0.815993, acc.: 54.69%] [Generator loss: 1.963955]\n",
      "1539 [Discriminator loss: 0.979078, acc.: 37.50%] [Generator loss: 1.727242]\n",
      "1540 [Discriminator loss: 1.449990, acc.: 19.53%] [Generator loss: 1.357511]\n",
      "1541 [Discriminator loss: 0.980857, acc.: 44.53%] [Generator loss: 1.935984]\n",
      "1542 [Discriminator loss: 0.904897, acc.: 52.34%] [Generator loss: 1.816052]\n",
      "1543 [Discriminator loss: 0.806116, acc.: 48.44%] [Generator loss: 1.679787]\n",
      "1544 [Discriminator loss: 0.789149, acc.: 50.00%] [Generator loss: 1.329968]\n",
      "1545 [Discriminator loss: 0.804335, acc.: 55.47%] [Generator loss: 1.310085]\n",
      "1546 [Discriminator loss: 0.705830, acc.: 57.03%] [Generator loss: 1.769624]\n",
      "1547 [Discriminator loss: 0.911672, acc.: 45.31%] [Generator loss: 1.347170]\n",
      "1548 [Discriminator loss: 0.711910, acc.: 59.38%] [Generator loss: 1.263771]\n",
      "1549 [Discriminator loss: 0.823144, acc.: 57.03%] [Generator loss: 1.121286]\n",
      "1550 [Discriminator loss: 0.761370, acc.: 54.69%] [Generator loss: 1.354104]\n",
      "1551 [Discriminator loss: 0.909551, acc.: 43.75%] [Generator loss: 0.872781]\n",
      "1552 [Discriminator loss: 0.655222, acc.: 68.75%] [Generator loss: 1.132923]\n",
      "1553 [Discriminator loss: 0.762359, acc.: 53.91%] [Generator loss: 1.648816]\n",
      "1554 [Discriminator loss: 0.652762, acc.: 60.94%] [Generator loss: 1.451967]\n",
      "1555 [Discriminator loss: 0.993929, acc.: 40.62%] [Generator loss: 1.235183]\n",
      "1556 [Discriminator loss: 0.647804, acc.: 66.41%] [Generator loss: 1.353480]\n",
      "1557 [Discriminator loss: 1.062041, acc.: 34.38%] [Generator loss: 1.194239]\n",
      "1558 [Discriminator loss: 0.763717, acc.: 59.38%] [Generator loss: 1.646415]\n",
      "1559 [Discriminator loss: 0.795668, acc.: 57.03%] [Generator loss: 1.412497]\n",
      "1560 [Discriminator loss: 0.690494, acc.: 58.59%] [Generator loss: 1.288833]\n",
      "1561 [Discriminator loss: 0.663636, acc.: 65.62%] [Generator loss: 0.940019]\n",
      "1562 [Discriminator loss: 0.775583, acc.: 56.25%] [Generator loss: 1.055495]\n",
      "1563 [Discriminator loss: 0.537169, acc.: 70.31%] [Generator loss: 0.827551]\n",
      "1564 [Discriminator loss: 0.779544, acc.: 56.25%] [Generator loss: 1.230180]\n",
      "1565 [Discriminator loss: 0.599591, acc.: 69.53%] [Generator loss: 1.007696]\n",
      "1566 [Discriminator loss: 0.612675, acc.: 67.19%] [Generator loss: 0.864898]\n",
      "1567 [Discriminator loss: 0.599132, acc.: 64.06%] [Generator loss: 1.231005]\n",
      "1568 [Discriminator loss: 0.933018, acc.: 43.75%] [Generator loss: 1.281691]\n",
      "1569 [Discriminator loss: 0.687308, acc.: 63.28%] [Generator loss: 1.557059]\n",
      "1570 [Discriminator loss: 0.843031, acc.: 51.56%] [Generator loss: 1.455962]\n",
      "1571 [Discriminator loss: 1.098407, acc.: 35.94%] [Generator loss: 1.283214]\n",
      "1572 [Discriminator loss: 0.807762, acc.: 51.56%] [Generator loss: 1.582842]\n",
      "1573 [Discriminator loss: 1.113824, acc.: 32.81%] [Generator loss: 1.186103]\n",
      "1574 [Discriminator loss: 0.777876, acc.: 53.91%] [Generator loss: 1.046240]\n",
      "1575 [Discriminator loss: 0.755855, acc.: 58.59%] [Generator loss: 0.790335]\n",
      "1576 [Discriminator loss: 0.918791, acc.: 49.22%] [Generator loss: 0.920537]\n",
      "1577 [Discriminator loss: 0.616616, acc.: 67.97%] [Generator loss: 1.343260]\n",
      "1578 [Discriminator loss: 0.504002, acc.: 77.34%] [Generator loss: 0.693000]\n",
      "1579 [Discriminator loss: 0.962455, acc.: 47.66%] [Generator loss: 0.797948]\n",
      "1580 [Discriminator loss: 0.609801, acc.: 71.09%] [Generator loss: 1.528035]\n",
      "1581 [Discriminator loss: 0.672411, acc.: 64.06%] [Generator loss: 1.541585]\n",
      "1582 [Discriminator loss: 0.780320, acc.: 60.16%] [Generator loss: 1.519191]\n",
      "1583 [Discriminator loss: 0.726812, acc.: 55.47%] [Generator loss: 1.022800]\n",
      "1584 [Discriminator loss: 0.490813, acc.: 78.12%] [Generator loss: 0.647129]\n",
      "1585 [Discriminator loss: 0.591804, acc.: 69.53%] [Generator loss: 0.972873]\n",
      "1586 [Discriminator loss: 0.474764, acc.: 76.56%] [Generator loss: 0.795167]\n",
      "1587 [Discriminator loss: 0.868951, acc.: 49.22%] [Generator loss: 0.936148]\n",
      "1588 [Discriminator loss: 0.471632, acc.: 78.12%] [Generator loss: 0.829655]\n",
      "1589 [Discriminator loss: 0.883612, acc.: 53.12%] [Generator loss: 0.858456]\n",
      "1590 [Discriminator loss: 0.634084, acc.: 71.88%] [Generator loss: 0.519407]\n",
      "1591 [Discriminator loss: 0.583482, acc.: 70.31%] [Generator loss: 0.762616]\n",
      "1592 [Discriminator loss: 0.485383, acc.: 80.47%] [Generator loss: 1.028951]\n",
      "1593 [Discriminator loss: 0.880481, acc.: 43.75%] [Generator loss: 1.445312]\n",
      "1594 [Discriminator loss: 0.477739, acc.: 77.34%] [Generator loss: 1.908702]\n",
      "1595 [Discriminator loss: 0.565260, acc.: 73.44%] [Generator loss: 1.184035]\n",
      "1596 [Discriminator loss: 0.908555, acc.: 51.56%] [Generator loss: 1.162483]\n",
      "1597 [Discriminator loss: 0.408218, acc.: 84.38%] [Generator loss: 1.287363]\n",
      "1598 [Discriminator loss: 0.823693, acc.: 57.03%] [Generator loss: 1.723386]\n",
      "1599 [Discriminator loss: 0.496218, acc.: 75.78%] [Generator loss: 1.323648]\n",
      "1600 [Discriminator loss: 0.457123, acc.: 79.69%] [Generator loss: 0.387537]\n",
      "1601 [Discriminator loss: 0.355662, acc.: 85.94%] [Generator loss: 0.233398]\n",
      "1602 [Discriminator loss: 0.265587, acc.: 89.84%] [Generator loss: 0.493408]\n",
      "1603 [Discriminator loss: 0.925690, acc.: 50.78%] [Generator loss: 1.197748]\n",
      "1604 [Discriminator loss: 0.397240, acc.: 78.12%] [Generator loss: 1.207381]\n",
      "1605 [Discriminator loss: 0.480587, acc.: 74.22%] [Generator loss: 0.471330]\n",
      "1606 [Discriminator loss: 1.879746, acc.: 20.31%] [Generator loss: 0.976243]\n",
      "1607 [Discriminator loss: 0.584394, acc.: 65.62%] [Generator loss: 2.605095]\n",
      "1608 [Discriminator loss: 0.872501, acc.: 55.47%] [Generator loss: 1.557281]\n",
      "1609 [Discriminator loss: 0.879972, acc.: 48.44%] [Generator loss: 1.591242]\n",
      "1610 [Discriminator loss: 0.775169, acc.: 59.38%] [Generator loss: 0.946986]\n",
      "1611 [Discriminator loss: 0.576297, acc.: 74.22%] [Generator loss: 0.830068]\n",
      "1612 [Discriminator loss: 0.374056, acc.: 82.81%] [Generator loss: 0.383769]\n",
      "1613 [Discriminator loss: 0.760821, acc.: 57.81%] [Generator loss: 1.039923]\n",
      "1614 [Discriminator loss: 0.240153, acc.: 88.28%] [Generator loss: 1.058140]\n",
      "1615 [Discriminator loss: 0.330640, acc.: 89.84%] [Generator loss: 0.548195]\n",
      "1616 [Discriminator loss: 0.341028, acc.: 85.94%] [Generator loss: 0.660331]\n",
      "1617 [Discriminator loss: 0.744037, acc.: 63.28%] [Generator loss: 1.924419]\n",
      "1618 [Discriminator loss: 0.827240, acc.: 57.81%] [Generator loss: 1.306302]\n",
      "1619 [Discriminator loss: 0.848204, acc.: 53.91%] [Generator loss: 1.353291]\n",
      "1620 [Discriminator loss: 0.491146, acc.: 76.56%] [Generator loss: 1.372921]\n",
      "1621 [Discriminator loss: 0.706206, acc.: 64.06%] [Generator loss: 0.942249]\n",
      "1622 [Discriminator loss: 0.603143, acc.: 66.41%] [Generator loss: 0.666121]\n",
      "1623 [Discriminator loss: 0.319018, acc.: 89.84%] [Generator loss: 0.472818]\n",
      "1624 [Discriminator loss: 0.342259, acc.: 85.94%] [Generator loss: 0.195955]\n",
      "1625 [Discriminator loss: 0.924449, acc.: 50.78%] [Generator loss: 1.116657]\n",
      "1626 [Discriminator loss: 0.588005, acc.: 70.31%] [Generator loss: 0.771371]\n",
      "1627 [Discriminator loss: 0.243441, acc.: 92.19%] [Generator loss: 0.312444]\n",
      "1628 [Discriminator loss: 0.817586, acc.: 53.12%] [Generator loss: 1.448471]\n",
      "1629 [Discriminator loss: 0.336804, acc.: 81.25%] [Generator loss: 1.896982]\n",
      "1630 [Discriminator loss: 1.510590, acc.: 34.38%] [Generator loss: 1.874912]\n",
      "1631 [Discriminator loss: 0.554172, acc.: 75.00%] [Generator loss: 2.297904]\n",
      "1632 [Discriminator loss: 1.112554, acc.: 43.75%] [Generator loss: 0.939781]\n",
      "1633 [Discriminator loss: 1.102898, acc.: 41.41%] [Generator loss: 1.948543]\n",
      "1634 [Discriminator loss: 0.643684, acc.: 66.41%] [Generator loss: 1.564745]\n",
      "1635 [Discriminator loss: 0.872040, acc.: 47.66%] [Generator loss: 1.188852]\n",
      "1636 [Discriminator loss: 0.721882, acc.: 57.81%] [Generator loss: 1.319648]\n",
      "1637 [Discriminator loss: 0.648599, acc.: 60.94%] [Generator loss: 1.099923]\n",
      "1638 [Discriminator loss: 1.002890, acc.: 41.41%] [Generator loss: 1.264335]\n",
      "1639 [Discriminator loss: 0.800382, acc.: 53.91%] [Generator loss: 1.800997]\n",
      "1640 [Discriminator loss: 0.929615, acc.: 42.97%] [Generator loss: 1.387161]\n",
      "1641 [Discriminator loss: 0.486893, acc.: 72.66%] [Generator loss: 1.304006]\n",
      "1642 [Discriminator loss: 0.781797, acc.: 53.91%] [Generator loss: 1.803578]\n",
      "1643 [Discriminator loss: 0.700823, acc.: 62.50%] [Generator loss: 1.659476]\n",
      "1644 [Discriminator loss: 0.845761, acc.: 52.34%] [Generator loss: 1.437327]\n",
      "1645 [Discriminator loss: 1.026023, acc.: 42.19%] [Generator loss: 1.865714]\n",
      "1646 [Discriminator loss: 0.926136, acc.: 45.31%] [Generator loss: 1.699957]\n",
      "1647 [Discriminator loss: 1.391459, acc.: 26.56%] [Generator loss: 1.402842]\n",
      "1648 [Discriminator loss: 0.867637, acc.: 59.38%] [Generator loss: 1.558185]\n",
      "1649 [Discriminator loss: 0.996086, acc.: 43.75%] [Generator loss: 2.231726]\n",
      "1650 [Discriminator loss: 0.818943, acc.: 50.78%] [Generator loss: 2.200247]\n",
      "1651 [Discriminator loss: 0.780794, acc.: 54.69%] [Generator loss: 2.273690]\n",
      "1652 [Discriminator loss: 0.702749, acc.: 58.59%] [Generator loss: 1.792247]\n",
      "1653 [Discriminator loss: 0.738000, acc.: 61.72%] [Generator loss: 1.614421]\n",
      "1654 [Discriminator loss: 0.549481, acc.: 77.34%] [Generator loss: 1.655993]\n",
      "1655 [Discriminator loss: 0.829667, acc.: 57.03%] [Generator loss: 1.418348]\n",
      "1656 [Discriminator loss: 1.112146, acc.: 33.59%] [Generator loss: 1.580809]\n",
      "1657 [Discriminator loss: 0.953784, acc.: 46.88%] [Generator loss: 1.833606]\n",
      "1658 [Discriminator loss: 0.800680, acc.: 60.94%] [Generator loss: 1.040613]\n",
      "1659 [Discriminator loss: 0.957795, acc.: 46.88%] [Generator loss: 1.347051]\n",
      "1660 [Discriminator loss: 0.625510, acc.: 67.97%] [Generator loss: 1.490341]\n",
      "1661 [Discriminator loss: 1.007340, acc.: 39.06%] [Generator loss: 1.218308]\n",
      "1662 [Discriminator loss: 0.809889, acc.: 49.22%] [Generator loss: 1.207975]\n",
      "1663 [Discriminator loss: 1.020543, acc.: 42.97%] [Generator loss: 1.113647]\n",
      "1664 [Discriminator loss: 0.713106, acc.: 61.72%] [Generator loss: 1.456860]\n",
      "1665 [Discriminator loss: 0.658118, acc.: 66.41%] [Generator loss: 1.336221]\n",
      "1666 [Discriminator loss: 0.572724, acc.: 74.22%] [Generator loss: 1.399918]\n",
      "1667 [Discriminator loss: 0.709975, acc.: 57.81%] [Generator loss: 1.333688]\n",
      "1668 [Discriminator loss: 0.889486, acc.: 49.22%] [Generator loss: 1.436750]\n",
      "1669 [Discriminator loss: 0.982231, acc.: 43.75%] [Generator loss: 1.108603]\n",
      "1670 [Discriminator loss: 0.991171, acc.: 42.19%] [Generator loss: 0.992466]\n",
      "1671 [Discriminator loss: 0.683918, acc.: 63.28%] [Generator loss: 1.569089]\n",
      "1672 [Discriminator loss: 0.716011, acc.: 60.16%] [Generator loss: 1.588912]\n",
      "1673 [Discriminator loss: 0.670400, acc.: 65.62%] [Generator loss: 1.526534]\n",
      "1674 [Discriminator loss: 0.633948, acc.: 71.09%] [Generator loss: 1.678879]\n",
      "1675 [Discriminator loss: 0.656150, acc.: 64.84%] [Generator loss: 1.176228]\n",
      "1676 [Discriminator loss: 0.816294, acc.: 48.44%] [Generator loss: 1.354924]\n",
      "1677 [Discriminator loss: 0.914725, acc.: 45.31%] [Generator loss: 1.230427]\n",
      "1678 [Discriminator loss: 0.786865, acc.: 56.25%] [Generator loss: 1.355473]\n",
      "1679 [Discriminator loss: 0.899426, acc.: 45.31%] [Generator loss: 0.985186]\n",
      "1680 [Discriminator loss: 0.665297, acc.: 65.62%] [Generator loss: 1.097311]\n",
      "1681 [Discriminator loss: 0.896984, acc.: 45.31%] [Generator loss: 1.442631]\n",
      "1682 [Discriminator loss: 0.542078, acc.: 71.09%] [Generator loss: 0.807436]\n",
      "1683 [Discriminator loss: 0.527969, acc.: 75.78%] [Generator loss: 0.598035]\n",
      "1684 [Discriminator loss: 0.403084, acc.: 83.59%] [Generator loss: 0.963538]\n",
      "1685 [Discriminator loss: 0.429939, acc.: 79.69%] [Generator loss: 1.147148]\n",
      "1686 [Discriminator loss: 0.952169, acc.: 43.75%] [Generator loss: 0.584668]\n",
      "1687 [Discriminator loss: 0.534022, acc.: 73.44%] [Generator loss: 0.836939]\n",
      "1688 [Discriminator loss: 0.324341, acc.: 88.28%] [Generator loss: 0.667202]\n",
      "1689 [Discriminator loss: 0.665613, acc.: 65.62%] [Generator loss: 1.061633]\n",
      "1690 [Discriminator loss: 0.246969, acc.: 90.62%] [Generator loss: 0.847648]\n",
      "1691 [Discriminator loss: 0.731337, acc.: 61.72%] [Generator loss: 1.029229]\n",
      "1692 [Discriminator loss: 0.680428, acc.: 69.53%] [Generator loss: 1.030186]\n",
      "1693 [Discriminator loss: 0.417897, acc.: 85.94%] [Generator loss: 1.152765]\n",
      "1694 [Discriminator loss: 2.124067, acc.: 7.03%] [Generator loss: 0.931683]\n",
      "1695 [Discriminator loss: 0.773492, acc.: 57.81%] [Generator loss: 1.665774]\n",
      "1696 [Discriminator loss: 0.875232, acc.: 46.09%] [Generator loss: 1.367035]\n",
      "1697 [Discriminator loss: 0.440782, acc.: 81.25%] [Generator loss: 1.205206]\n",
      "1698 [Discriminator loss: 0.592361, acc.: 71.09%] [Generator loss: 0.788751]\n",
      "1699 [Discriminator loss: 0.439524, acc.: 78.91%] [Generator loss: 0.807611]\n",
      "1700 [Discriminator loss: 0.850344, acc.: 48.44%] [Generator loss: 1.370841]\n",
      "1701 [Discriminator loss: 0.792922, acc.: 53.91%] [Generator loss: 1.254581]\n",
      "1702 [Discriminator loss: 0.605153, acc.: 70.31%] [Generator loss: 1.758199]\n",
      "1703 [Discriminator loss: 0.766249, acc.: 53.91%] [Generator loss: 1.334216]\n",
      "1704 [Discriminator loss: 0.640195, acc.: 61.72%] [Generator loss: 1.654508]\n",
      "1705 [Discriminator loss: 1.097742, acc.: 28.91%] [Generator loss: 1.503894]\n",
      "1706 [Discriminator loss: 0.571379, acc.: 67.19%] [Generator loss: 1.772650]\n",
      "1707 [Discriminator loss: 0.545588, acc.: 75.78%] [Generator loss: 1.694734]\n",
      "1708 [Discriminator loss: 0.633663, acc.: 70.31%] [Generator loss: 1.397638]\n",
      "1709 [Discriminator loss: 0.629946, acc.: 66.41%] [Generator loss: 1.753752]\n",
      "1710 [Discriminator loss: 0.840145, acc.: 55.47%] [Generator loss: 1.261034]\n",
      "1711 [Discriminator loss: 0.633229, acc.: 65.62%] [Generator loss: 1.543338]\n",
      "1712 [Discriminator loss: 0.807402, acc.: 52.34%] [Generator loss: 1.686225]\n",
      "1713 [Discriminator loss: 1.159094, acc.: 32.03%] [Generator loss: 1.762273]\n",
      "1714 [Discriminator loss: 1.165563, acc.: 29.69%] [Generator loss: 1.611950]\n",
      "1715 [Discriminator loss: 1.134725, acc.: 38.28%] [Generator loss: 1.686795]\n",
      "1716 [Discriminator loss: 1.064174, acc.: 35.94%] [Generator loss: 1.672256]\n",
      "1717 [Discriminator loss: 0.760889, acc.: 61.72%] [Generator loss: 1.176959]\n",
      "1718 [Discriminator loss: 0.473342, acc.: 79.69%] [Generator loss: 1.049942]\n",
      "1719 [Discriminator loss: 1.494211, acc.: 27.34%] [Generator loss: 1.355065]\n",
      "1720 [Discriminator loss: 0.555012, acc.: 72.66%] [Generator loss: 1.946174]\n",
      "1721 [Discriminator loss: 0.806425, acc.: 55.47%] [Generator loss: 1.672910]\n",
      "1722 [Discriminator loss: 0.475481, acc.: 77.34%] [Generator loss: 1.118131]\n",
      "1723 [Discriminator loss: 0.447671, acc.: 82.81%] [Generator loss: 1.045695]\n",
      "1724 [Discriminator loss: 0.479766, acc.: 79.69%] [Generator loss: 0.829390]\n",
      "1725 [Discriminator loss: 0.444139, acc.: 77.34%] [Generator loss: 0.622754]\n",
      "1726 [Discriminator loss: 0.497871, acc.: 75.78%] [Generator loss: 0.828891]\n",
      "1727 [Discriminator loss: 0.434104, acc.: 81.25%] [Generator loss: 0.766347]\n",
      "1728 [Discriminator loss: 0.546652, acc.: 71.09%] [Generator loss: 0.526516]\n",
      "1729 [Discriminator loss: 0.398761, acc.: 82.81%] [Generator loss: 1.117551]\n",
      "1730 [Discriminator loss: 0.404117, acc.: 85.16%] [Generator loss: 1.466990]\n",
      "1731 [Discriminator loss: 0.471295, acc.: 78.12%] [Generator loss: 0.939185]\n",
      "1732 [Discriminator loss: 0.500527, acc.: 75.00%] [Generator loss: 0.716523]\n",
      "1733 [Discriminator loss: 0.689373, acc.: 63.28%] [Generator loss: 2.033218]\n",
      "1734 [Discriminator loss: 0.661091, acc.: 71.09%] [Generator loss: 1.551544]\n",
      "1735 [Discriminator loss: 0.403931, acc.: 81.25%] [Generator loss: 0.631113]\n",
      "1736 [Discriminator loss: 0.384351, acc.: 83.59%] [Generator loss: 0.306850]\n",
      "1737 [Discriminator loss: 0.225526, acc.: 92.97%] [Generator loss: 0.381186]\n",
      "1738 [Discriminator loss: 0.219981, acc.: 92.19%] [Generator loss: 0.220858]\n",
      "1739 [Discriminator loss: 0.103027, acc.: 96.09%] [Generator loss: 0.229652]\n",
      "1740 [Discriminator loss: 0.080857, acc.: 99.22%] [Generator loss: 0.118561]\n",
      "1741 [Discriminator loss: 1.072965, acc.: 57.03%] [Generator loss: 2.660877]\n",
      "1742 [Discriminator loss: 0.665074, acc.: 68.75%] [Generator loss: 2.281902]\n",
      "1743 [Discriminator loss: 0.742350, acc.: 57.03%] [Generator loss: 1.447323]\n",
      "1744 [Discriminator loss: 0.541980, acc.: 71.88%] [Generator loss: 1.346752]\n",
      "1745 [Discriminator loss: 0.504293, acc.: 73.44%] [Generator loss: 2.130059]\n",
      "1746 [Discriminator loss: 0.755540, acc.: 52.34%] [Generator loss: 1.845424]\n",
      "1747 [Discriminator loss: 0.607833, acc.: 68.75%] [Generator loss: 1.358236]\n",
      "1748 [Discriminator loss: 0.871328, acc.: 52.34%] [Generator loss: 1.404534]\n",
      "1749 [Discriminator loss: 0.614180, acc.: 69.53%] [Generator loss: 0.567920]\n",
      "1750 [Discriminator loss: 0.408559, acc.: 81.25%] [Generator loss: 0.616334]\n",
      "1751 [Discriminator loss: 0.444786, acc.: 78.91%] [Generator loss: 1.299587]\n",
      "1752 [Discriminator loss: 0.324551, acc.: 88.28%] [Generator loss: 1.443337]\n",
      "1753 [Discriminator loss: 0.741502, acc.: 57.81%] [Generator loss: 1.744103]\n",
      "1754 [Discriminator loss: 0.656001, acc.: 64.06%] [Generator loss: 2.277751]\n",
      "1755 [Discriminator loss: 1.327066, acc.: 27.34%] [Generator loss: 1.314643]\n",
      "1756 [Discriminator loss: 0.313452, acc.: 85.94%] [Generator loss: 0.776261]\n",
      "1757 [Discriminator loss: 0.167832, acc.: 92.97%] [Generator loss: 0.356486]\n",
      "1758 [Discriminator loss: 0.332726, acc.: 86.72%] [Generator loss: 0.187857]\n",
      "1759 [Discriminator loss: 0.188574, acc.: 95.31%] [Generator loss: 0.272081]\n",
      "1760 [Discriminator loss: 0.109023, acc.: 99.22%] [Generator loss: 0.248294]\n",
      "1761 [Discriminator loss: 0.134537, acc.: 96.09%] [Generator loss: 0.256133]\n",
      "1762 [Discriminator loss: 0.428747, acc.: 79.69%] [Generator loss: 0.271643]\n",
      "1763 [Discriminator loss: 0.205942, acc.: 89.06%] [Generator loss: 0.410596]\n",
      "1764 [Discriminator loss: 0.618406, acc.: 72.66%] [Generator loss: 1.609613]\n",
      "1765 [Discriminator loss: 2.206528, acc.: 10.94%] [Generator loss: 1.436268]\n",
      "1766 [Discriminator loss: 0.709085, acc.: 63.28%] [Generator loss: 2.923174]\n",
      "1767 [Discriminator loss: 1.640425, acc.: 25.78%] [Generator loss: 1.710324]\n",
      "1768 [Discriminator loss: 0.941378, acc.: 50.78%] [Generator loss: 1.129185]\n",
      "1769 [Discriminator loss: 0.274061, acc.: 91.41%] [Generator loss: 1.093709]\n",
      "1770 [Discriminator loss: 0.553543, acc.: 71.09%] [Generator loss: 1.172880]\n",
      "1771 [Discriminator loss: 0.711343, acc.: 61.72%] [Generator loss: 1.022663]\n",
      "1772 [Discriminator loss: 0.391409, acc.: 78.12%] [Generator loss: 0.492236]\n",
      "1773 [Discriminator loss: 0.862381, acc.: 51.56%] [Generator loss: 1.220160]\n",
      "1774 [Discriminator loss: 0.712832, acc.: 59.38%] [Generator loss: 1.338449]\n",
      "1775 [Discriminator loss: 1.409551, acc.: 27.34%] [Generator loss: 1.243007]\n",
      "1776 [Discriminator loss: 0.928176, acc.: 41.41%] [Generator loss: 1.255129]\n",
      "1777 [Discriminator loss: 0.579529, acc.: 69.53%] [Generator loss: 1.527722]\n",
      "1778 [Discriminator loss: 0.665707, acc.: 71.09%] [Generator loss: 0.955925]\n",
      "1779 [Discriminator loss: 0.607577, acc.: 66.41%] [Generator loss: 1.164342]\n",
      "1780 [Discriminator loss: 0.577090, acc.: 69.53%] [Generator loss: 1.819214]\n",
      "1781 [Discriminator loss: 0.918922, acc.: 46.09%] [Generator loss: 1.476699]\n",
      "1782 [Discriminator loss: 0.791386, acc.: 51.56%] [Generator loss: 1.091087]\n",
      "1783 [Discriminator loss: 0.848476, acc.: 47.66%] [Generator loss: 1.545595]\n",
      "1784 [Discriminator loss: 0.502817, acc.: 71.09%] [Generator loss: 1.595564]\n",
      "1785 [Discriminator loss: 0.991151, acc.: 42.97%] [Generator loss: 1.131712]\n",
      "1786 [Discriminator loss: 0.663180, acc.: 57.03%] [Generator loss: 1.557085]\n",
      "1787 [Discriminator loss: 0.720822, acc.: 52.34%] [Generator loss: 1.549748]\n",
      "1788 [Discriminator loss: 0.845584, acc.: 53.91%] [Generator loss: 1.251967]\n",
      "1789 [Discriminator loss: 1.063057, acc.: 30.47%] [Generator loss: 1.029266]\n",
      "1790 [Discriminator loss: 0.802684, acc.: 54.69%] [Generator loss: 1.497853]\n",
      "1791 [Discriminator loss: 0.796428, acc.: 57.81%] [Generator loss: 1.303228]\n",
      "1792 [Discriminator loss: 0.800076, acc.: 57.03%] [Generator loss: 0.809200]\n",
      "1793 [Discriminator loss: 0.520459, acc.: 75.00%] [Generator loss: 1.103441]\n",
      "1794 [Discriminator loss: 0.768919, acc.: 57.81%] [Generator loss: 1.575082]\n",
      "1795 [Discriminator loss: 0.514679, acc.: 73.44%] [Generator loss: 1.253650]\n",
      "1796 [Discriminator loss: 0.612626, acc.: 70.31%] [Generator loss: 1.351293]\n",
      "1797 [Discriminator loss: 0.828686, acc.: 54.69%] [Generator loss: 1.261236]\n",
      "1798 [Discriminator loss: 0.731262, acc.: 56.25%] [Generator loss: 1.771772]\n",
      "1799 [Discriminator loss: 0.856201, acc.: 57.03%] [Generator loss: 1.452844]\n",
      "1800 [Discriminator loss: 0.595635, acc.: 69.53%] [Generator loss: 0.862579]\n",
      "1801 [Discriminator loss: 0.538874, acc.: 74.22%] [Generator loss: 0.453465]\n",
      "1802 [Discriminator loss: 0.668644, acc.: 58.59%] [Generator loss: 1.217345]\n",
      "1803 [Discriminator loss: 0.447304, acc.: 76.56%] [Generator loss: 0.712685]\n",
      "1804 [Discriminator loss: 0.540265, acc.: 74.22%] [Generator loss: 0.928349]\n",
      "1805 [Discriminator loss: 0.411202, acc.: 83.59%] [Generator loss: 0.544301]\n",
      "1806 [Discriminator loss: 0.600068, acc.: 68.75%] [Generator loss: 0.696617]\n",
      "1807 [Discriminator loss: 0.766494, acc.: 62.50%] [Generator loss: 0.410329]\n",
      "1808 [Discriminator loss: 0.827968, acc.: 60.94%] [Generator loss: 1.936840]\n",
      "1809 [Discriminator loss: 0.443922, acc.: 78.91%] [Generator loss: 1.988566]\n",
      "1810 [Discriminator loss: 1.244968, acc.: 39.06%] [Generator loss: 1.114077]\n",
      "1811 [Discriminator loss: 0.784251, acc.: 57.03%] [Generator loss: 1.540504]\n",
      "1812 [Discriminator loss: 1.097023, acc.: 39.06%] [Generator loss: 2.048274]\n",
      "1813 [Discriminator loss: 0.756271, acc.: 58.59%] [Generator loss: 2.007309]\n",
      "1814 [Discriminator loss: 0.726808, acc.: 56.25%] [Generator loss: 1.783787]\n",
      "1815 [Discriminator loss: 0.781496, acc.: 54.69%] [Generator loss: 1.644390]\n",
      "1816 [Discriminator loss: 0.762965, acc.: 57.81%] [Generator loss: 1.400003]\n",
      "1817 [Discriminator loss: 1.047724, acc.: 34.38%] [Generator loss: 1.720389]\n",
      "1818 [Discriminator loss: 0.863856, acc.: 47.66%] [Generator loss: 1.888678]\n",
      "1819 [Discriminator loss: 1.122020, acc.: 35.94%] [Generator loss: 1.548275]\n",
      "1820 [Discriminator loss: 1.042509, acc.: 38.28%] [Generator loss: 2.037658]\n",
      "1821 [Discriminator loss: 0.974834, acc.: 42.97%] [Generator loss: 1.477470]\n",
      "1822 [Discriminator loss: 1.051073, acc.: 35.16%] [Generator loss: 1.095239]\n",
      "1823 [Discriminator loss: 0.629823, acc.: 64.84%] [Generator loss: 1.873940]\n",
      "1824 [Discriminator loss: 0.627481, acc.: 70.31%] [Generator loss: 1.887038]\n",
      "1825 [Discriminator loss: 0.751119, acc.: 57.81%] [Generator loss: 1.506949]\n",
      "1826 [Discriminator loss: 0.669284, acc.: 60.94%] [Generator loss: 1.313046]\n",
      "1827 [Discriminator loss: 0.592363, acc.: 70.31%] [Generator loss: 1.259992]\n",
      "1828 [Discriminator loss: 0.675676, acc.: 59.38%] [Generator loss: 1.347254]\n",
      "1829 [Discriminator loss: 0.752071, acc.: 55.47%] [Generator loss: 1.066576]\n",
      "1830 [Discriminator loss: 0.509276, acc.: 75.00%] [Generator loss: 0.900188]\n",
      "1831 [Discriminator loss: 0.645971, acc.: 64.06%] [Generator loss: 0.932241]\n",
      "1832 [Discriminator loss: 0.756159, acc.: 53.91%] [Generator loss: 1.566691]\n",
      "1833 [Discriminator loss: 0.796296, acc.: 53.91%] [Generator loss: 1.253485]\n",
      "1834 [Discriminator loss: 0.906318, acc.: 50.00%] [Generator loss: 0.910353]\n",
      "1835 [Discriminator loss: 0.718919, acc.: 60.94%] [Generator loss: 0.990671]\n",
      "1836 [Discriminator loss: 0.774476, acc.: 60.94%] [Generator loss: 1.575869]\n",
      "1837 [Discriminator loss: 0.654191, acc.: 59.38%] [Generator loss: 1.255069]\n",
      "1838 [Discriminator loss: 0.924409, acc.: 43.75%] [Generator loss: 1.202289]\n",
      "1839 [Discriminator loss: 0.991116, acc.: 46.09%] [Generator loss: 1.277552]\n",
      "1840 [Discriminator loss: 0.716802, acc.: 60.16%] [Generator loss: 1.219406]\n",
      "1841 [Discriminator loss: 0.922547, acc.: 45.31%] [Generator loss: 1.205770]\n",
      "1842 [Discriminator loss: 0.851826, acc.: 50.78%] [Generator loss: 1.049055]\n",
      "1843 [Discriminator loss: 0.897361, acc.: 50.00%] [Generator loss: 1.310754]\n",
      "1844 [Discriminator loss: 0.644086, acc.: 67.19%] [Generator loss: 0.863264]\n",
      "1845 [Discriminator loss: 0.483307, acc.: 80.47%] [Generator loss: 0.754628]\n",
      "1846 [Discriminator loss: 1.307609, acc.: 35.16%] [Generator loss: 1.345268]\n",
      "1847 [Discriminator loss: 0.767441, acc.: 53.91%] [Generator loss: 2.363829]\n",
      "1848 [Discriminator loss: 0.797489, acc.: 58.59%] [Generator loss: 1.816063]\n",
      "1849 [Discriminator loss: 0.775324, acc.: 51.56%] [Generator loss: 1.141278]\n",
      "1850 [Discriminator loss: 0.805227, acc.: 44.53%] [Generator loss: 1.356540]\n",
      "1851 [Discriminator loss: 0.659897, acc.: 67.19%] [Generator loss: 1.369185]\n",
      "1852 [Discriminator loss: 0.774702, acc.: 56.25%] [Generator loss: 1.091439]\n",
      "1853 [Discriminator loss: 0.808125, acc.: 46.88%] [Generator loss: 1.578574]\n",
      "1854 [Discriminator loss: 0.805444, acc.: 53.12%] [Generator loss: 1.435157]\n",
      "1855 [Discriminator loss: 0.593760, acc.: 66.41%] [Generator loss: 1.438948]\n",
      "1856 [Discriminator loss: 0.842729, acc.: 51.56%] [Generator loss: 1.337346]\n",
      "1857 [Discriminator loss: 0.575925, acc.: 67.97%] [Generator loss: 1.456182]\n",
      "1858 [Discriminator loss: 0.911941, acc.: 41.41%] [Generator loss: 1.390384]\n",
      "1859 [Discriminator loss: 0.554265, acc.: 73.44%] [Generator loss: 0.688120]\n",
      "1860 [Discriminator loss: 0.709392, acc.: 63.28%] [Generator loss: 1.351862]\n",
      "1861 [Discriminator loss: 0.545746, acc.: 74.22%] [Generator loss: 0.928909]\n",
      "1862 [Discriminator loss: 0.704539, acc.: 60.94%] [Generator loss: 0.255978]\n",
      "1863 [Discriminator loss: 0.536149, acc.: 71.09%] [Generator loss: 0.663557]\n",
      "1864 [Discriminator loss: 0.459637, acc.: 75.78%] [Generator loss: 1.046696]\n",
      "1865 [Discriminator loss: 0.510115, acc.: 77.34%] [Generator loss: 0.448362]\n",
      "1866 [Discriminator loss: 0.734381, acc.: 55.47%] [Generator loss: 0.709243]\n",
      "1867 [Discriminator loss: 0.281837, acc.: 92.97%] [Generator loss: 0.720559]\n",
      "1868 [Discriminator loss: 0.338751, acc.: 85.94%] [Generator loss: 0.221905]\n",
      "1869 [Discriminator loss: 0.387720, acc.: 80.47%] [Generator loss: 0.380940]\n",
      "1870 [Discriminator loss: 0.389068, acc.: 82.81%] [Generator loss: 1.399608]\n",
      "1871 [Discriminator loss: 1.579690, acc.: 20.31%] [Generator loss: 1.503097]\n",
      "1872 [Discriminator loss: 0.761728, acc.: 57.81%] [Generator loss: 2.013077]\n",
      "1873 [Discriminator loss: 1.044797, acc.: 41.41%] [Generator loss: 1.403553]\n",
      "1874 [Discriminator loss: 0.784792, acc.: 53.12%] [Generator loss: 1.824537]\n",
      "1875 [Discriminator loss: 0.870513, acc.: 47.66%] [Generator loss: 1.593225]\n",
      "1876 [Discriminator loss: 0.914217, acc.: 44.53%] [Generator loss: 1.395021]\n",
      "1877 [Discriminator loss: 1.616456, acc.: 19.53%] [Generator loss: 1.140533]\n",
      "1878 [Discriminator loss: 0.671679, acc.: 60.94%] [Generator loss: 1.539995]\n",
      "1879 [Discriminator loss: 1.157702, acc.: 39.06%] [Generator loss: 1.492433]\n",
      "1880 [Discriminator loss: 0.749979, acc.: 58.59%] [Generator loss: 1.694388]\n",
      "1881 [Discriminator loss: 0.557716, acc.: 73.44%] [Generator loss: 1.346871]\n",
      "1882 [Discriminator loss: 0.787369, acc.: 55.47%] [Generator loss: 1.103162]\n",
      "1883 [Discriminator loss: 0.686010, acc.: 57.03%] [Generator loss: 1.384656]\n",
      "1884 [Discriminator loss: 0.795858, acc.: 55.47%] [Generator loss: 1.442263]\n",
      "1885 [Discriminator loss: 0.697222, acc.: 64.06%] [Generator loss: 0.993156]\n",
      "1886 [Discriminator loss: 0.634997, acc.: 65.62%] [Generator loss: 0.955762]\n",
      "1887 [Discriminator loss: 0.551485, acc.: 70.31%] [Generator loss: 0.996303]\n",
      "1888 [Discriminator loss: 0.742798, acc.: 53.12%] [Generator loss: 1.265289]\n",
      "1889 [Discriminator loss: 0.555509, acc.: 68.75%] [Generator loss: 1.453374]\n",
      "1890 [Discriminator loss: 0.608176, acc.: 68.75%] [Generator loss: 0.925830]\n",
      "1891 [Discriminator loss: 0.796704, acc.: 50.00%] [Generator loss: 1.578116]\n",
      "1892 [Discriminator loss: 0.593395, acc.: 68.75%] [Generator loss: 1.332886]\n",
      "1893 [Discriminator loss: 0.981669, acc.: 35.16%] [Generator loss: 1.384080]\n",
      "1894 [Discriminator loss: 0.808897, acc.: 50.78%] [Generator loss: 1.867839]\n",
      "1895 [Discriminator loss: 0.642685, acc.: 65.62%] [Generator loss: 1.498016]\n",
      "1896 [Discriminator loss: 0.578245, acc.: 68.75%] [Generator loss: 1.204951]\n",
      "1897 [Discriminator loss: 0.333849, acc.: 86.72%] [Generator loss: 1.078975]\n",
      "1898 [Discriminator loss: 1.177078, acc.: 35.16%] [Generator loss: 0.992688]\n",
      "1899 [Discriminator loss: 0.302940, acc.: 90.62%] [Generator loss: 0.778452]\n",
      "1900 [Discriminator loss: 0.352297, acc.: 82.81%] [Generator loss: 0.623284]\n",
      "1901 [Discriminator loss: 0.613012, acc.: 65.62%] [Generator loss: 0.883866]\n",
      "1902 [Discriminator loss: 0.441865, acc.: 80.47%] [Generator loss: 0.932969]\n",
      "1903 [Discriminator loss: 0.670778, acc.: 64.84%] [Generator loss: 1.539633]\n",
      "1904 [Discriminator loss: 0.546136, acc.: 69.53%] [Generator loss: 1.601750]\n",
      "1905 [Discriminator loss: 0.634234, acc.: 67.19%] [Generator loss: 1.651315]\n",
      "1906 [Discriminator loss: 0.647172, acc.: 61.72%] [Generator loss: 2.026623]\n",
      "1907 [Discriminator loss: 0.697633, acc.: 63.28%] [Generator loss: 1.143822]\n",
      "1908 [Discriminator loss: 0.843360, acc.: 50.78%] [Generator loss: 0.859509]\n",
      "1909 [Discriminator loss: 0.404286, acc.: 83.59%] [Generator loss: 0.831649]\n",
      "1910 [Discriminator loss: 0.786986, acc.: 54.69%] [Generator loss: 1.163167]\n",
      "1911 [Discriminator loss: 0.581714, acc.: 67.97%] [Generator loss: 1.032374]\n",
      "1912 [Discriminator loss: 0.716416, acc.: 56.25%] [Generator loss: 1.517815]\n",
      "1913 [Discriminator loss: 0.585769, acc.: 70.31%] [Generator loss: 1.440901]\n",
      "1914 [Discriminator loss: 0.487329, acc.: 74.22%] [Generator loss: 0.953315]\n",
      "1915 [Discriminator loss: 0.496902, acc.: 80.47%] [Generator loss: 1.057824]\n",
      "1916 [Discriminator loss: 1.160597, acc.: 34.38%] [Generator loss: 0.988624]\n",
      "1917 [Discriminator loss: 0.266283, acc.: 95.31%] [Generator loss: 1.181975]\n",
      "1918 [Discriminator loss: 0.616305, acc.: 62.50%] [Generator loss: 1.191014]\n",
      "1919 [Discriminator loss: 0.774353, acc.: 55.47%] [Generator loss: 0.954103]\n",
      "1920 [Discriminator loss: 0.438069, acc.: 82.81%] [Generator loss: 1.693637]\n",
      "1921 [Discriminator loss: 0.484468, acc.: 78.91%] [Generator loss: 1.368398]\n",
      "1922 [Discriminator loss: 0.659828, acc.: 64.06%] [Generator loss: 1.552969]\n",
      "1923 [Discriminator loss: 0.778163, acc.: 58.59%] [Generator loss: 0.911086]\n",
      "1924 [Discriminator loss: 0.553707, acc.: 72.66%] [Generator loss: 0.941623]\n",
      "1925 [Discriminator loss: 0.530764, acc.: 72.66%] [Generator loss: 1.142891]\n",
      "1926 [Discriminator loss: 0.404903, acc.: 84.38%] [Generator loss: 0.836026]\n",
      "1927 [Discriminator loss: 0.714142, acc.: 56.25%] [Generator loss: 1.132888]\n",
      "1928 [Discriminator loss: 0.932223, acc.: 45.31%] [Generator loss: 1.268584]\n",
      "1929 [Discriminator loss: 0.542122, acc.: 68.75%] [Generator loss: 1.720725]\n",
      "1930 [Discriminator loss: 0.872372, acc.: 54.69%] [Generator loss: 1.116863]\n",
      "1931 [Discriminator loss: 0.759105, acc.: 51.56%] [Generator loss: 1.989364]\n",
      "1932 [Discriminator loss: 1.215382, acc.: 38.28%] [Generator loss: 2.326643]\n",
      "1933 [Discriminator loss: 0.648970, acc.: 64.84%] [Generator loss: 2.420360]\n",
      "1934 [Discriminator loss: 0.702844, acc.: 60.16%] [Generator loss: 2.072298]\n",
      "1935 [Discriminator loss: 1.091943, acc.: 36.72%] [Generator loss: 1.743508]\n",
      "1936 [Discriminator loss: 0.629799, acc.: 62.50%] [Generator loss: 1.615375]\n",
      "1937 [Discriminator loss: 0.731298, acc.: 61.72%] [Generator loss: 1.653576]\n",
      "1938 [Discriminator loss: 0.900868, acc.: 52.34%] [Generator loss: 1.454467]\n",
      "1939 [Discriminator loss: 0.779417, acc.: 57.81%] [Generator loss: 1.011374]\n",
      "1940 [Discriminator loss: 0.682399, acc.: 64.84%] [Generator loss: 1.881616]\n",
      "1941 [Discriminator loss: 0.632432, acc.: 62.50%] [Generator loss: 1.736629]\n",
      "1942 [Discriminator loss: 0.446042, acc.: 78.91%] [Generator loss: 1.691386]\n",
      "1943 [Discriminator loss: 0.852935, acc.: 48.44%] [Generator loss: 1.461094]\n",
      "1944 [Discriminator loss: 0.740749, acc.: 60.16%] [Generator loss: 1.958584]\n",
      "1945 [Discriminator loss: 0.628363, acc.: 70.31%] [Generator loss: 0.892478]\n",
      "1946 [Discriminator loss: 0.618025, acc.: 67.19%] [Generator loss: 0.847093]\n",
      "1947 [Discriminator loss: 0.327830, acc.: 87.50%] [Generator loss: 0.665069]\n",
      "1948 [Discriminator loss: 0.384186, acc.: 84.38%] [Generator loss: 1.392771]\n",
      "1949 [Discriminator loss: 0.591470, acc.: 67.97%] [Generator loss: 1.508985]\n",
      "1950 [Discriminator loss: 0.566424, acc.: 72.66%] [Generator loss: 1.554133]\n",
      "1951 [Discriminator loss: 0.578703, acc.: 72.66%] [Generator loss: 1.766574]\n",
      "1952 [Discriminator loss: 0.846307, acc.: 49.22%] [Generator loss: 2.258301]\n",
      "1953 [Discriminator loss: 0.842643, acc.: 56.25%] [Generator loss: 1.882412]\n",
      "1954 [Discriminator loss: 0.595610, acc.: 67.19%] [Generator loss: 1.435751]\n",
      "1955 [Discriminator loss: 0.869764, acc.: 52.34%] [Generator loss: 1.857749]\n",
      "1956 [Discriminator loss: 0.700122, acc.: 64.84%] [Generator loss: 1.703367]\n",
      "1957 [Discriminator loss: 1.087279, acc.: 42.19%] [Generator loss: 1.716931]\n",
      "1958 [Discriminator loss: 0.428194, acc.: 82.03%] [Generator loss: 2.048791]\n",
      "1959 [Discriminator loss: 0.591669, acc.: 71.09%] [Generator loss: 1.777418]\n",
      "1960 [Discriminator loss: 0.982819, acc.: 42.97%] [Generator loss: 1.522000]\n",
      "1961 [Discriminator loss: 0.966491, acc.: 44.53%] [Generator loss: 1.418166]\n",
      "1962 [Discriminator loss: 0.657822, acc.: 63.28%] [Generator loss: 0.771447]\n",
      "1963 [Discriminator loss: 0.903178, acc.: 48.44%] [Generator loss: 1.515274]\n",
      "1964 [Discriminator loss: 1.075654, acc.: 35.16%] [Generator loss: 1.612743]\n",
      "1965 [Discriminator loss: 1.043912, acc.: 42.97%] [Generator loss: 1.194403]\n",
      "1966 [Discriminator loss: 1.119023, acc.: 34.38%] [Generator loss: 1.181686]\n",
      "1967 [Discriminator loss: 0.747792, acc.: 60.16%] [Generator loss: 1.577476]\n",
      "1968 [Discriminator loss: 0.897856, acc.: 46.09%] [Generator loss: 1.442360]\n",
      "1969 [Discriminator loss: 0.755960, acc.: 55.47%] [Generator loss: 1.550858]\n",
      "1970 [Discriminator loss: 0.654240, acc.: 60.16%] [Generator loss: 1.310994]\n",
      "1971 [Discriminator loss: 1.081071, acc.: 36.72%] [Generator loss: 1.092247]\n",
      "1972 [Discriminator loss: 1.093924, acc.: 30.47%] [Generator loss: 1.412077]\n",
      "1973 [Discriminator loss: 0.823055, acc.: 48.44%] [Generator loss: 1.301551]\n",
      "1974 [Discriminator loss: 0.772820, acc.: 55.47%] [Generator loss: 1.361968]\n",
      "1975 [Discriminator loss: 0.777781, acc.: 60.16%] [Generator loss: 1.279999]\n",
      "1976 [Discriminator loss: 0.891526, acc.: 56.25%] [Generator loss: 0.926169]\n",
      "1977 [Discriminator loss: 0.470712, acc.: 78.12%] [Generator loss: 0.872257]\n",
      "1978 [Discriminator loss: 0.494671, acc.: 75.78%] [Generator loss: 1.279646]\n",
      "1979 [Discriminator loss: 1.021758, acc.: 39.06%] [Generator loss: 1.018677]\n",
      "1980 [Discriminator loss: 0.735939, acc.: 58.59%] [Generator loss: 0.990445]\n",
      "1981 [Discriminator loss: 0.630706, acc.: 71.88%] [Generator loss: 0.887749]\n",
      "1982 [Discriminator loss: 0.584947, acc.: 67.19%] [Generator loss: 0.818978]\n",
      "1983 [Discriminator loss: 0.545307, acc.: 72.66%] [Generator loss: 2.144055]\n",
      "1984 [Discriminator loss: 1.389603, acc.: 32.81%] [Generator loss: 1.785428]\n",
      "1985 [Discriminator loss: 0.696876, acc.: 64.06%] [Generator loss: 1.543672]\n",
      "1986 [Discriminator loss: 1.280995, acc.: 30.47%] [Generator loss: 1.162326]\n",
      "1987 [Discriminator loss: 0.530641, acc.: 68.75%] [Generator loss: 1.247153]\n",
      "1988 [Discriminator loss: 0.933947, acc.: 43.75%] [Generator loss: 0.913497]\n",
      "1989 [Discriminator loss: 0.699289, acc.: 57.03%] [Generator loss: 1.237103]\n",
      "1990 [Discriminator loss: 0.875848, acc.: 53.12%] [Generator loss: 1.312656]\n",
      "1991 [Discriminator loss: 0.617538, acc.: 71.09%] [Generator loss: 1.048854]\n",
      "1992 [Discriminator loss: 0.739152, acc.: 55.47%] [Generator loss: 1.023046]\n",
      "1993 [Discriminator loss: 0.679258, acc.: 63.28%] [Generator loss: 1.207361]\n",
      "1994 [Discriminator loss: 0.950957, acc.: 44.53%] [Generator loss: 1.334713]\n",
      "1995 [Discriminator loss: 1.015708, acc.: 44.53%] [Generator loss: 1.038596]\n",
      "1996 [Discriminator loss: 0.703650, acc.: 60.94%] [Generator loss: 1.128698]\n",
      "1997 [Discriminator loss: 1.025192, acc.: 36.72%] [Generator loss: 1.325223]\n",
      "1998 [Discriminator loss: 0.602395, acc.: 68.75%] [Generator loss: 1.087391]\n",
      "1999 [Discriminator loss: 0.504875, acc.: 75.00%] [Generator loss: 1.074629]\n",
      "2000 [Discriminator loss: 0.751973, acc.: 53.91%] [Generator loss: 1.173983]\n",
      "2001 [Discriminator loss: 0.960064, acc.: 42.97%] [Generator loss: 1.674023]\n",
      "2002 [Discriminator loss: 0.758283, acc.: 57.03%] [Generator loss: 1.884679]\n",
      "2003 [Discriminator loss: 0.721013, acc.: 63.28%] [Generator loss: 0.771941]\n",
      "2004 [Discriminator loss: 0.724937, acc.: 57.81%] [Generator loss: 1.024948]\n",
      "2005 [Discriminator loss: 0.586014, acc.: 69.53%] [Generator loss: 1.631767]\n",
      "2006 [Discriminator loss: 0.860788, acc.: 47.66%] [Generator loss: 1.325136]\n",
      "2007 [Discriminator loss: 0.655930, acc.: 64.84%] [Generator loss: 1.637827]\n",
      "2008 [Discriminator loss: 0.728693, acc.: 53.91%] [Generator loss: 2.003410]\n",
      "2009 [Discriminator loss: 0.925499, acc.: 44.53%] [Generator loss: 1.365130]\n",
      "2010 [Discriminator loss: 0.696552, acc.: 60.16%] [Generator loss: 0.952296]\n",
      "2011 [Discriminator loss: 0.544428, acc.: 72.66%] [Generator loss: 1.025342]\n",
      "2012 [Discriminator loss: 0.496197, acc.: 76.56%] [Generator loss: 1.054445]\n",
      "2013 [Discriminator loss: 0.768480, acc.: 57.03%] [Generator loss: 0.517896]\n",
      "2014 [Discriminator loss: 0.826064, acc.: 53.91%] [Generator loss: 1.068129]\n",
      "2015 [Discriminator loss: 0.259563, acc.: 94.53%] [Generator loss: 1.211576]\n",
      "2016 [Discriminator loss: 1.009887, acc.: 45.31%] [Generator loss: 1.566432]\n",
      "2017 [Discriminator loss: 1.062434, acc.: 42.19%] [Generator loss: 1.318818]\n",
      "2018 [Discriminator loss: 1.017278, acc.: 36.72%] [Generator loss: 1.281527]\n",
      "2019 [Discriminator loss: 0.654211, acc.: 64.84%] [Generator loss: 1.724844]\n",
      "2020 [Discriminator loss: 0.768022, acc.: 57.81%] [Generator loss: 1.277935]\n",
      "2021 [Discriminator loss: 0.814304, acc.: 53.12%] [Generator loss: 1.320029]\n",
      "2022 [Discriminator loss: 0.634945, acc.: 64.06%] [Generator loss: 1.437722]\n",
      "2023 [Discriminator loss: 0.698239, acc.: 65.62%] [Generator loss: 1.116513]\n",
      "2024 [Discriminator loss: 0.517953, acc.: 71.88%] [Generator loss: 1.117834]\n",
      "2025 [Discriminator loss: 0.625036, acc.: 67.19%] [Generator loss: 1.195719]\n",
      "2026 [Discriminator loss: 0.694619, acc.: 65.62%] [Generator loss: 0.993325]\n",
      "2027 [Discriminator loss: 0.949907, acc.: 45.31%] [Generator loss: 1.013865]\n",
      "2028 [Discriminator loss: 0.818153, acc.: 57.81%] [Generator loss: 1.058575]\n",
      "2029 [Discriminator loss: 0.434470, acc.: 79.69%] [Generator loss: 1.020152]\n",
      "2030 [Discriminator loss: 0.760718, acc.: 55.47%] [Generator loss: 0.959469]\n",
      "2031 [Discriminator loss: 1.126876, acc.: 28.12%] [Generator loss: 1.238400]\n",
      "2032 [Discriminator loss: 0.869667, acc.: 46.09%] [Generator loss: 1.264927]\n",
      "2033 [Discriminator loss: 0.924269, acc.: 40.62%] [Generator loss: 0.955876]\n",
      "2034 [Discriminator loss: 0.856070, acc.: 42.19%] [Generator loss: 1.514136]\n",
      "2035 [Discriminator loss: 0.581602, acc.: 67.19%] [Generator loss: 1.224137]\n",
      "2036 [Discriminator loss: 0.568065, acc.: 70.31%] [Generator loss: 1.057831]\n",
      "2037 [Discriminator loss: 0.993881, acc.: 42.19%] [Generator loss: 1.561073]\n",
      "2038 [Discriminator loss: 1.058518, acc.: 38.28%] [Generator loss: 1.617919]\n",
      "2039 [Discriminator loss: 1.007570, acc.: 41.41%] [Generator loss: 1.347593]\n",
      "2040 [Discriminator loss: 0.763254, acc.: 55.47%] [Generator loss: 1.321028]\n",
      "2041 [Discriminator loss: 0.623546, acc.: 64.06%] [Generator loss: 1.067829]\n",
      "2042 [Discriminator loss: 0.400835, acc.: 87.50%] [Generator loss: 0.920168]\n",
      "2043 [Discriminator loss: 0.425068, acc.: 82.03%] [Generator loss: 1.120872]\n",
      "2044 [Discriminator loss: 0.628733, acc.: 63.28%] [Generator loss: 1.321518]\n",
      "2045 [Discriminator loss: 0.599567, acc.: 68.75%] [Generator loss: 1.263090]\n",
      "2046 [Discriminator loss: 0.330750, acc.: 85.94%] [Generator loss: 0.728998]\n",
      "2047 [Discriminator loss: 0.733789, acc.: 57.81%] [Generator loss: 1.313771]\n",
      "2048 [Discriminator loss: 0.691280, acc.: 67.97%] [Generator loss: 1.122224]\n",
      "2049 [Discriminator loss: 0.345706, acc.: 86.72%] [Generator loss: 0.469114]\n",
      "2050 [Discriminator loss: 0.482423, acc.: 77.34%] [Generator loss: 0.531390]\n",
      "2051 [Discriminator loss: 0.425413, acc.: 84.38%] [Generator loss: 0.576926]\n",
      "2052 [Discriminator loss: 0.298351, acc.: 89.06%] [Generator loss: 0.403829]\n",
      "2053 [Discriminator loss: 0.532510, acc.: 71.88%] [Generator loss: 0.420068]\n",
      "2054 [Discriminator loss: 0.986776, acc.: 39.84%] [Generator loss: 1.453785]\n",
      "2055 [Discriminator loss: 0.485994, acc.: 77.34%] [Generator loss: 3.335570]\n",
      "2056 [Discriminator loss: 1.808032, acc.: 28.12%] [Generator loss: 1.151727]\n",
      "2057 [Discriminator loss: 0.782754, acc.: 57.03%] [Generator loss: 1.550251]\n",
      "2058 [Discriminator loss: 0.340544, acc.: 86.72%] [Generator loss: 1.019844]\n",
      "2059 [Discriminator loss: 0.567652, acc.: 71.09%] [Generator loss: 0.925116]\n",
      "2060 [Discriminator loss: 0.325366, acc.: 87.50%] [Generator loss: 0.598496]\n",
      "2061 [Discriminator loss: 0.786025, acc.: 56.25%] [Generator loss: 0.917439]\n",
      "2062 [Discriminator loss: 0.354684, acc.: 82.81%] [Generator loss: 0.568246]\n",
      "2063 [Discriminator loss: 0.472287, acc.: 78.12%] [Generator loss: 0.527158]\n",
      "2064 [Discriminator loss: 1.051973, acc.: 46.88%] [Generator loss: 1.351362]\n",
      "2065 [Discriminator loss: 0.511812, acc.: 73.44%] [Generator loss: 1.667642]\n",
      "2066 [Discriminator loss: 1.193045, acc.: 28.12%] [Generator loss: 1.514013]\n",
      "2067 [Discriminator loss: 1.039662, acc.: 35.94%] [Generator loss: 1.521497]\n",
      "2068 [Discriminator loss: 0.615032, acc.: 67.97%] [Generator loss: 1.142817]\n",
      "2069 [Discriminator loss: 0.785868, acc.: 57.81%] [Generator loss: 1.987899]\n",
      "2070 [Discriminator loss: 0.326292, acc.: 85.94%] [Generator loss: 1.303450]\n",
      "2071 [Discriminator loss: 0.468474, acc.: 77.34%] [Generator loss: 0.746133]\n",
      "2072 [Discriminator loss: 0.367881, acc.: 82.03%] [Generator loss: 0.585744]\n",
      "2073 [Discriminator loss: 1.054052, acc.: 42.97%] [Generator loss: 1.372343]\n",
      "2074 [Discriminator loss: 0.498345, acc.: 79.69%] [Generator loss: 2.027989]\n",
      "2075 [Discriminator loss: 0.661237, acc.: 67.19%] [Generator loss: 0.970993]\n",
      "2076 [Discriminator loss: 0.787050, acc.: 56.25%] [Generator loss: 1.549316]\n",
      "2077 [Discriminator loss: 0.599831, acc.: 67.97%] [Generator loss: 1.259264]\n",
      "2078 [Discriminator loss: 0.823203, acc.: 54.69%] [Generator loss: 1.397241]\n",
      "2079 [Discriminator loss: 0.484053, acc.: 79.69%] [Generator loss: 1.028041]\n",
      "2080 [Discriminator loss: 1.235791, acc.: 28.12%] [Generator loss: 0.803582]\n",
      "2081 [Discriminator loss: 0.532135, acc.: 72.66%] [Generator loss: 0.876453]\n",
      "2082 [Discriminator loss: 0.757886, acc.: 57.81%] [Generator loss: 0.956842]\n",
      "2083 [Discriminator loss: 0.452757, acc.: 81.25%] [Generator loss: 1.335840]\n",
      "2084 [Discriminator loss: 0.965904, acc.: 44.53%] [Generator loss: 1.920217]\n",
      "2085 [Discriminator loss: 1.291210, acc.: 25.78%] [Generator loss: 1.437506]\n",
      "2086 [Discriminator loss: 0.529044, acc.: 76.56%] [Generator loss: 0.925659]\n",
      "2087 [Discriminator loss: 0.482490, acc.: 78.91%] [Generator loss: 0.806646]\n",
      "2088 [Discriminator loss: 1.094011, acc.: 41.41%] [Generator loss: 1.530403]\n",
      "2089 [Discriminator loss: 0.735961, acc.: 62.50%] [Generator loss: 1.045140]\n",
      "2090 [Discriminator loss: 0.989106, acc.: 40.62%] [Generator loss: 0.845222]\n",
      "2091 [Discriminator loss: 0.618230, acc.: 67.97%] [Generator loss: 1.667819]\n",
      "2092 [Discriminator loss: 0.802726, acc.: 52.34%] [Generator loss: 1.694240]\n",
      "2093 [Discriminator loss: 0.860840, acc.: 49.22%] [Generator loss: 2.275857]\n",
      "2094 [Discriminator loss: 1.050800, acc.: 42.19%] [Generator loss: 0.921311]\n",
      "2095 [Discriminator loss: 0.656076, acc.: 65.62%] [Generator loss: 1.350389]\n",
      "2096 [Discriminator loss: 0.602097, acc.: 68.75%] [Generator loss: 1.804031]\n",
      "2097 [Discriminator loss: 0.729068, acc.: 59.38%] [Generator loss: 1.465232]\n",
      "2098 [Discriminator loss: 1.013912, acc.: 42.19%] [Generator loss: 1.726025]\n",
      "2099 [Discriminator loss: 0.860901, acc.: 46.88%] [Generator loss: 1.123572]\n",
      "2100 [Discriminator loss: 0.607101, acc.: 71.09%] [Generator loss: 1.039775]\n",
      "2101 [Discriminator loss: 0.845497, acc.: 50.00%] [Generator loss: 1.540529]\n",
      "2102 [Discriminator loss: 0.696921, acc.: 62.50%] [Generator loss: 1.260319]\n",
      "2103 [Discriminator loss: 0.572526, acc.: 70.31%] [Generator loss: 0.971445]\n",
      "2104 [Discriminator loss: 1.018566, acc.: 40.62%] [Generator loss: 1.597510]\n",
      "2105 [Discriminator loss: 0.949091, acc.: 39.84%] [Generator loss: 2.148082]\n",
      "2106 [Discriminator loss: 0.958233, acc.: 41.41%] [Generator loss: 1.644075]\n",
      "2107 [Discriminator loss: 0.985159, acc.: 46.09%] [Generator loss: 1.680341]\n",
      "2108 [Discriminator loss: 0.687533, acc.: 59.38%] [Generator loss: 1.831859]\n",
      "2109 [Discriminator loss: 0.428048, acc.: 76.56%] [Generator loss: 0.729038]\n",
      "2110 [Discriminator loss: 0.579297, acc.: 71.88%] [Generator loss: 0.470472]\n",
      "2111 [Discriminator loss: 0.142503, acc.: 96.09%] [Generator loss: 0.520891]\n",
      "2112 [Discriminator loss: 0.497163, acc.: 76.56%] [Generator loss: 0.460593]\n",
      "2113 [Discriminator loss: 0.769037, acc.: 57.81%] [Generator loss: 1.471699]\n",
      "2114 [Discriminator loss: 0.367048, acc.: 81.25%] [Generator loss: 1.441762]\n",
      "2115 [Discriminator loss: 0.848673, acc.: 54.69%] [Generator loss: 1.246243]\n",
      "2116 [Discriminator loss: 0.312852, acc.: 89.06%] [Generator loss: 0.718757]\n",
      "2117 [Discriminator loss: 1.370319, acc.: 35.94%] [Generator loss: 1.753442]\n",
      "2118 [Discriminator loss: 0.594809, acc.: 69.53%] [Generator loss: 1.286665]\n",
      "2119 [Discriminator loss: 0.801256, acc.: 58.59%] [Generator loss: 1.260171]\n",
      "2120 [Discriminator loss: 0.743810, acc.: 64.06%] [Generator loss: 1.691092]\n",
      "2121 [Discriminator loss: 0.530310, acc.: 75.78%] [Generator loss: 1.253398]\n",
      "2122 [Discriminator loss: 0.665803, acc.: 62.50%] [Generator loss: 1.057607]\n",
      "2123 [Discriminator loss: 0.696543, acc.: 60.16%] [Generator loss: 1.142110]\n",
      "2124 [Discriminator loss: 0.814787, acc.: 56.25%] [Generator loss: 1.473301]\n",
      "2125 [Discriminator loss: 0.527848, acc.: 77.34%] [Generator loss: 1.171683]\n",
      "2126 [Discriminator loss: 0.999956, acc.: 39.84%] [Generator loss: 1.256805]\n",
      "2127 [Discriminator loss: 0.617098, acc.: 60.94%] [Generator loss: 1.706066]\n",
      "2128 [Discriminator loss: 0.779842, acc.: 61.72%] [Generator loss: 0.837907]\n",
      "2129 [Discriminator loss: 0.716335, acc.: 56.25%] [Generator loss: 1.075574]\n",
      "2130 [Discriminator loss: 0.567492, acc.: 71.88%] [Generator loss: 1.634464]\n",
      "2131 [Discriminator loss: 0.762667, acc.: 56.25%] [Generator loss: 1.211261]\n",
      "2132 [Discriminator loss: 0.813914, acc.: 48.44%] [Generator loss: 1.106647]\n",
      "2133 [Discriminator loss: 0.362485, acc.: 84.38%] [Generator loss: 0.736264]\n",
      "2134 [Discriminator loss: 0.846135, acc.: 51.56%] [Generator loss: 1.060136]\n",
      "2135 [Discriminator loss: 0.608251, acc.: 71.09%] [Generator loss: 1.183162]\n",
      "2136 [Discriminator loss: 0.828930, acc.: 51.56%] [Generator loss: 1.146603]\n",
      "2137 [Discriminator loss: 0.703555, acc.: 57.03%] [Generator loss: 1.733149]\n",
      "2138 [Discriminator loss: 0.731210, acc.: 55.47%] [Generator loss: 1.718998]\n",
      "2139 [Discriminator loss: 0.971568, acc.: 39.06%] [Generator loss: 0.998215]\n",
      "2140 [Discriminator loss: 1.002123, acc.: 36.72%] [Generator loss: 1.351923]\n",
      "2141 [Discriminator loss: 0.770584, acc.: 55.47%] [Generator loss: 1.712453]\n",
      "2142 [Discriminator loss: 0.846155, acc.: 46.88%] [Generator loss: 1.562200]\n",
      "2143 [Discriminator loss: 0.759680, acc.: 54.69%] [Generator loss: 1.642301]\n",
      "2144 [Discriminator loss: 0.774631, acc.: 50.78%] [Generator loss: 1.253268]\n",
      "2145 [Discriminator loss: 1.259163, acc.: 25.00%] [Generator loss: 1.264233]\n",
      "2146 [Discriminator loss: 0.709388, acc.: 64.06%] [Generator loss: 1.575707]\n",
      "2147 [Discriminator loss: 0.764179, acc.: 55.47%] [Generator loss: 1.168077]\n",
      "2148 [Discriminator loss: 0.923307, acc.: 47.66%] [Generator loss: 1.308637]\n",
      "2149 [Discriminator loss: 0.781598, acc.: 48.44%] [Generator loss: 1.248514]\n",
      "2150 [Discriminator loss: 0.561374, acc.: 67.97%] [Generator loss: 0.647779]\n",
      "2151 [Discriminator loss: 0.847256, acc.: 54.69%] [Generator loss: 1.113914]\n",
      "2152 [Discriminator loss: 0.377481, acc.: 82.81%] [Generator loss: 1.017117]\n",
      "2153 [Discriminator loss: 0.660606, acc.: 62.50%] [Generator loss: 0.733482]\n",
      "2154 [Discriminator loss: 0.690354, acc.: 59.38%] [Generator loss: 0.780599]\n",
      "2155 [Discriminator loss: 0.744879, acc.: 57.03%] [Generator loss: 0.668150]\n",
      "2156 [Discriminator loss: 0.837308, acc.: 50.78%] [Generator loss: 0.928746]\n",
      "2157 [Discriminator loss: 0.576599, acc.: 76.56%] [Generator loss: 1.306933]\n",
      "2158 [Discriminator loss: 1.506250, acc.: 18.75%] [Generator loss: 0.761185]\n",
      "2159 [Discriminator loss: 0.670427, acc.: 60.16%] [Generator loss: 1.530730]\n",
      "2160 [Discriminator loss: 0.684454, acc.: 63.28%] [Generator loss: 1.578752]\n",
      "2161 [Discriminator loss: 0.959766, acc.: 44.53%] [Generator loss: 1.192325]\n",
      "2162 [Discriminator loss: 0.844521, acc.: 53.12%] [Generator loss: 1.474349]\n",
      "2163 [Discriminator loss: 0.740948, acc.: 53.12%] [Generator loss: 1.538769]\n",
      "2164 [Discriminator loss: 1.136614, acc.: 39.84%] [Generator loss: 1.112406]\n",
      "2165 [Discriminator loss: 1.436908, acc.: 14.06%] [Generator loss: 1.043298]\n",
      "2166 [Discriminator loss: 0.647199, acc.: 64.06%] [Generator loss: 1.194090]\n",
      "2167 [Discriminator loss: 0.746831, acc.: 60.94%] [Generator loss: 1.212631]\n",
      "2168 [Discriminator loss: 0.673486, acc.: 62.50%] [Generator loss: 1.389083]\n",
      "2169 [Discriminator loss: 0.876061, acc.: 42.19%] [Generator loss: 1.034310]\n",
      "2170 [Discriminator loss: 0.723091, acc.: 57.03%] [Generator loss: 0.948196]\n",
      "2171 [Discriminator loss: 0.835232, acc.: 48.44%] [Generator loss: 1.069424]\n",
      "2172 [Discriminator loss: 0.959976, acc.: 46.09%] [Generator loss: 0.758287]\n",
      "2173 [Discriminator loss: 0.715994, acc.: 59.38%] [Generator loss: 0.870387]\n",
      "2174 [Discriminator loss: 0.796310, acc.: 55.47%] [Generator loss: 1.465858]\n",
      "2175 [Discriminator loss: 0.876017, acc.: 44.53%] [Generator loss: 1.600224]\n",
      "2176 [Discriminator loss: 0.767479, acc.: 54.69%] [Generator loss: 1.515441]\n",
      "2177 [Discriminator loss: 0.862990, acc.: 52.34%] [Generator loss: 1.339482]\n",
      "2178 [Discriminator loss: 1.141534, acc.: 32.03%] [Generator loss: 1.190952]\n",
      "2179 [Discriminator loss: 0.860416, acc.: 42.19%] [Generator loss: 1.655928]\n",
      "2180 [Discriminator loss: 0.861896, acc.: 41.41%] [Generator loss: 1.302627]\n",
      "2181 [Discriminator loss: 0.799456, acc.: 46.88%] [Generator loss: 1.368538]\n",
      "2182 [Discriminator loss: 0.945295, acc.: 39.06%] [Generator loss: 1.578357]\n",
      "2183 [Discriminator loss: 0.804255, acc.: 53.12%] [Generator loss: 1.456492]\n",
      "2184 [Discriminator loss: 0.750694, acc.: 57.81%] [Generator loss: 1.415428]\n",
      "2185 [Discriminator loss: 0.804609, acc.: 55.47%] [Generator loss: 1.279527]\n",
      "2186 [Discriminator loss: 0.964989, acc.: 40.62%] [Generator loss: 0.918058]\n",
      "2187 [Discriminator loss: 0.663201, acc.: 60.16%] [Generator loss: 1.130766]\n",
      "2188 [Discriminator loss: 0.732864, acc.: 52.34%] [Generator loss: 1.211235]\n",
      "2189 [Discriminator loss: 0.980461, acc.: 37.50%] [Generator loss: 0.984438]\n",
      "2190 [Discriminator loss: 0.872839, acc.: 39.06%] [Generator loss: 0.958164]\n",
      "2191 [Discriminator loss: 0.789728, acc.: 43.75%] [Generator loss: 1.267674]\n",
      "2192 [Discriminator loss: 1.090505, acc.: 29.69%] [Generator loss: 1.230749]\n",
      "2193 [Discriminator loss: 0.701225, acc.: 55.47%] [Generator loss: 1.394223]\n",
      "2194 [Discriminator loss: 0.948117, acc.: 39.84%] [Generator loss: 1.549716]\n",
      "2195 [Discriminator loss: 0.793436, acc.: 50.00%] [Generator loss: 1.361489]\n",
      "2196 [Discriminator loss: 0.755283, acc.: 59.38%] [Generator loss: 1.054904]\n",
      "2197 [Discriminator loss: 0.868400, acc.: 47.66%] [Generator loss: 1.364687]\n",
      "2198 [Discriminator loss: 0.972498, acc.: 37.50%] [Generator loss: 1.144918]\n",
      "2199 [Discriminator loss: 0.709715, acc.: 60.94%] [Generator loss: 1.291427]\n",
      "2200 [Discriminator loss: 0.826699, acc.: 43.75%] [Generator loss: 1.260297]\n",
      "2201 [Discriminator loss: 0.742913, acc.: 55.47%] [Generator loss: 0.795223]\n",
      "2202 [Discriminator loss: 0.833274, acc.: 45.31%] [Generator loss: 1.251559]\n",
      "2203 [Discriminator loss: 1.165204, acc.: 32.81%] [Generator loss: 1.476925]\n",
      "2204 [Discriminator loss: 0.800266, acc.: 53.12%] [Generator loss: 1.345271]\n",
      "2205 [Discriminator loss: 0.714778, acc.: 53.91%] [Generator loss: 1.260497]\n",
      "2206 [Discriminator loss: 0.981389, acc.: 33.59%] [Generator loss: 1.244500]\n",
      "2207 [Discriminator loss: 0.634219, acc.: 61.72%] [Generator loss: 1.693046]\n",
      "2208 [Discriminator loss: 0.867348, acc.: 53.91%] [Generator loss: 1.497183]\n",
      "2209 [Discriminator loss: 0.761533, acc.: 57.03%] [Generator loss: 1.509686]\n",
      "2210 [Discriminator loss: 0.856503, acc.: 49.22%] [Generator loss: 1.142942]\n",
      "2211 [Discriminator loss: 0.983468, acc.: 36.72%] [Generator loss: 1.280846]\n",
      "2212 [Discriminator loss: 1.017206, acc.: 35.16%] [Generator loss: 1.123611]\n",
      "2213 [Discriminator loss: 1.156107, acc.: 21.09%] [Generator loss: 0.899245]\n",
      "2214 [Discriminator loss: 0.819842, acc.: 48.44%] [Generator loss: 1.051807]\n",
      "2215 [Discriminator loss: 0.799890, acc.: 46.88%] [Generator loss: 1.291283]\n",
      "2216 [Discriminator loss: 0.685115, acc.: 62.50%] [Generator loss: 1.438806]\n",
      "2217 [Discriminator loss: 0.910837, acc.: 42.19%] [Generator loss: 1.117646]\n",
      "2218 [Discriminator loss: 0.814558, acc.: 49.22%] [Generator loss: 1.310546]\n",
      "2219 [Discriminator loss: 0.588721, acc.: 69.53%] [Generator loss: 0.943984]\n",
      "2220 [Discriminator loss: 0.671676, acc.: 61.72%] [Generator loss: 0.886190]\n",
      "2221 [Discriminator loss: 0.926706, acc.: 47.66%] [Generator loss: 1.242293]\n",
      "2222 [Discriminator loss: 0.718815, acc.: 58.59%] [Generator loss: 1.208228]\n",
      "2223 [Discriminator loss: 0.785228, acc.: 46.88%] [Generator loss: 1.473740]\n",
      "2224 [Discriminator loss: 0.864698, acc.: 46.09%] [Generator loss: 1.359768]\n",
      "2225 [Discriminator loss: 1.076624, acc.: 32.03%] [Generator loss: 1.107206]\n",
      "2226 [Discriminator loss: 0.805143, acc.: 44.53%] [Generator loss: 1.566905]\n",
      "2227 [Discriminator loss: 0.889108, acc.: 47.66%] [Generator loss: 1.464061]\n",
      "2228 [Discriminator loss: 1.438630, acc.: 18.75%] [Generator loss: 1.270647]\n",
      "2229 [Discriminator loss: 0.816005, acc.: 49.22%] [Generator loss: 1.528020]\n",
      "2230 [Discriminator loss: 0.800409, acc.: 50.78%] [Generator loss: 1.473891]\n",
      "2231 [Discriminator loss: 1.109696, acc.: 32.81%] [Generator loss: 1.094275]\n",
      "2232 [Discriminator loss: 0.782130, acc.: 53.91%] [Generator loss: 1.112386]\n",
      "2233 [Discriminator loss: 0.890202, acc.: 45.31%] [Generator loss: 1.153532]\n",
      "2234 [Discriminator loss: 0.743769, acc.: 54.69%] [Generator loss: 1.059738]\n",
      "2235 [Discriminator loss: 0.937995, acc.: 39.06%] [Generator loss: 1.108455]\n",
      "2236 [Discriminator loss: 0.554783, acc.: 70.31%] [Generator loss: 1.079743]\n",
      "2237 [Discriminator loss: 0.581388, acc.: 65.62%] [Generator loss: 0.978916]\n",
      "2238 [Discriminator loss: 0.699827, acc.: 57.03%] [Generator loss: 1.274754]\n",
      "2239 [Discriminator loss: 0.758605, acc.: 58.59%] [Generator loss: 1.206209]\n",
      "2240 [Discriminator loss: 0.733923, acc.: 58.59%] [Generator loss: 1.126621]\n",
      "2241 [Discriminator loss: 0.781712, acc.: 51.56%] [Generator loss: 1.030003]\n",
      "2242 [Discriminator loss: 0.865479, acc.: 46.88%] [Generator loss: 1.225159]\n",
      "2243 [Discriminator loss: 0.820458, acc.: 50.00%] [Generator loss: 1.215835]\n",
      "2244 [Discriminator loss: 0.844464, acc.: 50.78%] [Generator loss: 0.917154]\n",
      "2245 [Discriminator loss: 1.009119, acc.: 32.03%] [Generator loss: 0.890748]\n",
      "2246 [Discriminator loss: 0.679294, acc.: 61.72%] [Generator loss: 1.181601]\n",
      "2247 [Discriminator loss: 0.618776, acc.: 67.97%] [Generator loss: 1.105624]\n",
      "2248 [Discriminator loss: 0.925012, acc.: 42.97%] [Generator loss: 0.803681]\n",
      "2249 [Discriminator loss: 0.823100, acc.: 49.22%] [Generator loss: 1.053863]\n",
      "2250 [Discriminator loss: 0.649287, acc.: 60.16%] [Generator loss: 1.448856]\n",
      "2251 [Discriminator loss: 0.857464, acc.: 42.19%] [Generator loss: 1.132213]\n",
      "2252 [Discriminator loss: 0.745515, acc.: 57.03%] [Generator loss: 1.389852]\n",
      "2253 [Discriminator loss: 0.922100, acc.: 41.41%] [Generator loss: 1.126137]\n",
      "2254 [Discriminator loss: 0.670410, acc.: 61.72%] [Generator loss: 1.219346]\n",
      "2255 [Discriminator loss: 0.604356, acc.: 68.75%] [Generator loss: 1.191429]\n",
      "2256 [Discriminator loss: 0.649335, acc.: 60.94%] [Generator loss: 1.133661]\n",
      "2257 [Discriminator loss: 0.623705, acc.: 65.62%] [Generator loss: 1.172680]\n",
      "2258 [Discriminator loss: 0.797503, acc.: 51.56%] [Generator loss: 1.062693]\n",
      "2259 [Discriminator loss: 0.987558, acc.: 39.84%] [Generator loss: 1.321565]\n",
      "2260 [Discriminator loss: 0.777612, acc.: 57.03%] [Generator loss: 1.254082]\n",
      "2261 [Discriminator loss: 1.032715, acc.: 36.72%] [Generator loss: 1.256644]\n",
      "2262 [Discriminator loss: 0.858663, acc.: 48.44%] [Generator loss: 1.295273]\n",
      "2263 [Discriminator loss: 0.909205, acc.: 41.41%] [Generator loss: 1.407879]\n",
      "2264 [Discriminator loss: 0.851432, acc.: 47.66%] [Generator loss: 1.351083]\n",
      "2265 [Discriminator loss: 0.875472, acc.: 46.09%] [Generator loss: 1.340470]\n",
      "2266 [Discriminator loss: 0.947710, acc.: 44.53%] [Generator loss: 1.259295]\n",
      "2267 [Discriminator loss: 0.928023, acc.: 38.28%] [Generator loss: 1.183292]\n",
      "2268 [Discriminator loss: 0.893015, acc.: 39.06%] [Generator loss: 1.152217]\n",
      "2269 [Discriminator loss: 0.619960, acc.: 63.28%] [Generator loss: 0.958162]\n",
      "2270 [Discriminator loss: 1.014091, acc.: 32.03%] [Generator loss: 1.073917]\n",
      "2271 [Discriminator loss: 0.661812, acc.: 60.94%] [Generator loss: 1.208148]\n",
      "2272 [Discriminator loss: 0.763901, acc.: 48.44%] [Generator loss: 1.172590]\n",
      "2273 [Discriminator loss: 0.828922, acc.: 47.66%] [Generator loss: 1.238534]\n",
      "2274 [Discriminator loss: 0.830910, acc.: 45.31%] [Generator loss: 1.074275]\n",
      "2275 [Discriminator loss: 0.676206, acc.: 60.16%] [Generator loss: 1.290647]\n",
      "2276 [Discriminator loss: 0.718157, acc.: 57.03%] [Generator loss: 1.216421]\n",
      "2277 [Discriminator loss: 0.805653, acc.: 53.91%] [Generator loss: 1.402786]\n",
      "2278 [Discriminator loss: 0.753116, acc.: 53.12%] [Generator loss: 1.218839]\n",
      "2279 [Discriminator loss: 0.635612, acc.: 60.94%] [Generator loss: 1.275321]\n",
      "2280 [Discriminator loss: 0.700408, acc.: 59.38%] [Generator loss: 1.270568]\n",
      "2281 [Discriminator loss: 0.886686, acc.: 40.62%] [Generator loss: 1.211553]\n",
      "2282 [Discriminator loss: 1.084371, acc.: 32.81%] [Generator loss: 0.948769]\n",
      "2283 [Discriminator loss: 0.761277, acc.: 54.69%] [Generator loss: 1.101492]\n",
      "2284 [Discriminator loss: 0.868011, acc.: 53.91%] [Generator loss: 1.363250]\n",
      "2285 [Discriminator loss: 0.952736, acc.: 36.72%] [Generator loss: 1.090260]\n",
      "2286 [Discriminator loss: 0.881594, acc.: 45.31%] [Generator loss: 1.316193]\n",
      "2287 [Discriminator loss: 0.961351, acc.: 39.84%] [Generator loss: 1.432317]\n",
      "2288 [Discriminator loss: 0.795033, acc.: 48.44%] [Generator loss: 1.199890]\n",
      "2289 [Discriminator loss: 0.858689, acc.: 46.88%] [Generator loss: 1.245411]\n",
      "2290 [Discriminator loss: 0.599146, acc.: 70.31%] [Generator loss: 1.545215]\n",
      "2291 [Discriminator loss: 0.774266, acc.: 56.25%] [Generator loss: 1.312004]\n",
      "2292 [Discriminator loss: 0.702014, acc.: 58.59%] [Generator loss: 1.445886]\n",
      "2293 [Discriminator loss: 0.672132, acc.: 60.16%] [Generator loss: 1.184203]\n",
      "2294 [Discriminator loss: 0.655728, acc.: 61.72%] [Generator loss: 1.283791]\n",
      "2295 [Discriminator loss: 0.922829, acc.: 44.53%] [Generator loss: 1.215755]\n",
      "2296 [Discriminator loss: 0.699454, acc.: 60.94%] [Generator loss: 1.249768]\n",
      "2297 [Discriminator loss: 0.712304, acc.: 57.03%] [Generator loss: 1.208031]\n",
      "2298 [Discriminator loss: 0.852193, acc.: 45.31%] [Generator loss: 1.253438]\n",
      "2299 [Discriminator loss: 0.732823, acc.: 58.59%] [Generator loss: 1.025254]\n",
      "2300 [Discriminator loss: 0.830094, acc.: 49.22%] [Generator loss: 1.035943]\n",
      "2301 [Discriminator loss: 0.688920, acc.: 61.72%] [Generator loss: 1.095241]\n",
      "2302 [Discriminator loss: 0.751875, acc.: 60.16%] [Generator loss: 1.132224]\n",
      "2303 [Discriminator loss: 0.744284, acc.: 55.47%] [Generator loss: 1.308062]\n",
      "2304 [Discriminator loss: 0.821416, acc.: 51.56%] [Generator loss: 1.292480]\n",
      "2305 [Discriminator loss: 1.025681, acc.: 32.03%] [Generator loss: 0.955265]\n",
      "2306 [Discriminator loss: 0.735381, acc.: 50.78%] [Generator loss: 1.129433]\n",
      "2307 [Discriminator loss: 0.963441, acc.: 32.81%] [Generator loss: 1.050378]\n",
      "2308 [Discriminator loss: 0.960744, acc.: 38.28%] [Generator loss: 1.200015]\n",
      "2309 [Discriminator loss: 0.836623, acc.: 41.41%] [Generator loss: 1.422689]\n",
      "2310 [Discriminator loss: 0.800630, acc.: 48.44%] [Generator loss: 1.237163]\n",
      "2311 [Discriminator loss: 0.865037, acc.: 46.88%] [Generator loss: 1.199146]\n",
      "2312 [Discriminator loss: 0.773637, acc.: 57.03%] [Generator loss: 0.935787]\n",
      "2313 [Discriminator loss: 0.832824, acc.: 43.75%] [Generator loss: 1.097399]\n",
      "2314 [Discriminator loss: 0.856565, acc.: 46.88%] [Generator loss: 1.649883]\n",
      "2315 [Discriminator loss: 0.667020, acc.: 63.28%] [Generator loss: 1.684251]\n",
      "2316 [Discriminator loss: 0.720888, acc.: 56.25%] [Generator loss: 0.991420]\n",
      "2317 [Discriminator loss: 0.925050, acc.: 44.53%] [Generator loss: 1.004291]\n",
      "2318 [Discriminator loss: 0.623853, acc.: 63.28%] [Generator loss: 1.417415]\n",
      "2319 [Discriminator loss: 0.743377, acc.: 54.69%] [Generator loss: 1.331612]\n",
      "2320 [Discriminator loss: 0.975670, acc.: 35.16%] [Generator loss: 1.510215]\n",
      "2321 [Discriminator loss: 0.960149, acc.: 39.84%] [Generator loss: 1.325326]\n",
      "2322 [Discriminator loss: 0.736275, acc.: 53.91%] [Generator loss: 1.493789]\n",
      "2323 [Discriminator loss: 0.648284, acc.: 65.62%] [Generator loss: 1.009201]\n",
      "2324 [Discriminator loss: 0.816364, acc.: 55.47%] [Generator loss: 1.294593]\n",
      "2325 [Discriminator loss: 0.645221, acc.: 69.53%] [Generator loss: 1.536129]\n",
      "2326 [Discriminator loss: 0.677174, acc.: 60.94%] [Generator loss: 1.262889]\n",
      "2327 [Discriminator loss: 0.983313, acc.: 36.72%] [Generator loss: 1.041545]\n",
      "2328 [Discriminator loss: 0.802326, acc.: 51.56%] [Generator loss: 1.194026]\n",
      "2329 [Discriminator loss: 0.797040, acc.: 57.03%] [Generator loss: 0.955684]\n",
      "2330 [Discriminator loss: 0.814683, acc.: 51.56%] [Generator loss: 1.119702]\n",
      "2331 [Discriminator loss: 0.604666, acc.: 70.31%] [Generator loss: 1.130512]\n",
      "2332 [Discriminator loss: 0.834822, acc.: 47.66%] [Generator loss: 1.275159]\n",
      "2333 [Discriminator loss: 0.765942, acc.: 50.00%] [Generator loss: 1.334631]\n",
      "2334 [Discriminator loss: 0.777150, acc.: 53.91%] [Generator loss: 1.303431]\n",
      "2335 [Discriminator loss: 0.833145, acc.: 48.44%] [Generator loss: 1.457724]\n",
      "2336 [Discriminator loss: 0.617417, acc.: 69.53%] [Generator loss: 1.140016]\n",
      "2337 [Discriminator loss: 0.644415, acc.: 64.84%] [Generator loss: 1.413085]\n",
      "2338 [Discriminator loss: 0.768543, acc.: 58.59%] [Generator loss: 1.173071]\n",
      "2339 [Discriminator loss: 0.931867, acc.: 39.06%] [Generator loss: 1.168509]\n",
      "2340 [Discriminator loss: 0.688531, acc.: 54.69%] [Generator loss: 1.284802]\n",
      "2341 [Discriminator loss: 0.963448, acc.: 42.97%] [Generator loss: 1.275589]\n",
      "2342 [Discriminator loss: 0.613743, acc.: 66.41%] [Generator loss: 1.214535]\n",
      "2343 [Discriminator loss: 0.612066, acc.: 69.53%] [Generator loss: 1.120615]\n",
      "2344 [Discriminator loss: 0.751141, acc.: 57.03%] [Generator loss: 1.346371]\n",
      "2345 [Discriminator loss: 0.879772, acc.: 41.41%] [Generator loss: 1.648224]\n",
      "2346 [Discriminator loss: 0.851888, acc.: 46.88%] [Generator loss: 1.599790]\n",
      "2347 [Discriminator loss: 0.926360, acc.: 42.97%] [Generator loss: 1.479718]\n",
      "2348 [Discriminator loss: 0.768360, acc.: 51.56%] [Generator loss: 1.321746]\n",
      "2349 [Discriminator loss: 1.123505, acc.: 25.00%] [Generator loss: 1.157764]\n",
      "2350 [Discriminator loss: 0.889257, acc.: 50.00%] [Generator loss: 1.329335]\n",
      "2351 [Discriminator loss: 0.741005, acc.: 55.47%] [Generator loss: 1.444644]\n",
      "2352 [Discriminator loss: 0.764878, acc.: 46.09%] [Generator loss: 1.383059]\n",
      "2353 [Discriminator loss: 1.079073, acc.: 30.47%] [Generator loss: 1.121986]\n",
      "2354 [Discriminator loss: 0.706908, acc.: 55.47%] [Generator loss: 1.252005]\n",
      "2355 [Discriminator loss: 0.884201, acc.: 42.97%] [Generator loss: 1.389869]\n",
      "2356 [Discriminator loss: 0.732377, acc.: 54.69%] [Generator loss: 1.216266]\n",
      "2357 [Discriminator loss: 0.596891, acc.: 68.75%] [Generator loss: 1.233791]\n",
      "2358 [Discriminator loss: 0.762084, acc.: 51.56%] [Generator loss: 1.119734]\n",
      "2359 [Discriminator loss: 0.692112, acc.: 57.03%] [Generator loss: 1.093372]\n",
      "2360 [Discriminator loss: 0.777891, acc.: 50.78%] [Generator loss: 1.155676]\n",
      "2361 [Discriminator loss: 0.882942, acc.: 44.53%] [Generator loss: 1.236682]\n",
      "2362 [Discriminator loss: 0.693594, acc.: 60.16%] [Generator loss: 1.265708]\n",
      "2363 [Discriminator loss: 0.954357, acc.: 40.62%] [Generator loss: 1.069793]\n",
      "2364 [Discriminator loss: 0.856172, acc.: 46.88%] [Generator loss: 1.010974]\n",
      "2365 [Discriminator loss: 0.891665, acc.: 41.41%] [Generator loss: 1.188761]\n",
      "2366 [Discriminator loss: 0.595582, acc.: 73.44%] [Generator loss: 1.055435]\n",
      "2367 [Discriminator loss: 0.753112, acc.: 55.47%] [Generator loss: 0.943459]\n",
      "2368 [Discriminator loss: 0.600359, acc.: 67.97%] [Generator loss: 1.212065]\n",
      "2369 [Discriminator loss: 0.753028, acc.: 48.44%] [Generator loss: 0.915960]\n",
      "2370 [Discriminator loss: 0.849626, acc.: 49.22%] [Generator loss: 0.792070]\n",
      "2371 [Discriminator loss: 0.729089, acc.: 59.38%] [Generator loss: 1.208836]\n",
      "2372 [Discriminator loss: 0.743560, acc.: 57.81%] [Generator loss: 1.274718]\n",
      "2373 [Discriminator loss: 0.689877, acc.: 63.28%] [Generator loss: 0.966510]\n",
      "2374 [Discriminator loss: 0.853711, acc.: 48.44%] [Generator loss: 1.255752]\n",
      "2375 [Discriminator loss: 0.879874, acc.: 43.75%] [Generator loss: 1.253032]\n",
      "2376 [Discriminator loss: 0.894225, acc.: 46.09%] [Generator loss: 1.327156]\n",
      "2377 [Discriminator loss: 0.588930, acc.: 73.44%] [Generator loss: 1.023530]\n",
      "2378 [Discriminator loss: 0.880575, acc.: 48.44%] [Generator loss: 1.334007]\n",
      "2379 [Discriminator loss: 0.953763, acc.: 40.62%] [Generator loss: 1.416553]\n",
      "2380 [Discriminator loss: 0.740236, acc.: 60.16%] [Generator loss: 1.268686]\n",
      "2381 [Discriminator loss: 0.983796, acc.: 32.81%] [Generator loss: 1.264544]\n",
      "2382 [Discriminator loss: 0.879466, acc.: 47.66%] [Generator loss: 1.179491]\n",
      "2383 [Discriminator loss: 0.841429, acc.: 50.78%] [Generator loss: 1.407558]\n",
      "2384 [Discriminator loss: 0.751339, acc.: 58.59%] [Generator loss: 1.367841]\n",
      "2385 [Discriminator loss: 0.530364, acc.: 75.00%] [Generator loss: 1.511098]\n",
      "2386 [Discriminator loss: 0.739657, acc.: 57.03%] [Generator loss: 1.267880]\n",
      "2387 [Discriminator loss: 0.657835, acc.: 64.06%] [Generator loss: 1.163647]\n",
      "2388 [Discriminator loss: 0.802851, acc.: 50.00%] [Generator loss: 1.356370]\n",
      "2389 [Discriminator loss: 0.827228, acc.: 48.44%] [Generator loss: 1.197902]\n",
      "2390 [Discriminator loss: 0.843838, acc.: 46.88%] [Generator loss: 1.150752]\n",
      "2391 [Discriminator loss: 0.775348, acc.: 53.91%] [Generator loss: 1.137952]\n",
      "2392 [Discriminator loss: 0.935906, acc.: 36.72%] [Generator loss: 1.077484]\n",
      "2393 [Discriminator loss: 1.026476, acc.: 30.47%] [Generator loss: 1.160110]\n",
      "2394 [Discriminator loss: 0.844399, acc.: 45.31%] [Generator loss: 1.239225]\n",
      "2395 [Discriminator loss: 1.041763, acc.: 31.25%] [Generator loss: 1.239540]\n",
      "2396 [Discriminator loss: 0.828501, acc.: 45.31%] [Generator loss: 1.274624]\n",
      "2397 [Discriminator loss: 1.001965, acc.: 37.50%] [Generator loss: 1.140211]\n",
      "2398 [Discriminator loss: 0.948592, acc.: 40.62%] [Generator loss: 1.307208]\n",
      "2399 [Discriminator loss: 0.713404, acc.: 53.91%] [Generator loss: 1.377088]\n",
      "2400 [Discriminator loss: 0.724668, acc.: 55.47%] [Generator loss: 1.311730]\n",
      "2401 [Discriminator loss: 0.634423, acc.: 66.41%] [Generator loss: 1.308825]\n",
      "2402 [Discriminator loss: 0.647183, acc.: 60.94%] [Generator loss: 1.381035]\n",
      "2403 [Discriminator loss: 0.569058, acc.: 67.19%] [Generator loss: 1.475695]\n",
      "2404 [Discriminator loss: 0.745658, acc.: 53.91%] [Generator loss: 1.289865]\n",
      "2405 [Discriminator loss: 0.720412, acc.: 59.38%] [Generator loss: 1.516813]\n",
      "2406 [Discriminator loss: 0.728824, acc.: 57.03%] [Generator loss: 1.348329]\n",
      "2407 [Discriminator loss: 0.815469, acc.: 48.44%] [Generator loss: 1.169477]\n",
      "2408 [Discriminator loss: 0.789313, acc.: 48.44%] [Generator loss: 1.328450]\n",
      "2409 [Discriminator loss: 0.749799, acc.: 55.47%] [Generator loss: 1.291707]\n",
      "2410 [Discriminator loss: 0.908045, acc.: 44.53%] [Generator loss: 1.110048]\n",
      "2411 [Discriminator loss: 0.720867, acc.: 58.59%] [Generator loss: 1.395469]\n",
      "2412 [Discriminator loss: 0.745104, acc.: 52.34%] [Generator loss: 1.252707]\n",
      "2413 [Discriminator loss: 0.763729, acc.: 52.34%] [Generator loss: 1.093638]\n",
      "2414 [Discriminator loss: 0.853207, acc.: 39.84%] [Generator loss: 1.269216]\n",
      "2415 [Discriminator loss: 0.757251, acc.: 57.03%] [Generator loss: 1.084230]\n",
      "2416 [Discriminator loss: 0.897309, acc.: 40.62%] [Generator loss: 1.154170]\n",
      "2417 [Discriminator loss: 0.665714, acc.: 60.94%] [Generator loss: 1.170282]\n",
      "2418 [Discriminator loss: 0.812252, acc.: 47.66%] [Generator loss: 1.162717]\n",
      "2419 [Discriminator loss: 0.719975, acc.: 58.59%] [Generator loss: 1.217220]\n",
      "2420 [Discriminator loss: 0.716247, acc.: 57.03%] [Generator loss: 1.251719]\n",
      "2421 [Discriminator loss: 0.798103, acc.: 48.44%] [Generator loss: 1.234778]\n",
      "2422 [Discriminator loss: 0.909245, acc.: 38.28%] [Generator loss: 1.277745]\n",
      "2423 [Discriminator loss: 0.739292, acc.: 58.59%] [Generator loss: 1.028570]\n",
      "2424 [Discriminator loss: 0.769518, acc.: 50.00%] [Generator loss: 1.246352]\n",
      "2425 [Discriminator loss: 0.689636, acc.: 62.50%] [Generator loss: 1.361477]\n",
      "2426 [Discriminator loss: 0.610476, acc.: 64.06%] [Generator loss: 1.159200]\n",
      "2427 [Discriminator loss: 0.658894, acc.: 61.72%] [Generator loss: 1.444794]\n",
      "2428 [Discriminator loss: 0.679733, acc.: 57.03%] [Generator loss: 1.302117]\n",
      "2429 [Discriminator loss: 0.682605, acc.: 56.25%] [Generator loss: 1.478815]\n",
      "2430 [Discriminator loss: 0.648083, acc.: 64.06%] [Generator loss: 1.274214]\n",
      "2431 [Discriminator loss: 0.834771, acc.: 49.22%] [Generator loss: 1.399409]\n",
      "2432 [Discriminator loss: 0.922625, acc.: 41.41%] [Generator loss: 1.332534]\n",
      "2433 [Discriminator loss: 0.737614, acc.: 62.50%] [Generator loss: 0.999740]\n",
      "2434 [Discriminator loss: 0.855306, acc.: 53.12%] [Generator loss: 1.059503]\n",
      "2435 [Discriminator loss: 0.755340, acc.: 53.91%] [Generator loss: 0.692249]\n",
      "2436 [Discriminator loss: 0.654426, acc.: 68.75%] [Generator loss: 1.086530]\n",
      "2437 [Discriminator loss: 0.709020, acc.: 60.94%] [Generator loss: 0.617394]\n",
      "2438 [Discriminator loss: 0.961719, acc.: 45.31%] [Generator loss: 0.824264]\n",
      "2439 [Discriminator loss: 0.504929, acc.: 77.34%] [Generator loss: 0.906005]\n",
      "2440 [Discriminator loss: 0.373881, acc.: 83.59%] [Generator loss: 0.675940]\n",
      "2441 [Discriminator loss: 0.649143, acc.: 63.28%] [Generator loss: 1.055055]\n",
      "2442 [Discriminator loss: 0.739246, acc.: 56.25%] [Generator loss: 1.460520]\n",
      "2443 [Discriminator loss: 0.916388, acc.: 48.44%] [Generator loss: 1.227921]\n",
      "2444 [Discriminator loss: 0.714057, acc.: 66.41%] [Generator loss: 1.964258]\n",
      "2445 [Discriminator loss: 0.712516, acc.: 64.84%] [Generator loss: 1.250153]\n",
      "2446 [Discriminator loss: 0.873404, acc.: 46.88%] [Generator loss: 1.048245]\n",
      "2447 [Discriminator loss: 0.705366, acc.: 58.59%] [Generator loss: 0.826471]\n",
      "2448 [Discriminator loss: 0.561599, acc.: 72.66%] [Generator loss: 0.895514]\n",
      "2449 [Discriminator loss: 0.558304, acc.: 74.22%] [Generator loss: 0.945331]\n",
      "2450 [Discriminator loss: 0.623238, acc.: 65.62%] [Generator loss: 0.725431]\n",
      "2451 [Discriminator loss: 0.649724, acc.: 64.06%] [Generator loss: 1.262052]\n",
      "2452 [Discriminator loss: 0.715606, acc.: 57.81%] [Generator loss: 1.214339]\n",
      "2453 [Discriminator loss: 0.714195, acc.: 60.94%] [Generator loss: 1.130527]\n",
      "2454 [Discriminator loss: 0.690177, acc.: 59.38%] [Generator loss: 1.154119]\n",
      "2455 [Discriminator loss: 0.621106, acc.: 64.06%] [Generator loss: 1.330246]\n",
      "2456 [Discriminator loss: 0.821186, acc.: 52.34%] [Generator loss: 1.128822]\n",
      "2457 [Discriminator loss: 0.612288, acc.: 62.50%] [Generator loss: 1.327522]\n",
      "2458 [Discriminator loss: 0.878117, acc.: 44.53%] [Generator loss: 1.024637]\n",
      "2459 [Discriminator loss: 0.987445, acc.: 36.72%] [Generator loss: 1.105281]\n",
      "2460 [Discriminator loss: 0.658539, acc.: 63.28%] [Generator loss: 1.060335]\n",
      "2461 [Discriminator loss: 0.562534, acc.: 66.41%] [Generator loss: 1.046258]\n",
      "2462 [Discriminator loss: 0.467171, acc.: 78.12%] [Generator loss: 0.859821]\n",
      "2463 [Discriminator loss: 0.505603, acc.: 73.44%] [Generator loss: 0.975499]\n",
      "2464 [Discriminator loss: 0.672191, acc.: 60.16%] [Generator loss: 1.591783]\n",
      "2465 [Discriminator loss: 0.740518, acc.: 59.38%] [Generator loss: 1.050103]\n",
      "2466 [Discriminator loss: 1.108514, acc.: 39.84%] [Generator loss: 1.122486]\n",
      "2467 [Discriminator loss: 0.588960, acc.: 70.31%] [Generator loss: 1.338803]\n",
      "2468 [Discriminator loss: 1.251839, acc.: 19.53%] [Generator loss: 1.193092]\n",
      "2469 [Discriminator loss: 0.774061, acc.: 54.69%] [Generator loss: 1.204035]\n",
      "2470 [Discriminator loss: 0.941869, acc.: 39.84%] [Generator loss: 1.235613]\n",
      "2471 [Discriminator loss: 0.775508, acc.: 51.56%] [Generator loss: 1.421028]\n",
      "2472 [Discriminator loss: 0.791036, acc.: 46.88%] [Generator loss: 1.336521]\n",
      "2473 [Discriminator loss: 0.949448, acc.: 37.50%] [Generator loss: 1.010882]\n",
      "2474 [Discriminator loss: 0.850496, acc.: 50.78%] [Generator loss: 1.278061]\n",
      "2475 [Discriminator loss: 0.673900, acc.: 58.59%] [Generator loss: 1.291967]\n",
      "2476 [Discriminator loss: 0.835052, acc.: 50.00%] [Generator loss: 1.057315]\n",
      "2477 [Discriminator loss: 1.073565, acc.: 28.12%] [Generator loss: 1.486507]\n",
      "2478 [Discriminator loss: 1.003758, acc.: 37.50%] [Generator loss: 1.289011]\n",
      "2479 [Discriminator loss: 0.659578, acc.: 66.41%] [Generator loss: 1.516143]\n",
      "2480 [Discriminator loss: 0.786870, acc.: 54.69%] [Generator loss: 1.382065]\n",
      "2481 [Discriminator loss: 0.600273, acc.: 65.62%] [Generator loss: 1.406832]\n",
      "2482 [Discriminator loss: 0.577860, acc.: 64.06%] [Generator loss: 1.317484]\n",
      "2483 [Discriminator loss: 0.909675, acc.: 47.66%] [Generator loss: 1.417506]\n",
      "2484 [Discriminator loss: 0.814433, acc.: 48.44%] [Generator loss: 1.441205]\n",
      "2485 [Discriminator loss: 0.712759, acc.: 59.38%] [Generator loss: 1.627690]\n",
      "2486 [Discriminator loss: 0.589175, acc.: 68.75%] [Generator loss: 1.462417]\n",
      "2487 [Discriminator loss: 0.651006, acc.: 63.28%] [Generator loss: 1.621591]\n",
      "2488 [Discriminator loss: 0.588468, acc.: 73.44%] [Generator loss: 1.326706]\n",
      "2489 [Discriminator loss: 0.829939, acc.: 52.34%] [Generator loss: 1.667598]\n",
      "2490 [Discriminator loss: 0.738293, acc.: 60.16%] [Generator loss: 1.458795]\n",
      "2491 [Discriminator loss: 0.722709, acc.: 59.38%] [Generator loss: 1.176150]\n",
      "2492 [Discriminator loss: 0.892638, acc.: 44.53%] [Generator loss: 1.268004]\n",
      "2493 [Discriminator loss: 0.867669, acc.: 50.78%] [Generator loss: 1.577533]\n",
      "2494 [Discriminator loss: 0.882661, acc.: 51.56%] [Generator loss: 1.821532]\n",
      "2495 [Discriminator loss: 0.767685, acc.: 57.81%] [Generator loss: 1.522965]\n",
      "2496 [Discriminator loss: 0.870689, acc.: 48.44%] [Generator loss: 1.659129]\n",
      "2497 [Discriminator loss: 0.611092, acc.: 66.41%] [Generator loss: 1.395202]\n",
      "2498 [Discriminator loss: 0.772082, acc.: 53.12%] [Generator loss: 1.421541]\n",
      "2499 [Discriminator loss: 0.580045, acc.: 68.75%] [Generator loss: 1.082661]\n",
      "2500 [Discriminator loss: 0.654077, acc.: 58.59%] [Generator loss: 0.941047]\n",
      "2501 [Discriminator loss: 0.470677, acc.: 78.12%] [Generator loss: 0.847788]\n",
      "2502 [Discriminator loss: 0.758899, acc.: 57.81%] [Generator loss: 0.613967]\n",
      "2503 [Discriminator loss: 1.035661, acc.: 37.50%] [Generator loss: 0.716043]\n",
      "2504 [Discriminator loss: 0.644713, acc.: 62.50%] [Generator loss: 0.964550]\n",
      "2505 [Discriminator loss: 0.808449, acc.: 47.66%] [Generator loss: 0.971775]\n",
      "2506 [Discriminator loss: 0.765375, acc.: 49.22%] [Generator loss: 1.107994]\n",
      "2507 [Discriminator loss: 0.737689, acc.: 55.47%] [Generator loss: 1.401581]\n",
      "2508 [Discriminator loss: 1.036861, acc.: 41.41%] [Generator loss: 1.225246]\n",
      "2509 [Discriminator loss: 0.804984, acc.: 49.22%] [Generator loss: 1.210309]\n",
      "2510 [Discriminator loss: 0.566066, acc.: 72.66%] [Generator loss: 1.206439]\n",
      "2511 [Discriminator loss: 0.814323, acc.: 50.00%] [Generator loss: 1.138086]\n",
      "2512 [Discriminator loss: 0.487495, acc.: 76.56%] [Generator loss: 1.022280]\n",
      "2513 [Discriminator loss: 0.700183, acc.: 60.94%] [Generator loss: 1.216948]\n",
      "2514 [Discriminator loss: 0.733178, acc.: 51.56%] [Generator loss: 1.363873]\n",
      "2515 [Discriminator loss: 0.868677, acc.: 45.31%] [Generator loss: 1.270770]\n",
      "2516 [Discriminator loss: 1.085047, acc.: 27.34%] [Generator loss: 1.269994]\n",
      "2517 [Discriminator loss: 0.745627, acc.: 49.22%] [Generator loss: 1.749605]\n",
      "2518 [Discriminator loss: 0.632786, acc.: 67.97%] [Generator loss: 1.605870]\n",
      "2519 [Discriminator loss: 1.078133, acc.: 35.16%] [Generator loss: 1.437451]\n",
      "2520 [Discriminator loss: 0.664614, acc.: 57.03%] [Generator loss: 1.530408]\n",
      "2521 [Discriminator loss: 0.790389, acc.: 49.22%] [Generator loss: 1.165983]\n",
      "2522 [Discriminator loss: 0.918755, acc.: 39.06%] [Generator loss: 1.284148]\n",
      "2523 [Discriminator loss: 0.732972, acc.: 53.12%] [Generator loss: 1.293468]\n",
      "2524 [Discriminator loss: 0.557978, acc.: 71.88%] [Generator loss: 1.286022]\n",
      "2525 [Discriminator loss: 0.972494, acc.: 34.38%] [Generator loss: 1.128164]\n",
      "2526 [Discriminator loss: 0.702500, acc.: 58.59%] [Generator loss: 1.241050]\n",
      "2527 [Discriminator loss: 0.743506, acc.: 50.00%] [Generator loss: 1.137247]\n",
      "2528 [Discriminator loss: 0.715387, acc.: 56.25%] [Generator loss: 1.167227]\n",
      "2529 [Discriminator loss: 0.691999, acc.: 58.59%] [Generator loss: 1.337245]\n",
      "2530 [Discriminator loss: 0.913700, acc.: 45.31%] [Generator loss: 1.289148]\n",
      "2531 [Discriminator loss: 1.027821, acc.: 32.03%] [Generator loss: 1.366608]\n",
      "2532 [Discriminator loss: 0.690152, acc.: 60.94%] [Generator loss: 1.154898]\n",
      "2533 [Discriminator loss: 0.862785, acc.: 45.31%] [Generator loss: 0.958287]\n",
      "2534 [Discriminator loss: 1.034228, acc.: 35.94%] [Generator loss: 1.140779]\n",
      "2535 [Discriminator loss: 0.697707, acc.: 56.25%] [Generator loss: 1.275059]\n",
      "2536 [Discriminator loss: 0.605948, acc.: 67.19%] [Generator loss: 0.654867]\n",
      "2537 [Discriminator loss: 0.718678, acc.: 57.03%] [Generator loss: 0.943225]\n",
      "2538 [Discriminator loss: 0.546471, acc.: 70.31%] [Generator loss: 1.197814]\n",
      "2539 [Discriminator loss: 0.525600, acc.: 75.00%] [Generator loss: 1.139603]\n",
      "2540 [Discriminator loss: 0.880244, acc.: 43.75%] [Generator loss: 1.218635]\n",
      "2541 [Discriminator loss: 0.803619, acc.: 53.12%] [Generator loss: 1.605192]\n",
      "2542 [Discriminator loss: 0.862617, acc.: 45.31%] [Generator loss: 1.713549]\n",
      "2543 [Discriminator loss: 1.005826, acc.: 42.19%] [Generator loss: 1.432374]\n",
      "2544 [Discriminator loss: 0.679631, acc.: 62.50%] [Generator loss: 1.339156]\n",
      "2545 [Discriminator loss: 0.639961, acc.: 65.62%] [Generator loss: 0.996726]\n",
      "2546 [Discriminator loss: 0.694155, acc.: 59.38%] [Generator loss: 1.364274]\n",
      "2547 [Discriminator loss: 0.621974, acc.: 67.19%] [Generator loss: 1.150350]\n",
      "2548 [Discriminator loss: 0.789385, acc.: 52.34%] [Generator loss: 0.906868]\n",
      "2549 [Discriminator loss: 0.488137, acc.: 75.78%] [Generator loss: 0.787306]\n",
      "2550 [Discriminator loss: 0.578684, acc.: 69.53%] [Generator loss: 0.729516]\n",
      "2551 [Discriminator loss: 0.672985, acc.: 60.16%] [Generator loss: 1.059529]\n",
      "2552 [Discriminator loss: 0.854305, acc.: 50.78%] [Generator loss: 1.355255]\n",
      "2553 [Discriminator loss: 0.651860, acc.: 65.62%] [Generator loss: 1.008942]\n",
      "2554 [Discriminator loss: 0.852852, acc.: 46.09%] [Generator loss: 1.185988]\n",
      "2555 [Discriminator loss: 0.725675, acc.: 55.47%] [Generator loss: 1.228628]\n",
      "2556 [Discriminator loss: 0.794518, acc.: 53.91%] [Generator loss: 1.362490]\n",
      "2557 [Discriminator loss: 1.139048, acc.: 25.00%] [Generator loss: 1.132030]\n",
      "2558 [Discriminator loss: 0.603204, acc.: 69.53%] [Generator loss: 1.264828]\n",
      "2559 [Discriminator loss: 0.768218, acc.: 51.56%] [Generator loss: 1.208733]\n",
      "2560 [Discriminator loss: 0.737528, acc.: 61.72%] [Generator loss: 0.984967]\n",
      "2561 [Discriminator loss: 0.738679, acc.: 59.38%] [Generator loss: 0.626980]\n",
      "2562 [Discriminator loss: 0.768034, acc.: 52.34%] [Generator loss: 0.877841]\n",
      "2563 [Discriminator loss: 0.570223, acc.: 73.44%] [Generator loss: 0.851876]\n",
      "2564 [Discriminator loss: 0.675715, acc.: 61.72%] [Generator loss: 0.877271]\n",
      "2565 [Discriminator loss: 0.327450, acc.: 88.28%] [Generator loss: 0.709869]\n",
      "2566 [Discriminator loss: 0.799464, acc.: 49.22%] [Generator loss: 0.770179]\n",
      "2567 [Discriminator loss: 0.479773, acc.: 84.38%] [Generator loss: 1.359930]\n",
      "2568 [Discriminator loss: 0.585972, acc.: 73.44%] [Generator loss: 0.582726]\n",
      "2569 [Discriminator loss: 1.067751, acc.: 50.78%] [Generator loss: 1.556946]\n",
      "2570 [Discriminator loss: 0.413476, acc.: 77.34%] [Generator loss: 1.339575]\n",
      "2571 [Discriminator loss: 1.241387, acc.: 28.12%] [Generator loss: 0.979699]\n",
      "2572 [Discriminator loss: 0.672051, acc.: 61.72%] [Generator loss: 1.885587]\n",
      "2573 [Discriminator loss: 1.008768, acc.: 48.44%] [Generator loss: 1.366965]\n",
      "2574 [Discriminator loss: 0.932970, acc.: 41.41%] [Generator loss: 0.901070]\n",
      "2575 [Discriminator loss: 0.854487, acc.: 50.78%] [Generator loss: 1.061588]\n",
      "2576 [Discriminator loss: 0.589027, acc.: 66.41%] [Generator loss: 1.214540]\n",
      "2577 [Discriminator loss: 0.533296, acc.: 70.31%] [Generator loss: 1.304477]\n",
      "2578 [Discriminator loss: 0.582097, acc.: 67.97%] [Generator loss: 0.869879]\n",
      "2579 [Discriminator loss: 0.553312, acc.: 69.53%] [Generator loss: 0.941886]\n",
      "2580 [Discriminator loss: 0.627949, acc.: 66.41%] [Generator loss: 0.862829]\n",
      "2581 [Discriminator loss: 0.794619, acc.: 50.78%] [Generator loss: 0.815987]\n",
      "2582 [Discriminator loss: 0.605457, acc.: 69.53%] [Generator loss: 0.923958]\n",
      "2583 [Discriminator loss: 0.827568, acc.: 47.66%] [Generator loss: 0.921842]\n",
      "2584 [Discriminator loss: 1.067667, acc.: 31.25%] [Generator loss: 1.033914]\n",
      "2585 [Discriminator loss: 0.561890, acc.: 70.31%] [Generator loss: 1.030177]\n",
      "2586 [Discriminator loss: 0.976109, acc.: 35.94%] [Generator loss: 0.929964]\n",
      "2587 [Discriminator loss: 0.686520, acc.: 57.81%] [Generator loss: 1.194861]\n",
      "2588 [Discriminator loss: 0.735189, acc.: 60.16%] [Generator loss: 1.126426]\n",
      "2589 [Discriminator loss: 0.897196, acc.: 39.84%] [Generator loss: 1.084534]\n",
      "2590 [Discriminator loss: 0.765651, acc.: 50.00%] [Generator loss: 1.147763]\n",
      "2591 [Discriminator loss: 0.715760, acc.: 52.34%] [Generator loss: 1.307029]\n",
      "2592 [Discriminator loss: 0.977342, acc.: 39.06%] [Generator loss: 1.595848]\n",
      "2593 [Discriminator loss: 0.473909, acc.: 76.56%] [Generator loss: 1.611187]\n",
      "2594 [Discriminator loss: 0.557456, acc.: 78.12%] [Generator loss: 0.870019]\n",
      "2595 [Discriminator loss: 0.744789, acc.: 60.16%] [Generator loss: 1.234881]\n",
      "2596 [Discriminator loss: 0.653112, acc.: 64.06%] [Generator loss: 1.070621]\n",
      "2597 [Discriminator loss: 0.714771, acc.: 56.25%] [Generator loss: 0.982222]\n",
      "2598 [Discriminator loss: 0.755156, acc.: 48.44%] [Generator loss: 1.383610]\n",
      "2599 [Discriminator loss: 0.552024, acc.: 71.09%] [Generator loss: 1.147490]\n",
      "2600 [Discriminator loss: 1.219893, acc.: 26.56%] [Generator loss: 0.872188]\n",
      "2601 [Discriminator loss: 0.885831, acc.: 44.53%] [Generator loss: 1.372702]\n",
      "2602 [Discriminator loss: 0.879988, acc.: 44.53%] [Generator loss: 1.432195]\n",
      "2603 [Discriminator loss: 0.752676, acc.: 46.88%] [Generator loss: 1.398713]\n",
      "2604 [Discriminator loss: 0.993902, acc.: 38.28%] [Generator loss: 1.174337]\n",
      "2605 [Discriminator loss: 0.818475, acc.: 53.12%] [Generator loss: 1.269382]\n",
      "2606 [Discriminator loss: 0.649602, acc.: 62.50%] [Generator loss: 1.283028]\n",
      "2607 [Discriminator loss: 0.716097, acc.: 62.50%] [Generator loss: 1.188911]\n",
      "2608 [Discriminator loss: 0.471366, acc.: 76.56%] [Generator loss: 1.071622]\n",
      "2609 [Discriminator loss: 0.731469, acc.: 57.03%] [Generator loss: 1.233031]\n",
      "2610 [Discriminator loss: 0.612135, acc.: 64.06%] [Generator loss: 1.119557]\n",
      "2611 [Discriminator loss: 0.543768, acc.: 71.09%] [Generator loss: 1.115315]\n",
      "2612 [Discriminator loss: 0.838081, acc.: 53.91%] [Generator loss: 1.091269]\n",
      "2613 [Discriminator loss: 0.789484, acc.: 51.56%] [Generator loss: 1.205441]\n",
      "2614 [Discriminator loss: 0.561311, acc.: 72.66%] [Generator loss: 1.700786]\n",
      "2615 [Discriminator loss: 0.619627, acc.: 64.06%] [Generator loss: 1.312058]\n",
      "2616 [Discriminator loss: 0.723977, acc.: 51.56%] [Generator loss: 1.101163]\n",
      "2617 [Discriminator loss: 0.577671, acc.: 75.00%] [Generator loss: 0.913118]\n",
      "2618 [Discriminator loss: 0.786777, acc.: 57.03%] [Generator loss: 1.117346]\n",
      "2619 [Discriminator loss: 0.628636, acc.: 65.62%] [Generator loss: 1.295793]\n",
      "2620 [Discriminator loss: 0.826221, acc.: 51.56%] [Generator loss: 0.925727]\n",
      "2621 [Discriminator loss: 0.792107, acc.: 49.22%] [Generator loss: 1.027068]\n",
      "2622 [Discriminator loss: 0.462233, acc.: 74.22%] [Generator loss: 0.862887]\n",
      "2623 [Discriminator loss: 0.974635, acc.: 48.44%] [Generator loss: 1.470742]\n",
      "2624 [Discriminator loss: 0.700843, acc.: 57.81%] [Generator loss: 1.517333]\n",
      "2625 [Discriminator loss: 0.910707, acc.: 46.88%] [Generator loss: 1.270226]\n",
      "2626 [Discriminator loss: 0.671095, acc.: 63.28%] [Generator loss: 1.476792]\n",
      "2627 [Discriminator loss: 1.031520, acc.: 36.72%] [Generator loss: 1.271447]\n",
      "2628 [Discriminator loss: 0.574975, acc.: 67.19%] [Generator loss: 1.208011]\n",
      "2629 [Discriminator loss: 0.620321, acc.: 65.62%] [Generator loss: 1.186911]\n",
      "2630 [Discriminator loss: 0.727986, acc.: 58.59%] [Generator loss: 1.203210]\n",
      "2631 [Discriminator loss: 0.596467, acc.: 66.41%] [Generator loss: 1.154948]\n",
      "2632 [Discriminator loss: 0.869293, acc.: 44.53%] [Generator loss: 1.048048]\n",
      "2633 [Discriminator loss: 0.599242, acc.: 67.97%] [Generator loss: 1.216152]\n",
      "2634 [Discriminator loss: 0.680833, acc.: 62.50%] [Generator loss: 1.049055]\n",
      "2635 [Discriminator loss: 0.775692, acc.: 47.66%] [Generator loss: 1.005485]\n",
      "2636 [Discriminator loss: 0.771251, acc.: 50.00%] [Generator loss: 1.127207]\n",
      "2637 [Discriminator loss: 0.711802, acc.: 62.50%] [Generator loss: 1.513377]\n",
      "2638 [Discriminator loss: 0.924968, acc.: 46.09%] [Generator loss: 1.285201]\n",
      "2639 [Discriminator loss: 0.690786, acc.: 68.75%] [Generator loss: 1.095880]\n",
      "2640 [Discriminator loss: 0.845753, acc.: 46.88%] [Generator loss: 0.977077]\n",
      "2641 [Discriminator loss: 0.559285, acc.: 72.66%] [Generator loss: 1.207632]\n",
      "2642 [Discriminator loss: 0.637750, acc.: 64.84%] [Generator loss: 1.470881]\n",
      "2643 [Discriminator loss: 0.472113, acc.: 80.47%] [Generator loss: 1.270776]\n",
      "2644 [Discriminator loss: 0.346493, acc.: 87.50%] [Generator loss: 0.685109]\n",
      "2645 [Discriminator loss: 0.791245, acc.: 49.22%] [Generator loss: 1.196075]\n",
      "2646 [Discriminator loss: 0.560071, acc.: 75.78%] [Generator loss: 1.578313]\n",
      "2647 [Discriminator loss: 0.782812, acc.: 49.22%] [Generator loss: 1.233724]\n",
      "2648 [Discriminator loss: 0.814702, acc.: 46.88%] [Generator loss: 1.064234]\n",
      "2649 [Discriminator loss: 0.743773, acc.: 55.47%] [Generator loss: 1.513204]\n",
      "2650 [Discriminator loss: 0.796711, acc.: 44.53%] [Generator loss: 1.831423]\n",
      "2651 [Discriminator loss: 0.809513, acc.: 57.03%] [Generator loss: 1.579135]\n",
      "2652 [Discriminator loss: 0.718093, acc.: 58.59%] [Generator loss: 1.472894]\n",
      "2653 [Discriminator loss: 0.686566, acc.: 58.59%] [Generator loss: 1.708914]\n",
      "2654 [Discriminator loss: 0.816981, acc.: 53.12%] [Generator loss: 1.198434]\n",
      "2655 [Discriminator loss: 0.695838, acc.: 60.16%] [Generator loss: 1.391948]\n",
      "2656 [Discriminator loss: 0.716357, acc.: 60.16%] [Generator loss: 1.393490]\n",
      "2657 [Discriminator loss: 0.815428, acc.: 47.66%] [Generator loss: 1.084708]\n",
      "2658 [Discriminator loss: 0.666173, acc.: 62.50%] [Generator loss: 1.462852]\n",
      "2659 [Discriminator loss: 0.550413, acc.: 74.22%] [Generator loss: 1.343364]\n",
      "2660 [Discriminator loss: 0.866635, acc.: 42.97%] [Generator loss: 1.643645]\n",
      "2661 [Discriminator loss: 0.720994, acc.: 57.81%] [Generator loss: 1.577599]\n",
      "2662 [Discriminator loss: 1.129264, acc.: 29.69%] [Generator loss: 1.166383]\n",
      "2663 [Discriminator loss: 0.650164, acc.: 66.41%] [Generator loss: 1.450356]\n",
      "2664 [Discriminator loss: 0.896932, acc.: 42.19%] [Generator loss: 0.987034]\n",
      "2665 [Discriminator loss: 0.735998, acc.: 50.78%] [Generator loss: 1.232609]\n",
      "2666 [Discriminator loss: 1.178452, acc.: 28.12%] [Generator loss: 0.985293]\n",
      "2667 [Discriminator loss: 0.688962, acc.: 57.03%] [Generator loss: 1.307540]\n",
      "2668 [Discriminator loss: 0.837353, acc.: 44.53%] [Generator loss: 1.277488]\n",
      "2669 [Discriminator loss: 0.680324, acc.: 52.34%] [Generator loss: 1.162538]\n",
      "2670 [Discriminator loss: 0.949577, acc.: 43.75%] [Generator loss: 0.966516]\n",
      "2671 [Discriminator loss: 0.723949, acc.: 59.38%] [Generator loss: 1.033868]\n",
      "2672 [Discriminator loss: 1.048571, acc.: 28.91%] [Generator loss: 1.163089]\n",
      "2673 [Discriminator loss: 0.628136, acc.: 62.50%] [Generator loss: 1.548018]\n",
      "2674 [Discriminator loss: 0.969401, acc.: 42.97%] [Generator loss: 1.017403]\n",
      "2675 [Discriminator loss: 0.770287, acc.: 53.91%] [Generator loss: 1.167146]\n",
      "2676 [Discriminator loss: 0.917637, acc.: 47.66%] [Generator loss: 1.266734]\n",
      "2677 [Discriminator loss: 0.962319, acc.: 35.94%] [Generator loss: 1.470321]\n",
      "2678 [Discriminator loss: 0.812495, acc.: 50.78%] [Generator loss: 1.296535]\n",
      "2679 [Discriminator loss: 0.753218, acc.: 55.47%] [Generator loss: 1.519332]\n",
      "2680 [Discriminator loss: 0.783934, acc.: 46.09%] [Generator loss: 1.292432]\n",
      "2681 [Discriminator loss: 0.563225, acc.: 71.88%] [Generator loss: 1.113488]\n",
      "2682 [Discriminator loss: 0.808990, acc.: 42.97%] [Generator loss: 1.140064]\n",
      "2683 [Discriminator loss: 0.587654, acc.: 69.53%] [Generator loss: 1.277393]\n",
      "2684 [Discriminator loss: 1.114049, acc.: 28.12%] [Generator loss: 1.061095]\n",
      "2685 [Discriminator loss: 0.567672, acc.: 72.66%] [Generator loss: 1.171418]\n",
      "2686 [Discriminator loss: 0.481944, acc.: 80.47%] [Generator loss: 1.005157]\n",
      "2687 [Discriminator loss: 0.839009, acc.: 54.69%] [Generator loss: 0.974453]\n",
      "2688 [Discriminator loss: 1.036453, acc.: 35.16%] [Generator loss: 1.173798]\n",
      "2689 [Discriminator loss: 0.516985, acc.: 75.78%] [Generator loss: 1.011589]\n",
      "2690 [Discriminator loss: 0.876638, acc.: 43.75%] [Generator loss: 0.555892]\n",
      "2691 [Discriminator loss: 0.447484, acc.: 82.03%] [Generator loss: 0.970500]\n",
      "2692 [Discriminator loss: 1.107350, acc.: 30.47%] [Generator loss: 1.281046]\n",
      "2693 [Discriminator loss: 0.824483, acc.: 50.00%] [Generator loss: 1.670898]\n",
      "2694 [Discriminator loss: 0.700700, acc.: 57.81%] [Generator loss: 1.390720]\n",
      "2695 [Discriminator loss: 0.606809, acc.: 64.84%] [Generator loss: 1.318953]\n",
      "2696 [Discriminator loss: 0.781337, acc.: 53.12%] [Generator loss: 1.040748]\n",
      "2697 [Discriminator loss: 0.510998, acc.: 77.34%] [Generator loss: 1.070194]\n",
      "2698 [Discriminator loss: 0.922231, acc.: 37.50%] [Generator loss: 0.870490]\n",
      "2699 [Discriminator loss: 0.480658, acc.: 78.12%] [Generator loss: 0.755566]\n",
      "2700 [Discriminator loss: 0.651708, acc.: 55.47%] [Generator loss: 0.598413]\n",
      "2701 [Discriminator loss: 0.718962, acc.: 60.16%] [Generator loss: 0.923830]\n",
      "2702 [Discriminator loss: 0.878505, acc.: 46.88%] [Generator loss: 1.313649]\n",
      "2703 [Discriminator loss: 0.903757, acc.: 45.31%] [Generator loss: 1.055132]\n",
      "2704 [Discriminator loss: 0.530707, acc.: 74.22%] [Generator loss: 0.848489]\n",
      "2705 [Discriminator loss: 0.681380, acc.: 59.38%] [Generator loss: 1.303177]\n",
      "2706 [Discriminator loss: 0.484659, acc.: 75.78%] [Generator loss: 1.365014]\n",
      "2707 [Discriminator loss: 0.628871, acc.: 70.31%] [Generator loss: 1.104926]\n",
      "2708 [Discriminator loss: 0.974936, acc.: 41.41%] [Generator loss: 1.160058]\n",
      "2709 [Discriminator loss: 0.830156, acc.: 49.22%] [Generator loss: 1.087496]\n",
      "2710 [Discriminator loss: 0.826313, acc.: 48.44%] [Generator loss: 0.941104]\n",
      "2711 [Discriminator loss: 0.573239, acc.: 72.66%] [Generator loss: 0.975147]\n",
      "2712 [Discriminator loss: 0.672246, acc.: 60.16%] [Generator loss: 0.825748]\n",
      "2713 [Discriminator loss: 0.705093, acc.: 57.81%] [Generator loss: 1.056576]\n",
      "2714 [Discriminator loss: 0.540685, acc.: 74.22%] [Generator loss: 0.929989]\n",
      "2715 [Discriminator loss: 0.533934, acc.: 72.66%] [Generator loss: 0.737607]\n",
      "2716 [Discriminator loss: 0.475192, acc.: 73.44%] [Generator loss: 0.514350]\n",
      "2717 [Discriminator loss: 0.438348, acc.: 81.25%] [Generator loss: 0.707025]\n",
      "2718 [Discriminator loss: 0.371526, acc.: 84.38%] [Generator loss: 0.842725]\n",
      "2719 [Discriminator loss: 0.796494, acc.: 55.47%] [Generator loss: 1.426997]\n",
      "2720 [Discriminator loss: 1.109594, acc.: 29.69%] [Generator loss: 1.639515]\n",
      "2721 [Discriminator loss: 0.964898, acc.: 41.41%] [Generator loss: 1.594353]\n",
      "2722 [Discriminator loss: 0.590808, acc.: 67.19%] [Generator loss: 1.536654]\n",
      "2723 [Discriminator loss: 0.606935, acc.: 61.72%] [Generator loss: 1.152785]\n",
      "2724 [Discriminator loss: 0.649493, acc.: 66.41%] [Generator loss: 0.731736]\n",
      "2725 [Discriminator loss: 0.656238, acc.: 62.50%] [Generator loss: 1.325219]\n",
      "2726 [Discriminator loss: 0.574333, acc.: 70.31%] [Generator loss: 1.197701]\n",
      "2727 [Discriminator loss: 0.705022, acc.: 57.81%] [Generator loss: 1.093205]\n",
      "2728 [Discriminator loss: 0.781859, acc.: 57.03%] [Generator loss: 1.641822]\n",
      "2729 [Discriminator loss: 0.852601, acc.: 51.56%] [Generator loss: 1.343794]\n",
      "2730 [Discriminator loss: 0.562669, acc.: 71.88%] [Generator loss: 1.474179]\n",
      "2731 [Discriminator loss: 0.683904, acc.: 60.16%] [Generator loss: 1.618655]\n",
      "2732 [Discriminator loss: 0.570440, acc.: 73.44%] [Generator loss: 1.400226]\n",
      "2733 [Discriminator loss: 0.815788, acc.: 53.12%] [Generator loss: 1.311302]\n",
      "2734 [Discriminator loss: 0.722338, acc.: 57.03%] [Generator loss: 0.967178]\n",
      "2735 [Discriminator loss: 0.579157, acc.: 65.62%] [Generator loss: 1.002732]\n",
      "2736 [Discriminator loss: 0.588252, acc.: 70.31%] [Generator loss: 1.169657]\n",
      "2737 [Discriminator loss: 0.938722, acc.: 46.88%] [Generator loss: 0.791028]\n",
      "2738 [Discriminator loss: 0.554333, acc.: 70.31%] [Generator loss: 0.736100]\n",
      "2739 [Discriminator loss: 0.565747, acc.: 67.19%] [Generator loss: 1.303161]\n",
      "2740 [Discriminator loss: 0.758854, acc.: 49.22%] [Generator loss: 1.741789]\n",
      "2741 [Discriminator loss: 0.941184, acc.: 40.62%] [Generator loss: 1.402443]\n",
      "2742 [Discriminator loss: 0.637620, acc.: 66.41%] [Generator loss: 1.546417]\n",
      "2743 [Discriminator loss: 0.835863, acc.: 47.66%] [Generator loss: 1.305967]\n",
      "2744 [Discriminator loss: 0.809850, acc.: 45.31%] [Generator loss: 1.140772]\n",
      "2745 [Discriminator loss: 0.784251, acc.: 53.91%] [Generator loss: 0.833051]\n",
      "2746 [Discriminator loss: 0.909083, acc.: 45.31%] [Generator loss: 0.969276]\n",
      "2747 [Discriminator loss: 0.619289, acc.: 67.97%] [Generator loss: 1.520494]\n",
      "2748 [Discriminator loss: 0.708107, acc.: 60.16%] [Generator loss: 1.667482]\n",
      "2749 [Discriminator loss: 1.191701, acc.: 32.03%] [Generator loss: 1.115391]\n",
      "2750 [Discriminator loss: 0.657101, acc.: 63.28%] [Generator loss: 1.495984]\n",
      "2751 [Discriminator loss: 0.630199, acc.: 63.28%] [Generator loss: 1.510059]\n",
      "2752 [Discriminator loss: 0.664517, acc.: 64.06%] [Generator loss: 1.060018]\n",
      "2753 [Discriminator loss: 0.486652, acc.: 72.66%] [Generator loss: 0.414297]\n",
      "2754 [Discriminator loss: 0.549261, acc.: 71.88%] [Generator loss: 0.705140]\n",
      "2755 [Discriminator loss: 0.783060, acc.: 52.34%] [Generator loss: 1.202945]\n",
      "2756 [Discriminator loss: 0.549290, acc.: 70.31%] [Generator loss: 0.799114]\n",
      "2757 [Discriminator loss: 0.634809, acc.: 64.06%] [Generator loss: 0.458290]\n",
      "2758 [Discriminator loss: 0.724110, acc.: 61.72%] [Generator loss: 1.379178]\n",
      "2759 [Discriminator loss: 0.551237, acc.: 70.31%] [Generator loss: 0.948005]\n",
      "2760 [Discriminator loss: 0.653541, acc.: 66.41%] [Generator loss: 0.845586]\n",
      "2761 [Discriminator loss: 0.678195, acc.: 60.94%] [Generator loss: 1.235873]\n",
      "2762 [Discriminator loss: 0.853853, acc.: 51.56%] [Generator loss: 1.330651]\n",
      "2763 [Discriminator loss: 0.799040, acc.: 57.81%] [Generator loss: 0.646557]\n",
      "2764 [Discriminator loss: 1.121723, acc.: 44.53%] [Generator loss: 1.216637]\n",
      "2765 [Discriminator loss: 0.611547, acc.: 69.53%] [Generator loss: 1.113409]\n",
      "2766 [Discriminator loss: 0.850605, acc.: 50.00%] [Generator loss: 1.184931]\n",
      "2767 [Discriminator loss: 0.497906, acc.: 75.00%] [Generator loss: 1.416037]\n",
      "2768 [Discriminator loss: 0.576487, acc.: 70.31%] [Generator loss: 0.850808]\n",
      "2769 [Discriminator loss: 1.243820, acc.: 28.91%] [Generator loss: 1.067787]\n",
      "2770 [Discriminator loss: 0.715368, acc.: 60.94%] [Generator loss: 1.190475]\n",
      "2771 [Discriminator loss: 0.480588, acc.: 82.03%] [Generator loss: 0.988412]\n",
      "2772 [Discriminator loss: 0.708479, acc.: 57.03%] [Generator loss: 0.893898]\n",
      "2773 [Discriminator loss: 0.655955, acc.: 64.06%] [Generator loss: 0.958641]\n",
      "2774 [Discriminator loss: 0.488971, acc.: 81.25%] [Generator loss: 1.250251]\n",
      "2775 [Discriminator loss: 0.792976, acc.: 49.22%] [Generator loss: 1.093726]\n",
      "2776 [Discriminator loss: 0.522706, acc.: 75.78%] [Generator loss: 1.191538]\n",
      "2777 [Discriminator loss: 0.592351, acc.: 66.41%] [Generator loss: 0.862966]\n",
      "2778 [Discriminator loss: 0.790498, acc.: 53.12%] [Generator loss: 1.061598]\n",
      "2779 [Discriminator loss: 0.594107, acc.: 67.97%] [Generator loss: 1.433136]\n",
      "2780 [Discriminator loss: 0.597766, acc.: 69.53%] [Generator loss: 1.022357]\n",
      "2781 [Discriminator loss: 0.552025, acc.: 73.44%] [Generator loss: 0.754622]\n",
      "2782 [Discriminator loss: 0.403780, acc.: 83.59%] [Generator loss: 0.677805]\n",
      "2783 [Discriminator loss: 0.557931, acc.: 69.53%] [Generator loss: 0.715981]\n",
      "2784 [Discriminator loss: 0.476658, acc.: 78.12%] [Generator loss: 1.171407]\n",
      "2785 [Discriminator loss: 1.230036, acc.: 30.47%] [Generator loss: 1.657780]\n",
      "2786 [Discriminator loss: 1.137875, acc.: 34.38%] [Generator loss: 1.634321]\n",
      "2787 [Discriminator loss: 0.608787, acc.: 66.41%] [Generator loss: 0.950545]\n",
      "2788 [Discriminator loss: 0.760920, acc.: 50.78%] [Generator loss: 0.915290]\n",
      "2789 [Discriminator loss: 0.679089, acc.: 57.81%] [Generator loss: 0.940040]\n",
      "2790 [Discriminator loss: 0.964839, acc.: 40.62%] [Generator loss: 1.403934]\n",
      "2791 [Discriminator loss: 0.771767, acc.: 57.03%] [Generator loss: 1.215663]\n",
      "2792 [Discriminator loss: 1.168415, acc.: 25.78%] [Generator loss: 1.001063]\n",
      "2793 [Discriminator loss: 0.662824, acc.: 65.62%] [Generator loss: 1.069594]\n",
      "2794 [Discriminator loss: 0.694728, acc.: 59.38%] [Generator loss: 1.311879]\n",
      "2795 [Discriminator loss: 0.747790, acc.: 57.03%] [Generator loss: 1.143374]\n",
      "2796 [Discriminator loss: 0.698351, acc.: 59.38%] [Generator loss: 1.077931]\n",
      "2797 [Discriminator loss: 0.648805, acc.: 66.41%] [Generator loss: 1.226538]\n",
      "2798 [Discriminator loss: 0.880872, acc.: 48.44%] [Generator loss: 1.119582]\n",
      "2799 [Discriminator loss: 0.699246, acc.: 57.81%] [Generator loss: 1.285143]\n",
      "2800 [Discriminator loss: 0.431728, acc.: 83.59%] [Generator loss: 0.987290]\n",
      "2801 [Discriminator loss: 0.972026, acc.: 39.84%] [Generator loss: 1.218466]\n",
      "2802 [Discriminator loss: 0.553657, acc.: 76.56%] [Generator loss: 1.213162]\n",
      "2803 [Discriminator loss: 0.699520, acc.: 58.59%] [Generator loss: 1.113759]\n",
      "2804 [Discriminator loss: 0.587834, acc.: 69.53%] [Generator loss: 1.080445]\n",
      "2805 [Discriminator loss: 0.726310, acc.: 57.81%] [Generator loss: 1.373081]\n",
      "2806 [Discriminator loss: 0.763778, acc.: 57.81%] [Generator loss: 1.311193]\n",
      "2807 [Discriminator loss: 1.017259, acc.: 42.19%] [Generator loss: 1.105100]\n",
      "2808 [Discriminator loss: 0.628410, acc.: 63.28%] [Generator loss: 1.287155]\n",
      "2809 [Discriminator loss: 0.521707, acc.: 72.66%] [Generator loss: 1.251767]\n",
      "2810 [Discriminator loss: 0.923891, acc.: 42.19%] [Generator loss: 1.346262]\n",
      "2811 [Discriminator loss: 0.666199, acc.: 65.62%] [Generator loss: 1.069041]\n",
      "2812 [Discriminator loss: 0.596901, acc.: 75.78%] [Generator loss: 0.904757]\n",
      "2813 [Discriminator loss: 0.914118, acc.: 38.28%] [Generator loss: 0.813897]\n",
      "2814 [Discriminator loss: 0.658496, acc.: 61.72%] [Generator loss: 1.159193]\n",
      "2815 [Discriminator loss: 0.599718, acc.: 65.62%] [Generator loss: 1.272973]\n",
      "2816 [Discriminator loss: 1.293170, acc.: 27.34%] [Generator loss: 1.109709]\n",
      "2817 [Discriminator loss: 0.588908, acc.: 70.31%] [Generator loss: 1.628275]\n",
      "2818 [Discriminator loss: 0.915643, acc.: 41.41%] [Generator loss: 1.289227]\n",
      "2819 [Discriminator loss: 0.685693, acc.: 55.47%] [Generator loss: 1.168797]\n",
      "2820 [Discriminator loss: 0.820507, acc.: 45.31%] [Generator loss: 1.330239]\n",
      "2821 [Discriminator loss: 0.808310, acc.: 46.09%] [Generator loss: 1.201956]\n",
      "2822 [Discriminator loss: 0.786788, acc.: 50.00%] [Generator loss: 1.215759]\n",
      "2823 [Discriminator loss: 0.673686, acc.: 61.72%] [Generator loss: 1.091881]\n",
      "2824 [Discriminator loss: 0.841048, acc.: 50.00%] [Generator loss: 1.328401]\n",
      "2825 [Discriminator loss: 0.656711, acc.: 66.41%] [Generator loss: 2.340343]\n",
      "2826 [Discriminator loss: 0.643545, acc.: 64.06%] [Generator loss: 0.680901]\n",
      "2827 [Discriminator loss: 0.584286, acc.: 67.97%] [Generator loss: 1.055311]\n",
      "2828 [Discriminator loss: 0.590328, acc.: 66.41%] [Generator loss: 1.132461]\n",
      "2829 [Discriminator loss: 1.115255, acc.: 25.00%] [Generator loss: 1.421006]\n",
      "2830 [Discriminator loss: 1.017435, acc.: 35.94%] [Generator loss: 1.697273]\n",
      "2831 [Discriminator loss: 0.808201, acc.: 56.25%] [Generator loss: 1.528455]\n",
      "2832 [Discriminator loss: 1.039734, acc.: 30.47%] [Generator loss: 1.128443]\n",
      "2833 [Discriminator loss: 0.914108, acc.: 45.31%] [Generator loss: 1.003058]\n",
      "2834 [Discriminator loss: 0.819576, acc.: 53.91%] [Generator loss: 1.377720]\n",
      "2835 [Discriminator loss: 0.760368, acc.: 60.16%] [Generator loss: 1.717740]\n",
      "2836 [Discriminator loss: 0.729434, acc.: 57.81%] [Generator loss: 1.391689]\n",
      "2837 [Discriminator loss: 0.901908, acc.: 39.84%] [Generator loss: 1.445813]\n",
      "2838 [Discriminator loss: 0.638449, acc.: 64.06%] [Generator loss: 1.185675]\n",
      "2839 [Discriminator loss: 0.667703, acc.: 64.06%] [Generator loss: 0.709182]\n",
      "2840 [Discriminator loss: 0.877275, acc.: 50.00%] [Generator loss: 1.285998]\n",
      "2841 [Discriminator loss: 0.749879, acc.: 55.47%] [Generator loss: 1.403642]\n",
      "2842 [Discriminator loss: 0.510347, acc.: 77.34%] [Generator loss: 1.089403]\n",
      "2843 [Discriminator loss: 0.816031, acc.: 47.66%] [Generator loss: 1.457087]\n",
      "2844 [Discriminator loss: 0.720061, acc.: 54.69%] [Generator loss: 1.759390]\n",
      "2845 [Discriminator loss: 0.780481, acc.: 54.69%] [Generator loss: 1.154779]\n",
      "2846 [Discriminator loss: 1.632464, acc.: 16.41%] [Generator loss: 0.922805]\n",
      "2847 [Discriminator loss: 1.010709, acc.: 35.16%] [Generator loss: 1.231372]\n",
      "2848 [Discriminator loss: 0.878660, acc.: 43.75%] [Generator loss: 1.389354]\n",
      "2849 [Discriminator loss: 0.605549, acc.: 64.84%] [Generator loss: 1.330976]\n",
      "2850 [Discriminator loss: 0.838683, acc.: 47.66%] [Generator loss: 1.136349]\n",
      "2851 [Discriminator loss: 0.401229, acc.: 82.81%] [Generator loss: 0.757773]\n",
      "2852 [Discriminator loss: 0.588811, acc.: 68.75%] [Generator loss: 0.991148]\n",
      "2853 [Discriminator loss: 0.582299, acc.: 67.19%] [Generator loss: 1.376103]\n",
      "2854 [Discriminator loss: 0.644498, acc.: 66.41%] [Generator loss: 0.611303]\n",
      "2855 [Discriminator loss: 0.614384, acc.: 62.50%] [Generator loss: 0.854108]\n",
      "2856 [Discriminator loss: 0.485895, acc.: 77.34%] [Generator loss: 0.942447]\n",
      "2857 [Discriminator loss: 0.671569, acc.: 63.28%] [Generator loss: 1.120722]\n",
      "2858 [Discriminator loss: 0.521538, acc.: 76.56%] [Generator loss: 0.974967]\n",
      "2859 [Discriminator loss: 0.478197, acc.: 75.00%] [Generator loss: 0.799238]\n",
      "2860 [Discriminator loss: 0.651244, acc.: 61.72%] [Generator loss: 0.827343]\n",
      "2861 [Discriminator loss: 0.784181, acc.: 55.47%] [Generator loss: 1.318051]\n",
      "2862 [Discriminator loss: 0.769437, acc.: 50.78%] [Generator loss: 1.265295]\n",
      "2863 [Discriminator loss: 0.694410, acc.: 57.03%] [Generator loss: 1.179907]\n",
      "2864 [Discriminator loss: 0.723783, acc.: 55.47%] [Generator loss: 1.589158]\n",
      "2865 [Discriminator loss: 0.803351, acc.: 55.47%] [Generator loss: 1.284856]\n",
      "2866 [Discriminator loss: 0.719391, acc.: 55.47%] [Generator loss: 0.881926]\n",
      "2867 [Discriminator loss: 0.447489, acc.: 80.47%] [Generator loss: 0.751257]\n",
      "2868 [Discriminator loss: 0.378862, acc.: 86.72%] [Generator loss: 0.384739]\n",
      "2869 [Discriminator loss: 0.393982, acc.: 82.81%] [Generator loss: 0.620031]\n",
      "2870 [Discriminator loss: 0.381419, acc.: 83.59%] [Generator loss: 0.718358]\n",
      "2871 [Discriminator loss: 0.392376, acc.: 83.59%] [Generator loss: 0.514605]\n",
      "2872 [Discriminator loss: 0.744942, acc.: 54.69%] [Generator loss: 1.104569]\n",
      "2873 [Discriminator loss: 0.363687, acc.: 84.38%] [Generator loss: 0.845039]\n",
      "2874 [Discriminator loss: 0.456030, acc.: 81.25%] [Generator loss: 0.402354]\n",
      "2875 [Discriminator loss: 0.236307, acc.: 94.53%] [Generator loss: 0.574858]\n",
      "2876 [Discriminator loss: 1.204847, acc.: 30.47%] [Generator loss: 0.369220]\n",
      "2877 [Discriminator loss: 0.183401, acc.: 96.09%] [Generator loss: 0.373595]\n",
      "2878 [Discriminator loss: 0.249100, acc.: 92.19%] [Generator loss: 0.666046]\n",
      "2879 [Discriminator loss: 0.234253, acc.: 92.97%] [Generator loss: 1.054109]\n",
      "2880 [Discriminator loss: 0.638657, acc.: 67.19%] [Generator loss: 2.791193]\n",
      "2881 [Discriminator loss: 0.714260, acc.: 57.81%] [Generator loss: 2.404064]\n",
      "2882 [Discriminator loss: 1.973464, acc.: 4.69%] [Generator loss: 0.990875]\n",
      "2883 [Discriminator loss: 0.370685, acc.: 79.69%] [Generator loss: 1.088617]\n",
      "2884 [Discriminator loss: 0.629965, acc.: 63.28%] [Generator loss: 1.013684]\n",
      "2885 [Discriminator loss: 0.711657, acc.: 58.59%] [Generator loss: 0.852981]\n",
      "2886 [Discriminator loss: 0.538974, acc.: 72.66%] [Generator loss: 0.650602]\n",
      "2887 [Discriminator loss: 0.698118, acc.: 60.94%] [Generator loss: 0.530581]\n",
      "2888 [Discriminator loss: 0.526217, acc.: 75.78%] [Generator loss: 0.774736]\n",
      "2889 [Discriminator loss: 0.605477, acc.: 67.19%] [Generator loss: 0.769897]\n",
      "2890 [Discriminator loss: 0.280340, acc.: 93.75%] [Generator loss: 0.640568]\n",
      "2891 [Discriminator loss: 0.679654, acc.: 60.94%] [Generator loss: 0.744937]\n",
      "2892 [Discriminator loss: 0.432597, acc.: 82.03%] [Generator loss: 0.978342]\n",
      "2893 [Discriminator loss: 0.587613, acc.: 71.88%] [Generator loss: 0.905573]\n",
      "2894 [Discriminator loss: 1.018035, acc.: 39.84%] [Generator loss: 1.003323]\n",
      "2895 [Discriminator loss: 0.907500, acc.: 42.19%] [Generator loss: 1.121154]\n",
      "2896 [Discriminator loss: 1.431432, acc.: 26.56%] [Generator loss: 0.819470]\n",
      "2897 [Discriminator loss: 0.638583, acc.: 61.72%] [Generator loss: 1.423685]\n",
      "2898 [Discriminator loss: 0.591139, acc.: 67.97%] [Generator loss: 1.624505]\n",
      "2899 [Discriminator loss: 0.665316, acc.: 58.59%] [Generator loss: 1.206789]\n",
      "2900 [Discriminator loss: 0.688289, acc.: 54.69%] [Generator loss: 1.243367]\n",
      "2901 [Discriminator loss: 0.587269, acc.: 65.62%] [Generator loss: 0.863425]\n",
      "2902 [Discriminator loss: 0.527334, acc.: 69.53%] [Generator loss: 0.817464]\n",
      "2903 [Discriminator loss: 0.850843, acc.: 54.69%] [Generator loss: 0.830603]\n",
      "2904 [Discriminator loss: 0.608300, acc.: 70.31%] [Generator loss: 0.956202]\n",
      "2905 [Discriminator loss: 0.751010, acc.: 51.56%] [Generator loss: 1.254999]\n",
      "2906 [Discriminator loss: 0.461929, acc.: 72.66%] [Generator loss: 0.574914]\n",
      "2907 [Discriminator loss: 0.628071, acc.: 67.97%] [Generator loss: 0.484019]\n",
      "2908 [Discriminator loss: 0.574063, acc.: 69.53%] [Generator loss: 0.373643]\n",
      "2909 [Discriminator loss: 1.156028, acc.: 51.56%] [Generator loss: 1.635123]\n",
      "2910 [Discriminator loss: 0.866413, acc.: 46.09%] [Generator loss: 1.873246]\n",
      "2911 [Discriminator loss: 1.049217, acc.: 41.41%] [Generator loss: 1.386166]\n",
      "2912 [Discriminator loss: 0.827319, acc.: 52.34%] [Generator loss: 1.410164]\n",
      "2913 [Discriminator loss: 0.754104, acc.: 59.38%] [Generator loss: 1.545480]\n",
      "2914 [Discriminator loss: 0.989988, acc.: 42.19%] [Generator loss: 1.634829]\n",
      "2915 [Discriminator loss: 1.048979, acc.: 39.06%] [Generator loss: 1.184876]\n",
      "2916 [Discriminator loss: 0.672731, acc.: 65.62%] [Generator loss: 0.819316]\n",
      "2917 [Discriminator loss: 0.543942, acc.: 72.66%] [Generator loss: 0.805781]\n",
      "2918 [Discriminator loss: 0.888011, acc.: 44.53%] [Generator loss: 1.294764]\n",
      "2919 [Discriminator loss: 0.676005, acc.: 60.16%] [Generator loss: 1.625491]\n",
      "2920 [Discriminator loss: 0.560517, acc.: 64.84%] [Generator loss: 1.287415]\n",
      "2921 [Discriminator loss: 0.704673, acc.: 57.03%] [Generator loss: 1.365520]\n",
      "2922 [Discriminator loss: 0.623665, acc.: 61.72%] [Generator loss: 1.468428]\n",
      "2923 [Discriminator loss: 0.581025, acc.: 67.97%] [Generator loss: 1.289025]\n",
      "2924 [Discriminator loss: 0.736582, acc.: 60.16%] [Generator loss: 1.121940]\n",
      "2925 [Discriminator loss: 0.911394, acc.: 50.00%] [Generator loss: 0.875688]\n",
      "2926 [Discriminator loss: 0.514504, acc.: 77.34%] [Generator loss: 0.651979]\n",
      "2927 [Discriminator loss: 0.831618, acc.: 50.78%] [Generator loss: 1.232221]\n",
      "2928 [Discriminator loss: 0.617654, acc.: 68.75%] [Generator loss: 1.407115]\n",
      "2929 [Discriminator loss: 0.800458, acc.: 53.12%] [Generator loss: 1.090053]\n",
      "2930 [Discriminator loss: 0.805761, acc.: 54.69%] [Generator loss: 1.433581]\n",
      "2931 [Discriminator loss: 0.704100, acc.: 57.81%] [Generator loss: 1.200672]\n",
      "2932 [Discriminator loss: 0.777141, acc.: 56.25%] [Generator loss: 1.632886]\n",
      "2933 [Discriminator loss: 0.640476, acc.: 64.84%] [Generator loss: 1.339309]\n",
      "2934 [Discriminator loss: 0.718256, acc.: 59.38%] [Generator loss: 1.242129]\n",
      "2935 [Discriminator loss: 0.401008, acc.: 82.03%] [Generator loss: 1.167459]\n",
      "2936 [Discriminator loss: 0.477430, acc.: 78.91%] [Generator loss: 0.993473]\n",
      "2937 [Discriminator loss: 0.407404, acc.: 84.38%] [Generator loss: 0.607323]\n",
      "2938 [Discriminator loss: 0.921161, acc.: 46.88%] [Generator loss: 1.262521]\n",
      "2939 [Discriminator loss: 1.369812, acc.: 25.00%] [Generator loss: 0.911952]\n",
      "2940 [Discriminator loss: 0.701437, acc.: 57.03%] [Generator loss: 1.567332]\n",
      "2941 [Discriminator loss: 0.627402, acc.: 67.19%] [Generator loss: 1.740456]\n",
      "2942 [Discriminator loss: 0.864891, acc.: 53.12%] [Generator loss: 0.960243]\n",
      "2943 [Discriminator loss: 0.899247, acc.: 42.19%] [Generator loss: 0.865519]\n",
      "2944 [Discriminator loss: 0.875029, acc.: 42.97%] [Generator loss: 1.054594]\n",
      "2945 [Discriminator loss: 0.737075, acc.: 58.59%] [Generator loss: 1.286981]\n",
      "2946 [Discriminator loss: 1.166858, acc.: 29.69%] [Generator loss: 0.978686]\n",
      "2947 [Discriminator loss: 0.634511, acc.: 67.97%] [Generator loss: 0.864850]\n",
      "2948 [Discriminator loss: 0.597284, acc.: 66.41%] [Generator loss: 0.841428]\n",
      "2949 [Discriminator loss: 0.709779, acc.: 55.47%] [Generator loss: 0.932291]\n",
      "2950 [Discriminator loss: 0.463297, acc.: 76.56%] [Generator loss: 0.790951]\n",
      "2951 [Discriminator loss: 0.615071, acc.: 68.75%] [Generator loss: 0.689613]\n",
      "2952 [Discriminator loss: 0.549805, acc.: 71.88%] [Generator loss: 0.983979]\n",
      "2953 [Discriminator loss: 0.452577, acc.: 81.25%] [Generator loss: 0.810292]\n",
      "2954 [Discriminator loss: 0.453835, acc.: 80.47%] [Generator loss: 0.546976]\n",
      "2955 [Discriminator loss: 1.146257, acc.: 33.59%] [Generator loss: 0.951448]\n",
      "2956 [Discriminator loss: 0.626128, acc.: 68.75%] [Generator loss: 1.168337]\n",
      "2957 [Discriminator loss: 0.481934, acc.: 78.91%] [Generator loss: 1.252600]\n",
      "2958 [Discriminator loss: 0.820930, acc.: 52.34%] [Generator loss: 1.387167]\n",
      "2959 [Discriminator loss: 0.864520, acc.: 43.75%] [Generator loss: 1.663500]\n",
      "2960 [Discriminator loss: 1.173550, acc.: 25.78%] [Generator loss: 1.286227]\n",
      "2961 [Discriminator loss: 1.002494, acc.: 34.38%] [Generator loss: 1.173694]\n",
      "2962 [Discriminator loss: 0.555827, acc.: 66.41%] [Generator loss: 0.988457]\n",
      "2963 [Discriminator loss: 0.756565, acc.: 49.22%] [Generator loss: 1.287735]\n",
      "2964 [Discriminator loss: 0.851056, acc.: 42.19%] [Generator loss: 1.232792]\n",
      "2965 [Discriminator loss: 0.729464, acc.: 53.91%] [Generator loss: 1.339789]\n",
      "2966 [Discriminator loss: 0.721770, acc.: 57.03%] [Generator loss: 1.004146]\n",
      "2967 [Discriminator loss: 0.877867, acc.: 45.31%] [Generator loss: 1.073014]\n",
      "2968 [Discriminator loss: 0.707780, acc.: 59.38%] [Generator loss: 1.536784]\n",
      "2969 [Discriminator loss: 0.702823, acc.: 58.59%] [Generator loss: 1.695098]\n",
      "2970 [Discriminator loss: 0.986178, acc.: 37.50%] [Generator loss: 1.113532]\n",
      "2971 [Discriminator loss: 0.850748, acc.: 44.53%] [Generator loss: 1.584289]\n",
      "2972 [Discriminator loss: 0.883447, acc.: 38.28%] [Generator loss: 1.461660]\n",
      "2973 [Discriminator loss: 1.243538, acc.: 27.34%] [Generator loss: 1.033777]\n",
      "2974 [Discriminator loss: 0.742313, acc.: 57.03%] [Generator loss: 1.240793]\n",
      "2975 [Discriminator loss: 0.750599, acc.: 54.69%] [Generator loss: 0.860152]\n",
      "2976 [Discriminator loss: 0.758567, acc.: 54.69%] [Generator loss: 0.997833]\n",
      "2977 [Discriminator loss: 0.456818, acc.: 81.25%] [Generator loss: 1.023556]\n",
      "2978 [Discriminator loss: 0.782371, acc.: 49.22%] [Generator loss: 1.034551]\n",
      "2979 [Discriminator loss: 0.772951, acc.: 52.34%] [Generator loss: 1.228344]\n",
      "2980 [Discriminator loss: 0.598194, acc.: 64.84%] [Generator loss: 1.033999]\n",
      "2981 [Discriminator loss: 0.891801, acc.: 42.19%] [Generator loss: 1.262515]\n",
      "2982 [Discriminator loss: 0.836855, acc.: 45.31%] [Generator loss: 1.478637]\n",
      "2983 [Discriminator loss: 0.745998, acc.: 57.81%] [Generator loss: 1.747414]\n",
      "2984 [Discriminator loss: 0.645907, acc.: 61.72%] [Generator loss: 1.230560]\n",
      "2985 [Discriminator loss: 0.805360, acc.: 50.00%] [Generator loss: 1.256373]\n",
      "2986 [Discriminator loss: 0.617925, acc.: 65.62%] [Generator loss: 1.185009]\n",
      "2987 [Discriminator loss: 0.785641, acc.: 52.34%] [Generator loss: 1.234411]\n",
      "2988 [Discriminator loss: 0.711682, acc.: 57.03%] [Generator loss: 1.120106]\n",
      "2989 [Discriminator loss: 0.479400, acc.: 78.91%] [Generator loss: 0.889456]\n",
      "2990 [Discriminator loss: 0.698930, acc.: 60.16%] [Generator loss: 1.371005]\n",
      "2991 [Discriminator loss: 0.684278, acc.: 57.81%] [Generator loss: 1.383686]\n",
      "2992 [Discriminator loss: 0.345388, acc.: 90.62%] [Generator loss: 1.573847]\n",
      "2993 [Discriminator loss: 1.044058, acc.: 36.72%] [Generator loss: 1.263522]\n",
      "2994 [Discriminator loss: 0.569526, acc.: 69.53%] [Generator loss: 1.257637]\n",
      "2995 [Discriminator loss: 0.853953, acc.: 48.44%] [Generator loss: 1.105739]\n",
      "2996 [Discriminator loss: 0.903274, acc.: 40.62%] [Generator loss: 1.050951]\n",
      "2997 [Discriminator loss: 0.848671, acc.: 43.75%] [Generator loss: 0.953185]\n",
      "2998 [Discriminator loss: 0.892455, acc.: 43.75%] [Generator loss: 0.994495]\n",
      "2999 [Discriminator loss: 0.501749, acc.: 77.34%] [Generator loss: 1.539139]\n",
      "3000 [Discriminator loss: 0.904926, acc.: 44.53%] [Generator loss: 1.164177]\n",
      "3001 [Discriminator loss: 0.803993, acc.: 49.22%] [Generator loss: 1.215747]\n",
      "3002 [Discriminator loss: 0.743192, acc.: 52.34%] [Generator loss: 0.925092]\n",
      "3003 [Discriminator loss: 0.897557, acc.: 46.88%] [Generator loss: 1.147051]\n",
      "3004 [Discriminator loss: 0.727574, acc.: 60.94%] [Generator loss: 1.225395]\n",
      "3005 [Discriminator loss: 0.970096, acc.: 39.06%] [Generator loss: 1.026922]\n",
      "3006 [Discriminator loss: 0.828063, acc.: 53.12%] [Generator loss: 1.169777]\n",
      "3007 [Discriminator loss: 0.972979, acc.: 43.75%] [Generator loss: 0.925464]\n",
      "3008 [Discriminator loss: 0.921574, acc.: 34.38%] [Generator loss: 1.494962]\n",
      "3009 [Discriminator loss: 0.635310, acc.: 64.06%] [Generator loss: 1.499127]\n",
      "3010 [Discriminator loss: 0.911127, acc.: 51.56%] [Generator loss: 1.063986]\n",
      "3011 [Discriminator loss: 0.860360, acc.: 42.97%] [Generator loss: 1.160798]\n",
      "3012 [Discriminator loss: 0.762134, acc.: 51.56%] [Generator loss: 1.375059]\n",
      "3013 [Discriminator loss: 0.767382, acc.: 53.91%] [Generator loss: 1.313930]\n",
      "3014 [Discriminator loss: 0.883554, acc.: 42.19%] [Generator loss: 1.110184]\n",
      "3015 [Discriminator loss: 0.898773, acc.: 42.97%] [Generator loss: 1.222647]\n",
      "3016 [Discriminator loss: 0.778733, acc.: 46.88%] [Generator loss: 1.305901]\n",
      "3017 [Discriminator loss: 0.754124, acc.: 55.47%] [Generator loss: 1.082281]\n",
      "3018 [Discriminator loss: 0.797266, acc.: 48.44%] [Generator loss: 1.417631]\n",
      "3019 [Discriminator loss: 0.884832, acc.: 50.00%] [Generator loss: 1.200668]\n",
      "3020 [Discriminator loss: 0.800462, acc.: 48.44%] [Generator loss: 1.462933]\n",
      "3021 [Discriminator loss: 0.699672, acc.: 53.91%] [Generator loss: 1.470373]\n",
      "3022 [Discriminator loss: 0.697605, acc.: 58.59%] [Generator loss: 1.165626]\n",
      "3023 [Discriminator loss: 0.794351, acc.: 50.78%] [Generator loss: 1.039314]\n",
      "3024 [Discriminator loss: 0.690595, acc.: 60.16%] [Generator loss: 1.289499]\n",
      "3025 [Discriminator loss: 0.761560, acc.: 55.47%] [Generator loss: 1.325466]\n",
      "3026 [Discriminator loss: 0.779048, acc.: 50.00%] [Generator loss: 1.568476]\n",
      "3027 [Discriminator loss: 0.684922, acc.: 60.94%] [Generator loss: 1.355826]\n",
      "3028 [Discriminator loss: 0.641437, acc.: 65.62%] [Generator loss: 1.222250]\n",
      "3029 [Discriminator loss: 0.624870, acc.: 69.53%] [Generator loss: 0.859118]\n",
      "3030 [Discriminator loss: 0.615062, acc.: 64.06%] [Generator loss: 1.105058]\n",
      "3031 [Discriminator loss: 0.539257, acc.: 75.00%] [Generator loss: 1.201458]\n",
      "3032 [Discriminator loss: 0.522784, acc.: 73.44%] [Generator loss: 0.968259]\n",
      "3033 [Discriminator loss: 0.472112, acc.: 82.03%] [Generator loss: 0.841040]\n",
      "3034 [Discriminator loss: 0.700715, acc.: 60.94%] [Generator loss: 1.000383]\n",
      "3035 [Discriminator loss: 0.315791, acc.: 88.28%] [Generator loss: 0.778022]\n",
      "3036 [Discriminator loss: 0.333879, acc.: 85.16%] [Generator loss: 0.571133]\n",
      "3037 [Discriminator loss: 0.683554, acc.: 60.16%] [Generator loss: 0.830395]\n",
      "3038 [Discriminator loss: 0.613512, acc.: 62.50%] [Generator loss: 0.901070]\n",
      "3039 [Discriminator loss: 1.429403, acc.: 18.75%] [Generator loss: 1.201928]\n",
      "3040 [Discriminator loss: 1.302642, acc.: 28.12%] [Generator loss: 1.626696]\n",
      "3041 [Discriminator loss: 0.669427, acc.: 60.16%] [Generator loss: 1.575375]\n",
      "3042 [Discriminator loss: 0.864939, acc.: 46.09%] [Generator loss: 1.537091]\n",
      "3043 [Discriminator loss: 0.912079, acc.: 40.62%] [Generator loss: 1.195127]\n",
      "3044 [Discriminator loss: 0.805464, acc.: 52.34%] [Generator loss: 1.369798]\n",
      "3045 [Discriminator loss: 0.764155, acc.: 53.12%] [Generator loss: 1.345598]\n",
      "3046 [Discriminator loss: 0.715032, acc.: 64.84%] [Generator loss: 1.198809]\n",
      "3047 [Discriminator loss: 0.950894, acc.: 40.62%] [Generator loss: 1.038905]\n",
      "3048 [Discriminator loss: 0.830304, acc.: 44.53%] [Generator loss: 0.992758]\n",
      "3049 [Discriminator loss: 0.731936, acc.: 60.94%] [Generator loss: 0.929559]\n",
      "3050 [Discriminator loss: 0.679046, acc.: 60.94%] [Generator loss: 1.061163]\n",
      "3051 [Discriminator loss: 0.724234, acc.: 53.91%] [Generator loss: 1.254698]\n",
      "3052 [Discriminator loss: 0.651804, acc.: 68.75%] [Generator loss: 1.031939]\n",
      "3053 [Discriminator loss: 0.863488, acc.: 46.09%] [Generator loss: 1.187206]\n",
      "3054 [Discriminator loss: 0.621501, acc.: 65.62%] [Generator loss: 0.920895]\n",
      "3055 [Discriminator loss: 0.827871, acc.: 48.44%] [Generator loss: 0.952945]\n",
      "3056 [Discriminator loss: 0.697134, acc.: 57.81%] [Generator loss: 0.988424]\n",
      "3057 [Discriminator loss: 0.703680, acc.: 54.69%] [Generator loss: 0.950023]\n",
      "3058 [Discriminator loss: 0.752120, acc.: 58.59%] [Generator loss: 1.020890]\n",
      "3059 [Discriminator loss: 0.782676, acc.: 57.03%] [Generator loss: 1.418758]\n",
      "3060 [Discriminator loss: 0.772497, acc.: 56.25%] [Generator loss: 1.154143]\n",
      "3061 [Discriminator loss: 0.632176, acc.: 64.84%] [Generator loss: 1.194610]\n",
      "3062 [Discriminator loss: 0.715903, acc.: 58.59%] [Generator loss: 1.478003]\n",
      "3063 [Discriminator loss: 0.767400, acc.: 50.78%] [Generator loss: 1.484296]\n",
      "3064 [Discriminator loss: 0.702162, acc.: 61.72%] [Generator loss: 0.970294]\n",
      "3065 [Discriminator loss: 0.534883, acc.: 74.22%] [Generator loss: 0.662506]\n",
      "3066 [Discriminator loss: 0.702709, acc.: 59.38%] [Generator loss: 1.088480]\n",
      "3067 [Discriminator loss: 1.013314, acc.: 37.50%] [Generator loss: 1.887387]\n",
      "3068 [Discriminator loss: 0.561035, acc.: 72.66%] [Generator loss: 2.057545]\n",
      "3069 [Discriminator loss: 1.093944, acc.: 28.12%] [Generator loss: 0.872502]\n",
      "3070 [Discriminator loss: 0.455603, acc.: 79.69%] [Generator loss: 1.262648]\n",
      "3071 [Discriminator loss: 0.545256, acc.: 70.31%] [Generator loss: 1.225885]\n",
      "3072 [Discriminator loss: 0.444166, acc.: 85.16%] [Generator loss: 1.037615]\n",
      "3073 [Discriminator loss: 0.588602, acc.: 69.53%] [Generator loss: 0.840221]\n",
      "3074 [Discriminator loss: 0.802314, acc.: 48.44%] [Generator loss: 0.970181]\n",
      "3075 [Discriminator loss: 0.457981, acc.: 81.25%] [Generator loss: 1.200871]\n",
      "3076 [Discriminator loss: 0.762576, acc.: 53.12%] [Generator loss: 1.362171]\n",
      "3077 [Discriminator loss: 0.659875, acc.: 63.28%] [Generator loss: 1.339616]\n",
      "3078 [Discriminator loss: 0.926528, acc.: 38.28%] [Generator loss: 0.956095]\n",
      "3079 [Discriminator loss: 0.706642, acc.: 56.25%] [Generator loss: 0.938890]\n",
      "3080 [Discriminator loss: 0.743637, acc.: 52.34%] [Generator loss: 1.053531]\n",
      "3081 [Discriminator loss: 0.608173, acc.: 67.97%] [Generator loss: 1.001519]\n",
      "3082 [Discriminator loss: 0.698457, acc.: 60.94%] [Generator loss: 1.157790]\n",
      "3083 [Discriminator loss: 0.594739, acc.: 69.53%] [Generator loss: 1.377040]\n",
      "3084 [Discriminator loss: 0.642799, acc.: 60.94%] [Generator loss: 1.286043]\n",
      "3085 [Discriminator loss: 0.693511, acc.: 57.03%] [Generator loss: 1.465279]\n",
      "3086 [Discriminator loss: 0.640504, acc.: 64.06%] [Generator loss: 1.448020]\n",
      "3087 [Discriminator loss: 0.398148, acc.: 84.38%] [Generator loss: 0.752492]\n",
      "3088 [Discriminator loss: 0.834949, acc.: 53.12%] [Generator loss: 0.826053]\n",
      "3089 [Discriminator loss: 0.442247, acc.: 78.91%] [Generator loss: 0.892262]\n",
      "3090 [Discriminator loss: 1.118369, acc.: 27.34%] [Generator loss: 0.864738]\n",
      "3091 [Discriminator loss: 0.963265, acc.: 40.62%] [Generator loss: 0.880440]\n",
      "3092 [Discriminator loss: 1.387069, acc.: 17.97%] [Generator loss: 1.022689]\n",
      "3093 [Discriminator loss: 0.557737, acc.: 74.22%] [Generator loss: 1.545387]\n",
      "3094 [Discriminator loss: 0.739574, acc.: 61.72%] [Generator loss: 1.606015]\n",
      "3095 [Discriminator loss: 0.780079, acc.: 54.69%] [Generator loss: 1.553760]\n",
      "3096 [Discriminator loss: 0.815295, acc.: 50.78%] [Generator loss: 1.269322]\n",
      "3097 [Discriminator loss: 0.652499, acc.: 60.94%] [Generator loss: 1.543459]\n",
      "3098 [Discriminator loss: 0.601497, acc.: 65.62%] [Generator loss: 1.215697]\n",
      "3099 [Discriminator loss: 0.888991, acc.: 50.00%] [Generator loss: 1.240577]\n",
      "3100 [Discriminator loss: 0.692453, acc.: 57.03%] [Generator loss: 1.181171]\n",
      "3101 [Discriminator loss: 0.763748, acc.: 56.25%] [Generator loss: 1.199399]\n",
      "3102 [Discriminator loss: 0.530423, acc.: 75.00%] [Generator loss: 1.361512]\n",
      "3103 [Discriminator loss: 0.715619, acc.: 60.16%] [Generator loss: 1.422075]\n",
      "3104 [Discriminator loss: 0.684355, acc.: 61.72%] [Generator loss: 1.263261]\n",
      "3105 [Discriminator loss: 0.713568, acc.: 63.28%] [Generator loss: 1.392953]\n",
      "3106 [Discriminator loss: 0.867858, acc.: 45.31%] [Generator loss: 1.128272]\n",
      "3107 [Discriminator loss: 0.848604, acc.: 49.22%] [Generator loss: 1.278967]\n",
      "3108 [Discriminator loss: 0.658943, acc.: 61.72%] [Generator loss: 1.373584]\n",
      "3109 [Discriminator loss: 1.005947, acc.: 38.28%] [Generator loss: 1.215812]\n",
      "3110 [Discriminator loss: 0.870939, acc.: 46.09%] [Generator loss: 1.046117]\n",
      "3111 [Discriminator loss: 0.720786, acc.: 57.03%] [Generator loss: 1.145947]\n",
      "3112 [Discriminator loss: 0.730370, acc.: 60.16%] [Generator loss: 1.264377]\n",
      "3113 [Discriminator loss: 0.730337, acc.: 58.59%] [Generator loss: 1.034645]\n",
      "3114 [Discriminator loss: 0.661502, acc.: 60.94%] [Generator loss: 0.849765]\n",
      "3115 [Discriminator loss: 0.721522, acc.: 59.38%] [Generator loss: 1.176754]\n",
      "3116 [Discriminator loss: 0.528548, acc.: 70.31%] [Generator loss: 1.389038]\n",
      "3117 [Discriminator loss: 0.765280, acc.: 50.78%] [Generator loss: 1.752216]\n",
      "3118 [Discriminator loss: 0.665579, acc.: 65.62%] [Generator loss: 1.602172]\n",
      "3119 [Discriminator loss: 1.036039, acc.: 43.75%] [Generator loss: 1.055976]\n",
      "3120 [Discriminator loss: 0.731168, acc.: 58.59%] [Generator loss: 1.263942]\n",
      "3121 [Discriminator loss: 0.820090, acc.: 49.22%] [Generator loss: 1.693750]\n",
      "3122 [Discriminator loss: 0.726930, acc.: 57.03%] [Generator loss: 1.598927]\n",
      "3123 [Discriminator loss: 0.457223, acc.: 80.47%] [Generator loss: 1.635224]\n",
      "3124 [Discriminator loss: 0.842578, acc.: 42.97%] [Generator loss: 1.517716]\n",
      "3125 [Discriminator loss: 0.526025, acc.: 76.56%] [Generator loss: 1.275407]\n",
      "3126 [Discriminator loss: 0.784822, acc.: 50.00%] [Generator loss: 1.248613]\n",
      "3127 [Discriminator loss: 0.453168, acc.: 78.12%] [Generator loss: 1.179463]\n",
      "3128 [Discriminator loss: 0.765439, acc.: 50.00%] [Generator loss: 1.093991]\n",
      "3129 [Discriminator loss: 0.576021, acc.: 73.44%] [Generator loss: 1.225585]\n",
      "3130 [Discriminator loss: 0.603363, acc.: 64.84%] [Generator loss: 1.114002]\n",
      "3131 [Discriminator loss: 0.824843, acc.: 47.66%] [Generator loss: 0.940841]\n",
      "3132 [Discriminator loss: 0.520304, acc.: 80.47%] [Generator loss: 0.997893]\n",
      "3133 [Discriminator loss: 0.845338, acc.: 46.09%] [Generator loss: 1.323096]\n",
      "3134 [Discriminator loss: 0.706778, acc.: 61.72%] [Generator loss: 1.136248]\n",
      "3135 [Discriminator loss: 0.823604, acc.: 47.66%] [Generator loss: 1.219221]\n",
      "3136 [Discriminator loss: 0.637619, acc.: 60.16%] [Generator loss: 1.429810]\n",
      "3137 [Discriminator loss: 0.875917, acc.: 42.19%] [Generator loss: 1.179034]\n",
      "3138 [Discriminator loss: 0.940985, acc.: 39.06%] [Generator loss: 1.053251]\n",
      "3139 [Discriminator loss: 0.852614, acc.: 44.53%] [Generator loss: 1.242490]\n",
      "3140 [Discriminator loss: 0.615929, acc.: 71.88%] [Generator loss: 1.401562]\n",
      "3141 [Discriminator loss: 0.611831, acc.: 66.41%] [Generator loss: 1.243050]\n",
      "3142 [Discriminator loss: 1.009500, acc.: 35.16%] [Generator loss: 1.219574]\n",
      "3143 [Discriminator loss: 0.589037, acc.: 65.62%] [Generator loss: 1.296800]\n",
      "3144 [Discriminator loss: 0.513584, acc.: 70.31%] [Generator loss: 0.874006]\n",
      "3145 [Discriminator loss: 0.541890, acc.: 73.44%] [Generator loss: 0.810038]\n",
      "3146 [Discriminator loss: 0.718197, acc.: 53.12%] [Generator loss: 1.134675]\n",
      "3147 [Discriminator loss: 0.617096, acc.: 65.62%] [Generator loss: 1.402681]\n",
      "3148 [Discriminator loss: 0.624684, acc.: 64.84%] [Generator loss: 1.231668]\n",
      "3149 [Discriminator loss: 0.751347, acc.: 60.94%] [Generator loss: 1.354802]\n",
      "3150 [Discriminator loss: 0.547107, acc.: 72.66%] [Generator loss: 1.675326]\n",
      "3151 [Discriminator loss: 0.686840, acc.: 64.84%] [Generator loss: 1.532074]\n",
      "3152 [Discriminator loss: 0.736251, acc.: 57.81%] [Generator loss: 1.290833]\n",
      "3153 [Discriminator loss: 0.726509, acc.: 57.81%] [Generator loss: 1.569826]\n",
      "3154 [Discriminator loss: 0.855766, acc.: 45.31%] [Generator loss: 1.428433]\n",
      "3155 [Discriminator loss: 0.592534, acc.: 67.19%] [Generator loss: 1.580123]\n",
      "3156 [Discriminator loss: 0.664417, acc.: 60.16%] [Generator loss: 1.426348]\n",
      "3157 [Discriminator loss: 0.656093, acc.: 61.72%] [Generator loss: 1.375899]\n",
      "3158 [Discriminator loss: 0.774133, acc.: 56.25%] [Generator loss: 1.256015]\n",
      "3159 [Discriminator loss: 0.815612, acc.: 53.12%] [Generator loss: 1.145349]\n",
      "3160 [Discriminator loss: 0.772686, acc.: 52.34%] [Generator loss: 1.129951]\n",
      "3161 [Discriminator loss: 0.736575, acc.: 60.94%] [Generator loss: 1.025740]\n",
      "3162 [Discriminator loss: 0.597086, acc.: 67.19%] [Generator loss: 0.751599]\n",
      "3163 [Discriminator loss: 0.606671, acc.: 65.62%] [Generator loss: 0.847986]\n",
      "3164 [Discriminator loss: 0.548365, acc.: 70.31%] [Generator loss: 0.381639]\n",
      "3165 [Discriminator loss: 0.808608, acc.: 50.78%] [Generator loss: 0.862694]\n",
      "3166 [Discriminator loss: 0.639645, acc.: 67.97%] [Generator loss: 1.750266]\n",
      "3167 [Discriminator loss: 1.198518, acc.: 51.56%] [Generator loss: 0.687057]\n",
      "3168 [Discriminator loss: 0.720000, acc.: 64.06%] [Generator loss: 0.966246]\n",
      "3169 [Discriminator loss: 0.820476, acc.: 49.22%] [Generator loss: 1.286301]\n",
      "3170 [Discriminator loss: 1.096431, acc.: 33.59%] [Generator loss: 1.465305]\n",
      "3171 [Discriminator loss: 0.853977, acc.: 55.47%] [Generator loss: 1.152251]\n",
      "3172 [Discriminator loss: 0.776796, acc.: 49.22%] [Generator loss: 1.619243]\n",
      "3173 [Discriminator loss: 0.897911, acc.: 42.97%] [Generator loss: 1.261379]\n",
      "3174 [Discriminator loss: 0.800747, acc.: 53.91%] [Generator loss: 1.445524]\n",
      "3175 [Discriminator loss: 0.849701, acc.: 47.66%] [Generator loss: 1.419373]\n",
      "3176 [Discriminator loss: 0.694063, acc.: 59.38%] [Generator loss: 1.294889]\n",
      "3177 [Discriminator loss: 0.745800, acc.: 54.69%] [Generator loss: 1.303221]\n",
      "3178 [Discriminator loss: 0.822272, acc.: 50.00%] [Generator loss: 1.397887]\n",
      "3179 [Discriminator loss: 0.776973, acc.: 56.25%] [Generator loss: 1.470510]\n",
      "3180 [Discriminator loss: 0.878406, acc.: 42.19%] [Generator loss: 0.987821]\n",
      "3181 [Discriminator loss: 0.614344, acc.: 69.53%] [Generator loss: 1.238183]\n",
      "3182 [Discriminator loss: 0.578801, acc.: 74.22%] [Generator loss: 1.580041]\n",
      "3183 [Discriminator loss: 0.609555, acc.: 67.19%] [Generator loss: 1.565939]\n",
      "3184 [Discriminator loss: 0.680872, acc.: 64.84%] [Generator loss: 1.280689]\n",
      "3185 [Discriminator loss: 1.055619, acc.: 34.38%] [Generator loss: 0.996427]\n",
      "3186 [Discriminator loss: 0.613548, acc.: 62.50%] [Generator loss: 1.257181]\n",
      "3187 [Discriminator loss: 0.835364, acc.: 46.88%] [Generator loss: 1.354669]\n",
      "3188 [Discriminator loss: 0.654262, acc.: 58.59%] [Generator loss: 1.275528]\n",
      "3189 [Discriminator loss: 0.687255, acc.: 64.06%] [Generator loss: 1.686271]\n",
      "3190 [Discriminator loss: 0.700288, acc.: 59.38%] [Generator loss: 1.240071]\n",
      "3191 [Discriminator loss: 0.697146, acc.: 60.16%] [Generator loss: 1.116215]\n",
      "3192 [Discriminator loss: 0.736454, acc.: 57.03%] [Generator loss: 1.056326]\n",
      "3193 [Discriminator loss: 0.579182, acc.: 64.84%] [Generator loss: 1.128870]\n",
      "3194 [Discriminator loss: 0.566980, acc.: 73.44%] [Generator loss: 1.480089]\n",
      "3195 [Discriminator loss: 0.540729, acc.: 75.00%] [Generator loss: 1.276717]\n",
      "3196 [Discriminator loss: 0.563514, acc.: 71.88%] [Generator loss: 1.349501]\n",
      "3197 [Discriminator loss: 0.960081, acc.: 40.62%] [Generator loss: 1.215593]\n",
      "3198 [Discriminator loss: 0.629521, acc.: 64.84%] [Generator loss: 1.337687]\n",
      "3199 [Discriminator loss: 0.652340, acc.: 64.06%] [Generator loss: 1.372439]\n",
      "3200 [Discriminator loss: 0.733327, acc.: 56.25%] [Generator loss: 1.575381]\n",
      "3201 [Discriminator loss: 0.616332, acc.: 71.88%] [Generator loss: 1.442902]\n",
      "3202 [Discriminator loss: 1.107678, acc.: 29.69%] [Generator loss: 1.079771]\n",
      "3203 [Discriminator loss: 0.597508, acc.: 67.97%] [Generator loss: 1.316611]\n",
      "3204 [Discriminator loss: 0.723933, acc.: 59.38%] [Generator loss: 1.233821]\n",
      "3205 [Discriminator loss: 0.864924, acc.: 45.31%] [Generator loss: 1.180642]\n",
      "3206 [Discriminator loss: 0.933794, acc.: 40.62%] [Generator loss: 1.090199]\n",
      "3207 [Discriminator loss: 0.828476, acc.: 45.31%] [Generator loss: 1.284045]\n",
      "3208 [Discriminator loss: 0.807452, acc.: 47.66%] [Generator loss: 1.510514]\n",
      "3209 [Discriminator loss: 0.773372, acc.: 50.00%] [Generator loss: 1.462772]\n",
      "3210 [Discriminator loss: 0.806955, acc.: 52.34%] [Generator loss: 1.046446]\n",
      "3211 [Discriminator loss: 0.810904, acc.: 46.88%] [Generator loss: 1.055132]\n",
      "3212 [Discriminator loss: 0.762401, acc.: 57.03%] [Generator loss: 1.149696]\n",
      "3213 [Discriminator loss: 0.838813, acc.: 48.44%] [Generator loss: 1.090685]\n",
      "3214 [Discriminator loss: 0.620450, acc.: 60.94%] [Generator loss: 1.345838]\n",
      "3215 [Discriminator loss: 0.798866, acc.: 52.34%] [Generator loss: 1.061967]\n",
      "3216 [Discriminator loss: 0.776613, acc.: 47.66%] [Generator loss: 1.492642]\n",
      "3217 [Discriminator loss: 0.867427, acc.: 47.66%] [Generator loss: 1.565878]\n",
      "3218 [Discriminator loss: 0.677523, acc.: 64.06%] [Generator loss: 1.350504]\n",
      "3219 [Discriminator loss: 0.605283, acc.: 64.06%] [Generator loss: 1.453399]\n",
      "3220 [Discriminator loss: 0.730153, acc.: 55.47%] [Generator loss: 1.546813]\n",
      "3221 [Discriminator loss: 0.810012, acc.: 56.25%] [Generator loss: 1.210668]\n",
      "3222 [Discriminator loss: 1.441175, acc.: 17.19%] [Generator loss: 1.128743]\n",
      "3223 [Discriminator loss: 0.786502, acc.: 47.66%] [Generator loss: 1.241257]\n",
      "3224 [Discriminator loss: 0.763911, acc.: 51.56%] [Generator loss: 1.091890]\n",
      "3225 [Discriminator loss: 0.616542, acc.: 66.41%] [Generator loss: 1.046126]\n",
      "3226 [Discriminator loss: 0.923558, acc.: 42.97%] [Generator loss: 1.196244]\n",
      "3227 [Discriminator loss: 0.933034, acc.: 38.28%] [Generator loss: 0.965069]\n",
      "3228 [Discriminator loss: 0.719014, acc.: 54.69%] [Generator loss: 1.336182]\n",
      "3229 [Discriminator loss: 0.760733, acc.: 55.47%] [Generator loss: 1.321106]\n",
      "3230 [Discriminator loss: 0.740276, acc.: 52.34%] [Generator loss: 1.272408]\n",
      "3231 [Discriminator loss: 0.798576, acc.: 51.56%] [Generator loss: 1.208076]\n",
      "3232 [Discriminator loss: 0.629280, acc.: 66.41%] [Generator loss: 0.986492]\n",
      "3233 [Discriminator loss: 0.667072, acc.: 62.50%] [Generator loss: 0.921877]\n",
      "3234 [Discriminator loss: 0.754067, acc.: 53.91%] [Generator loss: 1.211891]\n",
      "3235 [Discriminator loss: 0.724048, acc.: 58.59%] [Generator loss: 1.178625]\n",
      "3236 [Discriminator loss: 0.797060, acc.: 52.34%] [Generator loss: 1.005360]\n",
      "3237 [Discriminator loss: 0.823455, acc.: 47.66%] [Generator loss: 1.200370]\n",
      "3238 [Discriminator loss: 0.634869, acc.: 67.19%] [Generator loss: 1.179967]\n",
      "3239 [Discriminator loss: 0.667484, acc.: 59.38%] [Generator loss: 1.156999]\n",
      "3240 [Discriminator loss: 0.742299, acc.: 53.12%] [Generator loss: 1.138918]\n",
      "3241 [Discriminator loss: 0.768184, acc.: 51.56%] [Generator loss: 1.282819]\n",
      "3242 [Discriminator loss: 0.763648, acc.: 55.47%] [Generator loss: 1.099187]\n",
      "3243 [Discriminator loss: 0.775425, acc.: 51.56%] [Generator loss: 1.225248]\n",
      "3244 [Discriminator loss: 0.685207, acc.: 61.72%] [Generator loss: 1.075500]\n",
      "3245 [Discriminator loss: 0.720667, acc.: 57.03%] [Generator loss: 1.292809]\n",
      "3246 [Discriminator loss: 0.577744, acc.: 70.31%] [Generator loss: 1.477172]\n",
      "3247 [Discriminator loss: 0.591264, acc.: 65.62%] [Generator loss: 1.377888]\n",
      "3248 [Discriminator loss: 0.809808, acc.: 47.66%] [Generator loss: 1.286291]\n",
      "3249 [Discriminator loss: 0.603630, acc.: 68.75%] [Generator loss: 1.484929]\n",
      "3250 [Discriminator loss: 0.808452, acc.: 48.44%] [Generator loss: 1.186886]\n",
      "3251 [Discriminator loss: 0.799347, acc.: 51.56%] [Generator loss: 1.408444]\n",
      "3252 [Discriminator loss: 0.699991, acc.: 56.25%] [Generator loss: 1.419000]\n",
      "3253 [Discriminator loss: 0.704703, acc.: 62.50%] [Generator loss: 1.308964]\n",
      "3254 [Discriminator loss: 0.734133, acc.: 53.91%] [Generator loss: 1.077769]\n",
      "3255 [Discriminator loss: 0.846744, acc.: 49.22%] [Generator loss: 1.114713]\n",
      "3256 [Discriminator loss: 0.889920, acc.: 42.19%] [Generator loss: 0.850048]\n",
      "3257 [Discriminator loss: 0.593560, acc.: 70.31%] [Generator loss: 1.041162]\n",
      "3258 [Discriminator loss: 0.561944, acc.: 75.00%] [Generator loss: 0.858815]\n",
      "3259 [Discriminator loss: 0.814794, acc.: 49.22%] [Generator loss: 0.851976]\n",
      "3260 [Discriminator loss: 0.654188, acc.: 64.06%] [Generator loss: 1.234573]\n",
      "3261 [Discriminator loss: 0.863114, acc.: 46.88%] [Generator loss: 0.908581]\n",
      "3262 [Discriminator loss: 0.588346, acc.: 63.28%] [Generator loss: 1.080316]\n",
      "3263 [Discriminator loss: 0.804632, acc.: 54.69%] [Generator loss: 1.097008]\n",
      "3264 [Discriminator loss: 0.705860, acc.: 60.94%] [Generator loss: 1.059658]\n",
      "3265 [Discriminator loss: 0.919940, acc.: 35.16%] [Generator loss: 0.834066]\n",
      "3266 [Discriminator loss: 0.652550, acc.: 64.06%] [Generator loss: 1.113833]\n",
      "3267 [Discriminator loss: 0.755958, acc.: 52.34%] [Generator loss: 1.095076]\n",
      "3268 [Discriminator loss: 0.669962, acc.: 64.06%] [Generator loss: 1.140836]\n",
      "3269 [Discriminator loss: 0.736021, acc.: 57.03%] [Generator loss: 1.002465]\n",
      "3270 [Discriminator loss: 0.844150, acc.: 46.88%] [Generator loss: 1.048786]\n",
      "3271 [Discriminator loss: 0.764097, acc.: 54.69%] [Generator loss: 1.199109]\n",
      "3272 [Discriminator loss: 0.851536, acc.: 41.41%] [Generator loss: 1.346898]\n",
      "3273 [Discriminator loss: 0.778417, acc.: 52.34%] [Generator loss: 1.027235]\n",
      "3274 [Discriminator loss: 0.730408, acc.: 55.47%] [Generator loss: 0.994898]\n",
      "3275 [Discriminator loss: 0.762502, acc.: 52.34%] [Generator loss: 0.820963]\n",
      "3276 [Discriminator loss: 0.844489, acc.: 47.66%] [Generator loss: 1.136347]\n",
      "3277 [Discriminator loss: 0.517770, acc.: 74.22%] [Generator loss: 1.139617]\n",
      "3278 [Discriminator loss: 0.636989, acc.: 72.66%] [Generator loss: 0.739926]\n",
      "3279 [Discriminator loss: 0.448997, acc.: 82.81%] [Generator loss: 0.916184]\n",
      "3280 [Discriminator loss: 0.794461, acc.: 59.38%] [Generator loss: 1.014683]\n",
      "3281 [Discriminator loss: 0.596138, acc.: 71.88%] [Generator loss: 1.935462]\n",
      "3282 [Discriminator loss: 1.399447, acc.: 24.22%] [Generator loss: 1.340642]\n",
      "3283 [Discriminator loss: 1.271931, acc.: 24.22%] [Generator loss: 1.211436]\n",
      "3284 [Discriminator loss: 0.672572, acc.: 62.50%] [Generator loss: 1.437187]\n",
      "3285 [Discriminator loss: 0.834967, acc.: 47.66%] [Generator loss: 1.421460]\n",
      "3286 [Discriminator loss: 0.706886, acc.: 52.34%] [Generator loss: 1.115038]\n",
      "3287 [Discriminator loss: 0.712970, acc.: 57.81%] [Generator loss: 1.311102]\n",
      "3288 [Discriminator loss: 0.605036, acc.: 69.53%] [Generator loss: 1.195078]\n",
      "3289 [Discriminator loss: 0.578877, acc.: 72.66%] [Generator loss: 1.385406]\n",
      "3290 [Discriminator loss: 0.730136, acc.: 54.69%] [Generator loss: 1.391362]\n",
      "3291 [Discriminator loss: 0.691871, acc.: 51.56%] [Generator loss: 1.181397]\n",
      "3292 [Discriminator loss: 0.559506, acc.: 67.19%] [Generator loss: 1.424201]\n",
      "3293 [Discriminator loss: 0.647925, acc.: 65.62%] [Generator loss: 1.080578]\n",
      "3294 [Discriminator loss: 0.616822, acc.: 65.62%] [Generator loss: 1.108275]\n",
      "3295 [Discriminator loss: 0.701922, acc.: 57.81%] [Generator loss: 1.252192]\n",
      "3296 [Discriminator loss: 0.551891, acc.: 74.22%] [Generator loss: 1.215797]\n",
      "3297 [Discriminator loss: 0.704706, acc.: 58.59%] [Generator loss: 1.008139]\n",
      "3298 [Discriminator loss: 0.890987, acc.: 39.84%] [Generator loss: 0.998942]\n",
      "3299 [Discriminator loss: 0.612926, acc.: 62.50%] [Generator loss: 1.209309]\n",
      "3300 [Discriminator loss: 0.624087, acc.: 64.84%] [Generator loss: 1.214519]\n",
      "3301 [Discriminator loss: 0.518879, acc.: 72.66%] [Generator loss: 1.149286]\n",
      "3302 [Discriminator loss: 0.592832, acc.: 67.97%] [Generator loss: 0.961650]\n",
      "3303 [Discriminator loss: 0.474963, acc.: 78.91%] [Generator loss: 1.130197]\n",
      "3304 [Discriminator loss: 0.573564, acc.: 66.41%] [Generator loss: 1.064312]\n",
      "3305 [Discriminator loss: 0.457276, acc.: 81.25%] [Generator loss: 0.554231]\n",
      "3306 [Discriminator loss: 0.377045, acc.: 82.81%] [Generator loss: 0.561106]\n",
      "3307 [Discriminator loss: 0.530102, acc.: 74.22%] [Generator loss: 0.518530]\n",
      "3308 [Discriminator loss: 0.606010, acc.: 60.16%] [Generator loss: 1.313645]\n",
      "3309 [Discriminator loss: 0.620444, acc.: 70.31%] [Generator loss: 1.278021]\n",
      "3310 [Discriminator loss: 0.909323, acc.: 43.75%] [Generator loss: 0.941600]\n",
      "3311 [Discriminator loss: 0.880441, acc.: 39.84%] [Generator loss: 0.993271]\n",
      "3312 [Discriminator loss: 0.534950, acc.: 76.56%] [Generator loss: 1.545164]\n",
      "3313 [Discriminator loss: 0.956334, acc.: 39.84%] [Generator loss: 1.232454]\n",
      "3314 [Discriminator loss: 0.752883, acc.: 54.69%] [Generator loss: 1.552092]\n",
      "3315 [Discriminator loss: 0.962048, acc.: 45.31%] [Generator loss: 1.158448]\n",
      "3316 [Discriminator loss: 0.517274, acc.: 75.78%] [Generator loss: 1.399901]\n",
      "3317 [Discriminator loss: 0.902748, acc.: 44.53%] [Generator loss: 0.983504]\n",
      "3318 [Discriminator loss: 0.695864, acc.: 59.38%] [Generator loss: 0.973707]\n",
      "3319 [Discriminator loss: 0.969508, acc.: 40.62%] [Generator loss: 0.889378]\n",
      "3320 [Discriminator loss: 0.832551, acc.: 51.56%] [Generator loss: 0.856221]\n",
      "3321 [Discriminator loss: 0.771674, acc.: 50.78%] [Generator loss: 1.132465]\n",
      "3322 [Discriminator loss: 0.660372, acc.: 60.16%] [Generator loss: 0.934895]\n",
      "3323 [Discriminator loss: 0.687282, acc.: 59.38%] [Generator loss: 1.030276]\n",
      "3324 [Discriminator loss: 0.683179, acc.: 60.16%] [Generator loss: 1.028421]\n",
      "3325 [Discriminator loss: 0.804546, acc.: 50.78%] [Generator loss: 0.829365]\n",
      "3326 [Discriminator loss: 0.758458, acc.: 53.91%] [Generator loss: 1.203210]\n",
      "3327 [Discriminator loss: 0.653394, acc.: 63.28%] [Generator loss: 1.227388]\n",
      "3328 [Discriminator loss: 0.795557, acc.: 49.22%] [Generator loss: 1.037497]\n",
      "3329 [Discriminator loss: 0.799611, acc.: 52.34%] [Generator loss: 1.135062]\n",
      "3330 [Discriminator loss: 0.550912, acc.: 71.09%] [Generator loss: 0.976175]\n",
      "3331 [Discriminator loss: 0.704721, acc.: 55.47%] [Generator loss: 0.987877]\n",
      "3332 [Discriminator loss: 0.659087, acc.: 63.28%] [Generator loss: 0.930483]\n",
      "3333 [Discriminator loss: 0.507276, acc.: 72.66%] [Generator loss: 0.872838]\n",
      "3334 [Discriminator loss: 1.155192, acc.: 34.38%] [Generator loss: 1.164447]\n",
      "3335 [Discriminator loss: 0.551217, acc.: 71.88%] [Generator loss: 1.036830]\n",
      "3336 [Discriminator loss: 0.876040, acc.: 42.97%] [Generator loss: 1.350471]\n",
      "3337 [Discriminator loss: 0.921505, acc.: 36.72%] [Generator loss: 1.561692]\n",
      "3338 [Discriminator loss: 0.639695, acc.: 64.84%] [Generator loss: 1.443384]\n",
      "3339 [Discriminator loss: 0.560164, acc.: 71.09%] [Generator loss: 1.535198]\n",
      "3340 [Discriminator loss: 0.866111, acc.: 46.88%] [Generator loss: 1.351666]\n",
      "3341 [Discriminator loss: 0.649907, acc.: 65.62%] [Generator loss: 1.144907]\n",
      "3342 [Discriminator loss: 0.771545, acc.: 52.34%] [Generator loss: 1.415443]\n",
      "3343 [Discriminator loss: 0.583125, acc.: 64.84%] [Generator loss: 1.471280]\n",
      "3344 [Discriminator loss: 0.741820, acc.: 53.91%] [Generator loss: 1.338836]\n",
      "3345 [Discriminator loss: 0.601770, acc.: 68.75%] [Generator loss: 1.104211]\n",
      "3346 [Discriminator loss: 0.759497, acc.: 53.12%] [Generator loss: 1.216084]\n",
      "3347 [Discriminator loss: 0.687996, acc.: 60.16%] [Generator loss: 1.411012]\n",
      "3348 [Discriminator loss: 0.969287, acc.: 41.41%] [Generator loss: 1.257753]\n",
      "3349 [Discriminator loss: 0.762081, acc.: 50.78%] [Generator loss: 1.541499]\n",
      "3350 [Discriminator loss: 0.766788, acc.: 49.22%] [Generator loss: 1.138141]\n",
      "3351 [Discriminator loss: 0.981629, acc.: 40.62%] [Generator loss: 1.100498]\n",
      "3352 [Discriminator loss: 0.957538, acc.: 37.50%] [Generator loss: 1.208916]\n",
      "3353 [Discriminator loss: 0.690651, acc.: 61.72%] [Generator loss: 1.161543]\n",
      "3354 [Discriminator loss: 0.745279, acc.: 57.81%] [Generator loss: 1.016538]\n",
      "3355 [Discriminator loss: 0.686817, acc.: 62.50%] [Generator loss: 1.161083]\n",
      "3356 [Discriminator loss: 0.826244, acc.: 45.31%] [Generator loss: 1.336933]\n",
      "3357 [Discriminator loss: 0.668810, acc.: 66.41%] [Generator loss: 1.284556]\n",
      "3358 [Discriminator loss: 0.730322, acc.: 57.03%] [Generator loss: 1.054865]\n",
      "3359 [Discriminator loss: 0.887125, acc.: 42.97%] [Generator loss: 1.135675]\n",
      "3360 [Discriminator loss: 0.648589, acc.: 66.41%] [Generator loss: 1.617720]\n",
      "3361 [Discriminator loss: 0.758171, acc.: 57.03%] [Generator loss: 1.235098]\n",
      "3362 [Discriminator loss: 0.852454, acc.: 42.97%] [Generator loss: 0.995372]\n",
      "3363 [Discriminator loss: 1.050155, acc.: 34.38%] [Generator loss: 0.796835]\n",
      "3364 [Discriminator loss: 0.482503, acc.: 78.12%] [Generator loss: 0.778047]\n",
      "3365 [Discriminator loss: 0.869920, acc.: 49.22%] [Generator loss: 1.131267]\n",
      "3366 [Discriminator loss: 0.609391, acc.: 67.19%] [Generator loss: 1.192696]\n",
      "3367 [Discriminator loss: 0.670122, acc.: 61.72%] [Generator loss: 1.010763]\n",
      "3368 [Discriminator loss: 0.555961, acc.: 69.53%] [Generator loss: 0.570565]\n",
      "3369 [Discriminator loss: 1.155431, acc.: 42.19%] [Generator loss: 1.206745]\n",
      "3370 [Discriminator loss: 0.603953, acc.: 69.53%] [Generator loss: 1.498612]\n",
      "3371 [Discriminator loss: 0.868946, acc.: 50.00%] [Generator loss: 1.220814]\n",
      "3372 [Discriminator loss: 0.736580, acc.: 52.34%] [Generator loss: 1.043350]\n",
      "3373 [Discriminator loss: 0.654597, acc.: 60.94%] [Generator loss: 1.138139]\n",
      "3374 [Discriminator loss: 1.171219, acc.: 30.47%] [Generator loss: 1.006135]\n",
      "3375 [Discriminator loss: 0.690877, acc.: 57.81%] [Generator loss: 1.164670]\n",
      "3376 [Discriminator loss: 0.624329, acc.: 65.62%] [Generator loss: 1.253014]\n",
      "3377 [Discriminator loss: 0.745776, acc.: 51.56%] [Generator loss: 0.817594]\n",
      "3378 [Discriminator loss: 0.587318, acc.: 71.09%] [Generator loss: 0.907746]\n",
      "3379 [Discriminator loss: 0.482035, acc.: 77.34%] [Generator loss: 0.611985]\n",
      "3380 [Discriminator loss: 0.539086, acc.: 72.66%] [Generator loss: 0.556647]\n",
      "3381 [Discriminator loss: 0.311917, acc.: 91.41%] [Generator loss: 0.582341]\n",
      "3382 [Discriminator loss: 0.333637, acc.: 87.50%] [Generator loss: 0.339619]\n",
      "3383 [Discriminator loss: 0.516124, acc.: 76.56%] [Generator loss: 0.379264]\n",
      "3384 [Discriminator loss: 0.425672, acc.: 82.81%] [Generator loss: 0.435624]\n",
      "3385 [Discriminator loss: 0.789977, acc.: 46.88%] [Generator loss: 0.888474]\n",
      "3386 [Discriminator loss: 0.307247, acc.: 85.16%] [Generator loss: 0.924725]\n",
      "3387 [Discriminator loss: 1.637255, acc.: 20.31%] [Generator loss: 0.993721]\n",
      "3388 [Discriminator loss: 0.692089, acc.: 62.50%] [Generator loss: 1.756963]\n",
      "3389 [Discriminator loss: 0.809339, acc.: 47.66%] [Generator loss: 1.620416]\n",
      "3390 [Discriminator loss: 0.802191, acc.: 53.91%] [Generator loss: 1.187968]\n",
      "3391 [Discriminator loss: 0.602348, acc.: 67.19%] [Generator loss: 1.094753]\n",
      "3392 [Discriminator loss: 0.696043, acc.: 58.59%] [Generator loss: 1.125952]\n",
      "3393 [Discriminator loss: 0.418098, acc.: 80.47%] [Generator loss: 0.922242]\n",
      "3394 [Discriminator loss: 0.595896, acc.: 68.75%] [Generator loss: 0.743643]\n",
      "3395 [Discriminator loss: 0.453778, acc.: 79.69%] [Generator loss: 0.984615]\n",
      "3396 [Discriminator loss: 0.556671, acc.: 72.66%] [Generator loss: 0.881903]\n",
      "3397 [Discriminator loss: 0.928032, acc.: 45.31%] [Generator loss: 0.896195]\n",
      "3398 [Discriminator loss: 0.777423, acc.: 53.12%] [Generator loss: 0.611298]\n",
      "3399 [Discriminator loss: 0.644974, acc.: 62.50%] [Generator loss: 1.012773]\n",
      "3400 [Discriminator loss: 0.650355, acc.: 63.28%] [Generator loss: 1.382184]\n",
      "3401 [Discriminator loss: 0.632639, acc.: 63.28%] [Generator loss: 1.326167]\n",
      "3402 [Discriminator loss: 0.750576, acc.: 60.16%] [Generator loss: 1.275306]\n",
      "3403 [Discriminator loss: 0.643306, acc.: 64.84%] [Generator loss: 0.824781]\n",
      "3404 [Discriminator loss: 0.641610, acc.: 58.59%] [Generator loss: 0.844386]\n",
      "3405 [Discriminator loss: 0.521049, acc.: 72.66%] [Generator loss: 0.930830]\n",
      "3406 [Discriminator loss: 0.423924, acc.: 81.25%] [Generator loss: 0.566545]\n",
      "3407 [Discriminator loss: 0.742946, acc.: 49.22%] [Generator loss: 0.567430]\n",
      "3408 [Discriminator loss: 0.525993, acc.: 68.75%] [Generator loss: 1.126254]\n",
      "3409 [Discriminator loss: 0.358875, acc.: 87.50%] [Generator loss: 1.325943]\n",
      "3410 [Discriminator loss: 0.944491, acc.: 37.50%] [Generator loss: 1.479462]\n",
      "3411 [Discriminator loss: 0.691753, acc.: 63.28%] [Generator loss: 0.941508]\n",
      "3412 [Discriminator loss: 0.684582, acc.: 58.59%] [Generator loss: 0.967853]\n",
      "3413 [Discriminator loss: 0.829113, acc.: 52.34%] [Generator loss: 1.701189]\n",
      "3414 [Discriminator loss: 0.628928, acc.: 67.19%] [Generator loss: 1.441022]\n",
      "3415 [Discriminator loss: 0.802309, acc.: 54.69%] [Generator loss: 1.175974]\n",
      "3416 [Discriminator loss: 0.709208, acc.: 56.25%] [Generator loss: 0.972299]\n",
      "3417 [Discriminator loss: 0.930263, acc.: 46.09%] [Generator loss: 1.264483]\n",
      "3418 [Discriminator loss: 0.830476, acc.: 45.31%] [Generator loss: 1.405305]\n",
      "3419 [Discriminator loss: 0.733339, acc.: 62.50%] [Generator loss: 0.834709]\n",
      "3420 [Discriminator loss: 0.993001, acc.: 39.06%] [Generator loss: 0.867271]\n",
      "3421 [Discriminator loss: 0.572373, acc.: 64.84%] [Generator loss: 1.044998]\n",
      "3422 [Discriminator loss: 0.638543, acc.: 67.19%] [Generator loss: 1.254737]\n",
      "3423 [Discriminator loss: 1.164751, acc.: 29.69%] [Generator loss: 1.375372]\n",
      "3424 [Discriminator loss: 0.495758, acc.: 76.56%] [Generator loss: 1.215142]\n",
      "3425 [Discriminator loss: 0.766173, acc.: 55.47%] [Generator loss: 1.350038]\n",
      "3426 [Discriminator loss: 0.704074, acc.: 64.06%] [Generator loss: 0.819639]\n",
      "3427 [Discriminator loss: 0.866694, acc.: 44.53%] [Generator loss: 0.847325]\n",
      "3428 [Discriminator loss: 1.051753, acc.: 35.16%] [Generator loss: 1.080471]\n",
      "3429 [Discriminator loss: 0.791932, acc.: 50.78%] [Generator loss: 1.544127]\n",
      "3430 [Discriminator loss: 0.720373, acc.: 60.16%] [Generator loss: 1.530130]\n",
      "3431 [Discriminator loss: 0.757248, acc.: 57.81%] [Generator loss: 1.084645]\n",
      "3432 [Discriminator loss: 0.628794, acc.: 63.28%] [Generator loss: 1.364872]\n",
      "3433 [Discriminator loss: 0.797782, acc.: 50.78%] [Generator loss: 1.314842]\n",
      "3434 [Discriminator loss: 0.714011, acc.: 60.16%] [Generator loss: 1.336388]\n",
      "3435 [Discriminator loss: 0.601442, acc.: 66.41%] [Generator loss: 1.076994]\n",
      "3436 [Discriminator loss: 0.880500, acc.: 45.31%] [Generator loss: 1.113085]\n",
      "3437 [Discriminator loss: 0.670334, acc.: 55.47%] [Generator loss: 1.091249]\n",
      "3438 [Discriminator loss: 0.725130, acc.: 53.12%] [Generator loss: 1.267079]\n",
      "3439 [Discriminator loss: 0.752799, acc.: 54.69%] [Generator loss: 1.421683]\n",
      "3440 [Discriminator loss: 0.619317, acc.: 67.19%] [Generator loss: 1.148145]\n",
      "3441 [Discriminator loss: 0.593711, acc.: 68.75%] [Generator loss: 1.125568]\n",
      "3442 [Discriminator loss: 0.717176, acc.: 59.38%] [Generator loss: 1.202543]\n",
      "3443 [Discriminator loss: 0.854143, acc.: 45.31%] [Generator loss: 1.395488]\n",
      "3444 [Discriminator loss: 0.861635, acc.: 45.31%] [Generator loss: 1.478395]\n",
      "3445 [Discriminator loss: 0.515657, acc.: 71.88%] [Generator loss: 1.384969]\n",
      "3446 [Discriminator loss: 0.861089, acc.: 42.97%] [Generator loss: 1.283265]\n",
      "3447 [Discriminator loss: 0.578536, acc.: 67.19%] [Generator loss: 1.401816]\n",
      "3448 [Discriminator loss: 0.625762, acc.: 63.28%] [Generator loss: 1.402903]\n",
      "3449 [Discriminator loss: 0.865645, acc.: 42.97%] [Generator loss: 1.201807]\n",
      "3450 [Discriminator loss: 0.867133, acc.: 47.66%] [Generator loss: 1.167117]\n",
      "3451 [Discriminator loss: 0.592438, acc.: 75.00%] [Generator loss: 1.012852]\n",
      "3452 [Discriminator loss: 0.759040, acc.: 49.22%] [Generator loss: 1.099785]\n",
      "3453 [Discriminator loss: 0.872909, acc.: 46.88%] [Generator loss: 1.390891]\n",
      "3454 [Discriminator loss: 0.709178, acc.: 61.72%] [Generator loss: 1.519295]\n",
      "3455 [Discriminator loss: 0.632840, acc.: 63.28%] [Generator loss: 1.891132]\n",
      "3456 [Discriminator loss: 0.707334, acc.: 62.50%] [Generator loss: 1.821292]\n",
      "3457 [Discriminator loss: 0.521364, acc.: 80.47%] [Generator loss: 1.802202]\n",
      "3458 [Discriminator loss: 0.722784, acc.: 57.03%] [Generator loss: 1.745981]\n",
      "3459 [Discriminator loss: 0.500504, acc.: 75.00%] [Generator loss: 1.668544]\n",
      "3460 [Discriminator loss: 0.436431, acc.: 83.59%] [Generator loss: 1.253782]\n",
      "3461 [Discriminator loss: 0.604705, acc.: 66.41%] [Generator loss: 1.509235]\n",
      "3462 [Discriminator loss: 0.571001, acc.: 68.75%] [Generator loss: 1.771267]\n",
      "3463 [Discriminator loss: 0.641993, acc.: 63.28%] [Generator loss: 1.688883]\n",
      "3464 [Discriminator loss: 0.404428, acc.: 81.25%] [Generator loss: 0.980243]\n",
      "3465 [Discriminator loss: 0.714008, acc.: 54.69%] [Generator loss: 0.899943]\n",
      "3466 [Discriminator loss: 0.777671, acc.: 55.47%] [Generator loss: 1.370062]\n",
      "3467 [Discriminator loss: 0.425820, acc.: 85.16%] [Generator loss: 1.737113]\n",
      "3468 [Discriminator loss: 0.576313, acc.: 70.31%] [Generator loss: 0.973554]\n",
      "3469 [Discriminator loss: 0.738377, acc.: 60.94%] [Generator loss: 0.853930]\n",
      "3470 [Discriminator loss: 0.667019, acc.: 61.72%] [Generator loss: 1.088731]\n",
      "3471 [Discriminator loss: 0.554457, acc.: 73.44%] [Generator loss: 1.045193]\n",
      "3472 [Discriminator loss: 1.184180, acc.: 24.22%] [Generator loss: 0.809277]\n",
      "3473 [Discriminator loss: 0.469393, acc.: 75.78%] [Generator loss: 0.993636]\n",
      "3474 [Discriminator loss: 0.951597, acc.: 42.19%] [Generator loss: 0.858158]\n",
      "3475 [Discriminator loss: 0.842827, acc.: 45.31%] [Generator loss: 0.901124]\n",
      "3476 [Discriminator loss: 0.625092, acc.: 66.41%] [Generator loss: 1.074117]\n",
      "3477 [Discriminator loss: 0.539302, acc.: 71.88%] [Generator loss: 0.810780]\n",
      "3478 [Discriminator loss: 0.802359, acc.: 48.44%] [Generator loss: 1.221436]\n",
      "3479 [Discriminator loss: 0.629194, acc.: 66.41%] [Generator loss: 1.583532]\n",
      "3480 [Discriminator loss: 0.656504, acc.: 62.50%] [Generator loss: 1.483300]\n",
      "3481 [Discriminator loss: 0.560013, acc.: 68.75%] [Generator loss: 1.087498]\n",
      "3482 [Discriminator loss: 0.535716, acc.: 69.53%] [Generator loss: 1.709910]\n",
      "3483 [Discriminator loss: 0.639279, acc.: 68.75%] [Generator loss: 1.334821]\n",
      "3484 [Discriminator loss: 0.678750, acc.: 58.59%] [Generator loss: 1.217566]\n",
      "3485 [Discriminator loss: 0.745170, acc.: 53.91%] [Generator loss: 1.366711]\n",
      "3486 [Discriminator loss: 0.822251, acc.: 50.78%] [Generator loss: 1.215213]\n",
      "3487 [Discriminator loss: 0.669230, acc.: 64.84%] [Generator loss: 1.196698]\n",
      "3488 [Discriminator loss: 0.612696, acc.: 67.97%] [Generator loss: 1.270952]\n",
      "3489 [Discriminator loss: 0.857086, acc.: 46.88%] [Generator loss: 1.357202]\n",
      "3490 [Discriminator loss: 0.786838, acc.: 53.91%] [Generator loss: 1.302759]\n",
      "3491 [Discriminator loss: 0.535975, acc.: 76.56%] [Generator loss: 0.642467]\n",
      "3492 [Discriminator loss: 1.012780, acc.: 41.41%] [Generator loss: 0.903637]\n",
      "3493 [Discriminator loss: 0.271789, acc.: 89.06%] [Generator loss: 1.046904]\n",
      "3494 [Discriminator loss: 0.652845, acc.: 64.84%] [Generator loss: 0.437262]\n",
      "3495 [Discriminator loss: 0.196488, acc.: 93.75%] [Generator loss: 0.213706]\n",
      "3496 [Discriminator loss: 0.203708, acc.: 97.66%] [Generator loss: 0.144168]\n",
      "3497 [Discriminator loss: 0.228447, acc.: 93.75%] [Generator loss: 0.184341]\n",
      "3498 [Discriminator loss: 0.179611, acc.: 95.31%] [Generator loss: 0.118826]\n",
      "3499 [Discriminator loss: 0.775788, acc.: 58.59%] [Generator loss: 0.666357]\n",
      "3500 [Discriminator loss: 0.885154, acc.: 58.59%] [Generator loss: 0.385736]\n",
      "3501 [Discriminator loss: 0.467761, acc.: 74.22%] [Generator loss: 1.758662]\n",
      "3502 [Discriminator loss: 0.465492, acc.: 75.00%] [Generator loss: 2.038559]\n",
      "3503 [Discriminator loss: 0.921725, acc.: 56.25%] [Generator loss: 1.016921]\n",
      "3504 [Discriminator loss: 0.673486, acc.: 63.28%] [Generator loss: 1.322183]\n",
      "3505 [Discriminator loss: 1.133841, acc.: 39.84%] [Generator loss: 1.213137]\n",
      "3506 [Discriminator loss: 0.420882, acc.: 82.03%] [Generator loss: 1.012371]\n",
      "3507 [Discriminator loss: 0.757029, acc.: 51.56%] [Generator loss: 0.597820]\n",
      "3508 [Discriminator loss: 0.297574, acc.: 89.84%] [Generator loss: 0.653474]\n",
      "3509 [Discriminator loss: 0.489029, acc.: 77.34%] [Generator loss: 0.706596]\n",
      "3510 [Discriminator loss: 0.585668, acc.: 67.19%] [Generator loss: 1.586197]\n",
      "3511 [Discriminator loss: 0.781374, acc.: 60.16%] [Generator loss: 1.006370]\n",
      "3512 [Discriminator loss: 1.310881, acc.: 25.00%] [Generator loss: 1.041933]\n",
      "3513 [Discriminator loss: 0.511159, acc.: 71.88%] [Generator loss: 1.336987]\n",
      "3514 [Discriminator loss: 0.520096, acc.: 72.66%] [Generator loss: 1.196413]\n",
      "3515 [Discriminator loss: 0.916647, acc.: 42.19%] [Generator loss: 1.170065]\n",
      "3516 [Discriminator loss: 0.349341, acc.: 85.16%] [Generator loss: 1.561725]\n",
      "3517 [Discriminator loss: 0.612860, acc.: 67.19%] [Generator loss: 1.181068]\n",
      "3518 [Discriminator loss: 0.853955, acc.: 41.41%] [Generator loss: 1.065503]\n",
      "3519 [Discriminator loss: 0.796669, acc.: 50.00%] [Generator loss: 1.112980]\n",
      "3520 [Discriminator loss: 0.464191, acc.: 80.47%] [Generator loss: 1.275567]\n",
      "3521 [Discriminator loss: 0.650027, acc.: 61.72%] [Generator loss: 0.887137]\n",
      "3522 [Discriminator loss: 0.584297, acc.: 68.75%] [Generator loss: 0.947673]\n",
      "3523 [Discriminator loss: 0.463503, acc.: 78.91%] [Generator loss: 1.125605]\n",
      "3524 [Discriminator loss: 0.534250, acc.: 71.88%] [Generator loss: 0.962177]\n",
      "3525 [Discriminator loss: 0.536752, acc.: 71.88%] [Generator loss: 1.064569]\n",
      "3526 [Discriminator loss: 0.422245, acc.: 79.69%] [Generator loss: 1.209579]\n",
      "3527 [Discriminator loss: 0.623365, acc.: 64.84%] [Generator loss: 0.736053]\n",
      "3528 [Discriminator loss: 1.153607, acc.: 45.31%] [Generator loss: 1.071708]\n",
      "3529 [Discriminator loss: 1.322143, acc.: 25.78%] [Generator loss: 0.986935]\n",
      "3530 [Discriminator loss: 0.854190, acc.: 50.78%] [Generator loss: 1.280254]\n",
      "3531 [Discriminator loss: 0.864926, acc.: 45.31%] [Generator loss: 1.397410]\n",
      "3532 [Discriminator loss: 0.884861, acc.: 44.53%] [Generator loss: 1.479764]\n",
      "3533 [Discriminator loss: 0.753047, acc.: 57.81%] [Generator loss: 1.405193]\n",
      "3534 [Discriminator loss: 0.774043, acc.: 52.34%] [Generator loss: 1.331842]\n",
      "3535 [Discriminator loss: 0.907829, acc.: 43.75%] [Generator loss: 1.262818]\n",
      "3536 [Discriminator loss: 0.808813, acc.: 47.66%] [Generator loss: 1.413394]\n",
      "3537 [Discriminator loss: 0.627242, acc.: 66.41%] [Generator loss: 1.458621]\n",
      "3538 [Discriminator loss: 0.757036, acc.: 59.38%] [Generator loss: 1.174646]\n",
      "3539 [Discriminator loss: 0.833706, acc.: 46.09%] [Generator loss: 1.160204]\n",
      "3540 [Discriminator loss: 0.530369, acc.: 73.44%] [Generator loss: 1.246728]\n",
      "3541 [Discriminator loss: 0.508485, acc.: 71.88%] [Generator loss: 0.948764]\n",
      "3542 [Discriminator loss: 0.594172, acc.: 64.84%] [Generator loss: 0.817407]\n",
      "3543 [Discriminator loss: 0.665319, acc.: 63.28%] [Generator loss: 1.155569]\n",
      "3544 [Discriminator loss: 0.400261, acc.: 85.94%] [Generator loss: 1.371275]\n",
      "3545 [Discriminator loss: 0.597287, acc.: 66.41%] [Generator loss: 1.334787]\n",
      "3546 [Discriminator loss: 0.457237, acc.: 82.81%] [Generator loss: 1.135856]\n",
      "3547 [Discriminator loss: 0.566370, acc.: 73.44%] [Generator loss: 1.288015]\n",
      "3548 [Discriminator loss: 0.727942, acc.: 64.06%] [Generator loss: 0.888428]\n",
      "3549 [Discriminator loss: 0.365766, acc.: 82.03%] [Generator loss: 0.853669]\n",
      "3550 [Discriminator loss: 0.638687, acc.: 61.72%] [Generator loss: 0.793942]\n",
      "3551 [Discriminator loss: 0.584377, acc.: 70.31%] [Generator loss: 1.038647]\n",
      "3552 [Discriminator loss: 0.730141, acc.: 57.81%] [Generator loss: 1.029748]\n",
      "3553 [Discriminator loss: 0.848392, acc.: 46.88%] [Generator loss: 1.376590]\n",
      "3554 [Discriminator loss: 0.647477, acc.: 62.50%] [Generator loss: 1.086182]\n",
      "3555 [Discriminator loss: 1.065631, acc.: 42.97%] [Generator loss: 0.558458]\n",
      "3556 [Discriminator loss: 0.482739, acc.: 78.91%] [Generator loss: 0.680675]\n",
      "3557 [Discriminator loss: 0.797179, acc.: 54.69%] [Generator loss: 1.264458]\n",
      "3558 [Discriminator loss: 0.821881, acc.: 57.81%] [Generator loss: 1.568166]\n",
      "3559 [Discriminator loss: 0.650671, acc.: 67.97%] [Generator loss: 1.579242]\n",
      "3560 [Discriminator loss: 0.741740, acc.: 62.50%] [Generator loss: 1.043597]\n",
      "3561 [Discriminator loss: 0.738120, acc.: 58.59%] [Generator loss: 1.393741]\n",
      "3562 [Discriminator loss: 0.850177, acc.: 53.12%] [Generator loss: 1.667313]\n",
      "3563 [Discriminator loss: 0.791719, acc.: 51.56%] [Generator loss: 1.356746]\n",
      "3564 [Discriminator loss: 0.772265, acc.: 53.91%] [Generator loss: 1.286549]\n",
      "3565 [Discriminator loss: 0.690012, acc.: 61.72%] [Generator loss: 1.059299]\n",
      "3566 [Discriminator loss: 0.569399, acc.: 68.75%] [Generator loss: 0.986416]\n",
      "3567 [Discriminator loss: 0.737750, acc.: 57.81%] [Generator loss: 1.232929]\n",
      "3568 [Discriminator loss: 0.824852, acc.: 50.00%] [Generator loss: 0.959378]\n",
      "3569 [Discriminator loss: 0.939936, acc.: 41.41%] [Generator loss: 1.135817]\n",
      "3570 [Discriminator loss: 0.739416, acc.: 55.47%] [Generator loss: 0.652896]\n",
      "3571 [Discriminator loss: 0.788593, acc.: 56.25%] [Generator loss: 1.125439]\n",
      "3572 [Discriminator loss: 0.744089, acc.: 59.38%] [Generator loss: 0.939155]\n",
      "3573 [Discriminator loss: 0.633902, acc.: 66.41%] [Generator loss: 1.347810]\n",
      "3574 [Discriminator loss: 0.946985, acc.: 42.97%] [Generator loss: 1.248580]\n",
      "3575 [Discriminator loss: 0.458145, acc.: 77.34%] [Generator loss: 1.215339]\n",
      "3576 [Discriminator loss: 0.786365, acc.: 54.69%] [Generator loss: 0.993042]\n",
      "3577 [Discriminator loss: 0.635123, acc.: 68.75%] [Generator loss: 1.110761]\n",
      "3578 [Discriminator loss: 0.632785, acc.: 67.97%] [Generator loss: 0.881586]\n",
      "3579 [Discriminator loss: 0.727454, acc.: 55.47%] [Generator loss: 0.858405]\n",
      "3580 [Discriminator loss: 0.372087, acc.: 89.06%] [Generator loss: 0.865838]\n",
      "3581 [Discriminator loss: 0.559721, acc.: 66.41%] [Generator loss: 0.830130]\n",
      "3582 [Discriminator loss: 0.851875, acc.: 45.31%] [Generator loss: 0.687189]\n",
      "3583 [Discriminator loss: 0.524529, acc.: 75.00%] [Generator loss: 1.355860]\n",
      "3584 [Discriminator loss: 1.206202, acc.: 25.00%] [Generator loss: 1.405521]\n",
      "3585 [Discriminator loss: 0.690729, acc.: 56.25%] [Generator loss: 1.143542]\n",
      "3586 [Discriminator loss: 0.924470, acc.: 42.97%] [Generator loss: 1.508412]\n",
      "3587 [Discriminator loss: 0.532434, acc.: 68.75%] [Generator loss: 1.597977]\n",
      "3588 [Discriminator loss: 0.787203, acc.: 55.47%] [Generator loss: 1.439511]\n",
      "3589 [Discriminator loss: 0.715304, acc.: 54.69%] [Generator loss: 1.189749]\n",
      "3590 [Discriminator loss: 0.645473, acc.: 63.28%] [Generator loss: 0.939115]\n",
      "3591 [Discriminator loss: 0.715429, acc.: 53.12%] [Generator loss: 0.941941]\n",
      "3592 [Discriminator loss: 0.908406, acc.: 45.31%] [Generator loss: 1.032386]\n",
      "3593 [Discriminator loss: 0.751696, acc.: 50.78%] [Generator loss: 1.092013]\n",
      "3594 [Discriminator loss: 0.649537, acc.: 67.97%] [Generator loss: 0.915213]\n",
      "3595 [Discriminator loss: 0.622106, acc.: 63.28%] [Generator loss: 0.883715]\n",
      "3596 [Discriminator loss: 0.602332, acc.: 67.19%] [Generator loss: 0.997471]\n",
      "3597 [Discriminator loss: 0.603778, acc.: 67.19%] [Generator loss: 0.943250]\n",
      "3598 [Discriminator loss: 1.032216, acc.: 32.81%] [Generator loss: 1.262160]\n",
      "3599 [Discriminator loss: 0.607821, acc.: 67.97%] [Generator loss: 1.164689]\n",
      "3600 [Discriminator loss: 0.804293, acc.: 52.34%] [Generator loss: 1.156732]\n",
      "3601 [Discriminator loss: 0.591949, acc.: 66.41%] [Generator loss: 1.346002]\n",
      "3602 [Discriminator loss: 0.670778, acc.: 64.84%] [Generator loss: 1.230618]\n",
      "3603 [Discriminator loss: 1.134764, acc.: 37.50%] [Generator loss: 1.251495]\n",
      "3604 [Discriminator loss: 0.663259, acc.: 64.84%] [Generator loss: 1.294209]\n",
      "3605 [Discriminator loss: 0.721075, acc.: 60.16%] [Generator loss: 1.041347]\n",
      "3606 [Discriminator loss: 0.634446, acc.: 64.84%] [Generator loss: 1.573724]\n",
      "3607 [Discriminator loss: 0.775027, acc.: 53.12%] [Generator loss: 1.631350]\n",
      "3608 [Discriminator loss: 0.922159, acc.: 48.44%] [Generator loss: 1.239759]\n",
      "3609 [Discriminator loss: 0.816689, acc.: 49.22%] [Generator loss: 1.188825]\n",
      "3610 [Discriminator loss: 0.621544, acc.: 66.41%] [Generator loss: 1.174236]\n",
      "3611 [Discriminator loss: 0.781796, acc.: 54.69%] [Generator loss: 0.926016]\n",
      "3612 [Discriminator loss: 0.572056, acc.: 73.44%] [Generator loss: 1.011575]\n",
      "3613 [Discriminator loss: 0.625126, acc.: 63.28%] [Generator loss: 0.924075]\n",
      "3614 [Discriminator loss: 0.614580, acc.: 67.97%] [Generator loss: 0.818236]\n",
      "3615 [Discriminator loss: 0.627090, acc.: 62.50%] [Generator loss: 1.165733]\n",
      "3616 [Discriminator loss: 0.732013, acc.: 59.38%] [Generator loss: 1.220493]\n",
      "3617 [Discriminator loss: 0.608709, acc.: 67.19%] [Generator loss: 1.283560]\n",
      "3618 [Discriminator loss: 0.374640, acc.: 87.50%] [Generator loss: 0.867036]\n",
      "3619 [Discriminator loss: 0.411309, acc.: 86.72%] [Generator loss: 0.845879]\n",
      "3620 [Discriminator loss: 0.559586, acc.: 71.09%] [Generator loss: 1.195063]\n",
      "3621 [Discriminator loss: 0.358351, acc.: 86.72%] [Generator loss: 0.840840]\n",
      "3622 [Discriminator loss: 0.721250, acc.: 59.38%] [Generator loss: 1.001572]\n",
      "3623 [Discriminator loss: 0.712956, acc.: 59.38%] [Generator loss: 1.025234]\n",
      "3624 [Discriminator loss: 1.020953, acc.: 39.06%] [Generator loss: 1.384657]\n",
      "3625 [Discriminator loss: 0.578434, acc.: 69.53%] [Generator loss: 1.931136]\n",
      "3626 [Discriminator loss: 0.628622, acc.: 63.28%] [Generator loss: 1.591353]\n",
      "3627 [Discriminator loss: 0.544315, acc.: 71.09%] [Generator loss: 0.940538]\n",
      "3628 [Discriminator loss: 0.890756, acc.: 52.34%] [Generator loss: 0.845401]\n",
      "3629 [Discriminator loss: 0.367043, acc.: 86.72%] [Generator loss: 0.739573]\n",
      "3630 [Discriminator loss: 0.345572, acc.: 92.19%] [Generator loss: 0.678901]\n",
      "3631 [Discriminator loss: 0.281437, acc.: 91.41%] [Generator loss: 0.387593]\n",
      "3632 [Discriminator loss: 0.612339, acc.: 65.62%] [Generator loss: 0.588429]\n",
      "3633 [Discriminator loss: 0.403000, acc.: 84.38%] [Generator loss: 1.169185]\n",
      "3634 [Discriminator loss: 0.667762, acc.: 67.19%] [Generator loss: 0.357727]\n",
      "3635 [Discriminator loss: 0.170258, acc.: 96.88%] [Generator loss: 0.190128]\n",
      "3636 [Discriminator loss: 1.188876, acc.: 43.75%] [Generator loss: 0.981205]\n",
      "3637 [Discriminator loss: 0.296225, acc.: 87.50%] [Generator loss: 1.235954]\n",
      "3638 [Discriminator loss: 1.000116, acc.: 52.34%] [Generator loss: 0.275246]\n",
      "3639 [Discriminator loss: 0.761629, acc.: 60.94%] [Generator loss: 0.592403]\n",
      "3640 [Discriminator loss: 0.928515, acc.: 45.31%] [Generator loss: 2.381234]\n",
      "3641 [Discriminator loss: 0.965049, acc.: 56.25%] [Generator loss: 1.588284]\n",
      "3642 [Discriminator loss: 0.723081, acc.: 60.16%] [Generator loss: 1.009848]\n",
      "3643 [Discriminator loss: 0.585565, acc.: 71.09%] [Generator loss: 1.118544]\n",
      "3644 [Discriminator loss: 0.819234, acc.: 52.34%] [Generator loss: 0.816170]\n",
      "3645 [Discriminator loss: 0.359835, acc.: 87.50%] [Generator loss: 0.641625]\n",
      "3646 [Discriminator loss: 0.594918, acc.: 71.09%] [Generator loss: 0.606911]\n",
      "3647 [Discriminator loss: 0.725080, acc.: 57.81%] [Generator loss: 0.797275]\n",
      "3648 [Discriminator loss: 0.490646, acc.: 73.44%] [Generator loss: 0.612243]\n",
      "3649 [Discriminator loss: 0.349390, acc.: 86.72%] [Generator loss: 0.614953]\n",
      "3650 [Discriminator loss: 0.657079, acc.: 64.84%] [Generator loss: 0.727081]\n",
      "3651 [Discriminator loss: 0.274860, acc.: 91.41%] [Generator loss: 0.955897]\n",
      "3652 [Discriminator loss: 0.477433, acc.: 75.78%] [Generator loss: 0.545231]\n",
      "3653 [Discriminator loss: 0.385889, acc.: 84.38%] [Generator loss: 0.819895]\n",
      "3654 [Discriminator loss: 0.722635, acc.: 59.38%] [Generator loss: 0.952186]\n",
      "3655 [Discriminator loss: 0.512041, acc.: 74.22%] [Generator loss: 0.445538]\n",
      "3656 [Discriminator loss: 0.864683, acc.: 47.66%] [Generator loss: 0.788153]\n",
      "3657 [Discriminator loss: 0.665465, acc.: 67.19%] [Generator loss: 0.882195]\n",
      "3658 [Discriminator loss: 0.765068, acc.: 62.50%] [Generator loss: 1.642401]\n",
      "3659 [Discriminator loss: 0.994490, acc.: 45.31%] [Generator loss: 0.977244]\n",
      "3660 [Discriminator loss: 0.899169, acc.: 46.09%] [Generator loss: 1.060342]\n",
      "3661 [Discriminator loss: 0.964470, acc.: 46.09%] [Generator loss: 1.229038]\n",
      "3662 [Discriminator loss: 0.934104, acc.: 48.44%] [Generator loss: 1.251771]\n",
      "3663 [Discriminator loss: 0.586720, acc.: 69.53%] [Generator loss: 1.575322]\n",
      "3664 [Discriminator loss: 0.972193, acc.: 41.41%] [Generator loss: 1.762669]\n",
      "3665 [Discriminator loss: 0.732256, acc.: 62.50%] [Generator loss: 1.151684]\n",
      "3666 [Discriminator loss: 1.036641, acc.: 35.16%] [Generator loss: 1.167024]\n",
      "3667 [Discriminator loss: 1.238526, acc.: 25.00%] [Generator loss: 1.245927]\n",
      "3668 [Discriminator loss: 0.629242, acc.: 67.19%] [Generator loss: 1.621912]\n",
      "3669 [Discriminator loss: 0.705193, acc.: 59.38%] [Generator loss: 1.356284]\n",
      "3670 [Discriminator loss: 0.865316, acc.: 42.97%] [Generator loss: 1.308390]\n",
      "3671 [Discriminator loss: 0.477571, acc.: 80.47%] [Generator loss: 1.263100]\n",
      "3672 [Discriminator loss: 0.698289, acc.: 58.59%] [Generator loss: 1.187257]\n",
      "3673 [Discriminator loss: 0.743382, acc.: 50.00%] [Generator loss: 1.063414]\n",
      "3674 [Discriminator loss: 0.420647, acc.: 82.81%] [Generator loss: 1.127921]\n",
      "3675 [Discriminator loss: 0.650944, acc.: 69.53%] [Generator loss: 0.844028]\n",
      "3676 [Discriminator loss: 0.489109, acc.: 75.78%] [Generator loss: 0.796598]\n",
      "3677 [Discriminator loss: 0.546108, acc.: 72.66%] [Generator loss: 1.151557]\n",
      "3678 [Discriminator loss: 0.524949, acc.: 78.12%] [Generator loss: 1.007051]\n",
      "3679 [Discriminator loss: 0.731655, acc.: 58.59%] [Generator loss: 0.967093]\n",
      "3680 [Discriminator loss: 0.621526, acc.: 70.31%] [Generator loss: 0.759778]\n",
      "3681 [Discriminator loss: 0.998820, acc.: 34.38%] [Generator loss: 0.928591]\n",
      "3682 [Discriminator loss: 0.659628, acc.: 66.41%] [Generator loss: 1.078653]\n",
      "3683 [Discriminator loss: 0.764445, acc.: 53.91%] [Generator loss: 0.807351]\n",
      "3684 [Discriminator loss: 1.062520, acc.: 34.38%] [Generator loss: 1.039959]\n",
      "3685 [Discriminator loss: 0.601855, acc.: 69.53%] [Generator loss: 1.576383]\n",
      "3686 [Discriminator loss: 0.693654, acc.: 62.50%] [Generator loss: 1.641009]\n",
      "3687 [Discriminator loss: 1.074689, acc.: 34.38%] [Generator loss: 1.230356]\n",
      "3688 [Discriminator loss: 0.638708, acc.: 66.41%] [Generator loss: 1.473868]\n",
      "3689 [Discriminator loss: 0.721694, acc.: 57.81%] [Generator loss: 1.365863]\n",
      "3690 [Discriminator loss: 0.922284, acc.: 41.41%] [Generator loss: 1.439621]\n",
      "3691 [Discriminator loss: 0.621485, acc.: 65.62%] [Generator loss: 1.481954]\n",
      "3692 [Discriminator loss: 0.644780, acc.: 66.41%] [Generator loss: 0.796035]\n",
      "3693 [Discriminator loss: 0.726776, acc.: 60.94%] [Generator loss: 1.305464]\n",
      "3694 [Discriminator loss: 0.673547, acc.: 59.38%] [Generator loss: 1.169594]\n",
      "3695 [Discriminator loss: 0.650407, acc.: 63.28%] [Generator loss: 0.958468]\n",
      "3696 [Discriminator loss: 0.932441, acc.: 49.22%] [Generator loss: 0.887412]\n",
      "3697 [Discriminator loss: 0.712156, acc.: 58.59%] [Generator loss: 1.031962]\n",
      "3698 [Discriminator loss: 0.846217, acc.: 43.75%] [Generator loss: 1.110270]\n",
      "3699 [Discriminator loss: 0.778731, acc.: 56.25%] [Generator loss: 1.579783]\n",
      "3700 [Discriminator loss: 0.752877, acc.: 56.25%] [Generator loss: 1.582002]\n",
      "3701 [Discriminator loss: 0.496535, acc.: 76.56%] [Generator loss: 1.144668]\n",
      "3702 [Discriminator loss: 0.703148, acc.: 60.94%] [Generator loss: 0.984475]\n",
      "3703 [Discriminator loss: 0.556194, acc.: 71.09%] [Generator loss: 1.148276]\n",
      "3704 [Discriminator loss: 0.703234, acc.: 57.81%] [Generator loss: 0.993006]\n",
      "3705 [Discriminator loss: 0.598116, acc.: 70.31%] [Generator loss: 1.317295]\n",
      "3706 [Discriminator loss: 0.788834, acc.: 55.47%] [Generator loss: 1.280372]\n",
      "3707 [Discriminator loss: 0.668270, acc.: 64.06%] [Generator loss: 1.305252]\n",
      "3708 [Discriminator loss: 0.878296, acc.: 42.19%] [Generator loss: 1.151280]\n",
      "3709 [Discriminator loss: 1.010761, acc.: 35.94%] [Generator loss: 1.074296]\n",
      "3710 [Discriminator loss: 0.921297, acc.: 46.88%] [Generator loss: 1.053556]\n",
      "3711 [Discriminator loss: 0.771498, acc.: 57.03%] [Generator loss: 1.126536]\n",
      "3712 [Discriminator loss: 0.587711, acc.: 71.09%] [Generator loss: 0.996493]\n",
      "3713 [Discriminator loss: 0.735361, acc.: 53.91%] [Generator loss: 1.188055]\n",
      "3714 [Discriminator loss: 0.771810, acc.: 50.78%] [Generator loss: 1.143043]\n",
      "3715 [Discriminator loss: 0.621793, acc.: 66.41%] [Generator loss: 0.808986]\n",
      "3716 [Discriminator loss: 0.712356, acc.: 59.38%] [Generator loss: 0.844069]\n",
      "3717 [Discriminator loss: 1.088506, acc.: 29.69%] [Generator loss: 1.260266]\n",
      "3718 [Discriminator loss: 0.577468, acc.: 70.31%] [Generator loss: 1.292384]\n",
      "3719 [Discriminator loss: 0.802945, acc.: 48.44%] [Generator loss: 0.956439]\n",
      "3720 [Discriminator loss: 0.772185, acc.: 57.03%] [Generator loss: 1.266546]\n",
      "3721 [Discriminator loss: 0.593453, acc.: 71.88%] [Generator loss: 1.112341]\n",
      "3722 [Discriminator loss: 0.747822, acc.: 49.22%] [Generator loss: 1.320024]\n",
      "3723 [Discriminator loss: 0.813909, acc.: 52.34%] [Generator loss: 1.222495]\n",
      "3724 [Discriminator loss: 0.721137, acc.: 60.16%] [Generator loss: 1.387537]\n",
      "3725 [Discriminator loss: 0.793069, acc.: 54.69%] [Generator loss: 1.204658]\n",
      "3726 [Discriminator loss: 0.772243, acc.: 52.34%] [Generator loss: 1.078777]\n",
      "3727 [Discriminator loss: 0.549783, acc.: 71.09%] [Generator loss: 1.282861]\n",
      "3728 [Discriminator loss: 0.749311, acc.: 56.25%] [Generator loss: 1.154152]\n",
      "3729 [Discriminator loss: 0.722031, acc.: 56.25%] [Generator loss: 1.174636]\n",
      "3730 [Discriminator loss: 0.626240, acc.: 67.97%] [Generator loss: 1.280840]\n",
      "3731 [Discriminator loss: 0.664062, acc.: 56.25%] [Generator loss: 0.984139]\n",
      "3732 [Discriminator loss: 0.358171, acc.: 85.16%] [Generator loss: 0.585410]\n",
      "3733 [Discriminator loss: 0.379283, acc.: 82.81%] [Generator loss: 0.414322]\n",
      "3734 [Discriminator loss: 0.436502, acc.: 78.12%] [Generator loss: 0.726877]\n",
      "3735 [Discriminator loss: 0.307168, acc.: 85.94%] [Generator loss: 0.525648]\n",
      "3736 [Discriminator loss: 1.035935, acc.: 44.53%] [Generator loss: 0.667150]\n",
      "3737 [Discriminator loss: 0.563255, acc.: 67.97%] [Generator loss: 0.795425]\n",
      "3738 [Discriminator loss: 0.611339, acc.: 66.41%] [Generator loss: 1.478042]\n",
      "3739 [Discriminator loss: 0.682947, acc.: 61.72%] [Generator loss: 1.126391]\n",
      "3740 [Discriminator loss: 0.888302, acc.: 50.78%] [Generator loss: 0.995430]\n",
      "3741 [Discriminator loss: 0.955747, acc.: 42.19%] [Generator loss: 1.396327]\n",
      "3742 [Discriminator loss: 0.916037, acc.: 41.41%] [Generator loss: 1.363162]\n",
      "3743 [Discriminator loss: 0.789764, acc.: 55.47%] [Generator loss: 1.538288]\n",
      "3744 [Discriminator loss: 0.471269, acc.: 79.69%] [Generator loss: 1.688307]\n",
      "3745 [Discriminator loss: 0.747250, acc.: 57.03%] [Generator loss: 0.963142]\n",
      "3746 [Discriminator loss: 0.555406, acc.: 67.19%] [Generator loss: 0.679374]\n",
      "3747 [Discriminator loss: 0.556641, acc.: 71.09%] [Generator loss: 0.853892]\n",
      "3748 [Discriminator loss: 0.901702, acc.: 46.09%] [Generator loss: 0.953399]\n",
      "3749 [Discriminator loss: 0.733212, acc.: 58.59%] [Generator loss: 0.693178]\n",
      "3750 [Discriminator loss: 0.918379, acc.: 39.84%] [Generator loss: 0.916168]\n",
      "3751 [Discriminator loss: 0.769793, acc.: 52.34%] [Generator loss: 1.199194]\n",
      "3752 [Discriminator loss: 1.080244, acc.: 28.91%] [Generator loss: 1.175551]\n",
      "3753 [Discriminator loss: 0.687609, acc.: 58.59%] [Generator loss: 1.253835]\n",
      "3754 [Discriminator loss: 0.622109, acc.: 64.84%] [Generator loss: 1.486666]\n",
      "3755 [Discriminator loss: 0.827622, acc.: 43.75%] [Generator loss: 1.001621]\n",
      "3756 [Discriminator loss: 0.670418, acc.: 62.50%] [Generator loss: 1.115779]\n",
      "3757 [Discriminator loss: 0.852276, acc.: 45.31%] [Generator loss: 1.116923]\n",
      "3758 [Discriminator loss: 0.828098, acc.: 46.88%] [Generator loss: 1.086900]\n",
      "3759 [Discriminator loss: 0.809899, acc.: 53.91%] [Generator loss: 1.212087]\n",
      "3760 [Discriminator loss: 0.678115, acc.: 67.97%] [Generator loss: 1.552402]\n",
      "3761 [Discriminator loss: 0.858774, acc.: 43.75%] [Generator loss: 1.138133]\n",
      "3762 [Discriminator loss: 0.668987, acc.: 64.06%] [Generator loss: 1.272881]\n",
      "3763 [Discriminator loss: 0.874767, acc.: 46.09%] [Generator loss: 1.241842]\n",
      "3764 [Discriminator loss: 0.728657, acc.: 54.69%] [Generator loss: 1.152289]\n",
      "3765 [Discriminator loss: 0.648508, acc.: 62.50%] [Generator loss: 1.318320]\n",
      "3766 [Discriminator loss: 0.694001, acc.: 63.28%] [Generator loss: 1.089876]\n",
      "3767 [Discriminator loss: 0.665431, acc.: 61.72%] [Generator loss: 0.894419]\n",
      "3768 [Discriminator loss: 0.755752, acc.: 53.91%] [Generator loss: 1.175322]\n",
      "3769 [Discriminator loss: 0.621512, acc.: 64.06%] [Generator loss: 1.290451]\n",
      "3770 [Discriminator loss: 0.812434, acc.: 40.62%] [Generator loss: 1.297302]\n",
      "3771 [Discriminator loss: 0.707823, acc.: 64.06%] [Generator loss: 0.940838]\n",
      "3772 [Discriminator loss: 0.509224, acc.: 76.56%] [Generator loss: 0.770818]\n",
      "3773 [Discriminator loss: 1.045087, acc.: 38.28%] [Generator loss: 1.084298]\n",
      "3774 [Discriminator loss: 0.651511, acc.: 63.28%] [Generator loss: 1.049661]\n",
      "3775 [Discriminator loss: 0.599873, acc.: 67.19%] [Generator loss: 0.958066]\n",
      "3776 [Discriminator loss: 0.751213, acc.: 48.44%] [Generator loss: 1.093824]\n",
      "3777 [Discriminator loss: 0.651669, acc.: 57.81%] [Generator loss: 1.165069]\n",
      "3778 [Discriminator loss: 0.845594, acc.: 48.44%] [Generator loss: 0.913176]\n",
      "3779 [Discriminator loss: 0.687052, acc.: 61.72%] [Generator loss: 1.114124]\n",
      "3780 [Discriminator loss: 0.501419, acc.: 75.00%] [Generator loss: 1.158868]\n",
      "3781 [Discriminator loss: 0.712303, acc.: 57.81%] [Generator loss: 1.181856]\n",
      "3782 [Discriminator loss: 0.549960, acc.: 71.88%] [Generator loss: 1.136925]\n",
      "3783 [Discriminator loss: 0.603626, acc.: 69.53%] [Generator loss: 1.289804]\n",
      "3784 [Discriminator loss: 0.591174, acc.: 73.44%] [Generator loss: 1.089031]\n",
      "3785 [Discriminator loss: 0.377246, acc.: 84.38%] [Generator loss: 0.743770]\n",
      "3786 [Discriminator loss: 0.556646, acc.: 76.56%] [Generator loss: 0.655476]\n",
      "3787 [Discriminator loss: 0.422156, acc.: 85.16%] [Generator loss: 0.778109]\n",
      "3788 [Discriminator loss: 0.482229, acc.: 79.69%] [Generator loss: 0.677106]\n",
      "3789 [Discriminator loss: 0.291360, acc.: 90.62%] [Generator loss: 0.407304]\n",
      "3790 [Discriminator loss: 0.965933, acc.: 52.34%] [Generator loss: 1.080757]\n",
      "3791 [Discriminator loss: 0.827157, acc.: 49.22%] [Generator loss: 1.266969]\n",
      "3792 [Discriminator loss: 0.948569, acc.: 39.84%] [Generator loss: 1.094594]\n",
      "3793 [Discriminator loss: 1.009479, acc.: 32.03%] [Generator loss: 1.728926]\n",
      "3794 [Discriminator loss: 0.827487, acc.: 50.78%] [Generator loss: 1.433645]\n",
      "3795 [Discriminator loss: 0.664532, acc.: 60.94%] [Generator loss: 1.372190]\n",
      "3796 [Discriminator loss: 0.482110, acc.: 77.34%] [Generator loss: 0.889699]\n",
      "3797 [Discriminator loss: 0.866776, acc.: 43.75%] [Generator loss: 0.918505]\n",
      "3798 [Discriminator loss: 0.964606, acc.: 35.16%] [Generator loss: 1.117366]\n",
      "3799 [Discriminator loss: 0.879218, acc.: 49.22%] [Generator loss: 1.062882]\n",
      "3800 [Discriminator loss: 0.618904, acc.: 69.53%] [Generator loss: 0.802382]\n",
      "3801 [Discriminator loss: 0.669417, acc.: 60.16%] [Generator loss: 1.205635]\n",
      "3802 [Discriminator loss: 0.609594, acc.: 69.53%] [Generator loss: 1.268429]\n",
      "3803 [Discriminator loss: 0.856475, acc.: 46.09%] [Generator loss: 1.192043]\n",
      "3804 [Discriminator loss: 0.874369, acc.: 50.78%] [Generator loss: 1.140615]\n",
      "3805 [Discriminator loss: 0.596384, acc.: 67.19%] [Generator loss: 1.190400]\n",
      "3806 [Discriminator loss: 0.748728, acc.: 57.81%] [Generator loss: 1.193591]\n",
      "3807 [Discriminator loss: 0.910298, acc.: 45.31%] [Generator loss: 0.727827]\n",
      "3808 [Discriminator loss: 0.652582, acc.: 63.28%] [Generator loss: 0.808137]\n",
      "3809 [Discriminator loss: 0.696874, acc.: 61.72%] [Generator loss: 1.371960]\n",
      "3810 [Discriminator loss: 0.866127, acc.: 53.91%] [Generator loss: 1.386361]\n",
      "3811 [Discriminator loss: 0.786743, acc.: 51.56%] [Generator loss: 1.104028]\n",
      "3812 [Discriminator loss: 0.723284, acc.: 58.59%] [Generator loss: 1.387881]\n",
      "3813 [Discriminator loss: 0.628876, acc.: 64.06%] [Generator loss: 1.295280]\n",
      "3814 [Discriminator loss: 0.688939, acc.: 58.59%] [Generator loss: 1.042247]\n",
      "3815 [Discriminator loss: 0.624301, acc.: 67.19%] [Generator loss: 1.161806]\n",
      "3816 [Discriminator loss: 0.686724, acc.: 59.38%] [Generator loss: 1.142277]\n",
      "3817 [Discriminator loss: 0.682728, acc.: 59.38%] [Generator loss: 1.586238]\n",
      "3818 [Discriminator loss: 1.142405, acc.: 30.47%] [Generator loss: 0.852434]\n",
      "3819 [Discriminator loss: 0.685344, acc.: 59.38%] [Generator loss: 0.973564]\n",
      "3820 [Discriminator loss: 0.701287, acc.: 59.38%] [Generator loss: 1.217586]\n",
      "3821 [Discriminator loss: 0.766897, acc.: 53.91%] [Generator loss: 1.216835]\n",
      "3822 [Discriminator loss: 0.870809, acc.: 50.78%] [Generator loss: 1.165973]\n",
      "3823 [Discriminator loss: 0.626288, acc.: 65.62%] [Generator loss: 0.909060]\n",
      "3824 [Discriminator loss: 0.731988, acc.: 51.56%] [Generator loss: 1.111877]\n",
      "3825 [Discriminator loss: 0.660286, acc.: 61.72%] [Generator loss: 1.288622]\n",
      "3826 [Discriminator loss: 0.597189, acc.: 71.88%] [Generator loss: 0.922830]\n",
      "3827 [Discriminator loss: 0.766690, acc.: 51.56%] [Generator loss: 1.165197]\n",
      "3828 [Discriminator loss: 0.666996, acc.: 60.16%] [Generator loss: 1.214666]\n",
      "3829 [Discriminator loss: 0.509492, acc.: 74.22%] [Generator loss: 1.664947]\n",
      "3830 [Discriminator loss: 0.758424, acc.: 55.47%] [Generator loss: 1.194228]\n",
      "3831 [Discriminator loss: 0.742315, acc.: 63.28%] [Generator loss: 0.897495]\n",
      "3832 [Discriminator loss: 0.533705, acc.: 72.66%] [Generator loss: 0.875552]\n",
      "3833 [Discriminator loss: 0.711314, acc.: 66.41%] [Generator loss: 1.092895]\n",
      "3834 [Discriminator loss: 0.829847, acc.: 38.28%] [Generator loss: 1.310762]\n",
      "3835 [Discriminator loss: 0.993215, acc.: 39.84%] [Generator loss: 1.361615]\n",
      "3836 [Discriminator loss: 0.493051, acc.: 79.69%] [Generator loss: 1.115704]\n",
      "3837 [Discriminator loss: 0.615599, acc.: 68.75%] [Generator loss: 1.000348]\n",
      "3838 [Discriminator loss: 0.577552, acc.: 75.78%] [Generator loss: 1.071606]\n",
      "3839 [Discriminator loss: 0.627316, acc.: 65.62%] [Generator loss: 1.058211]\n",
      "3840 [Discriminator loss: 0.665178, acc.: 60.94%] [Generator loss: 1.217410]\n",
      "3841 [Discriminator loss: 0.772865, acc.: 57.03%] [Generator loss: 1.040890]\n",
      "3842 [Discriminator loss: 0.706271, acc.: 58.59%] [Generator loss: 1.085637]\n",
      "3843 [Discriminator loss: 0.570352, acc.: 68.75%] [Generator loss: 1.161825]\n",
      "3844 [Discriminator loss: 0.444988, acc.: 81.25%] [Generator loss: 0.836089]\n",
      "3845 [Discriminator loss: 0.584269, acc.: 72.66%] [Generator loss: 0.929045]\n",
      "3846 [Discriminator loss: 0.650343, acc.: 64.06%] [Generator loss: 1.203698]\n",
      "3847 [Discriminator loss: 0.405023, acc.: 83.59%] [Generator loss: 1.042705]\n",
      "3848 [Discriminator loss: 0.592256, acc.: 64.84%] [Generator loss: 0.847240]\n",
      "3849 [Discriminator loss: 0.696166, acc.: 58.59%] [Generator loss: 1.185434]\n",
      "3850 [Discriminator loss: 0.613710, acc.: 66.41%] [Generator loss: 1.686602]\n",
      "3851 [Discriminator loss: 0.865431, acc.: 53.91%] [Generator loss: 0.831456]\n",
      "3852 [Discriminator loss: 0.894430, acc.: 45.31%] [Generator loss: 0.879563]\n",
      "3853 [Discriminator loss: 0.741521, acc.: 52.34%] [Generator loss: 1.175794]\n",
      "3854 [Discriminator loss: 1.138706, acc.: 32.03%] [Generator loss: 1.351046]\n",
      "3855 [Discriminator loss: 0.655867, acc.: 62.50%] [Generator loss: 1.280309]\n",
      "3856 [Discriminator loss: 0.716408, acc.: 63.28%] [Generator loss: 1.382204]\n",
      "3857 [Discriminator loss: 0.578652, acc.: 63.28%] [Generator loss: 1.325865]\n",
      "3858 [Discriminator loss: 0.617035, acc.: 64.84%] [Generator loss: 1.153681]\n",
      "3859 [Discriminator loss: 0.679579, acc.: 62.50%] [Generator loss: 1.175093]\n",
      "3860 [Discriminator loss: 0.753432, acc.: 60.16%] [Generator loss: 1.086942]\n",
      "3861 [Discriminator loss: 0.726650, acc.: 53.12%] [Generator loss: 1.222990]\n",
      "3862 [Discriminator loss: 0.943009, acc.: 35.16%] [Generator loss: 1.273279]\n",
      "3863 [Discriminator loss: 0.680558, acc.: 57.81%] [Generator loss: 1.297439]\n",
      "3864 [Discriminator loss: 0.638342, acc.: 68.75%] [Generator loss: 0.965525]\n",
      "3865 [Discriminator loss: 0.696202, acc.: 59.38%] [Generator loss: 0.943384]\n",
      "3866 [Discriminator loss: 0.721870, acc.: 61.72%] [Generator loss: 0.624812]\n",
      "3867 [Discriminator loss: 0.716195, acc.: 56.25%] [Generator loss: 1.011557]\n",
      "3868 [Discriminator loss: 0.536960, acc.: 73.44%] [Generator loss: 1.055923]\n",
      "3869 [Discriminator loss: 0.826021, acc.: 55.47%] [Generator loss: 1.127005]\n",
      "3870 [Discriminator loss: 0.524217, acc.: 73.44%] [Generator loss: 1.092924]\n",
      "3871 [Discriminator loss: 0.600986, acc.: 69.53%] [Generator loss: 1.287327]\n",
      "3872 [Discriminator loss: 0.671684, acc.: 61.72%] [Generator loss: 0.803379]\n",
      "3873 [Discriminator loss: 0.807198, acc.: 54.69%] [Generator loss: 1.112025]\n",
      "3874 [Discriminator loss: 0.652915, acc.: 69.53%] [Generator loss: 1.253848]\n",
      "3875 [Discriminator loss: 0.635956, acc.: 62.50%] [Generator loss: 1.172576]\n",
      "3876 [Discriminator loss: 0.453417, acc.: 75.78%] [Generator loss: 1.303028]\n",
      "3877 [Discriminator loss: 0.544452, acc.: 71.88%] [Generator loss: 0.929624]\n",
      "3878 [Discriminator loss: 0.821895, acc.: 53.91%] [Generator loss: 0.872124]\n",
      "3879 [Discriminator loss: 0.580187, acc.: 69.53%] [Generator loss: 1.129959]\n",
      "3880 [Discriminator loss: 0.654712, acc.: 62.50%] [Generator loss: 1.139785]\n",
      "3881 [Discriminator loss: 0.830859, acc.: 53.91%] [Generator loss: 0.786173]\n",
      "3882 [Discriminator loss: 0.732645, acc.: 50.78%] [Generator loss: 1.171068]\n",
      "3883 [Discriminator loss: 1.097305, acc.: 33.59%] [Generator loss: 1.178326]\n",
      "3884 [Discriminator loss: 0.725430, acc.: 57.03%] [Generator loss: 1.267777]\n",
      "3885 [Discriminator loss: 0.599631, acc.: 65.62%] [Generator loss: 0.917431]\n",
      "3886 [Discriminator loss: 0.695227, acc.: 58.59%] [Generator loss: 1.128737]\n",
      "3887 [Discriminator loss: 0.521793, acc.: 71.88%] [Generator loss: 1.028110]\n",
      "3888 [Discriminator loss: 0.606269, acc.: 64.06%] [Generator loss: 1.504365]\n",
      "3889 [Discriminator loss: 0.613970, acc.: 63.28%] [Generator loss: 1.160418]\n",
      "3890 [Discriminator loss: 0.676729, acc.: 59.38%] [Generator loss: 1.071807]\n",
      "3891 [Discriminator loss: 0.465703, acc.: 79.69%] [Generator loss: 0.939652]\n",
      "3892 [Discriminator loss: 1.042891, acc.: 38.28%] [Generator loss: 1.003128]\n",
      "3893 [Discriminator loss: 0.765387, acc.: 53.12%] [Generator loss: 1.284522]\n",
      "3894 [Discriminator loss: 0.492229, acc.: 79.69%] [Generator loss: 1.113722]\n",
      "3895 [Discriminator loss: 0.610433, acc.: 66.41%] [Generator loss: 1.274224]\n",
      "3896 [Discriminator loss: 0.679472, acc.: 61.72%] [Generator loss: 1.154600]\n",
      "3897 [Discriminator loss: 0.827419, acc.: 49.22%] [Generator loss: 1.415293]\n",
      "3898 [Discriminator loss: 0.582369, acc.: 68.75%] [Generator loss: 1.749292]\n",
      "3899 [Discriminator loss: 0.769426, acc.: 49.22%] [Generator loss: 1.378832]\n",
      "3900 [Discriminator loss: 0.836804, acc.: 46.09%] [Generator loss: 1.192815]\n",
      "3901 [Discriminator loss: 0.731515, acc.: 56.25%] [Generator loss: 1.499573]\n",
      "3902 [Discriminator loss: 0.698446, acc.: 58.59%] [Generator loss: 1.346182]\n",
      "3903 [Discriminator loss: 0.575615, acc.: 64.84%] [Generator loss: 1.292465]\n",
      "3904 [Discriminator loss: 0.526381, acc.: 71.09%] [Generator loss: 1.422927]\n",
      "3905 [Discriminator loss: 0.639931, acc.: 66.41%] [Generator loss: 1.681155]\n",
      "3906 [Discriminator loss: 0.767917, acc.: 54.69%] [Generator loss: 1.154047]\n",
      "3907 [Discriminator loss: 0.520095, acc.: 71.09%] [Generator loss: 0.783156]\n",
      "3908 [Discriminator loss: 0.764192, acc.: 53.12%] [Generator loss: 0.780973]\n",
      "3909 [Discriminator loss: 0.760653, acc.: 59.38%] [Generator loss: 1.594401]\n",
      "3910 [Discriminator loss: 0.818434, acc.: 53.12%] [Generator loss: 1.572793]\n",
      "3911 [Discriminator loss: 0.764867, acc.: 52.34%] [Generator loss: 1.253558]\n",
      "3912 [Discriminator loss: 0.821375, acc.: 53.91%] [Generator loss: 1.192209]\n",
      "3913 [Discriminator loss: 0.538462, acc.: 71.88%] [Generator loss: 1.040619]\n",
      "3914 [Discriminator loss: 0.657468, acc.: 67.19%] [Generator loss: 1.025217]\n",
      "3915 [Discriminator loss: 0.944561, acc.: 44.53%] [Generator loss: 0.811727]\n",
      "3916 [Discriminator loss: 0.547006, acc.: 78.12%] [Generator loss: 0.954767]\n",
      "3917 [Discriminator loss: 0.379109, acc.: 87.50%] [Generator loss: 0.945357]\n",
      "3918 [Discriminator loss: 0.566278, acc.: 67.97%] [Generator loss: 0.894515]\n",
      "3919 [Discriminator loss: 0.477932, acc.: 80.47%] [Generator loss: 0.976768]\n",
      "3920 [Discriminator loss: 0.363486, acc.: 84.38%] [Generator loss: 0.631812]\n",
      "3921 [Discriminator loss: 0.961300, acc.: 46.09%] [Generator loss: 1.007215]\n",
      "3922 [Discriminator loss: 0.407251, acc.: 85.16%] [Generator loss: 1.323106]\n",
      "3923 [Discriminator loss: 0.599486, acc.: 68.75%] [Generator loss: 1.306204]\n",
      "3924 [Discriminator loss: 0.676126, acc.: 59.38%] [Generator loss: 1.093198]\n",
      "3925 [Discriminator loss: 0.783879, acc.: 53.91%] [Generator loss: 0.941222]\n",
      "3926 [Discriminator loss: 0.393219, acc.: 84.38%] [Generator loss: 0.947706]\n",
      "3927 [Discriminator loss: 0.394316, acc.: 85.94%] [Generator loss: 0.641306]\n",
      "3928 [Discriminator loss: 0.304419, acc.: 89.06%] [Generator loss: 0.542189]\n",
      "3929 [Discriminator loss: 0.396837, acc.: 84.38%] [Generator loss: 0.721750]\n",
      "3930 [Discriminator loss: 0.315603, acc.: 86.72%] [Generator loss: 0.240331]\n",
      "3931 [Discriminator loss: 0.327881, acc.: 90.62%] [Generator loss: 0.242030]\n",
      "3932 [Discriminator loss: 0.400065, acc.: 74.22%] [Generator loss: 0.556929]\n",
      "3933 [Discriminator loss: 0.714452, acc.: 60.94%] [Generator loss: 1.325684]\n",
      "3934 [Discriminator loss: 0.403368, acc.: 74.22%] [Generator loss: 1.829299]\n",
      "3935 [Discriminator loss: 0.888466, acc.: 50.00%] [Generator loss: 0.574515]\n",
      "3936 [Discriminator loss: 0.319046, acc.: 89.84%] [Generator loss: 0.849713]\n",
      "3937 [Discriminator loss: 0.470801, acc.: 78.91%] [Generator loss: 1.149453]\n",
      "3938 [Discriminator loss: 0.473028, acc.: 78.91%] [Generator loss: 1.236265]\n",
      "3939 [Discriminator loss: 0.886073, acc.: 48.44%] [Generator loss: 0.924740]\n",
      "3940 [Discriminator loss: 0.581307, acc.: 68.75%] [Generator loss: 1.714022]\n",
      "3941 [Discriminator loss: 0.726212, acc.: 56.25%] [Generator loss: 2.082302]\n",
      "3942 [Discriminator loss: 1.601161, acc.: 16.41%] [Generator loss: 1.367869]\n",
      "3943 [Discriminator loss: 0.443750, acc.: 79.69%] [Generator loss: 1.528395]\n",
      "3944 [Discriminator loss: 0.575980, acc.: 71.09%] [Generator loss: 1.043286]\n",
      "3945 [Discriminator loss: 0.387188, acc.: 85.16%] [Generator loss: 0.729453]\n",
      "3946 [Discriminator loss: 0.561650, acc.: 70.31%] [Generator loss: 0.614646]\n",
      "3947 [Discriminator loss: 0.568391, acc.: 67.97%] [Generator loss: 0.932672]\n",
      "3948 [Discriminator loss: 0.421456, acc.: 81.25%] [Generator loss: 0.935694]\n",
      "3949 [Discriminator loss: 0.390376, acc.: 83.59%] [Generator loss: 0.749576]\n",
      "3950 [Discriminator loss: 0.381387, acc.: 85.16%] [Generator loss: 0.530998]\n",
      "3951 [Discriminator loss: 0.717154, acc.: 58.59%] [Generator loss: 0.894842]\n",
      "3952 [Discriminator loss: 0.321573, acc.: 90.62%] [Generator loss: 1.238362]\n",
      "3953 [Discriminator loss: 0.683792, acc.: 59.38%] [Generator loss: 0.863849]\n",
      "3954 [Discriminator loss: 0.645919, acc.: 65.62%] [Generator loss: 1.237871]\n",
      "3955 [Discriminator loss: 0.736771, acc.: 60.94%] [Generator loss: 1.582452]\n",
      "3956 [Discriminator loss: 1.323748, acc.: 20.31%] [Generator loss: 1.282829]\n",
      "3957 [Discriminator loss: 0.868792, acc.: 46.88%] [Generator loss: 1.291819]\n",
      "3958 [Discriminator loss: 0.366300, acc.: 89.84%] [Generator loss: 1.156881]\n",
      "3959 [Discriminator loss: 0.582396, acc.: 63.28%] [Generator loss: 0.790656]\n",
      "3960 [Discriminator loss: 0.712426, acc.: 61.72%] [Generator loss: 1.354542]\n",
      "3961 [Discriminator loss: 0.508505, acc.: 67.97%] [Generator loss: 0.729584]\n",
      "3962 [Discriminator loss: 0.907646, acc.: 46.88%] [Generator loss: 1.005599]\n",
      "3963 [Discriminator loss: 0.509043, acc.: 75.78%] [Generator loss: 1.242718]\n",
      "3964 [Discriminator loss: 0.523220, acc.: 76.56%] [Generator loss: 0.948067]\n",
      "3965 [Discriminator loss: 0.825004, acc.: 53.12%] [Generator loss: 0.768542]\n",
      "3966 [Discriminator loss: 0.477613, acc.: 81.25%] [Generator loss: 1.269586]\n",
      "3967 [Discriminator loss: 0.404920, acc.: 80.47%] [Generator loss: 1.045811]\n",
      "3968 [Discriminator loss: 0.917058, acc.: 49.22%] [Generator loss: 1.766852]\n",
      "3969 [Discriminator loss: 0.767682, acc.: 53.12%] [Generator loss: 1.951188]\n",
      "3970 [Discriminator loss: 1.205212, acc.: 38.28%] [Generator loss: 1.389278]\n",
      "3971 [Discriminator loss: 0.609720, acc.: 69.53%] [Generator loss: 1.509461]\n",
      "3972 [Discriminator loss: 0.380912, acc.: 87.50%] [Generator loss: 1.266737]\n",
      "3973 [Discriminator loss: 0.527895, acc.: 72.66%] [Generator loss: 1.272121]\n",
      "3974 [Discriminator loss: 0.929570, acc.: 46.88%] [Generator loss: 1.205334]\n",
      "3975 [Discriminator loss: 0.868759, acc.: 52.34%] [Generator loss: 1.704684]\n",
      "3976 [Discriminator loss: 0.449699, acc.: 81.25%] [Generator loss: 0.817134]\n",
      "3977 [Discriminator loss: 1.017241, acc.: 41.41%] [Generator loss: 1.162541]\n",
      "3978 [Discriminator loss: 0.573240, acc.: 71.09%] [Generator loss: 1.420752]\n",
      "3979 [Discriminator loss: 0.704199, acc.: 62.50%] [Generator loss: 1.087758]\n",
      "3980 [Discriminator loss: 0.509430, acc.: 74.22%] [Generator loss: 1.398151]\n",
      "3981 [Discriminator loss: 0.465354, acc.: 82.81%] [Generator loss: 1.539028]\n",
      "3982 [Discriminator loss: 0.384419, acc.: 84.38%] [Generator loss: 1.281476]\n",
      "3983 [Discriminator loss: 0.539216, acc.: 75.78%] [Generator loss: 0.995090]\n",
      "3984 [Discriminator loss: 0.521707, acc.: 72.66%] [Generator loss: 0.974753]\n",
      "3985 [Discriminator loss: 0.628055, acc.: 64.84%] [Generator loss: 0.636362]\n",
      "3986 [Discriminator loss: 0.645984, acc.: 65.62%] [Generator loss: 0.861364]\n",
      "3987 [Discriminator loss: 0.773158, acc.: 57.03%] [Generator loss: 1.038610]\n",
      "3988 [Discriminator loss: 0.392004, acc.: 84.38%] [Generator loss: 1.105033]\n",
      "3989 [Discriminator loss: 0.638297, acc.: 63.28%] [Generator loss: 1.330442]\n",
      "3990 [Discriminator loss: 0.813906, acc.: 49.22%] [Generator loss: 1.178810]\n",
      "3991 [Discriminator loss: 1.292333, acc.: 27.34%] [Generator loss: 1.084549]\n",
      "3992 [Discriminator loss: 0.699743, acc.: 61.72%] [Generator loss: 1.752085]\n",
      "3993 [Discriminator loss: 0.865182, acc.: 47.66%] [Generator loss: 1.541025]\n",
      "3994 [Discriminator loss: 0.596030, acc.: 63.28%] [Generator loss: 1.761469]\n",
      "3995 [Discriminator loss: 0.796463, acc.: 53.91%] [Generator loss: 1.167994]\n",
      "3996 [Discriminator loss: 0.569473, acc.: 72.66%] [Generator loss: 1.053209]\n",
      "3997 [Discriminator loss: 0.503553, acc.: 73.44%] [Generator loss: 0.982102]\n",
      "3998 [Discriminator loss: 0.769459, acc.: 53.91%] [Generator loss: 0.711129]\n",
      "3999 [Discriminator loss: 0.593313, acc.: 71.09%] [Generator loss: 0.886863]\n",
      "4000 [Discriminator loss: 0.564239, acc.: 70.31%] [Generator loss: 1.059947]\n",
      "4001 [Discriminator loss: 0.799754, acc.: 54.69%] [Generator loss: 1.375826]\n",
      "4002 [Discriminator loss: 0.820749, acc.: 51.56%] [Generator loss: 1.165376]\n",
      "4003 [Discriminator loss: 0.820098, acc.: 57.81%] [Generator loss: 0.788911]\n",
      "4004 [Discriminator loss: 0.874077, acc.: 44.53%] [Generator loss: 1.031390]\n",
      "4005 [Discriminator loss: 0.651037, acc.: 60.16%] [Generator loss: 1.235869]\n",
      "4006 [Discriminator loss: 0.921419, acc.: 40.62%] [Generator loss: 1.364895]\n",
      "4007 [Discriminator loss: 0.613920, acc.: 62.50%] [Generator loss: 1.025386]\n",
      "4008 [Discriminator loss: 0.703820, acc.: 56.25%] [Generator loss: 1.247613]\n",
      "4009 [Discriminator loss: 0.559598, acc.: 71.09%] [Generator loss: 0.934517]\n",
      "4010 [Discriminator loss: 0.891353, acc.: 49.22%] [Generator loss: 1.435728]\n",
      "4011 [Discriminator loss: 0.666839, acc.: 64.84%] [Generator loss: 1.554668]\n",
      "4012 [Discriminator loss: 0.721973, acc.: 60.16%] [Generator loss: 0.842203]\n",
      "4013 [Discriminator loss: 0.760412, acc.: 54.69%] [Generator loss: 1.539756]\n",
      "4014 [Discriminator loss: 0.555423, acc.: 67.19%] [Generator loss: 1.270320]\n",
      "4015 [Discriminator loss: 0.721447, acc.: 62.50%] [Generator loss: 1.237110]\n",
      "4016 [Discriminator loss: 0.588696, acc.: 73.44%] [Generator loss: 1.125307]\n",
      "4017 [Discriminator loss: 0.834144, acc.: 49.22%] [Generator loss: 1.200524]\n",
      "4018 [Discriminator loss: 0.968619, acc.: 47.66%] [Generator loss: 1.040809]\n",
      "4019 [Discriminator loss: 0.729530, acc.: 51.56%] [Generator loss: 1.104992]\n",
      "4020 [Discriminator loss: 0.400319, acc.: 85.94%] [Generator loss: 0.809499]\n",
      "4021 [Discriminator loss: 0.775902, acc.: 49.22%] [Generator loss: 0.944089]\n",
      "4022 [Discriminator loss: 0.746436, acc.: 54.69%] [Generator loss: 1.528591]\n",
      "4023 [Discriminator loss: 0.780524, acc.: 54.69%] [Generator loss: 1.102404]\n",
      "4024 [Discriminator loss: 0.659014, acc.: 64.84%] [Generator loss: 1.043436]\n",
      "4025 [Discriminator loss: 0.493789, acc.: 72.66%] [Generator loss: 1.174099]\n",
      "4026 [Discriminator loss: 0.749798, acc.: 50.78%] [Generator loss: 1.144857]\n",
      "4027 [Discriminator loss: 0.978189, acc.: 38.28%] [Generator loss: 1.096342]\n",
      "4028 [Discriminator loss: 0.570202, acc.: 70.31%] [Generator loss: 1.046142]\n",
      "4029 [Discriminator loss: 0.575307, acc.: 64.84%] [Generator loss: 0.729111]\n",
      "4030 [Discriminator loss: 0.871339, acc.: 47.66%] [Generator loss: 0.813278]\n",
      "4031 [Discriminator loss: 0.901398, acc.: 39.84%] [Generator loss: 0.897629]\n",
      "4032 [Discriminator loss: 0.529706, acc.: 75.78%] [Generator loss: 0.884364]\n",
      "4033 [Discriminator loss: 0.668940, acc.: 60.94%] [Generator loss: 1.117079]\n",
      "4034 [Discriminator loss: 0.619376, acc.: 70.31%] [Generator loss: 0.908377]\n",
      "4035 [Discriminator loss: 0.734894, acc.: 53.12%] [Generator loss: 1.289927]\n",
      "4036 [Discriminator loss: 0.664453, acc.: 57.81%] [Generator loss: 1.359429]\n",
      "4037 [Discriminator loss: 0.402569, acc.: 82.81%] [Generator loss: 1.205578]\n",
      "4038 [Discriminator loss: 0.611118, acc.: 70.31%] [Generator loss: 0.832881]\n",
      "4039 [Discriminator loss: 0.469517, acc.: 78.91%] [Generator loss: 0.837100]\n",
      "4040 [Discriminator loss: 0.608642, acc.: 64.84%] [Generator loss: 0.973401]\n",
      "4041 [Discriminator loss: 0.488244, acc.: 78.91%] [Generator loss: 1.124622]\n",
      "4042 [Discriminator loss: 0.517176, acc.: 75.00%] [Generator loss: 1.154813]\n",
      "4043 [Discriminator loss: 0.783566, acc.: 46.09%] [Generator loss: 0.775394]\n",
      "4044 [Discriminator loss: 0.569020, acc.: 72.66%] [Generator loss: 0.962563]\n",
      "4045 [Discriminator loss: 0.496264, acc.: 76.56%] [Generator loss: 0.994143]\n",
      "4046 [Discriminator loss: 0.620525, acc.: 66.41%] [Generator loss: 1.024089]\n",
      "4047 [Discriminator loss: 1.111193, acc.: 31.25%] [Generator loss: 0.721875]\n",
      "4048 [Discriminator loss: 0.659137, acc.: 61.72%] [Generator loss: 1.298501]\n",
      "4049 [Discriminator loss: 0.494881, acc.: 80.47%] [Generator loss: 1.427199]\n",
      "4050 [Discriminator loss: 0.494875, acc.: 75.00%] [Generator loss: 0.644072]\n",
      "4051 [Discriminator loss: 0.565070, acc.: 68.75%] [Generator loss: 0.288035]\n",
      "4052 [Discriminator loss: 0.288576, acc.: 91.41%] [Generator loss: 0.567212]\n",
      "4053 [Discriminator loss: 0.258198, acc.: 92.97%] [Generator loss: 0.564037]\n",
      "4054 [Discriminator loss: 0.815347, acc.: 58.59%] [Generator loss: 1.579894]\n",
      "4055 [Discriminator loss: 0.409854, acc.: 77.34%] [Generator loss: 0.914489]\n",
      "4056 [Discriminator loss: 1.000325, acc.: 43.75%] [Generator loss: 0.656556]\n",
      "4057 [Discriminator loss: 0.789296, acc.: 59.38%] [Generator loss: 1.466185]\n",
      "4058 [Discriminator loss: 0.842715, acc.: 52.34%] [Generator loss: 1.583627]\n",
      "4059 [Discriminator loss: 0.826218, acc.: 45.31%] [Generator loss: 1.631894]\n",
      "4060 [Discriminator loss: 0.904494, acc.: 47.66%] [Generator loss: 1.407429]\n",
      "4061 [Discriminator loss: 0.881325, acc.: 47.66%] [Generator loss: 1.516274]\n",
      "4062 [Discriminator loss: 0.405253, acc.: 85.94%] [Generator loss: 1.023680]\n",
      "4063 [Discriminator loss: 0.375910, acc.: 85.94%] [Generator loss: 0.695462]\n",
      "4064 [Discriminator loss: 0.467662, acc.: 79.69%] [Generator loss: 0.579136]\n",
      "4065 [Discriminator loss: 0.360405, acc.: 86.72%] [Generator loss: 0.442129]\n",
      "4066 [Discriminator loss: 0.276849, acc.: 92.19%] [Generator loss: 0.627780]\n",
      "4067 [Discriminator loss: 0.317825, acc.: 86.72%] [Generator loss: 0.552579]\n",
      "4068 [Discriminator loss: 0.405818, acc.: 85.16%] [Generator loss: 0.701929]\n",
      "4069 [Discriminator loss: 0.499361, acc.: 76.56%] [Generator loss: 0.616745]\n",
      "4070 [Discriminator loss: 0.573371, acc.: 67.19%] [Generator loss: 1.203638]\n",
      "4071 [Discriminator loss: 1.098187, acc.: 34.38%] [Generator loss: 1.433428]\n",
      "4072 [Discriminator loss: 0.552342, acc.: 75.00%] [Generator loss: 1.069731]\n",
      "4073 [Discriminator loss: 0.702154, acc.: 60.16%] [Generator loss: 0.823469]\n",
      "4074 [Discriminator loss: 0.461463, acc.: 75.00%] [Generator loss: 1.104573]\n",
      "4075 [Discriminator loss: 0.371568, acc.: 82.81%] [Generator loss: 0.691810]\n",
      "4076 [Discriminator loss: 0.369594, acc.: 85.94%] [Generator loss: 0.519638]\n",
      "4077 [Discriminator loss: 0.278276, acc.: 87.50%] [Generator loss: 0.331597]\n",
      "4078 [Discriminator loss: 0.683697, acc.: 60.94%] [Generator loss: 0.496923]\n",
      "4079 [Discriminator loss: 0.584392, acc.: 61.72%] [Generator loss: 1.123875]\n",
      "4080 [Discriminator loss: 0.366911, acc.: 80.47%] [Generator loss: 0.814308]\n",
      "4081 [Discriminator loss: 1.549025, acc.: 19.53%] [Generator loss: 0.376735]\n",
      "4082 [Discriminator loss: 0.406963, acc.: 83.59%] [Generator loss: 1.048791]\n",
      "4083 [Discriminator loss: 0.673703, acc.: 57.81%] [Generator loss: 1.314352]\n",
      "4084 [Discriminator loss: 0.785352, acc.: 48.44%] [Generator loss: 1.119611]\n",
      "4085 [Discriminator loss: 1.141971, acc.: 28.12%] [Generator loss: 1.354282]\n",
      "4086 [Discriminator loss: 0.909685, acc.: 43.75%] [Generator loss: 1.777102]\n",
      "4087 [Discriminator loss: 0.682533, acc.: 62.50%] [Generator loss: 1.314956]\n",
      "4088 [Discriminator loss: 0.748660, acc.: 60.94%] [Generator loss: 1.397115]\n",
      "4089 [Discriminator loss: 0.898227, acc.: 50.78%] [Generator loss: 1.043455]\n",
      "4090 [Discriminator loss: 0.608438, acc.: 65.62%] [Generator loss: 1.113876]\n",
      "4091 [Discriminator loss: 0.619472, acc.: 71.09%] [Generator loss: 0.938911]\n",
      "4092 [Discriminator loss: 0.883880, acc.: 46.09%] [Generator loss: 0.869631]\n",
      "4093 [Discriminator loss: 0.469018, acc.: 78.91%] [Generator loss: 1.409740]\n",
      "4094 [Discriminator loss: 0.548464, acc.: 71.88%] [Generator loss: 0.902687]\n",
      "4095 [Discriminator loss: 0.402799, acc.: 85.16%] [Generator loss: 0.936514]\n",
      "4096 [Discriminator loss: 0.360789, acc.: 87.50%] [Generator loss: 1.103696]\n",
      "4097 [Discriminator loss: 0.645577, acc.: 64.06%] [Generator loss: 1.306341]\n",
      "4098 [Discriminator loss: 1.071155, acc.: 30.47%] [Generator loss: 1.093442]\n",
      "4099 [Discriminator loss: 0.762577, acc.: 58.59%] [Generator loss: 1.177766]\n",
      "4100 [Discriminator loss: 0.716259, acc.: 59.38%] [Generator loss: 1.285867]\n",
      "4101 [Discriminator loss: 0.644105, acc.: 65.62%] [Generator loss: 0.872028]\n",
      "4102 [Discriminator loss: 0.680716, acc.: 62.50%] [Generator loss: 1.040267]\n",
      "4103 [Discriminator loss: 0.629547, acc.: 63.28%] [Generator loss: 1.010407]\n",
      "4104 [Discriminator loss: 0.658476, acc.: 64.06%] [Generator loss: 0.778447]\n",
      "4105 [Discriminator loss: 0.344188, acc.: 89.84%] [Generator loss: 0.726143]\n",
      "4106 [Discriminator loss: 0.598695, acc.: 66.41%] [Generator loss: 0.540833]\n",
      "4107 [Discriminator loss: 0.533024, acc.: 72.66%] [Generator loss: 0.673822]\n",
      "4108 [Discriminator loss: 0.604225, acc.: 66.41%] [Generator loss: 0.931144]\n",
      "4109 [Discriminator loss: 0.546319, acc.: 77.34%] [Generator loss: 0.888575]\n",
      "4110 [Discriminator loss: 0.675702, acc.: 61.72%] [Generator loss: 0.812958]\n",
      "4111 [Discriminator loss: 0.325220, acc.: 90.62%] [Generator loss: 1.256805]\n",
      "4112 [Discriminator loss: 0.512527, acc.: 70.31%] [Generator loss: 1.610470]\n",
      "4113 [Discriminator loss: 0.324566, acc.: 89.06%] [Generator loss: 1.233662]\n",
      "4114 [Discriminator loss: 0.365106, acc.: 85.94%] [Generator loss: 0.531081]\n",
      "4115 [Discriminator loss: 0.389841, acc.: 82.03%] [Generator loss: 0.629044]\n",
      "4116 [Discriminator loss: 0.503228, acc.: 74.22%] [Generator loss: 1.190156]\n",
      "4117 [Discriminator loss: 0.916938, acc.: 45.31%] [Generator loss: 1.562778]\n",
      "4118 [Discriminator loss: 0.453298, acc.: 79.69%] [Generator loss: 1.030111]\n",
      "4119 [Discriminator loss: 0.827026, acc.: 50.00%] [Generator loss: 1.286140]\n",
      "4120 [Discriminator loss: 0.811662, acc.: 56.25%] [Generator loss: 1.420294]\n",
      "4121 [Discriminator loss: 0.451790, acc.: 78.91%] [Generator loss: 0.616042]\n",
      "4122 [Discriminator loss: 0.739671, acc.: 55.47%] [Generator loss: 1.314956]\n",
      "4123 [Discriminator loss: 0.932567, acc.: 43.75%] [Generator loss: 1.331237]\n",
      "4124 [Discriminator loss: 0.566927, acc.: 70.31%] [Generator loss: 1.413392]\n",
      "4125 [Discriminator loss: 0.724069, acc.: 57.03%] [Generator loss: 0.884564]\n",
      "4126 [Discriminator loss: 0.732225, acc.: 55.47%] [Generator loss: 1.110944]\n",
      "4127 [Discriminator loss: 0.796775, acc.: 50.00%] [Generator loss: 1.533651]\n",
      "4128 [Discriminator loss: 0.708098, acc.: 60.16%] [Generator loss: 1.282521]\n",
      "4129 [Discriminator loss: 0.608952, acc.: 69.53%] [Generator loss: 1.454856]\n",
      "4130 [Discriminator loss: 0.693379, acc.: 52.34%] [Generator loss: 1.548249]\n",
      "4131 [Discriminator loss: 0.606519, acc.: 66.41%] [Generator loss: 1.423420]\n",
      "4132 [Discriminator loss: 0.820681, acc.: 44.53%] [Generator loss: 1.392814]\n",
      "4133 [Discriminator loss: 0.485236, acc.: 81.25%] [Generator loss: 1.591686]\n",
      "4134 [Discriminator loss: 0.994730, acc.: 42.97%] [Generator loss: 1.079446]\n",
      "4135 [Discriminator loss: 0.519361, acc.: 75.78%] [Generator loss: 1.286463]\n",
      "4136 [Discriminator loss: 0.697816, acc.: 60.94%] [Generator loss: 1.162096]\n",
      "4137 [Discriminator loss: 0.761137, acc.: 56.25%] [Generator loss: 1.413618]\n",
      "4138 [Discriminator loss: 0.535558, acc.: 72.66%] [Generator loss: 0.929547]\n",
      "4139 [Discriminator loss: 0.885825, acc.: 50.00%] [Generator loss: 1.057541]\n",
      "4140 [Discriminator loss: 0.507588, acc.: 80.47%] [Generator loss: 1.073164]\n",
      "4141 [Discriminator loss: 0.662961, acc.: 64.06%] [Generator loss: 0.783405]\n",
      "4142 [Discriminator loss: 0.887153, acc.: 44.53%] [Generator loss: 0.915272]\n",
      "4143 [Discriminator loss: 0.911758, acc.: 44.53%] [Generator loss: 1.260162]\n",
      "4144 [Discriminator loss: 0.464583, acc.: 80.47%] [Generator loss: 0.995404]\n",
      "4145 [Discriminator loss: 0.934388, acc.: 47.66%] [Generator loss: 1.227149]\n",
      "4146 [Discriminator loss: 0.422423, acc.: 78.91%] [Generator loss: 1.195576]\n",
      "4147 [Discriminator loss: 0.865609, acc.: 46.09%] [Generator loss: 1.286373]\n",
      "4148 [Discriminator loss: 0.773132, acc.: 55.47%] [Generator loss: 1.729310]\n",
      "4149 [Discriminator loss: 1.138568, acc.: 33.59%] [Generator loss: 1.432016]\n",
      "4150 [Discriminator loss: 0.449731, acc.: 78.91%] [Generator loss: 1.059700]\n",
      "4151 [Discriminator loss: 0.961769, acc.: 41.41%] [Generator loss: 0.948190]\n",
      "4152 [Discriminator loss: 0.486993, acc.: 71.09%] [Generator loss: 1.038363]\n",
      "4153 [Discriminator loss: 0.592400, acc.: 66.41%] [Generator loss: 0.628429]\n",
      "4154 [Discriminator loss: 0.737238, acc.: 62.50%] [Generator loss: 0.710287]\n",
      "4155 [Discriminator loss: 0.542592, acc.: 72.66%] [Generator loss: 1.099401]\n",
      "4156 [Discriminator loss: 0.393947, acc.: 85.16%] [Generator loss: 1.552375]\n",
      "4157 [Discriminator loss: 0.889055, acc.: 48.44%] [Generator loss: 0.955690]\n",
      "4158 [Discriminator loss: 0.538520, acc.: 72.66%] [Generator loss: 0.996794]\n",
      "4159 [Discriminator loss: 0.482647, acc.: 78.91%] [Generator loss: 1.008100]\n",
      "4160 [Discriminator loss: 0.731822, acc.: 59.38%] [Generator loss: 1.189402]\n",
      "4161 [Discriminator loss: 1.050455, acc.: 44.53%] [Generator loss: 1.177165]\n",
      "4162 [Discriminator loss: 0.736322, acc.: 60.16%] [Generator loss: 1.261935]\n",
      "4163 [Discriminator loss: 0.506585, acc.: 75.78%] [Generator loss: 1.253243]\n",
      "4164 [Discriminator loss: 0.704385, acc.: 60.16%] [Generator loss: 1.093836]\n",
      "4165 [Discriminator loss: 0.668701, acc.: 60.16%] [Generator loss: 1.346321]\n",
      "4166 [Discriminator loss: 0.541805, acc.: 71.09%] [Generator loss: 1.518475]\n",
      "4167 [Discriminator loss: 0.684793, acc.: 60.94%] [Generator loss: 1.177563]\n",
      "4168 [Discriminator loss: 0.745267, acc.: 58.59%] [Generator loss: 1.221538]\n",
      "4169 [Discriminator loss: 0.688327, acc.: 63.28%] [Generator loss: 1.156454]\n",
      "4170 [Discriminator loss: 0.762208, acc.: 53.12%] [Generator loss: 1.356096]\n",
      "4171 [Discriminator loss: 0.658668, acc.: 59.38%] [Generator loss: 1.405974]\n",
      "4172 [Discriminator loss: 0.751586, acc.: 55.47%] [Generator loss: 1.417061]\n",
      "4173 [Discriminator loss: 0.840830, acc.: 42.19%] [Generator loss: 1.064645]\n",
      "4174 [Discriminator loss: 0.872698, acc.: 45.31%] [Generator loss: 1.242075]\n",
      "4175 [Discriminator loss: 0.708656, acc.: 61.72%] [Generator loss: 1.260744]\n",
      "4176 [Discriminator loss: 0.812770, acc.: 46.09%] [Generator loss: 1.157486]\n",
      "4177 [Discriminator loss: 0.776138, acc.: 53.12%] [Generator loss: 1.145795]\n",
      "4178 [Discriminator loss: 0.505935, acc.: 77.34%] [Generator loss: 1.275634]\n",
      "4179 [Discriminator loss: 0.526663, acc.: 71.88%] [Generator loss: 0.951251]\n",
      "4180 [Discriminator loss: 0.559724, acc.: 67.97%] [Generator loss: 0.949266]\n",
      "4181 [Discriminator loss: 0.503590, acc.: 75.78%] [Generator loss: 1.227979]\n",
      "4182 [Discriminator loss: 0.710068, acc.: 53.91%] [Generator loss: 1.055622]\n",
      "4183 [Discriminator loss: 0.556790, acc.: 73.44%] [Generator loss: 1.351520]\n",
      "4184 [Discriminator loss: 0.926289, acc.: 37.50%] [Generator loss: 1.015178]\n",
      "4185 [Discriminator loss: 0.711007, acc.: 53.12%] [Generator loss: 1.196859]\n",
      "4186 [Discriminator loss: 0.900084, acc.: 42.97%] [Generator loss: 1.047717]\n",
      "4187 [Discriminator loss: 0.361680, acc.: 87.50%] [Generator loss: 1.174279]\n",
      "4188 [Discriminator loss: 1.137499, acc.: 25.78%] [Generator loss: 0.705098]\n",
      "4189 [Discriminator loss: 0.716234, acc.: 56.25%] [Generator loss: 1.012402]\n",
      "4190 [Discriminator loss: 0.665400, acc.: 60.94%] [Generator loss: 1.410979]\n",
      "4191 [Discriminator loss: 0.872641, acc.: 48.44%] [Generator loss: 0.996792]\n",
      "4192 [Discriminator loss: 0.575927, acc.: 67.97%] [Generator loss: 1.457451]\n",
      "4193 [Discriminator loss: 0.587913, acc.: 67.19%] [Generator loss: 1.377527]\n",
      "4194 [Discriminator loss: 0.642295, acc.: 65.62%] [Generator loss: 1.336794]\n",
      "4195 [Discriminator loss: 0.482153, acc.: 77.34%] [Generator loss: 1.265180]\n",
      "4196 [Discriminator loss: 0.924173, acc.: 43.75%] [Generator loss: 1.024584]\n",
      "4197 [Discriminator loss: 0.569212, acc.: 68.75%] [Generator loss: 1.207549]\n",
      "4198 [Discriminator loss: 0.293774, acc.: 92.97%] [Generator loss: 0.711750]\n",
      "4199 [Discriminator loss: 0.442345, acc.: 82.03%] [Generator loss: 1.030947]\n",
      "4200 [Discriminator loss: 0.715582, acc.: 57.81%] [Generator loss: 1.161162]\n",
      "4201 [Discriminator loss: 0.902917, acc.: 45.31%] [Generator loss: 1.213769]\n",
      "4202 [Discriminator loss: 0.642796, acc.: 64.06%] [Generator loss: 1.396570]\n",
      "4203 [Discriminator loss: 0.535849, acc.: 71.09%] [Generator loss: 1.512222]\n",
      "4204 [Discriminator loss: 0.812216, acc.: 53.91%] [Generator loss: 1.287873]\n",
      "4205 [Discriminator loss: 0.530742, acc.: 75.00%] [Generator loss: 1.085989]\n",
      "4206 [Discriminator loss: 0.573241, acc.: 72.66%] [Generator loss: 1.199303]\n",
      "4207 [Discriminator loss: 0.902757, acc.: 46.09%] [Generator loss: 1.156654]\n",
      "4208 [Discriminator loss: 0.613495, acc.: 70.31%] [Generator loss: 1.390161]\n",
      "4209 [Discriminator loss: 0.871888, acc.: 47.66%] [Generator loss: 1.244735]\n",
      "4210 [Discriminator loss: 1.162039, acc.: 33.59%] [Generator loss: 1.610704]\n",
      "4211 [Discriminator loss: 0.932795, acc.: 50.00%] [Generator loss: 1.561841]\n",
      "4212 [Discriminator loss: 0.620038, acc.: 64.84%] [Generator loss: 0.981850]\n",
      "4213 [Discriminator loss: 0.669778, acc.: 61.72%] [Generator loss: 1.221634]\n",
      "4214 [Discriminator loss: 0.688157, acc.: 64.06%] [Generator loss: 1.215093]\n",
      "4215 [Discriminator loss: 0.751444, acc.: 54.69%] [Generator loss: 1.162507]\n",
      "4216 [Discriminator loss: 0.883890, acc.: 41.41%] [Generator loss: 0.976432]\n",
      "4217 [Discriminator loss: 0.492440, acc.: 79.69%] [Generator loss: 1.257185]\n",
      "4218 [Discriminator loss: 0.670899, acc.: 56.25%] [Generator loss: 0.983844]\n",
      "4219 [Discriminator loss: 0.634319, acc.: 62.50%] [Generator loss: 1.254609]\n",
      "4220 [Discriminator loss: 0.882778, acc.: 38.28%] [Generator loss: 1.118408]\n",
      "4221 [Discriminator loss: 0.973351, acc.: 32.81%] [Generator loss: 1.160947]\n",
      "4222 [Discriminator loss: 0.687028, acc.: 60.16%] [Generator loss: 1.089269]\n",
      "4223 [Discriminator loss: 0.673012, acc.: 61.72%] [Generator loss: 0.824569]\n",
      "4224 [Discriminator loss: 0.440168, acc.: 81.25%] [Generator loss: 1.086480]\n",
      "4225 [Discriminator loss: 0.608389, acc.: 64.06%] [Generator loss: 0.987706]\n",
      "4226 [Discriminator loss: 0.762305, acc.: 50.00%] [Generator loss: 1.254422]\n",
      "4227 [Discriminator loss: 0.688847, acc.: 56.25%] [Generator loss: 1.303552]\n",
      "4228 [Discriminator loss: 0.623027, acc.: 62.50%] [Generator loss: 1.381119]\n",
      "4229 [Discriminator loss: 0.603987, acc.: 65.62%] [Generator loss: 1.295092]\n",
      "4230 [Discriminator loss: 0.686955, acc.: 60.16%] [Generator loss: 0.990264]\n",
      "4231 [Discriminator loss: 0.430194, acc.: 80.47%] [Generator loss: 1.042439]\n",
      "4232 [Discriminator loss: 0.380599, acc.: 82.81%] [Generator loss: 1.438763]\n",
      "4233 [Discriminator loss: 0.633177, acc.: 67.19%] [Generator loss: 1.434485]\n",
      "4234 [Discriminator loss: 0.458287, acc.: 81.25%] [Generator loss: 1.388407]\n",
      "4235 [Discriminator loss: 0.669269, acc.: 66.41%] [Generator loss: 1.169784]\n",
      "4236 [Discriminator loss: 0.734697, acc.: 60.94%] [Generator loss: 1.196942]\n",
      "4237 [Discriminator loss: 0.690036, acc.: 57.03%] [Generator loss: 1.410763]\n",
      "4238 [Discriminator loss: 0.589898, acc.: 67.97%] [Generator loss: 1.064368]\n",
      "4239 [Discriminator loss: 0.460876, acc.: 78.12%] [Generator loss: 1.385473]\n",
      "4240 [Discriminator loss: 0.629960, acc.: 68.75%] [Generator loss: 0.898896]\n",
      "4241 [Discriminator loss: 0.547690, acc.: 75.00%] [Generator loss: 0.620319]\n",
      "4242 [Discriminator loss: 0.576481, acc.: 72.66%] [Generator loss: 0.839385]\n",
      "4243 [Discriminator loss: 0.789327, acc.: 51.56%] [Generator loss: 1.699008]\n",
      "4244 [Discriminator loss: 0.600802, acc.: 67.19%] [Generator loss: 1.313218]\n",
      "4245 [Discriminator loss: 0.580677, acc.: 67.97%] [Generator loss: 0.757696]\n",
      "4246 [Discriminator loss: 0.352785, acc.: 86.72%] [Generator loss: 0.714846]\n",
      "4247 [Discriminator loss: 0.317773, acc.: 88.28%] [Generator loss: 0.836939]\n",
      "4248 [Discriminator loss: 0.542907, acc.: 72.66%] [Generator loss: 1.991013]\n",
      "4249 [Discriminator loss: 1.018425, acc.: 39.84%] [Generator loss: 1.657197]\n",
      "4250 [Discriminator loss: 0.334533, acc.: 86.72%] [Generator loss: 1.325691]\n",
      "4251 [Discriminator loss: 0.382654, acc.: 84.38%] [Generator loss: 1.334535]\n",
      "4252 [Discriminator loss: 0.618554, acc.: 66.41%] [Generator loss: 1.997342]\n",
      "4253 [Discriminator loss: 0.538298, acc.: 77.34%] [Generator loss: 1.455783]\n",
      "4254 [Discriminator loss: 0.626073, acc.: 64.06%] [Generator loss: 1.205546]\n",
      "4255 [Discriminator loss: 0.569937, acc.: 72.66%] [Generator loss: 1.314058]\n",
      "4256 [Discriminator loss: 0.659831, acc.: 64.06%] [Generator loss: 1.265897]\n",
      "4257 [Discriminator loss: 0.518572, acc.: 78.12%] [Generator loss: 1.428980]\n",
      "4258 [Discriminator loss: 0.682716, acc.: 61.72%] [Generator loss: 1.070171]\n",
      "4259 [Discriminator loss: 0.751430, acc.: 56.25%] [Generator loss: 1.205569]\n",
      "4260 [Discriminator loss: 0.728107, acc.: 55.47%] [Generator loss: 1.039576]\n",
      "4261 [Discriminator loss: 0.630773, acc.: 64.84%] [Generator loss: 0.709261]\n",
      "4262 [Discriminator loss: 0.595259, acc.: 67.19%] [Generator loss: 1.463145]\n",
      "4263 [Discriminator loss: 0.722504, acc.: 50.78%] [Generator loss: 1.453972]\n",
      "4264 [Discriminator loss: 0.801537, acc.: 52.34%] [Generator loss: 0.879407]\n",
      "4265 [Discriminator loss: 0.689382, acc.: 60.94%] [Generator loss: 1.016759]\n",
      "4266 [Discriminator loss: 0.786459, acc.: 55.47%] [Generator loss: 1.071009]\n",
      "4267 [Discriminator loss: 1.107464, acc.: 28.91%] [Generator loss: 0.899944]\n",
      "4268 [Discriminator loss: 0.593897, acc.: 71.09%] [Generator loss: 1.225799]\n",
      "4269 [Discriminator loss: 0.836889, acc.: 49.22%] [Generator loss: 1.288666]\n",
      "4270 [Discriminator loss: 0.709143, acc.: 58.59%] [Generator loss: 1.220948]\n",
      "4271 [Discriminator loss: 0.904185, acc.: 42.97%] [Generator loss: 1.046889]\n",
      "4272 [Discriminator loss: 0.632517, acc.: 64.06%] [Generator loss: 1.473158]\n",
      "4273 [Discriminator loss: 0.763283, acc.: 51.56%] [Generator loss: 1.398429]\n",
      "4274 [Discriminator loss: 0.438709, acc.: 79.69%] [Generator loss: 0.789469]\n",
      "4275 [Discriminator loss: 0.679915, acc.: 57.81%] [Generator loss: 0.917959]\n",
      "4276 [Discriminator loss: 0.345152, acc.: 89.06%] [Generator loss: 0.832107]\n",
      "4277 [Discriminator loss: 0.473561, acc.: 76.56%] [Generator loss: 0.621769]\n",
      "4278 [Discriminator loss: 0.357876, acc.: 84.38%] [Generator loss: 1.323322]\n",
      "4279 [Discriminator loss: 0.870669, acc.: 54.69%] [Generator loss: 0.520365]\n",
      "4280 [Discriminator loss: 0.871824, acc.: 48.44%] [Generator loss: 1.074481]\n",
      "4281 [Discriminator loss: 0.400706, acc.: 80.47%] [Generator loss: 1.197045]\n",
      "4282 [Discriminator loss: 0.479397, acc.: 80.47%] [Generator loss: 0.868417]\n",
      "4283 [Discriminator loss: 0.450555, acc.: 78.91%] [Generator loss: 0.885218]\n",
      "4284 [Discriminator loss: 0.735795, acc.: 59.38%] [Generator loss: 0.939540]\n",
      "4285 [Discriminator loss: 1.084633, acc.: 33.59%] [Generator loss: 0.891172]\n",
      "4286 [Discriminator loss: 0.732211, acc.: 57.81%] [Generator loss: 1.420625]\n",
      "4287 [Discriminator loss: 0.752924, acc.: 56.25%] [Generator loss: 1.206183]\n",
      "4288 [Discriminator loss: 0.771858, acc.: 50.78%] [Generator loss: 1.208294]\n",
      "4289 [Discriminator loss: 0.843076, acc.: 42.97%] [Generator loss: 1.088844]\n",
      "4290 [Discriminator loss: 0.519153, acc.: 79.69%] [Generator loss: 1.229617]\n",
      "4291 [Discriminator loss: 0.750743, acc.: 53.91%] [Generator loss: 1.140613]\n",
      "4292 [Discriminator loss: 0.812043, acc.: 46.09%] [Generator loss: 1.112828]\n",
      "4293 [Discriminator loss: 0.538817, acc.: 69.53%] [Generator loss: 1.139597]\n",
      "4294 [Discriminator loss: 0.763512, acc.: 52.34%] [Generator loss: 1.061560]\n",
      "4295 [Discriminator loss: 0.665412, acc.: 60.16%] [Generator loss: 1.466810]\n",
      "4296 [Discriminator loss: 0.762399, acc.: 50.78%] [Generator loss: 1.399667]\n",
      "4297 [Discriminator loss: 0.619298, acc.: 69.53%] [Generator loss: 1.803418]\n",
      "4298 [Discriminator loss: 0.848290, acc.: 47.66%] [Generator loss: 1.599002]\n",
      "4299 [Discriminator loss: 0.578984, acc.: 71.88%] [Generator loss: 0.920985]\n",
      "4300 [Discriminator loss: 0.689668, acc.: 61.72%] [Generator loss: 0.861689]\n",
      "4301 [Discriminator loss: 0.683455, acc.: 67.19%] [Generator loss: 1.053074]\n",
      "4302 [Discriminator loss: 0.409910, acc.: 81.25%] [Generator loss: 0.935346]\n",
      "4303 [Discriminator loss: 0.710538, acc.: 64.06%] [Generator loss: 1.079830]\n",
      "4304 [Discriminator loss: 0.865163, acc.: 48.44%] [Generator loss: 1.361295]\n",
      "4305 [Discriminator loss: 0.494289, acc.: 75.00%] [Generator loss: 1.606427]\n",
      "4306 [Discriminator loss: 0.678892, acc.: 58.59%] [Generator loss: 1.165101]\n",
      "4307 [Discriminator loss: 0.624173, acc.: 68.75%] [Generator loss: 1.943922]\n",
      "4308 [Discriminator loss: 0.522930, acc.: 70.31%] [Generator loss: 1.222723]\n",
      "4309 [Discriminator loss: 0.809239, acc.: 49.22%] [Generator loss: 0.981201]\n",
      "4310 [Discriminator loss: 0.606721, acc.: 73.44%] [Generator loss: 1.045728]\n",
      "4311 [Discriminator loss: 0.792211, acc.: 56.25%] [Generator loss: 0.627030]\n",
      "4312 [Discriminator loss: 0.565160, acc.: 67.97%] [Generator loss: 0.651468]\n",
      "4313 [Discriminator loss: 1.188408, acc.: 25.78%] [Generator loss: 1.071554]\n",
      "4314 [Discriminator loss: 0.400461, acc.: 85.94%] [Generator loss: 1.232332]\n",
      "4315 [Discriminator loss: 0.352485, acc.: 82.81%] [Generator loss: 0.616748]\n",
      "4316 [Discriminator loss: 0.780680, acc.: 53.12%] [Generator loss: 0.724456]\n",
      "4317 [Discriminator loss: 0.271858, acc.: 89.84%] [Generator loss: 0.726581]\n",
      "4318 [Discriminator loss: 0.771748, acc.: 55.47%] [Generator loss: 1.112758]\n",
      "4319 [Discriminator loss: 0.364548, acc.: 81.25%] [Generator loss: 0.740156]\n",
      "4320 [Discriminator loss: 0.675962, acc.: 64.84%] [Generator loss: 0.537561]\n",
      "4321 [Discriminator loss: 1.143602, acc.: 32.81%] [Generator loss: 0.842480]\n",
      "4322 [Discriminator loss: 1.303303, acc.: 21.88%] [Generator loss: 1.243750]\n",
      "4323 [Discriminator loss: 0.530418, acc.: 74.22%] [Generator loss: 1.421800]\n",
      "4324 [Discriminator loss: 0.576302, acc.: 67.97%] [Generator loss: 1.242458]\n",
      "4325 [Discriminator loss: 0.874359, acc.: 42.97%] [Generator loss: 0.991801]\n",
      "4326 [Discriminator loss: 0.508083, acc.: 78.12%] [Generator loss: 1.150004]\n",
      "4327 [Discriminator loss: 0.429148, acc.: 80.47%] [Generator loss: 0.904198]\n",
      "4328 [Discriminator loss: 0.618357, acc.: 63.28%] [Generator loss: 0.830601]\n",
      "4329 [Discriminator loss: 0.308439, acc.: 89.84%] [Generator loss: 0.705325]\n",
      "4330 [Discriminator loss: 0.839275, acc.: 52.34%] [Generator loss: 0.510651]\n",
      "4331 [Discriminator loss: 0.310896, acc.: 89.84%] [Generator loss: 0.824776]\n",
      "4332 [Discriminator loss: 0.295113, acc.: 88.28%] [Generator loss: 0.483755]\n",
      "4333 [Discriminator loss: 1.123336, acc.: 38.28%] [Generator loss: 0.805470]\n",
      "4334 [Discriminator loss: 0.561077, acc.: 74.22%] [Generator loss: 1.549394]\n",
      "4335 [Discriminator loss: 0.655435, acc.: 60.94%] [Generator loss: 1.823492]\n",
      "4336 [Discriminator loss: 0.367153, acc.: 81.25%] [Generator loss: 0.853128]\n",
      "4337 [Discriminator loss: 0.482121, acc.: 75.00%] [Generator loss: 0.889502]\n",
      "4338 [Discriminator loss: 0.825329, acc.: 53.91%] [Generator loss: 1.502572]\n",
      "4339 [Discriminator loss: 0.687216, acc.: 61.72%] [Generator loss: 1.557315]\n",
      "4340 [Discriminator loss: 0.557918, acc.: 75.78%] [Generator loss: 0.975511]\n",
      "4341 [Discriminator loss: 0.636633, acc.: 67.19%] [Generator loss: 1.016267]\n",
      "4342 [Discriminator loss: 0.491363, acc.: 74.22%] [Generator loss: 1.162886]\n",
      "4343 [Discriminator loss: 0.513940, acc.: 69.53%] [Generator loss: 1.452826]\n",
      "4344 [Discriminator loss: 0.559571, acc.: 77.34%] [Generator loss: 0.897032]\n",
      "4345 [Discriminator loss: 0.611624, acc.: 63.28%] [Generator loss: 1.420838]\n",
      "4346 [Discriminator loss: 0.705400, acc.: 64.06%] [Generator loss: 1.313566]\n",
      "4347 [Discriminator loss: 0.916141, acc.: 42.97%] [Generator loss: 1.174487]\n",
      "4348 [Discriminator loss: 0.971138, acc.: 46.09%] [Generator loss: 1.032729]\n",
      "4349 [Discriminator loss: 0.765935, acc.: 60.94%] [Generator loss: 1.425760]\n",
      "4350 [Discriminator loss: 0.509922, acc.: 71.88%] [Generator loss: 1.346464]\n",
      "4351 [Discriminator loss: 0.759361, acc.: 52.34%] [Generator loss: 1.327772]\n",
      "4352 [Discriminator loss: 0.365609, acc.: 86.72%] [Generator loss: 0.971144]\n",
      "4353 [Discriminator loss: 0.580737, acc.: 71.88%] [Generator loss: 0.624057]\n",
      "4354 [Discriminator loss: 0.411691, acc.: 82.81%] [Generator loss: 0.600069]\n",
      "4355 [Discriminator loss: 0.483415, acc.: 75.00%] [Generator loss: 0.505754]\n",
      "4356 [Discriminator loss: 0.254136, acc.: 92.19%] [Generator loss: 0.334338]\n",
      "4357 [Discriminator loss: 0.741187, acc.: 60.94%] [Generator loss: 1.443163]\n",
      "4358 [Discriminator loss: 0.898424, acc.: 53.12%] [Generator loss: 1.549638]\n",
      "4359 [Discriminator loss: 0.474301, acc.: 78.12%] [Generator loss: 1.566734]\n",
      "4360 [Discriminator loss: 0.596780, acc.: 69.53%] [Generator loss: 1.461895]\n",
      "4361 [Discriminator loss: 1.142333, acc.: 28.91%] [Generator loss: 1.549192]\n",
      "4362 [Discriminator loss: 0.459773, acc.: 75.78%] [Generator loss: 1.674548]\n",
      "4363 [Discriminator loss: 0.687060, acc.: 59.38%] [Generator loss: 1.182574]\n",
      "4364 [Discriminator loss: 0.618969, acc.: 65.62%] [Generator loss: 1.029117]\n",
      "4365 [Discriminator loss: 1.041301, acc.: 54.69%] [Generator loss: 0.541902]\n",
      "4366 [Discriminator loss: 0.422296, acc.: 85.16%] [Generator loss: 0.421657]\n",
      "4367 [Discriminator loss: 0.819577, acc.: 49.22%] [Generator loss: 0.985070]\n",
      "4368 [Discriminator loss: 0.428430, acc.: 77.34%] [Generator loss: 1.235889]\n",
      "4369 [Discriminator loss: 0.656280, acc.: 65.62%] [Generator loss: 0.853110]\n",
      "4370 [Discriminator loss: 0.736026, acc.: 56.25%] [Generator loss: 1.126844]\n",
      "4371 [Discriminator loss: 0.440117, acc.: 79.69%] [Generator loss: 1.681701]\n",
      "4372 [Discriminator loss: 0.569737, acc.: 67.97%] [Generator loss: 1.380421]\n",
      "4373 [Discriminator loss: 0.402357, acc.: 85.94%] [Generator loss: 1.282476]\n",
      "4374 [Discriminator loss: 0.504338, acc.: 77.34%] [Generator loss: 1.016041]\n",
      "4375 [Discriminator loss: 0.745150, acc.: 53.91%] [Generator loss: 1.306638]\n",
      "4376 [Discriminator loss: 0.601987, acc.: 65.62%] [Generator loss: 1.472851]\n",
      "4377 [Discriminator loss: 0.625012, acc.: 69.53%] [Generator loss: 1.027651]\n",
      "4378 [Discriminator loss: 0.842359, acc.: 46.09%] [Generator loss: 1.033751]\n",
      "4379 [Discriminator loss: 0.692540, acc.: 61.72%] [Generator loss: 1.344952]\n",
      "4380 [Discriminator loss: 0.710538, acc.: 62.50%] [Generator loss: 0.915433]\n",
      "4381 [Discriminator loss: 0.762699, acc.: 52.34%] [Generator loss: 0.837550]\n",
      "4382 [Discriminator loss: 0.689599, acc.: 56.25%] [Generator loss: 0.892245]\n",
      "4383 [Discriminator loss: 0.472379, acc.: 78.91%] [Generator loss: 0.814348]\n",
      "4384 [Discriminator loss: 1.042902, acc.: 40.62%] [Generator loss: 0.951410]\n",
      "4385 [Discriminator loss: 0.557664, acc.: 67.19%] [Generator loss: 1.012938]\n",
      "4386 [Discriminator loss: 1.043772, acc.: 41.41%] [Generator loss: 1.582712]\n",
      "4387 [Discriminator loss: 0.710944, acc.: 63.28%] [Generator loss: 1.297705]\n",
      "4388 [Discriminator loss: 0.986168, acc.: 35.16%] [Generator loss: 1.341905]\n",
      "4389 [Discriminator loss: 0.726314, acc.: 57.03%] [Generator loss: 1.394736]\n",
      "4390 [Discriminator loss: 0.659481, acc.: 61.72%] [Generator loss: 1.514743]\n",
      "4391 [Discriminator loss: 0.544090, acc.: 73.44%] [Generator loss: 1.301600]\n",
      "4392 [Discriminator loss: 0.958860, acc.: 40.62%] [Generator loss: 1.215123]\n",
      "4393 [Discriminator loss: 0.757761, acc.: 56.25%] [Generator loss: 0.859875]\n",
      "4394 [Discriminator loss: 0.648095, acc.: 60.16%] [Generator loss: 0.953602]\n",
      "4395 [Discriminator loss: 0.650978, acc.: 60.94%] [Generator loss: 1.102380]\n",
      "4396 [Discriminator loss: 0.809618, acc.: 46.88%] [Generator loss: 1.271688]\n",
      "4397 [Discriminator loss: 0.568283, acc.: 67.19%] [Generator loss: 1.310670]\n",
      "4398 [Discriminator loss: 0.556668, acc.: 74.22%] [Generator loss: 0.984852]\n",
      "4399 [Discriminator loss: 0.808921, acc.: 56.25%] [Generator loss: 1.162199]\n",
      "4400 [Discriminator loss: 0.666137, acc.: 60.94%] [Generator loss: 1.363498]\n",
      "4401 [Discriminator loss: 0.943798, acc.: 43.75%] [Generator loss: 1.119273]\n",
      "4402 [Discriminator loss: 0.587191, acc.: 69.53%] [Generator loss: 1.345878]\n",
      "4403 [Discriminator loss: 0.650515, acc.: 59.38%] [Generator loss: 1.099887]\n",
      "4404 [Discriminator loss: 0.611298, acc.: 66.41%] [Generator loss: 1.037252]\n",
      "4405 [Discriminator loss: 0.700287, acc.: 63.28%] [Generator loss: 1.549954]\n",
      "4406 [Discriminator loss: 0.583961, acc.: 67.19%] [Generator loss: 1.111475]\n",
      "4407 [Discriminator loss: 0.615148, acc.: 66.41%] [Generator loss: 1.136208]\n",
      "4408 [Discriminator loss: 0.704128, acc.: 54.69%] [Generator loss: 1.385203]\n",
      "4409 [Discriminator loss: 0.812309, acc.: 50.00%] [Generator loss: 2.005060]\n",
      "4410 [Discriminator loss: 0.689850, acc.: 60.94%] [Generator loss: 1.748798]\n",
      "4411 [Discriminator loss: 0.597679, acc.: 70.31%] [Generator loss: 2.075693]\n",
      "4412 [Discriminator loss: 0.579212, acc.: 71.09%] [Generator loss: 1.854723]\n",
      "4413 [Discriminator loss: 0.573577, acc.: 69.53%] [Generator loss: 1.501855]\n",
      "4414 [Discriminator loss: 0.604705, acc.: 65.62%] [Generator loss: 1.480401]\n",
      "4415 [Discriminator loss: 0.625530, acc.: 64.06%] [Generator loss: 1.552100]\n",
      "4416 [Discriminator loss: 0.663733, acc.: 66.41%] [Generator loss: 1.180568]\n",
      "4417 [Discriminator loss: 0.870249, acc.: 48.44%] [Generator loss: 1.341101]\n",
      "4418 [Discriminator loss: 0.698499, acc.: 62.50%] [Generator loss: 1.286665]\n",
      "4419 [Discriminator loss: 0.749026, acc.: 53.12%] [Generator loss: 1.227913]\n",
      "4420 [Discriminator loss: 0.872462, acc.: 41.41%] [Generator loss: 1.138743]\n",
      "4421 [Discriminator loss: 0.838455, acc.: 42.97%] [Generator loss: 0.931119]\n",
      "4422 [Discriminator loss: 0.832586, acc.: 51.56%] [Generator loss: 1.083831]\n",
      "4423 [Discriminator loss: 0.562373, acc.: 69.53%] [Generator loss: 1.184815]\n",
      "4424 [Discriminator loss: 0.597104, acc.: 67.97%] [Generator loss: 1.355292]\n",
      "4425 [Discriminator loss: 0.839849, acc.: 40.62%] [Generator loss: 1.114442]\n",
      "4426 [Discriminator loss: 0.659808, acc.: 63.28%] [Generator loss: 1.248469]\n",
      "4427 [Discriminator loss: 0.572629, acc.: 65.62%] [Generator loss: 1.048385]\n",
      "4428 [Discriminator loss: 0.611587, acc.: 68.75%] [Generator loss: 1.026504]\n",
      "4429 [Discriminator loss: 0.716036, acc.: 58.59%] [Generator loss: 1.366106]\n",
      "4430 [Discriminator loss: 0.811955, acc.: 49.22%] [Generator loss: 1.096619]\n",
      "4431 [Discriminator loss: 0.620203, acc.: 66.41%] [Generator loss: 1.189103]\n",
      "4432 [Discriminator loss: 0.557713, acc.: 75.78%] [Generator loss: 1.235614]\n",
      "4433 [Discriminator loss: 0.784808, acc.: 50.00%] [Generator loss: 0.966671]\n",
      "4434 [Discriminator loss: 0.555837, acc.: 74.22%] [Generator loss: 0.968606]\n",
      "4435 [Discriminator loss: 0.582015, acc.: 67.97%] [Generator loss: 1.145877]\n",
      "4436 [Discriminator loss: 0.825786, acc.: 50.78%] [Generator loss: 0.847600]\n",
      "4437 [Discriminator loss: 0.430001, acc.: 83.59%] [Generator loss: 0.970966]\n",
      "4438 [Discriminator loss: 0.821752, acc.: 45.31%] [Generator loss: 1.376808]\n",
      "4439 [Discriminator loss: 0.721418, acc.: 56.25%] [Generator loss: 1.721373]\n",
      "4440 [Discriminator loss: 0.730345, acc.: 58.59%] [Generator loss: 1.173256]\n",
      "4441 [Discriminator loss: 0.848034, acc.: 50.00%] [Generator loss: 0.838952]\n",
      "4442 [Discriminator loss: 0.662421, acc.: 60.16%] [Generator loss: 1.051316]\n",
      "4443 [Discriminator loss: 0.752530, acc.: 53.12%] [Generator loss: 1.119881]\n",
      "4444 [Discriminator loss: 0.602527, acc.: 67.19%] [Generator loss: 0.942316]\n",
      "4445 [Discriminator loss: 1.098305, acc.: 28.12%] [Generator loss: 0.903502]\n",
      "4446 [Discriminator loss: 0.555927, acc.: 69.53%] [Generator loss: 0.880130]\n",
      "4447 [Discriminator loss: 0.844319, acc.: 51.56%] [Generator loss: 1.130200]\n",
      "4448 [Discriminator loss: 0.461914, acc.: 79.69%] [Generator loss: 1.203538]\n",
      "4449 [Discriminator loss: 0.622424, acc.: 67.97%] [Generator loss: 1.058995]\n",
      "4450 [Discriminator loss: 0.541849, acc.: 78.12%] [Generator loss: 1.013768]\n",
      "4451 [Discriminator loss: 1.098741, acc.: 39.06%] [Generator loss: 0.936451]\n",
      "4452 [Discriminator loss: 0.730385, acc.: 57.03%] [Generator loss: 1.230396]\n",
      "4453 [Discriminator loss: 0.621511, acc.: 70.31%] [Generator loss: 1.259834]\n",
      "4454 [Discriminator loss: 1.010499, acc.: 29.69%] [Generator loss: 1.116303]\n",
      "4455 [Discriminator loss: 0.664869, acc.: 64.06%] [Generator loss: 1.171246]\n",
      "4456 [Discriminator loss: 0.754142, acc.: 55.47%] [Generator loss: 1.057263]\n",
      "4457 [Discriminator loss: 0.862620, acc.: 46.09%] [Generator loss: 0.920035]\n",
      "4458 [Discriminator loss: 0.769650, acc.: 53.12%] [Generator loss: 1.337408]\n",
      "4459 [Discriminator loss: 0.811365, acc.: 52.34%] [Generator loss: 1.124270]\n",
      "4460 [Discriminator loss: 0.671701, acc.: 61.72%] [Generator loss: 1.454165]\n",
      "4461 [Discriminator loss: 0.412127, acc.: 84.38%] [Generator loss: 1.448688]\n",
      "4462 [Discriminator loss: 0.668691, acc.: 64.06%] [Generator loss: 1.113430]\n",
      "4463 [Discriminator loss: 0.738743, acc.: 53.91%] [Generator loss: 1.172158]\n",
      "4464 [Discriminator loss: 0.819099, acc.: 50.78%] [Generator loss: 1.102860]\n",
      "4465 [Discriminator loss: 0.824168, acc.: 49.22%] [Generator loss: 1.425058]\n",
      "4466 [Discriminator loss: 0.738764, acc.: 54.69%] [Generator loss: 1.243043]\n",
      "4467 [Discriminator loss: 0.616610, acc.: 63.28%] [Generator loss: 0.938763]\n",
      "4468 [Discriminator loss: 0.823254, acc.: 47.66%] [Generator loss: 0.872452]\n",
      "4469 [Discriminator loss: 0.689328, acc.: 58.59%] [Generator loss: 0.866612]\n",
      "4470 [Discriminator loss: 0.747305, acc.: 50.00%] [Generator loss: 1.161173]\n",
      "4471 [Discriminator loss: 0.667141, acc.: 60.16%] [Generator loss: 1.188680]\n",
      "4472 [Discriminator loss: 0.980201, acc.: 35.94%] [Generator loss: 1.072297]\n",
      "4473 [Discriminator loss: 0.760976, acc.: 49.22%] [Generator loss: 1.406386]\n",
      "4474 [Discriminator loss: 0.932044, acc.: 42.97%] [Generator loss: 1.175669]\n",
      "4475 [Discriminator loss: 0.914431, acc.: 43.75%] [Generator loss: 1.343361]\n",
      "4476 [Discriminator loss: 0.950860, acc.: 46.09%] [Generator loss: 1.003959]\n",
      "4477 [Discriminator loss: 0.782848, acc.: 51.56%] [Generator loss: 1.030976]\n",
      "4478 [Discriminator loss: 0.616544, acc.: 67.97%] [Generator loss: 1.180752]\n",
      "4479 [Discriminator loss: 0.792540, acc.: 44.53%] [Generator loss: 1.155440]\n",
      "4480 [Discriminator loss: 0.669826, acc.: 60.94%] [Generator loss: 1.398507]\n",
      "4481 [Discriminator loss: 0.857461, acc.: 55.47%] [Generator loss: 1.035113]\n",
      "4482 [Discriminator loss: 0.681450, acc.: 63.28%] [Generator loss: 0.955410]\n",
      "4483 [Discriminator loss: 0.629108, acc.: 64.84%] [Generator loss: 1.103929]\n",
      "4484 [Discriminator loss: 0.517204, acc.: 79.69%] [Generator loss: 1.046037]\n",
      "4485 [Discriminator loss: 0.667984, acc.: 58.59%] [Generator loss: 1.112794]\n",
      "4486 [Discriminator loss: 0.649266, acc.: 65.62%] [Generator loss: 1.135413]\n",
      "4487 [Discriminator loss: 0.801908, acc.: 52.34%] [Generator loss: 1.255967]\n",
      "4488 [Discriminator loss: 0.774433, acc.: 54.69%] [Generator loss: 1.360471]\n",
      "4489 [Discriminator loss: 0.682632, acc.: 59.38%] [Generator loss: 1.333453]\n",
      "4490 [Discriminator loss: 0.605433, acc.: 73.44%] [Generator loss: 1.320896]\n",
      "4491 [Discriminator loss: 0.695450, acc.: 60.16%] [Generator loss: 1.372717]\n",
      "4492 [Discriminator loss: 0.531316, acc.: 71.88%] [Generator loss: 1.018130]\n",
      "4493 [Discriminator loss: 0.666263, acc.: 62.50%] [Generator loss: 0.918814]\n",
      "4494 [Discriminator loss: 1.028562, acc.: 29.69%] [Generator loss: 0.911542]\n",
      "4495 [Discriminator loss: 0.606804, acc.: 67.97%] [Generator loss: 1.189611]\n",
      "4496 [Discriminator loss: 0.649790, acc.: 67.97%] [Generator loss: 1.214022]\n",
      "4497 [Discriminator loss: 0.683097, acc.: 56.25%] [Generator loss: 1.246593]\n",
      "4498 [Discriminator loss: 0.727805, acc.: 56.25%] [Generator loss: 1.666664]\n",
      "4499 [Discriminator loss: 0.729120, acc.: 55.47%] [Generator loss: 1.504867]\n",
      "4500 [Discriminator loss: 0.732834, acc.: 55.47%] [Generator loss: 1.267643]\n",
      "4501 [Discriminator loss: 0.790192, acc.: 52.34%] [Generator loss: 1.226870]\n",
      "4502 [Discriminator loss: 0.646597, acc.: 63.28%] [Generator loss: 1.466049]\n",
      "4503 [Discriminator loss: 0.665524, acc.: 64.06%] [Generator loss: 1.246701]\n",
      "4504 [Discriminator loss: 1.035466, acc.: 30.47%] [Generator loss: 1.236208]\n",
      "4505 [Discriminator loss: 0.774615, acc.: 57.03%] [Generator loss: 1.454049]\n",
      "4506 [Discriminator loss: 0.836182, acc.: 46.88%] [Generator loss: 1.323090]\n",
      "4507 [Discriminator loss: 0.728378, acc.: 60.16%] [Generator loss: 1.454468]\n",
      "4508 [Discriminator loss: 0.771966, acc.: 51.56%] [Generator loss: 1.311640]\n",
      "4509 [Discriminator loss: 0.630037, acc.: 60.94%] [Generator loss: 1.118930]\n",
      "4510 [Discriminator loss: 0.651330, acc.: 61.72%] [Generator loss: 1.157238]\n",
      "4511 [Discriminator loss: 0.559282, acc.: 71.09%] [Generator loss: 1.103622]\n",
      "4512 [Discriminator loss: 0.724448, acc.: 57.03%] [Generator loss: 1.486224]\n",
      "4513 [Discriminator loss: 0.625531, acc.: 65.62%] [Generator loss: 1.220384]\n",
      "4514 [Discriminator loss: 0.550105, acc.: 72.66%] [Generator loss: 0.888587]\n",
      "4515 [Discriminator loss: 0.450646, acc.: 81.25%] [Generator loss: 0.638529]\n",
      "4516 [Discriminator loss: 0.326436, acc.: 91.41%] [Generator loss: 0.603567]\n",
      "4517 [Discriminator loss: 0.408685, acc.: 82.03%] [Generator loss: 0.786650]\n",
      "4518 [Discriminator loss: 0.588721, acc.: 72.66%] [Generator loss: 1.095526]\n",
      "4519 [Discriminator loss: 0.349210, acc.: 82.81%] [Generator loss: 1.029748]\n",
      "4520 [Discriminator loss: 0.841394, acc.: 53.91%] [Generator loss: 1.142082]\n",
      "4521 [Discriminator loss: 0.488606, acc.: 81.25%] [Generator loss: 1.401525]\n",
      "4522 [Discriminator loss: 0.744966, acc.: 52.34%] [Generator loss: 1.363523]\n",
      "4523 [Discriminator loss: 0.642337, acc.: 63.28%] [Generator loss: 1.405603]\n",
      "4524 [Discriminator loss: 0.777364, acc.: 53.91%] [Generator loss: 1.093109]\n",
      "4525 [Discriminator loss: 0.325548, acc.: 88.28%] [Generator loss: 0.961447]\n",
      "4526 [Discriminator loss: 0.532785, acc.: 75.00%] [Generator loss: 0.945738]\n",
      "4527 [Discriminator loss: 0.753484, acc.: 53.12%] [Generator loss: 1.108497]\n",
      "4528 [Discriminator loss: 0.670857, acc.: 61.72%] [Generator loss: 1.503493]\n",
      "4529 [Discriminator loss: 0.867571, acc.: 42.97%] [Generator loss: 1.021563]\n",
      "4530 [Discriminator loss: 0.597724, acc.: 75.78%] [Generator loss: 1.176704]\n",
      "4531 [Discriminator loss: 0.722964, acc.: 56.25%] [Generator loss: 1.350712]\n",
      "4532 [Discriminator loss: 0.778298, acc.: 55.47%] [Generator loss: 1.836026]\n",
      "4533 [Discriminator loss: 0.607962, acc.: 68.75%] [Generator loss: 1.339694]\n",
      "4534 [Discriminator loss: 0.777825, acc.: 51.56%] [Generator loss: 0.851104]\n",
      "4535 [Discriminator loss: 0.621938, acc.: 62.50%] [Generator loss: 0.993179]\n",
      "4536 [Discriminator loss: 0.847107, acc.: 47.66%] [Generator loss: 1.093483]\n",
      "4537 [Discriminator loss: 0.574738, acc.: 70.31%] [Generator loss: 1.152332]\n",
      "4538 [Discriminator loss: 0.739756, acc.: 54.69%] [Generator loss: 1.147194]\n",
      "4539 [Discriminator loss: 0.829903, acc.: 48.44%] [Generator loss: 0.937553]\n",
      "4540 [Discriminator loss: 0.746802, acc.: 53.91%] [Generator loss: 1.144663]\n",
      "4541 [Discriminator loss: 0.732710, acc.: 46.09%] [Generator loss: 1.193642]\n",
      "4542 [Discriminator loss: 0.785209, acc.: 52.34%] [Generator loss: 1.160676]\n",
      "4543 [Discriminator loss: 0.462842, acc.: 79.69%] [Generator loss: 1.159034]\n",
      "4544 [Discriminator loss: 0.706055, acc.: 54.69%] [Generator loss: 1.018079]\n",
      "4545 [Discriminator loss: 0.536591, acc.: 70.31%] [Generator loss: 0.911985]\n",
      "4546 [Discriminator loss: 0.742432, acc.: 54.69%] [Generator loss: 1.347830]\n",
      "4547 [Discriminator loss: 0.688095, acc.: 60.16%] [Generator loss: 1.452147]\n",
      "4548 [Discriminator loss: 0.766488, acc.: 53.12%] [Generator loss: 1.299121]\n",
      "4549 [Discriminator loss: 0.593977, acc.: 66.41%] [Generator loss: 1.015104]\n",
      "4550 [Discriminator loss: 0.591873, acc.: 69.53%] [Generator loss: 1.324224]\n",
      "4551 [Discriminator loss: 0.772778, acc.: 50.78%] [Generator loss: 1.277692]\n",
      "4552 [Discriminator loss: 0.656814, acc.: 60.16%] [Generator loss: 1.350593]\n",
      "4553 [Discriminator loss: 0.611004, acc.: 60.94%] [Generator loss: 1.319371]\n",
      "4554 [Discriminator loss: 0.641262, acc.: 66.41%] [Generator loss: 1.089870]\n",
      "4555 [Discriminator loss: 0.618537, acc.: 64.06%] [Generator loss: 1.232849]\n",
      "4556 [Discriminator loss: 0.689256, acc.: 53.91%] [Generator loss: 1.417494]\n",
      "4557 [Discriminator loss: 0.631941, acc.: 67.19%] [Generator loss: 1.083229]\n",
      "4558 [Discriminator loss: 0.823798, acc.: 54.69%] [Generator loss: 1.174159]\n",
      "4559 [Discriminator loss: 0.630414, acc.: 60.94%] [Generator loss: 1.298254]\n",
      "4560 [Discriminator loss: 0.519991, acc.: 73.44%] [Generator loss: 1.327842]\n",
      "4561 [Discriminator loss: 0.604812, acc.: 65.62%] [Generator loss: 0.978396]\n",
      "4562 [Discriminator loss: 0.619998, acc.: 64.84%] [Generator loss: 1.097796]\n",
      "4563 [Discriminator loss: 0.913358, acc.: 46.88%] [Generator loss: 1.419952]\n",
      "4564 [Discriminator loss: 0.845327, acc.: 53.91%] [Generator loss: 1.259864]\n",
      "4565 [Discriminator loss: 0.997207, acc.: 31.25%] [Generator loss: 1.572847]\n",
      "4566 [Discriminator loss: 0.788524, acc.: 51.56%] [Generator loss: 1.331163]\n",
      "4567 [Discriminator loss: 0.928516, acc.: 38.28%] [Generator loss: 1.400534]\n",
      "4568 [Discriminator loss: 0.767098, acc.: 48.44%] [Generator loss: 1.387781]\n",
      "4569 [Discriminator loss: 0.784029, acc.: 52.34%] [Generator loss: 1.456752]\n",
      "4570 [Discriminator loss: 0.590251, acc.: 67.97%] [Generator loss: 1.254789]\n",
      "4571 [Discriminator loss: 0.798611, acc.: 52.34%] [Generator loss: 1.375687]\n",
      "4572 [Discriminator loss: 0.600843, acc.: 74.22%] [Generator loss: 1.322386]\n",
      "4573 [Discriminator loss: 0.579186, acc.: 70.31%] [Generator loss: 1.016725]\n",
      "4574 [Discriminator loss: 0.609438, acc.: 64.84%] [Generator loss: 1.519650]\n",
      "4575 [Discriminator loss: 0.521749, acc.: 73.44%] [Generator loss: 1.602198]\n",
      "4576 [Discriminator loss: 0.812314, acc.: 48.44%] [Generator loss: 1.626452]\n",
      "4577 [Discriminator loss: 0.586913, acc.: 71.88%] [Generator loss: 1.437016]\n",
      "4578 [Discriminator loss: 0.724938, acc.: 59.38%] [Generator loss: 1.361986]\n",
      "4579 [Discriminator loss: 0.657497, acc.: 62.50%] [Generator loss: 1.206553]\n",
      "4580 [Discriminator loss: 0.799250, acc.: 54.69%] [Generator loss: 1.239676]\n",
      "4581 [Discriminator loss: 0.651954, acc.: 61.72%] [Generator loss: 1.510362]\n",
      "4582 [Discriminator loss: 0.632809, acc.: 67.97%] [Generator loss: 1.582612]\n",
      "4583 [Discriminator loss: 1.017183, acc.: 36.72%] [Generator loss: 1.208348]\n",
      "4584 [Discriminator loss: 0.621936, acc.: 64.84%] [Generator loss: 1.399653]\n",
      "4585 [Discriminator loss: 0.780219, acc.: 50.00%] [Generator loss: 1.267146]\n",
      "4586 [Discriminator loss: 0.521905, acc.: 78.91%] [Generator loss: 1.134747]\n",
      "4587 [Discriminator loss: 0.752245, acc.: 57.03%] [Generator loss: 1.228910]\n",
      "4588 [Discriminator loss: 0.532704, acc.: 73.44%] [Generator loss: 1.129467]\n",
      "4589 [Discriminator loss: 0.539057, acc.: 76.56%] [Generator loss: 1.317065]\n",
      "4590 [Discriminator loss: 0.417386, acc.: 83.59%] [Generator loss: 1.196227]\n",
      "4591 [Discriminator loss: 0.599168, acc.: 66.41%] [Generator loss: 1.545225]\n",
      "4592 [Discriminator loss: 0.524591, acc.: 75.78%] [Generator loss: 1.104848]\n",
      "4593 [Discriminator loss: 0.590424, acc.: 73.44%] [Generator loss: 1.044118]\n",
      "4594 [Discriminator loss: 0.501289, acc.: 77.34%] [Generator loss: 0.940808]\n",
      "4595 [Discriminator loss: 0.752059, acc.: 54.69%] [Generator loss: 1.208484]\n",
      "4596 [Discriminator loss: 0.779370, acc.: 53.12%] [Generator loss: 0.988654]\n",
      "4597 [Discriminator loss: 0.489195, acc.: 78.91%] [Generator loss: 1.137100]\n",
      "4598 [Discriminator loss: 0.692453, acc.: 57.03%] [Generator loss: 1.288780]\n",
      "4599 [Discriminator loss: 0.541473, acc.: 72.66%] [Generator loss: 1.418326]\n",
      "4600 [Discriminator loss: 0.662508, acc.: 57.03%] [Generator loss: 1.633953]\n",
      "4601 [Discriminator loss: 0.650943, acc.: 61.72%] [Generator loss: 1.413599]\n",
      "4602 [Discriminator loss: 0.630998, acc.: 62.50%] [Generator loss: 1.376630]\n",
      "4603 [Discriminator loss: 0.855854, acc.: 47.66%] [Generator loss: 1.110853]\n",
      "4604 [Discriminator loss: 0.660361, acc.: 58.59%] [Generator loss: 1.288451]\n",
      "4605 [Discriminator loss: 0.762229, acc.: 57.03%] [Generator loss: 1.246790]\n",
      "4606 [Discriminator loss: 0.646775, acc.: 64.84%] [Generator loss: 1.153803]\n",
      "4607 [Discriminator loss: 0.601792, acc.: 64.84%] [Generator loss: 1.379047]\n",
      "4608 [Discriminator loss: 0.703981, acc.: 53.91%] [Generator loss: 1.061369]\n",
      "4609 [Discriminator loss: 0.713690, acc.: 59.38%] [Generator loss: 1.218501]\n",
      "4610 [Discriminator loss: 0.761557, acc.: 51.56%] [Generator loss: 1.117237]\n",
      "4611 [Discriminator loss: 0.798748, acc.: 52.34%] [Generator loss: 0.943842]\n",
      "4612 [Discriminator loss: 0.706277, acc.: 57.81%] [Generator loss: 0.927265]\n",
      "4613 [Discriminator loss: 0.869878, acc.: 43.75%] [Generator loss: 1.108102]\n",
      "4614 [Discriminator loss: 0.733623, acc.: 56.25%] [Generator loss: 1.227542]\n",
      "4615 [Discriminator loss: 0.629593, acc.: 67.19%] [Generator loss: 1.181175]\n",
      "4616 [Discriminator loss: 0.971082, acc.: 38.28%] [Generator loss: 1.412092]\n",
      "4617 [Discriminator loss: 0.810714, acc.: 49.22%] [Generator loss: 1.492311]\n",
      "4618 [Discriminator loss: 0.602311, acc.: 66.41%] [Generator loss: 1.270301]\n",
      "4619 [Discriminator loss: 0.607087, acc.: 66.41%] [Generator loss: 1.117497]\n",
      "4620 [Discriminator loss: 0.929409, acc.: 40.62%] [Generator loss: 1.323917]\n",
      "4621 [Discriminator loss: 0.635279, acc.: 65.62%] [Generator loss: 1.604504]\n",
      "4622 [Discriminator loss: 0.719889, acc.: 55.47%] [Generator loss: 1.261558]\n",
      "4623 [Discriminator loss: 0.942063, acc.: 44.53%] [Generator loss: 1.156855]\n",
      "4624 [Discriminator loss: 0.630868, acc.: 64.84%] [Generator loss: 1.168667]\n",
      "4625 [Discriminator loss: 0.655152, acc.: 64.84%] [Generator loss: 1.202512]\n",
      "4626 [Discriminator loss: 0.628056, acc.: 64.06%] [Generator loss: 1.120147]\n",
      "4627 [Discriminator loss: 0.684474, acc.: 57.81%] [Generator loss: 1.270025]\n",
      "4628 [Discriminator loss: 0.530659, acc.: 71.09%] [Generator loss: 1.521270]\n",
      "4629 [Discriminator loss: 0.471952, acc.: 78.91%] [Generator loss: 0.972519]\n",
      "4630 [Discriminator loss: 0.532688, acc.: 75.78%] [Generator loss: 0.831599]\n",
      "4631 [Discriminator loss: 0.645575, acc.: 67.97%] [Generator loss: 1.514633]\n",
      "4632 [Discriminator loss: 1.077197, acc.: 36.72%] [Generator loss: 1.289289]\n",
      "4633 [Discriminator loss: 0.747180, acc.: 55.47%] [Generator loss: 1.386045]\n",
      "4634 [Discriminator loss: 0.846108, acc.: 50.00%] [Generator loss: 1.246741]\n",
      "4635 [Discriminator loss: 0.756236, acc.: 57.03%] [Generator loss: 1.458265]\n",
      "4636 [Discriminator loss: 0.597568, acc.: 67.97%] [Generator loss: 1.261512]\n",
      "4637 [Discriminator loss: 0.608286, acc.: 68.75%] [Generator loss: 1.320397]\n",
      "4638 [Discriminator loss: 0.578456, acc.: 67.97%] [Generator loss: 1.281963]\n",
      "4639 [Discriminator loss: 0.622812, acc.: 64.06%] [Generator loss: 1.533372]\n",
      "4640 [Discriminator loss: 0.733052, acc.: 54.69%] [Generator loss: 1.342366]\n",
      "4641 [Discriminator loss: 0.714161, acc.: 56.25%] [Generator loss: 1.379340]\n",
      "4642 [Discriminator loss: 0.947509, acc.: 42.97%] [Generator loss: 1.133586]\n",
      "4643 [Discriminator loss: 0.629540, acc.: 67.97%] [Generator loss: 1.185273]\n",
      "4644 [Discriminator loss: 0.649073, acc.: 60.94%] [Generator loss: 1.265945]\n",
      "4645 [Discriminator loss: 0.916510, acc.: 42.19%] [Generator loss: 0.957245]\n",
      "4646 [Discriminator loss: 0.609270, acc.: 66.41%] [Generator loss: 1.264920]\n",
      "4647 [Discriminator loss: 0.621305, acc.: 67.19%] [Generator loss: 0.974123]\n",
      "4648 [Discriminator loss: 0.824496, acc.: 48.44%] [Generator loss: 1.004711]\n",
      "4649 [Discriminator loss: 0.710350, acc.: 57.81%] [Generator loss: 1.157717]\n",
      "4650 [Discriminator loss: 0.896332, acc.: 44.53%] [Generator loss: 1.021901]\n",
      "4651 [Discriminator loss: 0.847659, acc.: 45.31%] [Generator loss: 1.022177]\n",
      "4652 [Discriminator loss: 0.616967, acc.: 69.53%] [Generator loss: 1.027894]\n",
      "4653 [Discriminator loss: 0.866355, acc.: 43.75%] [Generator loss: 1.070032]\n",
      "4654 [Discriminator loss: 0.656833, acc.: 65.62%] [Generator loss: 1.061120]\n",
      "4655 [Discriminator loss: 0.678576, acc.: 60.94%] [Generator loss: 1.007655]\n",
      "4656 [Discriminator loss: 0.658237, acc.: 65.62%] [Generator loss: 1.142683]\n",
      "4657 [Discriminator loss: 0.592697, acc.: 67.97%] [Generator loss: 1.209102]\n",
      "4658 [Discriminator loss: 0.547878, acc.: 74.22%] [Generator loss: 1.258438]\n",
      "4659 [Discriminator loss: 0.559101, acc.: 66.41%] [Generator loss: 1.370591]\n",
      "4660 [Discriminator loss: 0.622703, acc.: 60.94%] [Generator loss: 1.380924]\n",
      "4661 [Discriminator loss: 0.530769, acc.: 74.22%] [Generator loss: 1.525779]\n",
      "4662 [Discriminator loss: 0.387690, acc.: 85.94%] [Generator loss: 1.494114]\n",
      "4663 [Discriminator loss: 0.425669, acc.: 79.69%] [Generator loss: 1.008621]\n",
      "4664 [Discriminator loss: 0.584970, acc.: 74.22%] [Generator loss: 1.110116]\n",
      "4665 [Discriminator loss: 0.596452, acc.: 60.94%] [Generator loss: 1.091634]\n",
      "4666 [Discriminator loss: 0.730993, acc.: 57.81%] [Generator loss: 1.340490]\n",
      "4667 [Discriminator loss: 0.546156, acc.: 71.09%] [Generator loss: 1.278718]\n",
      "4668 [Discriminator loss: 0.473120, acc.: 80.47%] [Generator loss: 1.042297]\n",
      "4669 [Discriminator loss: 0.627003, acc.: 66.41%] [Generator loss: 0.982420]\n",
      "4670 [Discriminator loss: 0.362424, acc.: 87.50%] [Generator loss: 1.341962]\n",
      "4671 [Discriminator loss: 0.342017, acc.: 90.62%] [Generator loss: 1.161298]\n",
      "4672 [Discriminator loss: 0.668431, acc.: 68.75%] [Generator loss: 0.920462]\n",
      "4673 [Discriminator loss: 0.564396, acc.: 69.53%] [Generator loss: 1.454802]\n",
      "4674 [Discriminator loss: 0.637103, acc.: 68.75%] [Generator loss: 1.562754]\n",
      "4675 [Discriminator loss: 0.684339, acc.: 64.84%] [Generator loss: 1.583518]\n",
      "4676 [Discriminator loss: 0.961896, acc.: 41.41%] [Generator loss: 1.149441]\n",
      "4677 [Discriminator loss: 0.783077, acc.: 49.22%] [Generator loss: 1.715526]\n",
      "4678 [Discriminator loss: 0.719160, acc.: 59.38%] [Generator loss: 1.736051]\n",
      "4679 [Discriminator loss: 1.024244, acc.: 40.62%] [Generator loss: 1.140134]\n",
      "4680 [Discriminator loss: 0.566005, acc.: 68.75%] [Generator loss: 1.516770]\n",
      "4681 [Discriminator loss: 0.533510, acc.: 75.78%] [Generator loss: 1.273287]\n",
      "4682 [Discriminator loss: 0.861141, acc.: 46.88%] [Generator loss: 1.078564]\n",
      "4683 [Discriminator loss: 0.737117, acc.: 54.69%] [Generator loss: 0.894211]\n",
      "4684 [Discriminator loss: 0.743422, acc.: 54.69%] [Generator loss: 1.189049]\n",
      "4685 [Discriminator loss: 0.582917, acc.: 67.97%] [Generator loss: 1.228712]\n",
      "4686 [Discriminator loss: 0.612411, acc.: 67.19%] [Generator loss: 0.765969]\n",
      "4687 [Discriminator loss: 0.748862, acc.: 57.81%] [Generator loss: 0.762056]\n",
      "4688 [Discriminator loss: 0.448948, acc.: 82.81%] [Generator loss: 1.127409]\n",
      "4689 [Discriminator loss: 0.705416, acc.: 60.94%] [Generator loss: 0.804915]\n",
      "4690 [Discriminator loss: 1.077867, acc.: 35.94%] [Generator loss: 1.119662]\n",
      "4691 [Discriminator loss: 0.708330, acc.: 57.81%] [Generator loss: 1.264332]\n",
      "4692 [Discriminator loss: 0.727224, acc.: 62.50%] [Generator loss: 1.092366]\n",
      "4693 [Discriminator loss: 0.749850, acc.: 53.12%] [Generator loss: 0.844292]\n",
      "4694 [Discriminator loss: 0.612641, acc.: 67.19%] [Generator loss: 0.798520]\n",
      "4695 [Discriminator loss: 0.704377, acc.: 60.16%] [Generator loss: 0.736974]\n",
      "4696 [Discriminator loss: 0.662458, acc.: 67.97%] [Generator loss: 0.681029]\n",
      "4697 [Discriminator loss: 0.728878, acc.: 53.91%] [Generator loss: 0.671435]\n",
      "4698 [Discriminator loss: 0.836333, acc.: 53.12%] [Generator loss: 1.229618]\n",
      "4699 [Discriminator loss: 0.994131, acc.: 39.84%] [Generator loss: 1.330831]\n",
      "4700 [Discriminator loss: 0.665905, acc.: 63.28%] [Generator loss: 1.536138]\n",
      "4701 [Discriminator loss: 0.732000, acc.: 58.59%] [Generator loss: 1.466075]\n",
      "4702 [Discriminator loss: 0.742836, acc.: 56.25%] [Generator loss: 1.381343]\n",
      "4703 [Discriminator loss: 0.686809, acc.: 65.62%] [Generator loss: 1.187282]\n",
      "4704 [Discriminator loss: 0.694051, acc.: 57.03%] [Generator loss: 1.138470]\n",
      "4705 [Discriminator loss: 0.486697, acc.: 82.81%] [Generator loss: 1.016499]\n",
      "4706 [Discriminator loss: 0.470547, acc.: 78.12%] [Generator loss: 0.817585]\n",
      "4707 [Discriminator loss: 0.835410, acc.: 49.22%] [Generator loss: 0.852920]\n",
      "4708 [Discriminator loss: 0.583819, acc.: 64.84%] [Generator loss: 1.070055]\n",
      "4709 [Discriminator loss: 0.653809, acc.: 62.50%] [Generator loss: 1.482335]\n",
      "4710 [Discriminator loss: 0.704925, acc.: 60.16%] [Generator loss: 1.149063]\n",
      "4711 [Discriminator loss: 0.608523, acc.: 67.97%] [Generator loss: 1.075889]\n",
      "4712 [Discriminator loss: 0.802772, acc.: 50.00%] [Generator loss: 1.353062]\n",
      "4713 [Discriminator loss: 0.680500, acc.: 60.16%] [Generator loss: 1.444025]\n",
      "4714 [Discriminator loss: 0.691330, acc.: 57.81%] [Generator loss: 1.369177]\n",
      "4715 [Discriminator loss: 0.693276, acc.: 60.16%] [Generator loss: 1.360884]\n",
      "4716 [Discriminator loss: 1.000213, acc.: 41.41%] [Generator loss: 1.271575]\n",
      "4717 [Discriminator loss: 0.892029, acc.: 44.53%] [Generator loss: 1.239763]\n",
      "4718 [Discriminator loss: 0.706175, acc.: 62.50%] [Generator loss: 1.499231]\n",
      "4719 [Discriminator loss: 0.737631, acc.: 55.47%] [Generator loss: 1.247311]\n",
      "4720 [Discriminator loss: 0.620952, acc.: 67.19%] [Generator loss: 1.090715]\n",
      "4721 [Discriminator loss: 0.871778, acc.: 43.75%] [Generator loss: 1.327042]\n",
      "4722 [Discriminator loss: 0.653976, acc.: 60.94%] [Generator loss: 1.199700]\n",
      "4723 [Discriminator loss: 0.659680, acc.: 62.50%] [Generator loss: 1.099806]\n",
      "4724 [Discriminator loss: 0.616512, acc.: 64.06%] [Generator loss: 0.884032]\n",
      "4725 [Discriminator loss: 0.741985, acc.: 59.38%] [Generator loss: 1.023314]\n",
      "4726 [Discriminator loss: 0.454089, acc.: 77.34%] [Generator loss: 0.838594]\n",
      "4727 [Discriminator loss: 0.600392, acc.: 66.41%] [Generator loss: 0.756338]\n",
      "4728 [Discriminator loss: 0.664633, acc.: 68.75%] [Generator loss: 0.975166]\n",
      "4729 [Discriminator loss: 0.517381, acc.: 76.56%] [Generator loss: 1.133813]\n",
      "4730 [Discriminator loss: 0.802826, acc.: 46.09%] [Generator loss: 1.065493]\n",
      "4731 [Discriminator loss: 0.637462, acc.: 64.84%] [Generator loss: 1.064159]\n",
      "4732 [Discriminator loss: 0.703894, acc.: 57.03%] [Generator loss: 1.421867]\n",
      "4733 [Discriminator loss: 0.732716, acc.: 53.12%] [Generator loss: 1.275456]\n",
      "4734 [Discriminator loss: 0.846188, acc.: 43.75%] [Generator loss: 1.167101]\n",
      "4735 [Discriminator loss: 0.684331, acc.: 60.94%] [Generator loss: 1.305331]\n",
      "4736 [Discriminator loss: 0.635073, acc.: 69.53%] [Generator loss: 0.956638]\n",
      "4737 [Discriminator loss: 0.599535, acc.: 72.66%] [Generator loss: 1.124293]\n",
      "4738 [Discriminator loss: 0.599283, acc.: 67.19%] [Generator loss: 1.261657]\n",
      "4739 [Discriminator loss: 0.665882, acc.: 61.72%] [Generator loss: 1.092482]\n",
      "4740 [Discriminator loss: 0.570494, acc.: 68.75%] [Generator loss: 1.021789]\n",
      "4741 [Discriminator loss: 0.836524, acc.: 46.09%] [Generator loss: 1.055710]\n",
      "4742 [Discriminator loss: 0.547314, acc.: 74.22%] [Generator loss: 0.981244]\n",
      "4743 [Discriminator loss: 0.658780, acc.: 57.81%] [Generator loss: 1.096621]\n",
      "4744 [Discriminator loss: 0.890100, acc.: 38.28%] [Generator loss: 1.071006]\n",
      "4745 [Discriminator loss: 0.895132, acc.: 48.44%] [Generator loss: 1.036446]\n",
      "4746 [Discriminator loss: 0.511980, acc.: 77.34%] [Generator loss: 1.254763]\n",
      "4747 [Discriminator loss: 0.438142, acc.: 82.81%] [Generator loss: 1.168146]\n",
      "4748 [Discriminator loss: 0.633304, acc.: 63.28%] [Generator loss: 0.892198]\n",
      "4749 [Discriminator loss: 0.942581, acc.: 37.50%] [Generator loss: 1.021265]\n",
      "4750 [Discriminator loss: 0.536483, acc.: 69.53%] [Generator loss: 1.446505]\n",
      "4751 [Discriminator loss: 0.710302, acc.: 57.03%] [Generator loss: 1.381798]\n",
      "4752 [Discriminator loss: 0.702657, acc.: 56.25%] [Generator loss: 1.368600]\n",
      "4753 [Discriminator loss: 0.656724, acc.: 63.28%] [Generator loss: 1.500313]\n",
      "4754 [Discriminator loss: 0.707072, acc.: 58.59%] [Generator loss: 1.301953]\n",
      "4755 [Discriminator loss: 0.630568, acc.: 65.62%] [Generator loss: 1.453873]\n",
      "4756 [Discriminator loss: 0.608559, acc.: 64.84%] [Generator loss: 1.330593]\n",
      "4757 [Discriminator loss: 0.894220, acc.: 45.31%] [Generator loss: 0.800559]\n",
      "4758 [Discriminator loss: 0.461587, acc.: 79.69%] [Generator loss: 0.696057]\n",
      "4759 [Discriminator loss: 0.581739, acc.: 67.97%] [Generator loss: 1.242806]\n",
      "4760 [Discriminator loss: 0.393501, acc.: 82.03%] [Generator loss: 1.032025]\n",
      "4761 [Discriminator loss: 0.512604, acc.: 71.09%] [Generator loss: 0.607722]\n",
      "4762 [Discriminator loss: 0.656312, acc.: 64.84%] [Generator loss: 0.954596]\n",
      "4763 [Discriminator loss: 0.534648, acc.: 71.09%] [Generator loss: 0.998586]\n",
      "4764 [Discriminator loss: 0.343988, acc.: 91.41%] [Generator loss: 0.573981]\n",
      "4765 [Discriminator loss: 0.730599, acc.: 57.81%] [Generator loss: 1.124218]\n",
      "4766 [Discriminator loss: 1.079124, acc.: 35.16%] [Generator loss: 1.129215]\n",
      "4767 [Discriminator loss: 0.714497, acc.: 58.59%] [Generator loss: 1.416373]\n",
      "4768 [Discriminator loss: 0.512458, acc.: 77.34%] [Generator loss: 1.453180]\n",
      "4769 [Discriminator loss: 0.627856, acc.: 63.28%] [Generator loss: 0.703785]\n",
      "4770 [Discriminator loss: 0.963152, acc.: 49.22%] [Generator loss: 1.005434]\n",
      "4771 [Discriminator loss: 0.453482, acc.: 81.25%] [Generator loss: 1.048730]\n",
      "4772 [Discriminator loss: 0.467042, acc.: 76.56%] [Generator loss: 0.889118]\n",
      "4773 [Discriminator loss: 0.537414, acc.: 68.75%] [Generator loss: 0.843955]\n",
      "4774 [Discriminator loss: 0.851198, acc.: 47.66%] [Generator loss: 0.658475]\n",
      "4775 [Discriminator loss: 0.815251, acc.: 52.34%] [Generator loss: 0.912846]\n",
      "4776 [Discriminator loss: 0.784341, acc.: 60.94%] [Generator loss: 0.643645]\n",
      "4777 [Discriminator loss: 0.368924, acc.: 87.50%] [Generator loss: 0.668391]\n",
      "4778 [Discriminator loss: 0.463514, acc.: 78.12%] [Generator loss: 0.933944]\n",
      "4779 [Discriminator loss: 0.519794, acc.: 75.00%] [Generator loss: 1.209574]\n",
      "4780 [Discriminator loss: 0.546930, acc.: 69.53%] [Generator loss: 0.845876]\n",
      "4781 [Discriminator loss: 0.785650, acc.: 50.78%] [Generator loss: 0.667033]\n",
      "4782 [Discriminator loss: 0.672992, acc.: 66.41%] [Generator loss: 1.037215]\n",
      "4783 [Discriminator loss: 0.740731, acc.: 54.69%] [Generator loss: 0.922367]\n",
      "4784 [Discriminator loss: 0.642899, acc.: 65.62%] [Generator loss: 0.866284]\n",
      "4785 [Discriminator loss: 0.696394, acc.: 59.38%] [Generator loss: 0.817856]\n",
      "4786 [Discriminator loss: 1.089104, acc.: 25.78%] [Generator loss: 0.768037]\n",
      "4787 [Discriminator loss: 1.112097, acc.: 35.94%] [Generator loss: 0.996212]\n",
      "4788 [Discriminator loss: 0.880048, acc.: 42.97%] [Generator loss: 0.945465]\n",
      "4789 [Discriminator loss: 0.585301, acc.: 70.31%] [Generator loss: 1.200534]\n",
      "4790 [Discriminator loss: 0.591558, acc.: 70.31%] [Generator loss: 0.967509]\n",
      "4791 [Discriminator loss: 0.880888, acc.: 46.09%] [Generator loss: 1.319371]\n",
      "4792 [Discriminator loss: 0.514055, acc.: 78.12%] [Generator loss: 1.485397]\n",
      "4793 [Discriminator loss: 0.587601, acc.: 69.53%] [Generator loss: 1.000780]\n",
      "4794 [Discriminator loss: 0.642667, acc.: 64.84%] [Generator loss: 1.096756]\n",
      "4795 [Discriminator loss: 0.927363, acc.: 41.41%] [Generator loss: 1.203709]\n",
      "4796 [Discriminator loss: 0.618154, acc.: 67.19%] [Generator loss: 1.184554]\n",
      "4797 [Discriminator loss: 0.699947, acc.: 65.62%] [Generator loss: 1.241754]\n",
      "4798 [Discriminator loss: 0.957289, acc.: 41.41%] [Generator loss: 1.256305]\n",
      "4799 [Discriminator loss: 0.636772, acc.: 65.62%] [Generator loss: 1.297931]\n",
      "4800 [Discriminator loss: 0.814134, acc.: 46.88%] [Generator loss: 1.215666]\n",
      "4801 [Discriminator loss: 0.784402, acc.: 48.44%] [Generator loss: 1.605864]\n",
      "4802 [Discriminator loss: 0.754283, acc.: 57.03%] [Generator loss: 1.298624]\n",
      "4803 [Discriminator loss: 0.547154, acc.: 72.66%] [Generator loss: 1.377783]\n",
      "4804 [Discriminator loss: 0.600767, acc.: 68.75%] [Generator loss: 1.539401]\n",
      "4805 [Discriminator loss: 0.481664, acc.: 78.91%] [Generator loss: 1.214569]\n",
      "4806 [Discriminator loss: 0.619638, acc.: 64.84%] [Generator loss: 0.844268]\n",
      "4807 [Discriminator loss: 0.876562, acc.: 50.00%] [Generator loss: 1.359934]\n",
      "4808 [Discriminator loss: 0.652655, acc.: 59.38%] [Generator loss: 1.364230]\n",
      "4809 [Discriminator loss: 0.872302, acc.: 46.09%] [Generator loss: 1.335946]\n",
      "4810 [Discriminator loss: 0.812006, acc.: 49.22%] [Generator loss: 1.318824]\n",
      "4811 [Discriminator loss: 0.635212, acc.: 66.41%] [Generator loss: 1.296588]\n",
      "4812 [Discriminator loss: 0.750874, acc.: 50.78%] [Generator loss: 1.308732]\n",
      "4813 [Discriminator loss: 0.502104, acc.: 76.56%] [Generator loss: 1.458830]\n",
      "4814 [Discriminator loss: 0.763799, acc.: 53.91%] [Generator loss: 1.103421]\n",
      "4815 [Discriminator loss: 0.743288, acc.: 55.47%] [Generator loss: 1.230202]\n",
      "4816 [Discriminator loss: 0.572187, acc.: 70.31%] [Generator loss: 1.306186]\n",
      "4817 [Discriminator loss: 0.490717, acc.: 75.78%] [Generator loss: 0.970193]\n",
      "4818 [Discriminator loss: 0.725743, acc.: 56.25%] [Generator loss: 1.059724]\n",
      "4819 [Discriminator loss: 0.676845, acc.: 58.59%] [Generator loss: 1.114997]\n",
      "4820 [Discriminator loss: 0.644388, acc.: 65.62%] [Generator loss: 1.391528]\n",
      "4821 [Discriminator loss: 0.736493, acc.: 57.81%] [Generator loss: 1.038481]\n",
      "4822 [Discriminator loss: 0.645766, acc.: 66.41%] [Generator loss: 1.339068]\n",
      "4823 [Discriminator loss: 0.814723, acc.: 52.34%] [Generator loss: 1.434548]\n",
      "4824 [Discriminator loss: 0.664704, acc.: 62.50%] [Generator loss: 1.289474]\n",
      "4825 [Discriminator loss: 0.548367, acc.: 74.22%] [Generator loss: 1.084022]\n",
      "4826 [Discriminator loss: 0.845382, acc.: 50.78%] [Generator loss: 1.399527]\n",
      "4827 [Discriminator loss: 0.765370, acc.: 53.12%] [Generator loss: 1.587697]\n",
      "4828 [Discriminator loss: 0.740785, acc.: 60.16%] [Generator loss: 1.214450]\n",
      "4829 [Discriminator loss: 0.751816, acc.: 57.81%] [Generator loss: 1.339445]\n",
      "4830 [Discriminator loss: 0.617125, acc.: 65.62%] [Generator loss: 1.022158]\n",
      "4831 [Discriminator loss: 0.761362, acc.: 49.22%] [Generator loss: 0.922001]\n",
      "4832 [Discriminator loss: 0.844063, acc.: 49.22%] [Generator loss: 1.191803]\n",
      "4833 [Discriminator loss: 0.761025, acc.: 53.91%] [Generator loss: 1.224198]\n",
      "4834 [Discriminator loss: 0.705812, acc.: 53.12%] [Generator loss: 0.955055]\n",
      "4835 [Discriminator loss: 0.928785, acc.: 46.09%] [Generator loss: 0.913460]\n",
      "4836 [Discriminator loss: 0.709801, acc.: 54.69%] [Generator loss: 1.097962]\n",
      "4837 [Discriminator loss: 0.793417, acc.: 48.44%] [Generator loss: 1.019813]\n",
      "4838 [Discriminator loss: 1.028779, acc.: 32.81%] [Generator loss: 0.828322]\n",
      "4839 [Discriminator loss: 0.855899, acc.: 39.84%] [Generator loss: 1.029445]\n",
      "4840 [Discriminator loss: 0.768196, acc.: 52.34%] [Generator loss: 1.257592]\n",
      "4841 [Discriminator loss: 0.597896, acc.: 66.41%] [Generator loss: 1.472936]\n",
      "4842 [Discriminator loss: 0.499716, acc.: 78.12%] [Generator loss: 1.131785]\n",
      "4843 [Discriminator loss: 0.577109, acc.: 73.44%] [Generator loss: 1.441161]\n",
      "4844 [Discriminator loss: 0.683558, acc.: 57.81%] [Generator loss: 1.367397]\n",
      "4845 [Discriminator loss: 0.497638, acc.: 81.25%] [Generator loss: 1.385996]\n",
      "4846 [Discriminator loss: 0.626498, acc.: 68.75%] [Generator loss: 1.260753]\n",
      "4847 [Discriminator loss: 0.716428, acc.: 60.94%] [Generator loss: 1.035503]\n",
      "4848 [Discriminator loss: 0.680667, acc.: 60.16%] [Generator loss: 0.994983]\n",
      "4849 [Discriminator loss: 0.845489, acc.: 52.34%] [Generator loss: 1.147574]\n",
      "4850 [Discriminator loss: 0.647376, acc.: 65.62%] [Generator loss: 0.895128]\n",
      "4851 [Discriminator loss: 0.597881, acc.: 71.09%] [Generator loss: 1.139884]\n",
      "4852 [Discriminator loss: 0.587831, acc.: 64.84%] [Generator loss: 1.238145]\n",
      "4853 [Discriminator loss: 0.585467, acc.: 64.84%] [Generator loss: 1.354464]\n",
      "4854 [Discriminator loss: 0.925844, acc.: 46.09%] [Generator loss: 1.030644]\n",
      "4855 [Discriminator loss: 0.672228, acc.: 63.28%] [Generator loss: 1.123083]\n",
      "4856 [Discriminator loss: 0.574660, acc.: 70.31%] [Generator loss: 1.195048]\n",
      "4857 [Discriminator loss: 0.697867, acc.: 60.94%] [Generator loss: 1.135177]\n",
      "4858 [Discriminator loss: 0.526981, acc.: 72.66%] [Generator loss: 1.078775]\n",
      "4859 [Discriminator loss: 0.644492, acc.: 64.84%] [Generator loss: 0.997458]\n",
      "4860 [Discriminator loss: 0.660815, acc.: 64.06%] [Generator loss: 1.243895]\n",
      "4861 [Discriminator loss: 0.854635, acc.: 48.44%] [Generator loss: 1.056930]\n",
      "4862 [Discriminator loss: 0.797133, acc.: 47.66%] [Generator loss: 1.414565]\n",
      "4863 [Discriminator loss: 0.499938, acc.: 78.91%] [Generator loss: 1.180026]\n",
      "4864 [Discriminator loss: 0.739114, acc.: 57.03%] [Generator loss: 1.045754]\n",
      "4865 [Discriminator loss: 0.813730, acc.: 58.59%] [Generator loss: 1.158327]\n",
      "4866 [Discriminator loss: 0.669901, acc.: 56.25%] [Generator loss: 1.496075]\n",
      "4867 [Discriminator loss: 0.602504, acc.: 67.19%] [Generator loss: 1.225525]\n",
      "4868 [Discriminator loss: 0.719860, acc.: 57.81%] [Generator loss: 1.289267]\n",
      "4869 [Discriminator loss: 0.915861, acc.: 43.75%] [Generator loss: 1.109233]\n",
      "4870 [Discriminator loss: 0.736328, acc.: 60.16%] [Generator loss: 1.462766]\n",
      "4871 [Discriminator loss: 0.645196, acc.: 61.72%] [Generator loss: 1.443542]\n",
      "4872 [Discriminator loss: 0.700080, acc.: 58.59%] [Generator loss: 1.332510]\n",
      "4873 [Discriminator loss: 0.684023, acc.: 60.16%] [Generator loss: 1.235441]\n",
      "4874 [Discriminator loss: 0.831515, acc.: 46.09%] [Generator loss: 1.552647]\n",
      "4875 [Discriminator loss: 0.647876, acc.: 69.53%] [Generator loss: 1.215670]\n",
      "4876 [Discriminator loss: 0.700470, acc.: 62.50%] [Generator loss: 1.060601]\n",
      "4877 [Discriminator loss: 0.615412, acc.: 65.62%] [Generator loss: 1.307061]\n",
      "4878 [Discriminator loss: 0.735735, acc.: 55.47%] [Generator loss: 1.377510]\n",
      "4879 [Discriminator loss: 0.617826, acc.: 64.06%] [Generator loss: 1.392450]\n",
      "4880 [Discriminator loss: 0.560293, acc.: 71.09%] [Generator loss: 0.970535]\n",
      "4881 [Discriminator loss: 0.831107, acc.: 42.19%] [Generator loss: 0.895404]\n",
      "4882 [Discriminator loss: 0.809524, acc.: 53.12%] [Generator loss: 1.390566]\n",
      "4883 [Discriminator loss: 0.487388, acc.: 81.25%] [Generator loss: 1.469958]\n",
      "4884 [Discriminator loss: 0.609873, acc.: 64.06%] [Generator loss: 0.896076]\n",
      "4885 [Discriminator loss: 0.995241, acc.: 41.41%] [Generator loss: 0.934414]\n",
      "4886 [Discriminator loss: 0.716299, acc.: 53.12%] [Generator loss: 1.207164]\n",
      "4887 [Discriminator loss: 0.973097, acc.: 39.06%] [Generator loss: 1.069682]\n",
      "4888 [Discriminator loss: 0.745104, acc.: 48.44%] [Generator loss: 1.270170]\n",
      "4889 [Discriminator loss: 0.807549, acc.: 46.09%] [Generator loss: 0.937213]\n",
      "4890 [Discriminator loss: 0.657077, acc.: 61.72%] [Generator loss: 1.046029]\n",
      "4891 [Discriminator loss: 0.790169, acc.: 48.44%] [Generator loss: 0.912021]\n",
      "4892 [Discriminator loss: 0.747776, acc.: 54.69%] [Generator loss: 1.109482]\n",
      "4893 [Discriminator loss: 0.816987, acc.: 49.22%] [Generator loss: 1.118085]\n",
      "4894 [Discriminator loss: 0.871299, acc.: 43.75%] [Generator loss: 1.118301]\n",
      "4895 [Discriminator loss: 0.680510, acc.: 60.94%] [Generator loss: 1.358262]\n",
      "4896 [Discriminator loss: 0.652453, acc.: 63.28%] [Generator loss: 1.122595]\n",
      "4897 [Discriminator loss: 0.873321, acc.: 43.75%] [Generator loss: 1.072911]\n",
      "4898 [Discriminator loss: 0.828312, acc.: 41.41%] [Generator loss: 1.151978]\n",
      "4899 [Discriminator loss: 0.819509, acc.: 51.56%] [Generator loss: 1.101355]\n",
      "4900 [Discriminator loss: 0.745364, acc.: 56.25%] [Generator loss: 1.331666]\n",
      "4901 [Discriminator loss: 0.755465, acc.: 60.16%] [Generator loss: 1.083249]\n",
      "4902 [Discriminator loss: 0.616121, acc.: 62.50%] [Generator loss: 1.214382]\n",
      "4903 [Discriminator loss: 0.599730, acc.: 67.19%] [Generator loss: 1.149344]\n",
      "4904 [Discriminator loss: 0.716008, acc.: 50.00%] [Generator loss: 1.064907]\n",
      "4905 [Discriminator loss: 0.677242, acc.: 59.38%] [Generator loss: 0.933381]\n",
      "4906 [Discriminator loss: 0.566543, acc.: 70.31%] [Generator loss: 1.153592]\n",
      "4907 [Discriminator loss: 0.689279, acc.: 59.38%] [Generator loss: 1.216611]\n",
      "4908 [Discriminator loss: 0.605562, acc.: 65.62%] [Generator loss: 1.239189]\n",
      "4909 [Discriminator loss: 0.600080, acc.: 69.53%] [Generator loss: 1.444310]\n",
      "4910 [Discriminator loss: 0.516199, acc.: 76.56%] [Generator loss: 1.253842]\n",
      "4911 [Discriminator loss: 0.733779, acc.: 56.25%] [Generator loss: 1.071390]\n",
      "4912 [Discriminator loss: 0.592353, acc.: 64.84%] [Generator loss: 1.268376]\n",
      "4913 [Discriminator loss: 0.740408, acc.: 58.59%] [Generator loss: 1.098338]\n",
      "4914 [Discriminator loss: 0.671329, acc.: 60.16%] [Generator loss: 1.135433]\n",
      "4915 [Discriminator loss: 0.730054, acc.: 58.59%] [Generator loss: 1.299726]\n",
      "4916 [Discriminator loss: 0.701958, acc.: 63.28%] [Generator loss: 1.374101]\n",
      "4917 [Discriminator loss: 0.675507, acc.: 59.38%] [Generator loss: 1.072018]\n",
      "4918 [Discriminator loss: 0.740978, acc.: 57.03%] [Generator loss: 1.084110]\n",
      "4919 [Discriminator loss: 0.825938, acc.: 45.31%] [Generator loss: 1.109921]\n",
      "4920 [Discriminator loss: 0.689807, acc.: 59.38%] [Generator loss: 1.115482]\n",
      "4921 [Discriminator loss: 0.681300, acc.: 54.69%] [Generator loss: 1.281046]\n",
      "4922 [Discriminator loss: 0.537244, acc.: 75.00%] [Generator loss: 1.252336]\n",
      "4923 [Discriminator loss: 0.740028, acc.: 60.16%] [Generator loss: 1.195058]\n",
      "4924 [Discriminator loss: 0.618501, acc.: 63.28%] [Generator loss: 1.014577]\n",
      "4925 [Discriminator loss: 0.692562, acc.: 60.16%] [Generator loss: 0.947593]\n",
      "4926 [Discriminator loss: 0.707008, acc.: 58.59%] [Generator loss: 0.920508]\n",
      "4927 [Discriminator loss: 0.662027, acc.: 64.06%] [Generator loss: 1.076678]\n",
      "4928 [Discriminator loss: 0.939670, acc.: 39.06%] [Generator loss: 1.484574]\n",
      "4929 [Discriminator loss: 0.863704, acc.: 45.31%] [Generator loss: 1.642873]\n",
      "4930 [Discriminator loss: 0.659304, acc.: 57.03%] [Generator loss: 1.535241]\n",
      "4931 [Discriminator loss: 0.838068, acc.: 44.53%] [Generator loss: 1.394704]\n",
      "4932 [Discriminator loss: 0.680876, acc.: 58.59%] [Generator loss: 1.616279]\n",
      "4933 [Discriminator loss: 0.658127, acc.: 63.28%] [Generator loss: 1.355978]\n",
      "4934 [Discriminator loss: 0.694741, acc.: 60.16%] [Generator loss: 1.453415]\n",
      "4935 [Discriminator loss: 0.841478, acc.: 48.44%] [Generator loss: 1.536583]\n",
      "4936 [Discriminator loss: 0.691978, acc.: 60.16%] [Generator loss: 1.410244]\n",
      "4937 [Discriminator loss: 0.971475, acc.: 41.41%] [Generator loss: 1.225045]\n",
      "4938 [Discriminator loss: 0.708634, acc.: 59.38%] [Generator loss: 1.233397]\n",
      "4939 [Discriminator loss: 0.702015, acc.: 58.59%] [Generator loss: 1.289275]\n",
      "4940 [Discriminator loss: 0.694247, acc.: 57.03%] [Generator loss: 1.140188]\n",
      "4941 [Discriminator loss: 0.638910, acc.: 67.19%] [Generator loss: 1.305883]\n",
      "4942 [Discriminator loss: 0.609927, acc.: 69.53%] [Generator loss: 1.158710]\n",
      "4943 [Discriminator loss: 0.806265, acc.: 48.44%] [Generator loss: 0.828373]\n",
      "4944 [Discriminator loss: 0.753765, acc.: 45.31%] [Generator loss: 0.947287]\n",
      "4945 [Discriminator loss: 0.551754, acc.: 70.31%] [Generator loss: 0.952713]\n",
      "4946 [Discriminator loss: 0.711212, acc.: 54.69%] [Generator loss: 1.110515]\n",
      "4947 [Discriminator loss: 0.737977, acc.: 50.00%] [Generator loss: 1.015012]\n",
      "4948 [Discriminator loss: 0.787028, acc.: 50.78%] [Generator loss: 1.047504]\n",
      "4949 [Discriminator loss: 0.532547, acc.: 75.78%] [Generator loss: 0.941098]\n",
      "4950 [Discriminator loss: 0.561290, acc.: 70.31%] [Generator loss: 1.044048]\n",
      "4951 [Discriminator loss: 0.520640, acc.: 76.56%] [Generator loss: 0.968633]\n",
      "4952 [Discriminator loss: 0.522859, acc.: 75.78%] [Generator loss: 0.988445]\n",
      "4953 [Discriminator loss: 0.573746, acc.: 67.19%] [Generator loss: 0.841204]\n",
      "4954 [Discriminator loss: 0.538981, acc.: 69.53%] [Generator loss: 0.907092]\n",
      "4955 [Discriminator loss: 0.566591, acc.: 71.09%] [Generator loss: 0.985730]\n",
      "4956 [Discriminator loss: 0.671507, acc.: 65.62%] [Generator loss: 0.767982]\n",
      "4957 [Discriminator loss: 0.795241, acc.: 50.00%] [Generator loss: 1.018698]\n",
      "4958 [Discriminator loss: 1.251133, acc.: 26.56%] [Generator loss: 0.812900]\n",
      "4959 [Discriminator loss: 0.835714, acc.: 53.12%] [Generator loss: 1.075501]\n",
      "4960 [Discriminator loss: 0.695554, acc.: 59.38%] [Generator loss: 1.212796]\n",
      "4961 [Discriminator loss: 0.679823, acc.: 55.47%] [Generator loss: 1.239306]\n",
      "4962 [Discriminator loss: 0.765409, acc.: 51.56%] [Generator loss: 1.153989]\n",
      "4963 [Discriminator loss: 0.624369, acc.: 65.62%] [Generator loss: 1.233820]\n",
      "4964 [Discriminator loss: 0.632367, acc.: 63.28%] [Generator loss: 1.377534]\n",
      "4965 [Discriminator loss: 0.548105, acc.: 76.56%] [Generator loss: 1.021801]\n",
      "4966 [Discriminator loss: 0.553907, acc.: 71.88%] [Generator loss: 0.969233]\n",
      "4967 [Discriminator loss: 0.580931, acc.: 67.19%] [Generator loss: 0.913965]\n",
      "4968 [Discriminator loss: 0.705394, acc.: 56.25%] [Generator loss: 0.934221]\n",
      "4969 [Discriminator loss: 0.833602, acc.: 51.56%] [Generator loss: 1.222039]\n",
      "4970 [Discriminator loss: 0.507615, acc.: 77.34%] [Generator loss: 1.231593]\n",
      "4971 [Discriminator loss: 0.495361, acc.: 75.00%] [Generator loss: 1.034637]\n",
      "4972 [Discriminator loss: 0.748405, acc.: 54.69%] [Generator loss: 1.092295]\n",
      "4973 [Discriminator loss: 0.549300, acc.: 67.97%] [Generator loss: 1.270470]\n",
      "4974 [Discriminator loss: 0.574055, acc.: 69.53%] [Generator loss: 1.037255]\n",
      "4975 [Discriminator loss: 0.676224, acc.: 57.81%] [Generator loss: 1.068772]\n",
      "4976 [Discriminator loss: 0.897116, acc.: 42.97%] [Generator loss: 1.339716]\n",
      "4977 [Discriminator loss: 0.657648, acc.: 62.50%] [Generator loss: 1.465472]\n",
      "4978 [Discriminator loss: 0.569836, acc.: 69.53%] [Generator loss: 1.085995]\n",
      "4979 [Discriminator loss: 0.699900, acc.: 59.38%] [Generator loss: 0.894274]\n",
      "4980 [Discriminator loss: 0.598978, acc.: 63.28%] [Generator loss: 1.014404]\n",
      "4981 [Discriminator loss: 0.681845, acc.: 57.81%] [Generator loss: 1.117225]\n",
      "4982 [Discriminator loss: 0.843163, acc.: 50.00%] [Generator loss: 1.091063]\n",
      "4983 [Discriminator loss: 0.591132, acc.: 72.66%] [Generator loss: 0.954782]\n",
      "4984 [Discriminator loss: 0.577011, acc.: 71.09%] [Generator loss: 1.161019]\n",
      "4985 [Discriminator loss: 0.432414, acc.: 82.03%] [Generator loss: 1.043290]\n",
      "4986 [Discriminator loss: 0.678469, acc.: 59.38%] [Generator loss: 1.313291]\n",
      "4987 [Discriminator loss: 0.637796, acc.: 61.72%] [Generator loss: 1.183735]\n",
      "4988 [Discriminator loss: 0.619434, acc.: 70.31%] [Generator loss: 1.109861]\n",
      "4989 [Discriminator loss: 0.505834, acc.: 75.78%] [Generator loss: 1.041435]\n",
      "4990 [Discriminator loss: 0.785798, acc.: 53.91%] [Generator loss: 0.918724]\n",
      "4991 [Discriminator loss: 0.794925, acc.: 49.22%] [Generator loss: 1.183660]\n",
      "4992 [Discriminator loss: 0.880462, acc.: 42.97%] [Generator loss: 1.129901]\n",
      "4993 [Discriminator loss: 0.591552, acc.: 70.31%] [Generator loss: 1.184837]\n",
      "4994 [Discriminator loss: 0.757597, acc.: 53.12%] [Generator loss: 1.055810]\n",
      "4995 [Discriminator loss: 0.691199, acc.: 60.94%] [Generator loss: 1.227084]\n",
      "4996 [Discriminator loss: 0.482447, acc.: 79.69%] [Generator loss: 0.965663]\n",
      "4997 [Discriminator loss: 0.637087, acc.: 68.75%] [Generator loss: 1.259135]\n",
      "4998 [Discriminator loss: 0.558445, acc.: 71.88%] [Generator loss: 1.247693]\n",
      "4999 [Discriminator loss: 0.637354, acc.: 64.06%] [Generator loss: 1.460731]\n",
      "5000 [Discriminator loss: 0.681856, acc.: 60.16%] [Generator loss: 1.178686]\n",
      "5001 [Discriminator loss: 0.685779, acc.: 60.94%] [Generator loss: 1.219324]\n",
      "5002 [Discriminator loss: 0.789394, acc.: 51.56%] [Generator loss: 1.415874]\n",
      "5003 [Discriminator loss: 0.683677, acc.: 61.72%] [Generator loss: 1.226465]\n",
      "5004 [Discriminator loss: 0.733436, acc.: 51.56%] [Generator loss: 1.184328]\n",
      "5005 [Discriminator loss: 0.553534, acc.: 75.78%] [Generator loss: 1.453057]\n",
      "5006 [Discriminator loss: 0.524880, acc.: 75.00%] [Generator loss: 1.412302]\n",
      "5007 [Discriminator loss: 0.644617, acc.: 62.50%] [Generator loss: 1.157217]\n",
      "5008 [Discriminator loss: 0.648471, acc.: 63.28%] [Generator loss: 1.277770]\n",
      "5009 [Discriminator loss: 0.696813, acc.: 60.16%] [Generator loss: 1.110086]\n",
      "5010 [Discriminator loss: 0.733437, acc.: 53.12%] [Generator loss: 1.509726]\n",
      "5011 [Discriminator loss: 0.738248, acc.: 54.69%] [Generator loss: 1.215982]\n",
      "5012 [Discriminator loss: 0.999133, acc.: 42.19%] [Generator loss: 1.416952]\n",
      "5013 [Discriminator loss: 0.884992, acc.: 44.53%] [Generator loss: 1.669227]\n",
      "5014 [Discriminator loss: 0.933239, acc.: 39.06%] [Generator loss: 1.150533]\n",
      "5015 [Discriminator loss: 0.781136, acc.: 52.34%] [Generator loss: 1.182918]\n",
      "5016 [Discriminator loss: 0.683519, acc.: 58.59%] [Generator loss: 1.096516]\n",
      "5017 [Discriminator loss: 0.816404, acc.: 46.09%] [Generator loss: 1.337982]\n",
      "5018 [Discriminator loss: 0.704183, acc.: 62.50%] [Generator loss: 1.530689]\n",
      "5019 [Discriminator loss: 0.681155, acc.: 58.59%] [Generator loss: 1.364240]\n",
      "5020 [Discriminator loss: 0.756593, acc.: 55.47%] [Generator loss: 1.170105]\n",
      "5021 [Discriminator loss: 0.560479, acc.: 75.78%] [Generator loss: 1.071883]\n",
      "5022 [Discriminator loss: 0.751734, acc.: 56.25%] [Generator loss: 1.157897]\n",
      "5023 [Discriminator loss: 0.687782, acc.: 62.50%] [Generator loss: 1.257377]\n",
      "5024 [Discriminator loss: 0.649626, acc.: 64.06%] [Generator loss: 0.984026]\n",
      "5025 [Discriminator loss: 0.958630, acc.: 35.16%] [Generator loss: 1.118997]\n",
      "5026 [Discriminator loss: 0.709828, acc.: 56.25%] [Generator loss: 1.259474]\n",
      "5027 [Discriminator loss: 0.609226, acc.: 64.06%] [Generator loss: 0.940143]\n",
      "5028 [Discriminator loss: 0.758866, acc.: 57.03%] [Generator loss: 1.166072]\n",
      "5029 [Discriminator loss: 0.846179, acc.: 42.19%] [Generator loss: 1.318586]\n",
      "5030 [Discriminator loss: 0.635418, acc.: 65.62%] [Generator loss: 1.368152]\n",
      "5031 [Discriminator loss: 0.643097, acc.: 61.72%] [Generator loss: 1.050247]\n",
      "5032 [Discriminator loss: 0.526943, acc.: 72.66%] [Generator loss: 1.093855]\n",
      "5033 [Discriminator loss: 0.691444, acc.: 57.81%] [Generator loss: 1.019710]\n",
      "5034 [Discriminator loss: 0.535274, acc.: 76.56%] [Generator loss: 0.955325]\n",
      "5035 [Discriminator loss: 0.428745, acc.: 85.16%] [Generator loss: 0.690347]\n",
      "5036 [Discriminator loss: 0.610227, acc.: 67.97%] [Generator loss: 1.107068]\n",
      "5037 [Discriminator loss: 0.576544, acc.: 71.88%] [Generator loss: 1.096814]\n",
      "5038 [Discriminator loss: 0.759365, acc.: 56.25%] [Generator loss: 1.051336]\n",
      "5039 [Discriminator loss: 0.604302, acc.: 65.62%] [Generator loss: 0.931929]\n",
      "5040 [Discriminator loss: 0.789102, acc.: 53.12%] [Generator loss: 1.267456]\n",
      "5041 [Discriminator loss: 0.635582, acc.: 60.94%] [Generator loss: 1.369342]\n",
      "5042 [Discriminator loss: 0.597000, acc.: 70.31%] [Generator loss: 1.489240]\n",
      "5043 [Discriminator loss: 0.866419, acc.: 44.53%] [Generator loss: 1.533552]\n",
      "5044 [Discriminator loss: 0.620417, acc.: 66.41%] [Generator loss: 1.294646]\n",
      "5045 [Discriminator loss: 0.933298, acc.: 40.62%] [Generator loss: 1.624613]\n",
      "5046 [Discriminator loss: 0.676298, acc.: 60.94%] [Generator loss: 1.450575]\n",
      "5047 [Discriminator loss: 0.635986, acc.: 70.31%] [Generator loss: 0.997606]\n",
      "5048 [Discriminator loss: 0.494919, acc.: 78.12%] [Generator loss: 1.160971]\n",
      "5049 [Discriminator loss: 0.777976, acc.: 53.12%] [Generator loss: 1.187723]\n",
      "5050 [Discriminator loss: 0.702313, acc.: 64.84%] [Generator loss: 1.050576]\n",
      "5051 [Discriminator loss: 0.626288, acc.: 66.41%] [Generator loss: 1.093673]\n",
      "5052 [Discriminator loss: 0.613124, acc.: 68.75%] [Generator loss: 0.971746]\n",
      "5053 [Discriminator loss: 0.597686, acc.: 65.62%] [Generator loss: 0.937622]\n",
      "5054 [Discriminator loss: 0.615586, acc.: 65.62%] [Generator loss: 1.618074]\n",
      "5055 [Discriminator loss: 0.593504, acc.: 68.75%] [Generator loss: 1.276166]\n",
      "5056 [Discriminator loss: 0.730191, acc.: 60.16%] [Generator loss: 1.312741]\n",
      "5057 [Discriminator loss: 0.598167, acc.: 70.31%] [Generator loss: 1.201294]\n",
      "5058 [Discriminator loss: 0.508775, acc.: 73.44%] [Generator loss: 1.183354]\n",
      "5059 [Discriminator loss: 0.683065, acc.: 60.94%] [Generator loss: 1.103433]\n",
      "5060 [Discriminator loss: 0.669587, acc.: 63.28%] [Generator loss: 1.188786]\n",
      "5061 [Discriminator loss: 0.779075, acc.: 45.31%] [Generator loss: 1.151837]\n",
      "5062 [Discriminator loss: 0.627524, acc.: 61.72%] [Generator loss: 1.314243]\n",
      "5063 [Discriminator loss: 0.553437, acc.: 71.09%] [Generator loss: 1.221121]\n",
      "5064 [Discriminator loss: 0.607080, acc.: 66.41%] [Generator loss: 1.175073]\n",
      "5065 [Discriminator loss: 0.821580, acc.: 46.88%] [Generator loss: 1.501161]\n",
      "5066 [Discriminator loss: 0.704824, acc.: 54.69%] [Generator loss: 1.608401]\n",
      "5067 [Discriminator loss: 0.531960, acc.: 76.56%] [Generator loss: 0.926572]\n",
      "5068 [Discriminator loss: 0.620360, acc.: 62.50%] [Generator loss: 0.942232]\n",
      "5069 [Discriminator loss: 0.596245, acc.: 67.97%] [Generator loss: 1.230577]\n",
      "5070 [Discriminator loss: 0.637571, acc.: 64.06%] [Generator loss: 1.191073]\n",
      "5071 [Discriminator loss: 0.811490, acc.: 50.78%] [Generator loss: 1.389372]\n",
      "5072 [Discriminator loss: 0.851812, acc.: 44.53%] [Generator loss: 1.255564]\n",
      "5073 [Discriminator loss: 0.665196, acc.: 60.94%] [Generator loss: 0.988248]\n",
      "5074 [Discriminator loss: 0.827930, acc.: 44.53%] [Generator loss: 0.967831]\n",
      "5075 [Discriminator loss: 0.695591, acc.: 60.16%] [Generator loss: 1.063247]\n",
      "5076 [Discriminator loss: 0.673511, acc.: 60.94%] [Generator loss: 1.150440]\n",
      "5077 [Discriminator loss: 0.736461, acc.: 49.22%] [Generator loss: 1.052057]\n",
      "5078 [Discriminator loss: 0.844974, acc.: 43.75%] [Generator loss: 1.019854]\n",
      "5079 [Discriminator loss: 0.626317, acc.: 69.53%] [Generator loss: 1.397942]\n",
      "5080 [Discriminator loss: 0.674515, acc.: 61.72%] [Generator loss: 1.025385]\n",
      "5081 [Discriminator loss: 0.911076, acc.: 37.50%] [Generator loss: 1.328867]\n",
      "5082 [Discriminator loss: 0.498020, acc.: 77.34%] [Generator loss: 1.174044]\n",
      "5083 [Discriminator loss: 0.685951, acc.: 57.81%] [Generator loss: 1.066961]\n",
      "5084 [Discriminator loss: 0.595324, acc.: 71.88%] [Generator loss: 1.127568]\n",
      "5085 [Discriminator loss: 0.604260, acc.: 60.94%] [Generator loss: 1.356909]\n",
      "5086 [Discriminator loss: 0.686369, acc.: 59.38%] [Generator loss: 1.598260]\n",
      "5087 [Discriminator loss: 0.674706, acc.: 58.59%] [Generator loss: 1.195527]\n",
      "5088 [Discriminator loss: 0.697190, acc.: 56.25%] [Generator loss: 1.010257]\n",
      "5089 [Discriminator loss: 0.756712, acc.: 57.81%] [Generator loss: 1.336980]\n",
      "5090 [Discriminator loss: 0.765012, acc.: 59.38%] [Generator loss: 1.208740]\n",
      "5091 [Discriminator loss: 0.866943, acc.: 49.22%] [Generator loss: 1.148404]\n",
      "5092 [Discriminator loss: 0.760810, acc.: 52.34%] [Generator loss: 1.554922]\n",
      "5093 [Discriminator loss: 0.571549, acc.: 69.53%] [Generator loss: 1.295663]\n",
      "5094 [Discriminator loss: 0.642264, acc.: 60.16%] [Generator loss: 1.196347]\n",
      "5095 [Discriminator loss: 0.727577, acc.: 58.59%] [Generator loss: 1.201133]\n",
      "5096 [Discriminator loss: 0.577296, acc.: 71.09%] [Generator loss: 1.391034]\n",
      "5097 [Discriminator loss: 0.830048, acc.: 51.56%] [Generator loss: 1.201043]\n",
      "5098 [Discriminator loss: 0.543375, acc.: 72.66%] [Generator loss: 1.518256]\n",
      "5099 [Discriminator loss: 0.700859, acc.: 60.94%] [Generator loss: 1.228528]\n",
      "5100 [Discriminator loss: 0.907230, acc.: 45.31%] [Generator loss: 0.894363]\n",
      "5101 [Discriminator loss: 0.841037, acc.: 46.88%] [Generator loss: 1.012261]\n",
      "5102 [Discriminator loss: 0.705866, acc.: 60.16%] [Generator loss: 0.973419]\n",
      "5103 [Discriminator loss: 0.660322, acc.: 60.16%] [Generator loss: 1.296897]\n",
      "5104 [Discriminator loss: 0.633056, acc.: 66.41%] [Generator loss: 1.237683]\n",
      "5105 [Discriminator loss: 0.661834, acc.: 64.06%] [Generator loss: 1.383686]\n",
      "5106 [Discriminator loss: 0.588289, acc.: 66.41%] [Generator loss: 1.332530]\n",
      "5107 [Discriminator loss: 0.758040, acc.: 50.00%] [Generator loss: 1.413395]\n",
      "5108 [Discriminator loss: 0.501047, acc.: 78.91%] [Generator loss: 1.557872]\n",
      "5109 [Discriminator loss: 0.629589, acc.: 63.28%] [Generator loss: 1.232123]\n",
      "5110 [Discriminator loss: 0.573959, acc.: 67.97%] [Generator loss: 1.197358]\n",
      "5111 [Discriminator loss: 0.746415, acc.: 58.59%] [Generator loss: 1.065738]\n",
      "5112 [Discriminator loss: 0.670514, acc.: 62.50%] [Generator loss: 1.042055]\n",
      "5113 [Discriminator loss: 0.646635, acc.: 67.97%] [Generator loss: 1.193294]\n",
      "5114 [Discriminator loss: 0.708468, acc.: 62.50%] [Generator loss: 1.134369]\n",
      "5115 [Discriminator loss: 0.570311, acc.: 67.97%] [Generator loss: 1.163618]\n",
      "5116 [Discriminator loss: 0.755681, acc.: 59.38%] [Generator loss: 1.238004]\n",
      "5117 [Discriminator loss: 0.699993, acc.: 58.59%] [Generator loss: 1.073711]\n",
      "5118 [Discriminator loss: 0.708377, acc.: 56.25%] [Generator loss: 1.028019]\n",
      "5119 [Discriminator loss: 0.684155, acc.: 59.38%] [Generator loss: 1.138597]\n",
      "5120 [Discriminator loss: 0.719922, acc.: 59.38%] [Generator loss: 1.052305]\n",
      "5121 [Discriminator loss: 0.755580, acc.: 50.78%] [Generator loss: 1.016823]\n",
      "5122 [Discriminator loss: 0.919812, acc.: 40.62%] [Generator loss: 0.951097]\n",
      "5123 [Discriminator loss: 0.942842, acc.: 43.75%] [Generator loss: 1.137584]\n",
      "5124 [Discriminator loss: 0.647413, acc.: 64.84%] [Generator loss: 1.362557]\n",
      "5125 [Discriminator loss: 0.822544, acc.: 47.66%] [Generator loss: 1.130914]\n",
      "5126 [Discriminator loss: 0.597676, acc.: 67.19%] [Generator loss: 1.178399]\n",
      "5127 [Discriminator loss: 0.794081, acc.: 52.34%] [Generator loss: 1.046325]\n",
      "5128 [Discriminator loss: 0.737029, acc.: 57.81%] [Generator loss: 1.263210]\n",
      "5129 [Discriminator loss: 0.674581, acc.: 67.97%] [Generator loss: 1.101859]\n",
      "5130 [Discriminator loss: 0.614700, acc.: 64.06%] [Generator loss: 0.955543]\n",
      "5131 [Discriminator loss: 0.961613, acc.: 26.56%] [Generator loss: 1.180378]\n",
      "5132 [Discriminator loss: 0.712843, acc.: 57.81%] [Generator loss: 1.248697]\n",
      "5133 [Discriminator loss: 0.784970, acc.: 49.22%] [Generator loss: 1.118117]\n",
      "5134 [Discriminator loss: 0.821320, acc.: 52.34%] [Generator loss: 1.110292]\n",
      "5135 [Discriminator loss: 0.640601, acc.: 66.41%] [Generator loss: 1.095237]\n",
      "5136 [Discriminator loss: 0.625735, acc.: 60.16%] [Generator loss: 1.039291]\n",
      "5137 [Discriminator loss: 0.934776, acc.: 41.41%] [Generator loss: 0.906999]\n",
      "5138 [Discriminator loss: 0.518830, acc.: 72.66%] [Generator loss: 1.473851]\n",
      "5139 [Discriminator loss: 0.507246, acc.: 75.00%] [Generator loss: 1.515127]\n",
      "5140 [Discriminator loss: 0.650415, acc.: 63.28%] [Generator loss: 1.167715]\n",
      "5141 [Discriminator loss: 0.529692, acc.: 73.44%] [Generator loss: 1.170536]\n",
      "5142 [Discriminator loss: 0.696077, acc.: 59.38%] [Generator loss: 1.389590]\n",
      "5143 [Discriminator loss: 0.697767, acc.: 65.62%] [Generator loss: 1.272542]\n",
      "5144 [Discriminator loss: 0.546189, acc.: 71.88%] [Generator loss: 1.236287]\n",
      "5145 [Discriminator loss: 0.489675, acc.: 71.09%] [Generator loss: 0.937969]\n",
      "5146 [Discriminator loss: 0.596681, acc.: 63.28%] [Generator loss: 0.844052]\n",
      "5147 [Discriminator loss: 0.431108, acc.: 82.81%] [Generator loss: 0.956855]\n",
      "5148 [Discriminator loss: 0.703882, acc.: 60.94%] [Generator loss: 0.945272]\n",
      "5149 [Discriminator loss: 0.640490, acc.: 64.06%] [Generator loss: 0.662439]\n",
      "5150 [Discriminator loss: 0.771954, acc.: 50.78%] [Generator loss: 0.999230]\n",
      "5151 [Discriminator loss: 0.940829, acc.: 35.94%] [Generator loss: 1.216670]\n",
      "5152 [Discriminator loss: 0.892825, acc.: 46.09%] [Generator loss: 1.153003]\n",
      "5153 [Discriminator loss: 0.864237, acc.: 44.53%] [Generator loss: 1.111480]\n",
      "5154 [Discriminator loss: 0.959820, acc.: 36.72%] [Generator loss: 1.266980]\n",
      "5155 [Discriminator loss: 0.731278, acc.: 50.78%] [Generator loss: 1.472344]\n",
      "5156 [Discriminator loss: 0.710073, acc.: 59.38%] [Generator loss: 1.284359]\n",
      "5157 [Discriminator loss: 0.537634, acc.: 74.22%] [Generator loss: 1.428396]\n",
      "5158 [Discriminator loss: 0.756844, acc.: 50.00%] [Generator loss: 1.085030]\n",
      "5159 [Discriminator loss: 0.832205, acc.: 46.88%] [Generator loss: 0.982605]\n",
      "5160 [Discriminator loss: 0.600749, acc.: 70.31%] [Generator loss: 0.899342]\n",
      "5161 [Discriminator loss: 0.685459, acc.: 60.16%] [Generator loss: 1.044052]\n",
      "5162 [Discriminator loss: 0.525132, acc.: 74.22%] [Generator loss: 0.815581]\n",
      "5163 [Discriminator loss: 0.628940, acc.: 69.53%] [Generator loss: 0.884378]\n",
      "5164 [Discriminator loss: 0.675509, acc.: 59.38%] [Generator loss: 0.903055]\n",
      "5165 [Discriminator loss: 0.545821, acc.: 77.34%] [Generator loss: 1.261405]\n",
      "5166 [Discriminator loss: 0.780934, acc.: 53.91%] [Generator loss: 1.129243]\n",
      "5167 [Discriminator loss: 0.655017, acc.: 64.06%] [Generator loss: 1.167753]\n",
      "5168 [Discriminator loss: 0.809285, acc.: 42.97%] [Generator loss: 1.023746]\n",
      "5169 [Discriminator loss: 0.574533, acc.: 69.53%] [Generator loss: 1.245713]\n",
      "5170 [Discriminator loss: 0.579826, acc.: 73.44%] [Generator loss: 1.215826]\n",
      "5171 [Discriminator loss: 0.705574, acc.: 60.16%] [Generator loss: 1.232294]\n",
      "5172 [Discriminator loss: 0.602070, acc.: 64.06%] [Generator loss: 1.225412]\n",
      "5173 [Discriminator loss: 0.579531, acc.: 66.41%] [Generator loss: 1.136507]\n",
      "5174 [Discriminator loss: 0.705338, acc.: 62.50%] [Generator loss: 1.133504]\n",
      "5175 [Discriminator loss: 0.609764, acc.: 64.84%] [Generator loss: 1.284879]\n",
      "5176 [Discriminator loss: 0.624140, acc.: 64.84%] [Generator loss: 1.299903]\n",
      "5177 [Discriminator loss: 0.550035, acc.: 71.09%] [Generator loss: 1.365468]\n",
      "5178 [Discriminator loss: 0.497860, acc.: 77.34%] [Generator loss: 1.200640]\n",
      "5179 [Discriminator loss: 0.553436, acc.: 70.31%] [Generator loss: 1.136769]\n",
      "5180 [Discriminator loss: 0.728816, acc.: 59.38%] [Generator loss: 1.482744]\n",
      "5181 [Discriminator loss: 0.576865, acc.: 75.00%] [Generator loss: 1.372865]\n",
      "5182 [Discriminator loss: 0.700202, acc.: 57.03%] [Generator loss: 1.454775]\n",
      "5183 [Discriminator loss: 0.699787, acc.: 60.94%] [Generator loss: 1.292030]\n",
      "5184 [Discriminator loss: 0.631198, acc.: 67.19%] [Generator loss: 1.215390]\n",
      "5185 [Discriminator loss: 0.624208, acc.: 67.97%] [Generator loss: 1.340566]\n",
      "5186 [Discriminator loss: 0.788100, acc.: 50.78%] [Generator loss: 1.279176]\n",
      "5187 [Discriminator loss: 0.875923, acc.: 42.97%] [Generator loss: 1.205211]\n",
      "5188 [Discriminator loss: 0.577914, acc.: 72.66%] [Generator loss: 0.978989]\n",
      "5189 [Discriminator loss: 0.802777, acc.: 52.34%] [Generator loss: 1.018949]\n",
      "5190 [Discriminator loss: 0.536817, acc.: 75.78%] [Generator loss: 1.241548]\n",
      "5191 [Discriminator loss: 0.547385, acc.: 73.44%] [Generator loss: 1.597514]\n",
      "5192 [Discriminator loss: 0.692031, acc.: 64.06%] [Generator loss: 1.157256]\n",
      "5193 [Discriminator loss: 1.109467, acc.: 32.03%] [Generator loss: 1.052584]\n",
      "5194 [Discriminator loss: 0.512857, acc.: 78.12%] [Generator loss: 1.382281]\n",
      "5195 [Discriminator loss: 0.602339, acc.: 67.97%] [Generator loss: 1.515436]\n",
      "5196 [Discriminator loss: 0.569352, acc.: 74.22%] [Generator loss: 1.218907]\n",
      "5197 [Discriminator loss: 0.780353, acc.: 48.44%] [Generator loss: 1.362118]\n",
      "5198 [Discriminator loss: 0.761029, acc.: 55.47%] [Generator loss: 1.183906]\n",
      "5199 [Discriminator loss: 0.893808, acc.: 45.31%] [Generator loss: 1.276781]\n",
      "5200 [Discriminator loss: 0.641784, acc.: 66.41%] [Generator loss: 1.423140]\n",
      "5201 [Discriminator loss: 0.581370, acc.: 70.31%] [Generator loss: 1.293300]\n",
      "5202 [Discriminator loss: 0.648704, acc.: 64.84%] [Generator loss: 0.997997]\n",
      "5203 [Discriminator loss: 0.651177, acc.: 62.50%] [Generator loss: 1.293107]\n",
      "5204 [Discriminator loss: 0.699729, acc.: 57.03%] [Generator loss: 0.892121]\n",
      "5205 [Discriminator loss: 0.809337, acc.: 48.44%] [Generator loss: 0.924544]\n",
      "5206 [Discriminator loss: 0.644396, acc.: 64.84%] [Generator loss: 0.934255]\n",
      "5207 [Discriminator loss: 0.834713, acc.: 52.34%] [Generator loss: 0.995309]\n",
      "5208 [Discriminator loss: 0.816561, acc.: 44.53%] [Generator loss: 1.133346]\n",
      "5209 [Discriminator loss: 0.629557, acc.: 64.06%] [Generator loss: 1.206501]\n",
      "5210 [Discriminator loss: 0.756071, acc.: 52.34%] [Generator loss: 1.568320]\n",
      "5211 [Discriminator loss: 0.609488, acc.: 66.41%] [Generator loss: 1.482039]\n",
      "5212 [Discriminator loss: 0.778095, acc.: 51.56%] [Generator loss: 1.245011]\n",
      "5213 [Discriminator loss: 0.770128, acc.: 58.59%] [Generator loss: 1.455893]\n",
      "5214 [Discriminator loss: 0.730922, acc.: 57.81%] [Generator loss: 1.106512]\n",
      "5215 [Discriminator loss: 0.622367, acc.: 65.62%] [Generator loss: 0.991707]\n",
      "5216 [Discriminator loss: 0.827580, acc.: 51.56%] [Generator loss: 0.843238]\n",
      "5217 [Discriminator loss: 0.568891, acc.: 70.31%] [Generator loss: 1.161912]\n",
      "5218 [Discriminator loss: 0.747135, acc.: 57.03%] [Generator loss: 1.172924]\n",
      "5219 [Discriminator loss: 0.760317, acc.: 53.91%] [Generator loss: 1.182250]\n",
      "5220 [Discriminator loss: 0.687793, acc.: 60.16%] [Generator loss: 1.114326]\n",
      "5221 [Discriminator loss: 0.654194, acc.: 57.03%] [Generator loss: 0.901564]\n",
      "5222 [Discriminator loss: 0.713761, acc.: 56.25%] [Generator loss: 1.124817]\n",
      "5223 [Discriminator loss: 0.513893, acc.: 76.56%] [Generator loss: 0.927804]\n",
      "5224 [Discriminator loss: 0.761445, acc.: 51.56%] [Generator loss: 0.907686]\n",
      "5225 [Discriminator loss: 0.542872, acc.: 71.88%] [Generator loss: 1.039510]\n",
      "5226 [Discriminator loss: 0.723204, acc.: 60.16%] [Generator loss: 0.981089]\n",
      "5227 [Discriminator loss: 0.680102, acc.: 63.28%] [Generator loss: 0.986421]\n",
      "5228 [Discriminator loss: 0.629929, acc.: 65.62%] [Generator loss: 1.156864]\n",
      "5229 [Discriminator loss: 0.675146, acc.: 55.47%] [Generator loss: 1.173222]\n",
      "5230 [Discriminator loss: 0.600876, acc.: 66.41%] [Generator loss: 1.164534]\n",
      "5231 [Discriminator loss: 0.800941, acc.: 47.66%] [Generator loss: 1.107910]\n",
      "5232 [Discriminator loss: 0.607602, acc.: 65.62%] [Generator loss: 1.208633]\n",
      "5233 [Discriminator loss: 0.726612, acc.: 51.56%] [Generator loss: 1.017536]\n",
      "5234 [Discriminator loss: 0.620445, acc.: 66.41%] [Generator loss: 1.255464]\n",
      "5235 [Discriminator loss: 0.563991, acc.: 71.88%] [Generator loss: 1.300884]\n",
      "5236 [Discriminator loss: 0.731775, acc.: 54.69%] [Generator loss: 1.018395]\n",
      "5237 [Discriminator loss: 0.706201, acc.: 55.47%] [Generator loss: 0.991070]\n",
      "5238 [Discriminator loss: 0.617782, acc.: 66.41%] [Generator loss: 1.149601]\n",
      "5239 [Discriminator loss: 0.919262, acc.: 43.75%] [Generator loss: 1.099282]\n",
      "5240 [Discriminator loss: 0.735838, acc.: 52.34%] [Generator loss: 1.024306]\n",
      "5241 [Discriminator loss: 0.981702, acc.: 35.94%] [Generator loss: 1.219825]\n",
      "5242 [Discriminator loss: 0.595625, acc.: 71.09%] [Generator loss: 1.118166]\n",
      "5243 [Discriminator loss: 0.582470, acc.: 71.09%] [Generator loss: 0.882796]\n",
      "5244 [Discriminator loss: 0.744874, acc.: 54.69%] [Generator loss: 0.885655]\n",
      "5245 [Discriminator loss: 0.689364, acc.: 58.59%] [Generator loss: 1.066411]\n",
      "5246 [Discriminator loss: 0.566555, acc.: 74.22%] [Generator loss: 1.014797]\n",
      "5247 [Discriminator loss: 0.917608, acc.: 39.06%] [Generator loss: 0.956919]\n",
      "5248 [Discriminator loss: 0.629831, acc.: 64.84%] [Generator loss: 0.911639]\n",
      "5249 [Discriminator loss: 0.857302, acc.: 39.06%] [Generator loss: 0.924484]\n",
      "5250 [Discriminator loss: 0.654345, acc.: 64.06%] [Generator loss: 1.153588]\n",
      "5251 [Discriminator loss: 0.698344, acc.: 60.16%] [Generator loss: 1.199692]\n",
      "5252 [Discriminator loss: 0.686597, acc.: 63.28%] [Generator loss: 0.911415]\n",
      "5253 [Discriminator loss: 0.724098, acc.: 52.34%] [Generator loss: 1.027703]\n",
      "5254 [Discriminator loss: 0.649563, acc.: 66.41%] [Generator loss: 0.840592]\n",
      "5255 [Discriminator loss: 0.740845, acc.: 53.12%] [Generator loss: 0.987956]\n",
      "5256 [Discriminator loss: 0.732959, acc.: 54.69%] [Generator loss: 1.326810]\n",
      "5257 [Discriminator loss: 0.801078, acc.: 51.56%] [Generator loss: 1.156662]\n",
      "5258 [Discriminator loss: 0.572917, acc.: 70.31%] [Generator loss: 1.163754]\n",
      "5259 [Discriminator loss: 0.615929, acc.: 60.94%] [Generator loss: 0.957593]\n",
      "5260 [Discriminator loss: 0.616652, acc.: 67.19%] [Generator loss: 1.042568]\n",
      "5261 [Discriminator loss: 0.622545, acc.: 67.19%] [Generator loss: 1.120473]\n",
      "5262 [Discriminator loss: 0.643423, acc.: 71.09%] [Generator loss: 0.852618]\n",
      "5263 [Discriminator loss: 0.555614, acc.: 68.75%] [Generator loss: 0.789230]\n",
      "5264 [Discriminator loss: 0.623496, acc.: 67.19%] [Generator loss: 0.762610]\n",
      "5265 [Discriminator loss: 0.473036, acc.: 78.12%] [Generator loss: 0.959877]\n",
      "5266 [Discriminator loss: 0.926890, acc.: 38.28%] [Generator loss: 1.155021]\n",
      "5267 [Discriminator loss: 0.516032, acc.: 74.22%] [Generator loss: 1.264468]\n",
      "5268 [Discriminator loss: 0.727865, acc.: 56.25%] [Generator loss: 1.095294]\n",
      "5269 [Discriminator loss: 0.647835, acc.: 64.06%] [Generator loss: 1.123029]\n",
      "5270 [Discriminator loss: 0.658782, acc.: 65.62%] [Generator loss: 1.058944]\n",
      "5271 [Discriminator loss: 0.672623, acc.: 60.16%] [Generator loss: 0.971379]\n",
      "5272 [Discriminator loss: 0.476241, acc.: 83.59%] [Generator loss: 1.018815]\n",
      "5273 [Discriminator loss: 0.601325, acc.: 70.31%] [Generator loss: 0.971005]\n",
      "5274 [Discriminator loss: 0.570400, acc.: 70.31%] [Generator loss: 0.922567]\n",
      "5275 [Discriminator loss: 0.493293, acc.: 82.03%] [Generator loss: 1.068257]\n",
      "5276 [Discriminator loss: 0.493068, acc.: 75.78%] [Generator loss: 1.323541]\n",
      "5277 [Discriminator loss: 0.662411, acc.: 57.81%] [Generator loss: 1.360598]\n",
      "5278 [Discriminator loss: 0.649172, acc.: 67.19%] [Generator loss: 0.861100]\n",
      "5279 [Discriminator loss: 0.759703, acc.: 50.78%] [Generator loss: 0.756439]\n",
      "5280 [Discriminator loss: 0.628841, acc.: 66.41%] [Generator loss: 1.099566]\n",
      "5281 [Discriminator loss: 0.563038, acc.: 67.97%] [Generator loss: 1.163310]\n",
      "5282 [Discriminator loss: 0.397875, acc.: 84.38%] [Generator loss: 0.960038]\n",
      "5283 [Discriminator loss: 0.867892, acc.: 42.97%] [Generator loss: 0.804159]\n",
      "5284 [Discriminator loss: 0.804866, acc.: 55.47%] [Generator loss: 0.851570]\n",
      "5285 [Discriminator loss: 0.533681, acc.: 72.66%] [Generator loss: 0.905842]\n",
      "5286 [Discriminator loss: 0.829053, acc.: 39.84%] [Generator loss: 1.090209]\n",
      "5287 [Discriminator loss: 0.802052, acc.: 55.47%] [Generator loss: 1.051355]\n",
      "5288 [Discriminator loss: 0.802210, acc.: 46.88%] [Generator loss: 0.950265]\n",
      "5289 [Discriminator loss: 0.516441, acc.: 78.12%] [Generator loss: 1.168893]\n",
      "5290 [Discriminator loss: 0.734976, acc.: 60.16%] [Generator loss: 1.048865]\n",
      "5291 [Discriminator loss: 0.637853, acc.: 66.41%] [Generator loss: 1.267043]\n",
      "5292 [Discriminator loss: 0.527893, acc.: 74.22%] [Generator loss: 1.144139]\n",
      "5293 [Discriminator loss: 0.691698, acc.: 56.25%] [Generator loss: 1.150571]\n",
      "5294 [Discriminator loss: 0.461326, acc.: 79.69%] [Generator loss: 0.883874]\n",
      "5295 [Discriminator loss: 0.921691, acc.: 37.50%] [Generator loss: 0.628844]\n",
      "5296 [Discriminator loss: 0.516870, acc.: 77.34%] [Generator loss: 0.826666]\n",
      "5297 [Discriminator loss: 0.494917, acc.: 75.78%] [Generator loss: 0.883420]\n",
      "5298 [Discriminator loss: 0.686035, acc.: 60.16%] [Generator loss: 0.492006]\n",
      "5299 [Discriminator loss: 0.714442, acc.: 54.69%] [Generator loss: 0.817953]\n",
      "5300 [Discriminator loss: 0.337653, acc.: 89.84%] [Generator loss: 0.876724]\n",
      "5301 [Discriminator loss: 0.729355, acc.: 55.47%] [Generator loss: 0.983586]\n",
      "5302 [Discriminator loss: 0.668560, acc.: 60.16%] [Generator loss: 1.102958]\n",
      "5303 [Discriminator loss: 0.752234, acc.: 53.91%] [Generator loss: 1.217703]\n",
      "5304 [Discriminator loss: 0.581541, acc.: 68.75%] [Generator loss: 0.969993]\n",
      "5305 [Discriminator loss: 0.613490, acc.: 64.84%] [Generator loss: 0.847186]\n",
      "5306 [Discriminator loss: 0.431706, acc.: 84.38%] [Generator loss: 1.014990]\n",
      "5307 [Discriminator loss: 0.744783, acc.: 49.22%] [Generator loss: 0.828408]\n",
      "5308 [Discriminator loss: 0.543127, acc.: 77.34%] [Generator loss: 0.921599]\n",
      "5309 [Discriminator loss: 0.654629, acc.: 63.28%] [Generator loss: 0.907645]\n",
      "5310 [Discriminator loss: 0.411678, acc.: 85.16%] [Generator loss: 1.106385]\n",
      "5311 [Discriminator loss: 0.573454, acc.: 74.22%] [Generator loss: 1.178669]\n",
      "5312 [Discriminator loss: 0.541155, acc.: 71.88%] [Generator loss: 1.156144]\n",
      "5313 [Discriminator loss: 0.443223, acc.: 81.25%] [Generator loss: 0.719486]\n",
      "5314 [Discriminator loss: 0.418224, acc.: 81.25%] [Generator loss: 0.399887]\n",
      "5315 [Discriminator loss: 0.400155, acc.: 82.81%] [Generator loss: 1.042938]\n",
      "5316 [Discriminator loss: 0.718910, acc.: 59.38%] [Generator loss: 1.260807]\n",
      "5317 [Discriminator loss: 0.577439, acc.: 72.66%] [Generator loss: 1.022721]\n",
      "5318 [Discriminator loss: 0.574069, acc.: 64.06%] [Generator loss: 0.778038]\n",
      "5319 [Discriminator loss: 0.554588, acc.: 75.00%] [Generator loss: 1.230797]\n",
      "5320 [Discriminator loss: 0.453784, acc.: 79.69%] [Generator loss: 0.904960]\n",
      "5321 [Discriminator loss: 1.361488, acc.: 21.88%] [Generator loss: 1.088120]\n",
      "5322 [Discriminator loss: 0.572719, acc.: 71.09%] [Generator loss: 0.890427]\n",
      "5323 [Discriminator loss: 0.823085, acc.: 52.34%] [Generator loss: 1.077183]\n",
      "5324 [Discriminator loss: 0.692253, acc.: 60.16%] [Generator loss: 0.999753]\n",
      "5325 [Discriminator loss: 0.577281, acc.: 68.75%] [Generator loss: 1.158099]\n",
      "5326 [Discriminator loss: 1.092979, acc.: 32.81%] [Generator loss: 1.019886]\n",
      "5327 [Discriminator loss: 0.599992, acc.: 67.19%] [Generator loss: 1.221097]\n",
      "5328 [Discriminator loss: 0.435156, acc.: 88.28%] [Generator loss: 1.129044]\n",
      "5329 [Discriminator loss: 0.449363, acc.: 85.94%] [Generator loss: 1.080663]\n",
      "5330 [Discriminator loss: 0.600011, acc.: 66.41%] [Generator loss: 0.913207]\n",
      "5331 [Discriminator loss: 0.646946, acc.: 65.62%] [Generator loss: 1.184025]\n",
      "5332 [Discriminator loss: 0.592112, acc.: 70.31%] [Generator loss: 1.051471]\n",
      "5333 [Discriminator loss: 0.824981, acc.: 42.19%] [Generator loss: 0.792128]\n",
      "5334 [Discriminator loss: 0.428958, acc.: 82.81%] [Generator loss: 0.926499]\n",
      "5335 [Discriminator loss: 0.670647, acc.: 57.81%] [Generator loss: 0.879203]\n",
      "5336 [Discriminator loss: 0.516368, acc.: 76.56%] [Generator loss: 1.044388]\n",
      "5337 [Discriminator loss: 0.627095, acc.: 59.38%] [Generator loss: 1.098977]\n",
      "5338 [Discriminator loss: 0.673339, acc.: 63.28%] [Generator loss: 0.913478]\n",
      "5339 [Discriminator loss: 0.636371, acc.: 61.72%] [Generator loss: 1.071709]\n",
      "5340 [Discriminator loss: 0.638494, acc.: 62.50%] [Generator loss: 0.987841]\n",
      "5341 [Discriminator loss: 0.549825, acc.: 71.09%] [Generator loss: 1.021644]\n",
      "5342 [Discriminator loss: 0.710724, acc.: 53.91%] [Generator loss: 0.886351]\n",
      "5343 [Discriminator loss: 0.680811, acc.: 57.03%] [Generator loss: 1.133588]\n",
      "5344 [Discriminator loss: 0.659579, acc.: 59.38%] [Generator loss: 1.046795]\n",
      "5345 [Discriminator loss: 0.513130, acc.: 77.34%] [Generator loss: 0.951693]\n",
      "5346 [Discriminator loss: 0.730263, acc.: 53.12%] [Generator loss: 0.793779]\n",
      "5347 [Discriminator loss: 0.918184, acc.: 41.41%] [Generator loss: 0.907852]\n",
      "5348 [Discriminator loss: 0.544783, acc.: 71.88%] [Generator loss: 0.954736]\n",
      "5349 [Discriminator loss: 0.748663, acc.: 49.22%] [Generator loss: 0.990619]\n",
      "5350 [Discriminator loss: 0.665286, acc.: 59.38%] [Generator loss: 1.119688]\n",
      "5351 [Discriminator loss: 0.778849, acc.: 51.56%] [Generator loss: 1.166897]\n",
      "5352 [Discriminator loss: 0.708354, acc.: 59.38%] [Generator loss: 1.204319]\n",
      "5353 [Discriminator loss: 0.886863, acc.: 41.41%] [Generator loss: 1.200307]\n",
      "5354 [Discriminator loss: 1.020382, acc.: 36.72%] [Generator loss: 0.907874]\n",
      "5355 [Discriminator loss: 0.719567, acc.: 58.59%] [Generator loss: 1.243989]\n",
      "5356 [Discriminator loss: 0.640438, acc.: 67.97%] [Generator loss: 1.022494]\n",
      "5357 [Discriminator loss: 0.599683, acc.: 71.88%] [Generator loss: 0.804629]\n",
      "5358 [Discriminator loss: 0.753534, acc.: 51.56%] [Generator loss: 0.766225]\n",
      "5359 [Discriminator loss: 0.463691, acc.: 78.91%] [Generator loss: 0.783791]\n",
      "5360 [Discriminator loss: 0.680360, acc.: 58.59%] [Generator loss: 1.112531]\n",
      "5361 [Discriminator loss: 0.593750, acc.: 68.75%] [Generator loss: 0.961522]\n",
      "5362 [Discriminator loss: 0.772810, acc.: 48.44%] [Generator loss: 1.212061]\n",
      "5363 [Discriminator loss: 0.773145, acc.: 50.78%] [Generator loss: 1.239398]\n",
      "5364 [Discriminator loss: 0.552236, acc.: 68.75%] [Generator loss: 0.987975]\n",
      "5365 [Discriminator loss: 0.743327, acc.: 55.47%] [Generator loss: 1.155605]\n",
      "5366 [Discriminator loss: 0.944021, acc.: 39.06%] [Generator loss: 1.066217]\n",
      "5367 [Discriminator loss: 0.609788, acc.: 62.50%] [Generator loss: 1.192014]\n",
      "5368 [Discriminator loss: 0.673012, acc.: 62.50%] [Generator loss: 1.025254]\n",
      "5369 [Discriminator loss: 0.556066, acc.: 74.22%] [Generator loss: 1.146264]\n",
      "5370 [Discriminator loss: 0.814428, acc.: 44.53%] [Generator loss: 1.028791]\n",
      "5371 [Discriminator loss: 0.799149, acc.: 46.88%] [Generator loss: 0.976805]\n",
      "5372 [Discriminator loss: 0.883560, acc.: 44.53%] [Generator loss: 0.753008]\n",
      "5373 [Discriminator loss: 0.750702, acc.: 53.12%] [Generator loss: 1.139814]\n",
      "5374 [Discriminator loss: 0.643649, acc.: 64.84%] [Generator loss: 1.002705]\n",
      "5375 [Discriminator loss: 0.584977, acc.: 67.97%] [Generator loss: 0.921343]\n",
      "5376 [Discriminator loss: 0.556612, acc.: 74.22%] [Generator loss: 0.862318]\n",
      "5377 [Discriminator loss: 0.606429, acc.: 63.28%] [Generator loss: 1.021163]\n",
      "5378 [Discriminator loss: 0.542661, acc.: 71.88%] [Generator loss: 1.074906]\n",
      "5379 [Discriminator loss: 0.493662, acc.: 76.56%] [Generator loss: 1.325032]\n",
      "5380 [Discriminator loss: 0.568886, acc.: 65.62%] [Generator loss: 1.276131]\n",
      "5381 [Discriminator loss: 0.618581, acc.: 61.72%] [Generator loss: 1.032873]\n",
      "5382 [Discriminator loss: 0.528482, acc.: 73.44%] [Generator loss: 1.241271]\n",
      "5383 [Discriminator loss: 0.599658, acc.: 67.19%] [Generator loss: 1.289489]\n",
      "5384 [Discriminator loss: 0.627820, acc.: 65.62%] [Generator loss: 1.108398]\n",
      "5385 [Discriminator loss: 0.798178, acc.: 52.34%] [Generator loss: 0.914231]\n",
      "5386 [Discriminator loss: 0.615653, acc.: 62.50%] [Generator loss: 1.008136]\n",
      "5387 [Discriminator loss: 0.811265, acc.: 46.88%] [Generator loss: 0.806455]\n",
      "5388 [Discriminator loss: 0.816637, acc.: 50.00%] [Generator loss: 1.350011]\n",
      "5389 [Discriminator loss: 0.775598, acc.: 53.12%] [Generator loss: 1.279440]\n",
      "5390 [Discriminator loss: 0.715474, acc.: 59.38%] [Generator loss: 1.224646]\n",
      "5391 [Discriminator loss: 0.535089, acc.: 75.78%] [Generator loss: 1.066407]\n",
      "5392 [Discriminator loss: 0.672620, acc.: 61.72%] [Generator loss: 1.272689]\n",
      "5393 [Discriminator loss: 0.792550, acc.: 48.44%] [Generator loss: 0.999534]\n",
      "5394 [Discriminator loss: 0.613492, acc.: 69.53%] [Generator loss: 1.119188]\n",
      "5395 [Discriminator loss: 0.631613, acc.: 60.16%] [Generator loss: 1.135025]\n",
      "5396 [Discriminator loss: 0.633261, acc.: 71.09%] [Generator loss: 1.268811]\n",
      "5397 [Discriminator loss: 0.523376, acc.: 78.91%] [Generator loss: 1.136145]\n",
      "5398 [Discriminator loss: 0.570918, acc.: 74.22%] [Generator loss: 1.046700]\n",
      "5399 [Discriminator loss: 0.575610, acc.: 71.88%] [Generator loss: 0.928364]\n",
      "5400 [Discriminator loss: 0.605627, acc.: 64.84%] [Generator loss: 1.064966]\n",
      "5401 [Discriminator loss: 1.051463, acc.: 31.25%] [Generator loss: 0.940366]\n",
      "5402 [Discriminator loss: 0.738196, acc.: 55.47%] [Generator loss: 1.371750]\n",
      "5403 [Discriminator loss: 0.635393, acc.: 64.06%] [Generator loss: 1.043188]\n",
      "5404 [Discriminator loss: 0.693345, acc.: 64.06%] [Generator loss: 1.347413]\n",
      "5405 [Discriminator loss: 0.590368, acc.: 71.09%] [Generator loss: 1.183144]\n",
      "5406 [Discriminator loss: 0.661321, acc.: 61.72%] [Generator loss: 1.171421]\n",
      "5407 [Discriminator loss: 0.783516, acc.: 48.44%] [Generator loss: 1.098913]\n",
      "5408 [Discriminator loss: 0.757289, acc.: 46.88%] [Generator loss: 1.050013]\n",
      "5409 [Discriminator loss: 0.694770, acc.: 56.25%] [Generator loss: 1.111952]\n",
      "5410 [Discriminator loss: 0.617956, acc.: 64.84%] [Generator loss: 1.131994]\n",
      "5411 [Discriminator loss: 0.890878, acc.: 46.88%] [Generator loss: 1.110263]\n",
      "5412 [Discriminator loss: 0.663913, acc.: 60.16%] [Generator loss: 1.120435]\n",
      "5413 [Discriminator loss: 0.610571, acc.: 64.84%] [Generator loss: 1.037375]\n",
      "5414 [Discriminator loss: 0.816059, acc.: 46.88%] [Generator loss: 0.902203]\n",
      "5415 [Discriminator loss: 0.614403, acc.: 60.94%] [Generator loss: 0.986760]\n",
      "5416 [Discriminator loss: 0.509036, acc.: 75.78%] [Generator loss: 1.084909]\n",
      "5417 [Discriminator loss: 0.742843, acc.: 60.16%] [Generator loss: 0.994382]\n",
      "5418 [Discriminator loss: 0.677016, acc.: 61.72%] [Generator loss: 1.076274]\n",
      "5419 [Discriminator loss: 0.892634, acc.: 36.72%] [Generator loss: 1.259433]\n",
      "5420 [Discriminator loss: 0.739265, acc.: 53.12%] [Generator loss: 1.011154]\n",
      "5421 [Discriminator loss: 0.818416, acc.: 46.09%] [Generator loss: 1.237518]\n",
      "5422 [Discriminator loss: 0.831746, acc.: 49.22%] [Generator loss: 1.636967]\n",
      "5423 [Discriminator loss: 0.676759, acc.: 57.03%] [Generator loss: 1.552699]\n",
      "5424 [Discriminator loss: 0.819119, acc.: 48.44%] [Generator loss: 1.169212]\n",
      "5425 [Discriminator loss: 0.677229, acc.: 54.69%] [Generator loss: 1.014425]\n",
      "5426 [Discriminator loss: 0.655503, acc.: 61.72%] [Generator loss: 1.001009]\n",
      "5427 [Discriminator loss: 0.607730, acc.: 74.22%] [Generator loss: 0.979290]\n",
      "5428 [Discriminator loss: 0.639402, acc.: 63.28%] [Generator loss: 0.853361]\n",
      "5429 [Discriminator loss: 0.667384, acc.: 64.84%] [Generator loss: 1.182910]\n",
      "5430 [Discriminator loss: 0.558378, acc.: 72.66%] [Generator loss: 0.854698]\n",
      "5431 [Discriminator loss: 0.861067, acc.: 46.88%] [Generator loss: 0.844082]\n",
      "5432 [Discriminator loss: 0.623448, acc.: 63.28%] [Generator loss: 1.425981]\n",
      "5433 [Discriminator loss: 0.502820, acc.: 75.78%] [Generator loss: 0.907286]\n",
      "5434 [Discriminator loss: 0.749145, acc.: 50.78%] [Generator loss: 0.867944]\n",
      "5435 [Discriminator loss: 0.558321, acc.: 71.09%] [Generator loss: 0.982397]\n",
      "5436 [Discriminator loss: 0.638068, acc.: 62.50%] [Generator loss: 0.840073]\n",
      "5437 [Discriminator loss: 0.655850, acc.: 60.16%] [Generator loss: 0.931888]\n",
      "5438 [Discriminator loss: 0.716481, acc.: 51.56%] [Generator loss: 0.858495]\n",
      "5439 [Discriminator loss: 0.487573, acc.: 76.56%] [Generator loss: 1.633993]\n",
      "5440 [Discriminator loss: 0.649406, acc.: 65.62%] [Generator loss: 1.123089]\n",
      "5441 [Discriminator loss: 0.751733, acc.: 49.22%] [Generator loss: 1.036335]\n",
      "5442 [Discriminator loss: 0.888629, acc.: 39.06%] [Generator loss: 0.978754]\n",
      "5443 [Discriminator loss: 0.743696, acc.: 49.22%] [Generator loss: 1.029135]\n",
      "5444 [Discriminator loss: 0.687368, acc.: 54.69%] [Generator loss: 1.084611]\n",
      "5445 [Discriminator loss: 0.761454, acc.: 49.22%] [Generator loss: 0.996789]\n",
      "5446 [Discriminator loss: 0.676845, acc.: 58.59%] [Generator loss: 1.050687]\n",
      "5447 [Discriminator loss: 0.733684, acc.: 58.59%] [Generator loss: 0.932548]\n",
      "5448 [Discriminator loss: 0.780343, acc.: 48.44%] [Generator loss: 0.972171]\n",
      "5449 [Discriminator loss: 0.625378, acc.: 69.53%] [Generator loss: 0.959675]\n",
      "5450 [Discriminator loss: 0.593246, acc.: 61.72%] [Generator loss: 0.968139]\n",
      "5451 [Discriminator loss: 0.676568, acc.: 60.16%] [Generator loss: 1.001968]\n",
      "5452 [Discriminator loss: 0.685124, acc.: 61.72%] [Generator loss: 0.899180]\n",
      "5453 [Discriminator loss: 0.601616, acc.: 64.84%] [Generator loss: 0.919451]\n",
      "5454 [Discriminator loss: 0.573050, acc.: 71.09%] [Generator loss: 0.862905]\n",
      "5455 [Discriminator loss: 0.413486, acc.: 85.94%] [Generator loss: 1.012416]\n",
      "5456 [Discriminator loss: 0.620382, acc.: 64.84%] [Generator loss: 0.841576]\n",
      "5457 [Discriminator loss: 0.643714, acc.: 60.16%] [Generator loss: 0.956718]\n",
      "5458 [Discriminator loss: 0.481173, acc.: 82.81%] [Generator loss: 1.082854]\n",
      "5459 [Discriminator loss: 0.472314, acc.: 80.47%] [Generator loss: 0.990706]\n",
      "5460 [Discriminator loss: 0.783032, acc.: 50.78%] [Generator loss: 1.056021]\n",
      "5461 [Discriminator loss: 1.009762, acc.: 33.59%] [Generator loss: 1.019125]\n",
      "5462 [Discriminator loss: 0.578559, acc.: 71.09%] [Generator loss: 1.072348]\n",
      "5463 [Discriminator loss: 0.602609, acc.: 66.41%] [Generator loss: 1.096707]\n",
      "5464 [Discriminator loss: 0.603849, acc.: 74.22%] [Generator loss: 1.037919]\n",
      "5465 [Discriminator loss: 0.553881, acc.: 67.97%] [Generator loss: 0.987155]\n",
      "5466 [Discriminator loss: 0.747494, acc.: 50.78%] [Generator loss: 0.750187]\n",
      "5467 [Discriminator loss: 0.755912, acc.: 53.91%] [Generator loss: 1.060861]\n",
      "5468 [Discriminator loss: 0.642026, acc.: 66.41%] [Generator loss: 0.800945]\n",
      "5469 [Discriminator loss: 0.845053, acc.: 53.91%] [Generator loss: 0.513502]\n",
      "5470 [Discriminator loss: 0.787125, acc.: 54.69%] [Generator loss: 0.893638]\n",
      "5471 [Discriminator loss: 0.520775, acc.: 71.88%] [Generator loss: 0.978922]\n",
      "5472 [Discriminator loss: 0.627785, acc.: 64.84%] [Generator loss: 0.604057]\n",
      "5473 [Discriminator loss: 0.526276, acc.: 74.22%] [Generator loss: 0.680473]\n",
      "5474 [Discriminator loss: 0.482449, acc.: 76.56%] [Generator loss: 0.852039]\n",
      "5475 [Discriminator loss: 0.942602, acc.: 42.97%] [Generator loss: 0.956518]\n",
      "5476 [Discriminator loss: 0.605111, acc.: 70.31%] [Generator loss: 1.080926]\n",
      "5477 [Discriminator loss: 0.446765, acc.: 80.47%] [Generator loss: 0.856886]\n",
      "5478 [Discriminator loss: 0.718058, acc.: 50.78%] [Generator loss: 0.856916]\n",
      "5479 [Discriminator loss: 0.831148, acc.: 49.22%] [Generator loss: 1.090384]\n",
      "5480 [Discriminator loss: 0.587629, acc.: 67.97%] [Generator loss: 1.389100]\n",
      "5481 [Discriminator loss: 0.570079, acc.: 71.09%] [Generator loss: 1.130895]\n",
      "5482 [Discriminator loss: 0.852472, acc.: 43.75%] [Generator loss: 0.931246]\n",
      "5483 [Discriminator loss: 0.901952, acc.: 46.09%] [Generator loss: 0.856991]\n",
      "5484 [Discriminator loss: 0.475395, acc.: 80.47%] [Generator loss: 0.817514]\n",
      "5485 [Discriminator loss: 0.740254, acc.: 55.47%] [Generator loss: 0.826870]\n",
      "5486 [Discriminator loss: 0.624524, acc.: 64.84%] [Generator loss: 0.968683]\n",
      "5487 [Discriminator loss: 0.645144, acc.: 65.62%] [Generator loss: 0.995092]\n",
      "5488 [Discriminator loss: 0.526994, acc.: 75.78%] [Generator loss: 0.976437]\n",
      "5489 [Discriminator loss: 0.673916, acc.: 59.38%] [Generator loss: 1.072792]\n",
      "5490 [Discriminator loss: 0.569713, acc.: 71.09%] [Generator loss: 1.105300]\n",
      "5491 [Discriminator loss: 0.655214, acc.: 64.06%] [Generator loss: 1.535145]\n",
      "5492 [Discriminator loss: 0.688882, acc.: 59.38%] [Generator loss: 1.494226]\n",
      "5493 [Discriminator loss: 0.514840, acc.: 79.69%] [Generator loss: 1.039966]\n",
      "5494 [Discriminator loss: 0.519002, acc.: 76.56%] [Generator loss: 1.086225]\n",
      "5495 [Discriminator loss: 0.508087, acc.: 76.56%] [Generator loss: 1.051274]\n",
      "5496 [Discriminator loss: 0.650292, acc.: 62.50%] [Generator loss: 0.949353]\n",
      "5497 [Discriminator loss: 0.704678, acc.: 57.03%] [Generator loss: 1.274016]\n",
      "5498 [Discriminator loss: 0.861440, acc.: 48.44%] [Generator loss: 1.283178]\n",
      "5499 [Discriminator loss: 0.733934, acc.: 57.03%] [Generator loss: 1.123438]\n",
      "5500 [Discriminator loss: 0.925137, acc.: 32.81%] [Generator loss: 1.042935]\n",
      "5501 [Discriminator loss: 0.569784, acc.: 68.75%] [Generator loss: 1.074689]\n",
      "5502 [Discriminator loss: 0.914667, acc.: 39.06%] [Generator loss: 0.920943]\n",
      "5503 [Discriminator loss: 0.753934, acc.: 52.34%] [Generator loss: 0.846398]\n",
      "5504 [Discriminator loss: 0.731045, acc.: 52.34%] [Generator loss: 0.921761]\n",
      "5505 [Discriminator loss: 0.527935, acc.: 77.34%] [Generator loss: 1.015639]\n",
      "5506 [Discriminator loss: 0.501953, acc.: 78.91%] [Generator loss: 1.267860]\n",
      "5507 [Discriminator loss: 0.535920, acc.: 70.31%] [Generator loss: 1.202370]\n",
      "5508 [Discriminator loss: 0.579874, acc.: 71.09%] [Generator loss: 0.975246]\n",
      "5509 [Discriminator loss: 0.580363, acc.: 64.84%] [Generator loss: 1.020636]\n",
      "5510 [Discriminator loss: 0.639182, acc.: 58.59%] [Generator loss: 1.013810]\n",
      "5511 [Discriminator loss: 0.577889, acc.: 72.66%] [Generator loss: 1.275843]\n",
      "5512 [Discriminator loss: 0.907441, acc.: 40.62%] [Generator loss: 1.061514]\n",
      "5513 [Discriminator loss: 0.718005, acc.: 54.69%] [Generator loss: 0.949662]\n",
      "5514 [Discriminator loss: 0.783020, acc.: 55.47%] [Generator loss: 1.342868]\n",
      "5515 [Discriminator loss: 1.020688, acc.: 35.16%] [Generator loss: 1.262664]\n",
      "5516 [Discriminator loss: 0.836845, acc.: 46.09%] [Generator loss: 1.357022]\n",
      "5517 [Discriminator loss: 0.804522, acc.: 50.78%] [Generator loss: 1.172709]\n",
      "5518 [Discriminator loss: 0.694653, acc.: 55.47%] [Generator loss: 1.271541]\n",
      "5519 [Discriminator loss: 0.609707, acc.: 65.62%] [Generator loss: 1.277625]\n",
      "5520 [Discriminator loss: 0.545466, acc.: 72.66%] [Generator loss: 1.037837]\n",
      "5521 [Discriminator loss: 0.645018, acc.: 64.84%] [Generator loss: 1.126360]\n",
      "5522 [Discriminator loss: 0.609370, acc.: 66.41%] [Generator loss: 0.875149]\n",
      "5523 [Discriminator loss: 0.407342, acc.: 85.16%] [Generator loss: 0.675154]\n",
      "5524 [Discriminator loss: 0.491312, acc.: 73.44%] [Generator loss: 1.024459]\n",
      "5525 [Discriminator loss: 0.468801, acc.: 78.91%] [Generator loss: 0.840922]\n",
      "5526 [Discriminator loss: 0.834951, acc.: 53.91%] [Generator loss: 0.882163]\n",
      "5527 [Discriminator loss: 0.492395, acc.: 81.25%] [Generator loss: 0.747464]\n",
      "5528 [Discriminator loss: 0.665404, acc.: 67.97%] [Generator loss: 0.738111]\n",
      "5529 [Discriminator loss: 0.902517, acc.: 47.66%] [Generator loss: 1.023815]\n",
      "5530 [Discriminator loss: 0.631802, acc.: 67.97%] [Generator loss: 1.042500]\n",
      "5531 [Discriminator loss: 1.087368, acc.: 28.91%] [Generator loss: 0.908394]\n",
      "5532 [Discriminator loss: 0.678495, acc.: 64.06%] [Generator loss: 1.095784]\n",
      "5533 [Discriminator loss: 0.646946, acc.: 63.28%] [Generator loss: 1.393003]\n",
      "5534 [Discriminator loss: 0.540276, acc.: 76.56%] [Generator loss: 1.236531]\n",
      "5535 [Discriminator loss: 0.710788, acc.: 56.25%] [Generator loss: 1.509038]\n",
      "5536 [Discriminator loss: 0.664533, acc.: 58.59%] [Generator loss: 1.003487]\n",
      "5537 [Discriminator loss: 0.715547, acc.: 51.56%] [Generator loss: 1.056786]\n",
      "5538 [Discriminator loss: 0.592643, acc.: 67.19%] [Generator loss: 1.296024]\n",
      "5539 [Discriminator loss: 0.621094, acc.: 68.75%] [Generator loss: 1.102904]\n",
      "5540 [Discriminator loss: 0.553887, acc.: 67.97%] [Generator loss: 0.710964]\n",
      "5541 [Discriminator loss: 0.523839, acc.: 74.22%] [Generator loss: 0.712799]\n",
      "5542 [Discriminator loss: 0.523832, acc.: 77.34%] [Generator loss: 1.089058]\n",
      "5543 [Discriminator loss: 0.675591, acc.: 66.41%] [Generator loss: 0.665516]\n",
      "5544 [Discriminator loss: 0.549864, acc.: 71.09%] [Generator loss: 0.758877]\n",
      "5545 [Discriminator loss: 0.526067, acc.: 72.66%] [Generator loss: 0.439678]\n",
      "5546 [Discriminator loss: 0.903236, acc.: 50.78%] [Generator loss: 1.231343]\n",
      "5547 [Discriminator loss: 0.649787, acc.: 68.75%] [Generator loss: 1.489528]\n",
      "5548 [Discriminator loss: 0.571253, acc.: 68.75%] [Generator loss: 1.217827]\n",
      "5549 [Discriminator loss: 0.601883, acc.: 70.31%] [Generator loss: 1.116598]\n",
      "5550 [Discriminator loss: 0.728572, acc.: 59.38%] [Generator loss: 0.835797]\n",
      "5551 [Discriminator loss: 0.480813, acc.: 76.56%] [Generator loss: 0.924676]\n",
      "5552 [Discriminator loss: 0.463180, acc.: 80.47%] [Generator loss: 0.606836]\n",
      "5553 [Discriminator loss: 0.460751, acc.: 83.59%] [Generator loss: 0.762960]\n",
      "5554 [Discriminator loss: 0.439130, acc.: 79.69%] [Generator loss: 0.515668]\n",
      "5555 [Discriminator loss: 0.367073, acc.: 86.72%] [Generator loss: 0.569966]\n",
      "5556 [Discriminator loss: 0.381889, acc.: 87.50%] [Generator loss: 0.377820]\n",
      "5557 [Discriminator loss: 0.401042, acc.: 85.16%] [Generator loss: 0.297746]\n",
      "5558 [Discriminator loss: 1.066072, acc.: 46.88%] [Generator loss: 1.024696]\n",
      "5559 [Discriminator loss: 0.592240, acc.: 61.72%] [Generator loss: 1.279541]\n",
      "5560 [Discriminator loss: 0.606385, acc.: 65.62%] [Generator loss: 0.735799]\n",
      "5561 [Discriminator loss: 0.528964, acc.: 76.56%] [Generator loss: 0.912579]\n",
      "5562 [Discriminator loss: 0.748624, acc.: 51.56%] [Generator loss: 0.830531]\n",
      "5563 [Discriminator loss: 0.674043, acc.: 56.25%] [Generator loss: 0.703921]\n",
      "5564 [Discriminator loss: 0.776507, acc.: 50.00%] [Generator loss: 1.093478]\n",
      "5565 [Discriminator loss: 0.992581, acc.: 33.59%] [Generator loss: 1.248650]\n",
      "5566 [Discriminator loss: 0.662865, acc.: 61.72%] [Generator loss: 0.790087]\n",
      "5567 [Discriminator loss: 0.809906, acc.: 50.00%] [Generator loss: 1.098350]\n",
      "5568 [Discriminator loss: 0.638712, acc.: 68.75%] [Generator loss: 0.857066]\n",
      "5569 [Discriminator loss: 0.711922, acc.: 60.94%] [Generator loss: 0.684769]\n",
      "5570 [Discriminator loss: 0.751253, acc.: 57.03%] [Generator loss: 0.761834]\n",
      "5571 [Discriminator loss: 0.558868, acc.: 71.09%] [Generator loss: 0.924653]\n",
      "5572 [Discriminator loss: 0.497458, acc.: 84.38%] [Generator loss: 0.957238]\n",
      "5573 [Discriminator loss: 0.446346, acc.: 83.59%] [Generator loss: 0.962370]\n",
      "5574 [Discriminator loss: 0.510917, acc.: 79.69%] [Generator loss: 0.844458]\n",
      "5575 [Discriminator loss: 0.589622, acc.: 65.62%] [Generator loss: 0.985971]\n",
      "5576 [Discriminator loss: 0.447134, acc.: 84.38%] [Generator loss: 0.830915]\n",
      "5577 [Discriminator loss: 0.857075, acc.: 46.88%] [Generator loss: 0.627689]\n",
      "5578 [Discriminator loss: 0.485991, acc.: 78.12%] [Generator loss: 0.875082]\n",
      "5579 [Discriminator loss: 0.670607, acc.: 64.06%] [Generator loss: 1.291734]\n",
      "5580 [Discriminator loss: 0.587603, acc.: 70.31%] [Generator loss: 0.878661]\n",
      "5581 [Discriminator loss: 0.504373, acc.: 77.34%] [Generator loss: 1.102521]\n",
      "5582 [Discriminator loss: 1.114035, acc.: 29.69%] [Generator loss: 0.902098]\n",
      "5583 [Discriminator loss: 0.485252, acc.: 77.34%] [Generator loss: 0.986715]\n",
      "5584 [Discriminator loss: 0.711967, acc.: 56.25%] [Generator loss: 0.918346]\n",
      "5585 [Discriminator loss: 0.816775, acc.: 50.78%] [Generator loss: 0.828313]\n",
      "5586 [Discriminator loss: 0.716818, acc.: 54.69%] [Generator loss: 0.808411]\n",
      "5587 [Discriminator loss: 0.753929, acc.: 52.34%] [Generator loss: 0.847902]\n",
      "5588 [Discriminator loss: 0.543445, acc.: 76.56%] [Generator loss: 0.954442]\n",
      "5589 [Discriminator loss: 0.632049, acc.: 62.50%] [Generator loss: 0.653622]\n",
      "5590 [Discriminator loss: 0.573487, acc.: 73.44%] [Generator loss: 0.893878]\n",
      "5591 [Discriminator loss: 0.431701, acc.: 82.03%] [Generator loss: 0.741827]\n",
      "5592 [Discriminator loss: 0.597964, acc.: 66.41%] [Generator loss: 1.229585]\n",
      "5593 [Discriminator loss: 0.964129, acc.: 38.28%] [Generator loss: 0.857655]\n",
      "5594 [Discriminator loss: 0.810119, acc.: 50.78%] [Generator loss: 0.810231]\n",
      "5595 [Discriminator loss: 0.854735, acc.: 40.62%] [Generator loss: 1.114236]\n",
      "5596 [Discriminator loss: 0.815533, acc.: 45.31%] [Generator loss: 1.029390]\n",
      "5597 [Discriminator loss: 0.915144, acc.: 48.44%] [Generator loss: 0.874356]\n",
      "5598 [Discriminator loss: 0.768303, acc.: 52.34%] [Generator loss: 0.914717]\n",
      "5599 [Discriminator loss: 0.816926, acc.: 56.25%] [Generator loss: 1.300438]\n",
      "5600 [Discriminator loss: 0.656172, acc.: 64.84%] [Generator loss: 1.388954]\n",
      "5601 [Discriminator loss: 0.702427, acc.: 58.59%] [Generator loss: 1.330501]\n",
      "5602 [Discriminator loss: 0.665520, acc.: 61.72%] [Generator loss: 1.063934]\n",
      "5603 [Discriminator loss: 0.734590, acc.: 57.81%] [Generator loss: 1.259529]\n",
      "5604 [Discriminator loss: 0.734235, acc.: 54.69%] [Generator loss: 1.504084]\n",
      "5605 [Discriminator loss: 0.753332, acc.: 53.91%] [Generator loss: 0.999783]\n",
      "5606 [Discriminator loss: 0.723510, acc.: 50.00%] [Generator loss: 0.833556]\n",
      "5607 [Discriminator loss: 0.778306, acc.: 57.03%] [Generator loss: 0.767972]\n",
      "5608 [Discriminator loss: 0.720291, acc.: 50.78%] [Generator loss: 0.814046]\n",
      "5609 [Discriminator loss: 0.584072, acc.: 68.75%] [Generator loss: 1.047450]\n",
      "5610 [Discriminator loss: 0.724274, acc.: 53.91%] [Generator loss: 0.926416]\n",
      "5611 [Discriminator loss: 0.729225, acc.: 55.47%] [Generator loss: 0.916562]\n",
      "5612 [Discriminator loss: 0.664309, acc.: 64.84%] [Generator loss: 1.098480]\n",
      "5613 [Discriminator loss: 0.643979, acc.: 60.94%] [Generator loss: 0.857728]\n",
      "5614 [Discriminator loss: 0.780958, acc.: 47.66%] [Generator loss: 0.657181]\n",
      "5615 [Discriminator loss: 0.740041, acc.: 54.69%] [Generator loss: 0.833166]\n",
      "5616 [Discriminator loss: 0.600078, acc.: 67.19%] [Generator loss: 0.976711]\n",
      "5617 [Discriminator loss: 0.558870, acc.: 73.44%] [Generator loss: 0.831018]\n",
      "5618 [Discriminator loss: 0.536569, acc.: 78.12%] [Generator loss: 0.739136]\n",
      "5619 [Discriminator loss: 0.745274, acc.: 50.00%] [Generator loss: 0.669643]\n",
      "5620 [Discriminator loss: 0.660099, acc.: 59.38%] [Generator loss: 0.678336]\n",
      "5621 [Discriminator loss: 0.652962, acc.: 58.59%] [Generator loss: 0.953267]\n",
      "5622 [Discriminator loss: 0.589091, acc.: 67.97%] [Generator loss: 0.976228]\n",
      "5623 [Discriminator loss: 0.626041, acc.: 64.84%] [Generator loss: 1.088462]\n",
      "5624 [Discriminator loss: 0.577717, acc.: 71.09%] [Generator loss: 0.842578]\n",
      "5625 [Discriminator loss: 0.565459, acc.: 71.09%] [Generator loss: 0.971110]\n",
      "5626 [Discriminator loss: 0.702422, acc.: 60.94%] [Generator loss: 1.152070]\n",
      "5627 [Discriminator loss: 0.579708, acc.: 67.19%] [Generator loss: 1.072082]\n",
      "5628 [Discriminator loss: 0.676932, acc.: 65.62%] [Generator loss: 0.930180]\n",
      "5629 [Discriminator loss: 0.938956, acc.: 48.44%] [Generator loss: 1.397402]\n",
      "5630 [Discriminator loss: 0.533837, acc.: 75.00%] [Generator loss: 1.280164]\n",
      "5631 [Discriminator loss: 0.799043, acc.: 55.47%] [Generator loss: 1.153049]\n",
      "5632 [Discriminator loss: 0.587830, acc.: 64.84%] [Generator loss: 1.128287]\n",
      "5633 [Discriminator loss: 0.841484, acc.: 47.66%] [Generator loss: 0.974507]\n",
      "5634 [Discriminator loss: 0.630786, acc.: 72.66%] [Generator loss: 0.993265]\n",
      "5635 [Discriminator loss: 0.490676, acc.: 79.69%] [Generator loss: 1.369150]\n",
      "5636 [Discriminator loss: 0.673668, acc.: 57.03%] [Generator loss: 1.255438]\n",
      "5637 [Discriminator loss: 0.638496, acc.: 60.94%] [Generator loss: 1.070766]\n",
      "5638 [Discriminator loss: 0.910420, acc.: 44.53%] [Generator loss: 0.949662]\n",
      "5639 [Discriminator loss: 0.710617, acc.: 55.47%] [Generator loss: 1.026464]\n",
      "5640 [Discriminator loss: 0.628523, acc.: 63.28%] [Generator loss: 1.127311]\n",
      "5641 [Discriminator loss: 0.761842, acc.: 53.12%] [Generator loss: 1.124680]\n",
      "5642 [Discriminator loss: 0.737840, acc.: 54.69%] [Generator loss: 0.834758]\n",
      "5643 [Discriminator loss: 0.379018, acc.: 86.72%] [Generator loss: 0.609872]\n",
      "5644 [Discriminator loss: 0.694513, acc.: 60.94%] [Generator loss: 0.577843]\n",
      "5645 [Discriminator loss: 0.637994, acc.: 62.50%] [Generator loss: 0.747716]\n",
      "5646 [Discriminator loss: 0.461639, acc.: 78.91%] [Generator loss: 0.953504]\n",
      "5647 [Discriminator loss: 0.582003, acc.: 69.53%] [Generator loss: 0.611670]\n",
      "5648 [Discriminator loss: 0.721956, acc.: 51.56%] [Generator loss: 0.712160]\n",
      "5649 [Discriminator loss: 0.923303, acc.: 46.88%] [Generator loss: 0.856067]\n",
      "5650 [Discriminator loss: 0.506973, acc.: 73.44%] [Generator loss: 1.108034]\n",
      "5651 [Discriminator loss: 0.949638, acc.: 46.09%] [Generator loss: 0.813826]\n",
      "5652 [Discriminator loss: 0.627109, acc.: 60.16%] [Generator loss: 0.911071]\n",
      "5653 [Discriminator loss: 0.538336, acc.: 75.00%] [Generator loss: 0.967465]\n",
      "5654 [Discriminator loss: 0.710331, acc.: 56.25%] [Generator loss: 0.968329]\n",
      "5655 [Discriminator loss: 0.516832, acc.: 75.78%] [Generator loss: 0.934492]\n",
      "5656 [Discriminator loss: 0.748228, acc.: 57.03%] [Generator loss: 0.841497]\n",
      "5657 [Discriminator loss: 0.714169, acc.: 57.03%] [Generator loss: 0.903291]\n",
      "5658 [Discriminator loss: 0.723559, acc.: 55.47%] [Generator loss: 1.097185]\n",
      "5659 [Discriminator loss: 0.819875, acc.: 47.66%] [Generator loss: 0.831376]\n",
      "5660 [Discriminator loss: 0.854778, acc.: 44.53%] [Generator loss: 0.820923]\n",
      "5661 [Discriminator loss: 0.827324, acc.: 46.09%] [Generator loss: 0.853799]\n",
      "5662 [Discriminator loss: 0.673740, acc.: 60.94%] [Generator loss: 1.090803]\n",
      "5663 [Discriminator loss: 0.863948, acc.: 46.09%] [Generator loss: 0.985603]\n",
      "5664 [Discriminator loss: 0.570848, acc.: 71.88%] [Generator loss: 1.110531]\n",
      "5665 [Discriminator loss: 0.635155, acc.: 62.50%] [Generator loss: 1.037533]\n",
      "5666 [Discriminator loss: 0.498539, acc.: 78.12%] [Generator loss: 1.023232]\n",
      "5667 [Discriminator loss: 0.622849, acc.: 69.53%] [Generator loss: 1.015101]\n",
      "5668 [Discriminator loss: 0.629004, acc.: 60.94%] [Generator loss: 1.047224]\n",
      "5669 [Discriminator loss: 0.582583, acc.: 74.22%] [Generator loss: 0.970790]\n",
      "5670 [Discriminator loss: 0.613059, acc.: 63.28%] [Generator loss: 1.004125]\n",
      "5671 [Discriminator loss: 0.763407, acc.: 53.12%] [Generator loss: 1.097315]\n",
      "5672 [Discriminator loss: 0.420996, acc.: 83.59%] [Generator loss: 0.908973]\n",
      "5673 [Discriminator loss: 0.607774, acc.: 67.97%] [Generator loss: 0.854951]\n",
      "5674 [Discriminator loss: 0.622395, acc.: 64.06%] [Generator loss: 1.025866]\n",
      "5675 [Discriminator loss: 0.569161, acc.: 68.75%] [Generator loss: 0.804512]\n",
      "5676 [Discriminator loss: 0.920460, acc.: 35.94%] [Generator loss: 1.108105]\n",
      "5677 [Discriminator loss: 0.872441, acc.: 41.41%] [Generator loss: 1.261149]\n",
      "5678 [Discriminator loss: 0.651001, acc.: 56.25%] [Generator loss: 1.244718]\n",
      "5679 [Discriminator loss: 0.778038, acc.: 50.00%] [Generator loss: 0.931403]\n",
      "5680 [Discriminator loss: 0.629632, acc.: 64.06%] [Generator loss: 0.888657]\n",
      "5681 [Discriminator loss: 0.765886, acc.: 46.09%] [Generator loss: 0.752373]\n",
      "5682 [Discriminator loss: 0.859756, acc.: 42.19%] [Generator loss: 0.723744]\n",
      "5683 [Discriminator loss: 0.486892, acc.: 75.00%] [Generator loss: 0.939965]\n",
      "5684 [Discriminator loss: 0.656890, acc.: 61.72%] [Generator loss: 0.986697]\n",
      "5685 [Discriminator loss: 0.510542, acc.: 78.91%] [Generator loss: 1.219059]\n",
      "5686 [Discriminator loss: 0.656951, acc.: 67.19%] [Generator loss: 0.919297]\n",
      "5687 [Discriminator loss: 0.607368, acc.: 70.31%] [Generator loss: 1.066970]\n",
      "5688 [Discriminator loss: 0.680295, acc.: 61.72%] [Generator loss: 1.014991]\n",
      "5689 [Discriminator loss: 0.441927, acc.: 82.81%] [Generator loss: 0.754878]\n",
      "5690 [Discriminator loss: 0.863511, acc.: 39.84%] [Generator loss: 0.795315]\n",
      "5691 [Discriminator loss: 0.654489, acc.: 68.75%] [Generator loss: 1.166670]\n",
      "5692 [Discriminator loss: 0.606182, acc.: 67.97%] [Generator loss: 1.378742]\n",
      "5693 [Discriminator loss: 0.669556, acc.: 63.28%] [Generator loss: 1.011003]\n",
      "5694 [Discriminator loss: 0.793181, acc.: 56.25%] [Generator loss: 1.068508]\n",
      "5695 [Discriminator loss: 0.884885, acc.: 37.50%] [Generator loss: 0.904593]\n",
      "5696 [Discriminator loss: 0.626565, acc.: 62.50%] [Generator loss: 0.998035]\n",
      "5697 [Discriminator loss: 0.806377, acc.: 49.22%] [Generator loss: 0.814441]\n",
      "5698 [Discriminator loss: 0.725323, acc.: 51.56%] [Generator loss: 0.745003]\n",
      "5699 [Discriminator loss: 0.885650, acc.: 49.22%] [Generator loss: 1.097450]\n",
      "5700 [Discriminator loss: 0.589378, acc.: 71.09%] [Generator loss: 1.053029]\n",
      "5701 [Discriminator loss: 0.503942, acc.: 73.44%] [Generator loss: 0.894384]\n",
      "5702 [Discriminator loss: 0.576676, acc.: 74.22%] [Generator loss: 0.697826]\n",
      "5703 [Discriminator loss: 0.498912, acc.: 77.34%] [Generator loss: 1.018720]\n",
      "5704 [Discriminator loss: 0.941029, acc.: 47.66%] [Generator loss: 0.959144]\n",
      "5705 [Discriminator loss: 0.472942, acc.: 76.56%] [Generator loss: 0.677580]\n",
      "5706 [Discriminator loss: 0.443801, acc.: 85.16%] [Generator loss: 0.751316]\n",
      "5707 [Discriminator loss: 0.420852, acc.: 84.38%] [Generator loss: 0.444307]\n",
      "5708 [Discriminator loss: 0.890019, acc.: 49.22%] [Generator loss: 0.777810]\n",
      "5709 [Discriminator loss: 0.728895, acc.: 51.56%] [Generator loss: 1.145226]\n",
      "5710 [Discriminator loss: 0.748999, acc.: 53.91%] [Generator loss: 0.831510]\n",
      "5711 [Discriminator loss: 0.745297, acc.: 53.12%] [Generator loss: 1.261272]\n",
      "5712 [Discriminator loss: 0.841652, acc.: 48.44%] [Generator loss: 0.755476]\n",
      "5713 [Discriminator loss: 0.757558, acc.: 52.34%] [Generator loss: 0.990429]\n",
      "5714 [Discriminator loss: 0.607161, acc.: 68.75%] [Generator loss: 0.797990]\n",
      "5715 [Discriminator loss: 0.471923, acc.: 77.34%] [Generator loss: 0.961512]\n",
      "5716 [Discriminator loss: 0.662103, acc.: 59.38%] [Generator loss: 0.871349]\n",
      "5717 [Discriminator loss: 0.721284, acc.: 57.81%] [Generator loss: 0.861183]\n",
      "5718 [Discriminator loss: 0.750264, acc.: 53.91%] [Generator loss: 0.993989]\n",
      "5719 [Discriminator loss: 0.777488, acc.: 46.09%] [Generator loss: 0.967051]\n",
      "5720 [Discriminator loss: 0.589943, acc.: 65.62%] [Generator loss: 0.989867]\n",
      "5721 [Discriminator loss: 0.635615, acc.: 55.47%] [Generator loss: 0.957290]\n",
      "5722 [Discriminator loss: 0.742934, acc.: 50.78%] [Generator loss: 0.980393]\n",
      "5723 [Discriminator loss: 0.725405, acc.: 54.69%] [Generator loss: 1.015770]\n",
      "5724 [Discriminator loss: 0.654724, acc.: 64.06%] [Generator loss: 1.060951]\n",
      "5725 [Discriminator loss: 0.944977, acc.: 36.72%] [Generator loss: 0.846659]\n",
      "5726 [Discriminator loss: 0.587379, acc.: 64.84%] [Generator loss: 0.916868]\n",
      "5727 [Discriminator loss: 0.577221, acc.: 70.31%] [Generator loss: 0.897300]\n",
      "5728 [Discriminator loss: 0.613599, acc.: 63.28%] [Generator loss: 0.877514]\n",
      "5729 [Discriminator loss: 0.590193, acc.: 67.19%] [Generator loss: 0.809656]\n",
      "5730 [Discriminator loss: 0.720809, acc.: 58.59%] [Generator loss: 0.758286]\n",
      "5731 [Discriminator loss: 0.785364, acc.: 55.47%] [Generator loss: 0.913934]\n",
      "5732 [Discriminator loss: 0.859150, acc.: 49.22%] [Generator loss: 0.842420]\n",
      "5733 [Discriminator loss: 0.843042, acc.: 40.62%] [Generator loss: 0.863784]\n",
      "5734 [Discriminator loss: 0.771060, acc.: 49.22%] [Generator loss: 0.935393]\n",
      "5735 [Discriminator loss: 0.567249, acc.: 66.41%] [Generator loss: 1.147228]\n",
      "5736 [Discriminator loss: 0.704714, acc.: 57.03%] [Generator loss: 1.147509]\n",
      "5737 [Discriminator loss: 0.694280, acc.: 55.47%] [Generator loss: 1.183814]\n",
      "5738 [Discriminator loss: 0.770777, acc.: 53.12%] [Generator loss: 1.352905]\n",
      "5739 [Discriminator loss: 0.713740, acc.: 56.25%] [Generator loss: 1.260883]\n",
      "5740 [Discriminator loss: 0.736859, acc.: 53.12%] [Generator loss: 0.966022]\n",
      "5741 [Discriminator loss: 0.759488, acc.: 56.25%] [Generator loss: 1.227547]\n",
      "5742 [Discriminator loss: 0.635452, acc.: 66.41%] [Generator loss: 0.938309]\n",
      "5743 [Discriminator loss: 0.718609, acc.: 57.03%] [Generator loss: 1.010676]\n",
      "5744 [Discriminator loss: 0.699447, acc.: 58.59%] [Generator loss: 0.926201]\n",
      "5745 [Discriminator loss: 0.703588, acc.: 53.91%] [Generator loss: 0.795875]\n",
      "5746 [Discriminator loss: 0.680740, acc.: 54.69%] [Generator loss: 0.847375]\n",
      "5747 [Discriminator loss: 0.581641, acc.: 77.34%] [Generator loss: 1.017459]\n",
      "5748 [Discriminator loss: 0.645284, acc.: 63.28%] [Generator loss: 0.966416]\n",
      "5749 [Discriminator loss: 0.580514, acc.: 73.44%] [Generator loss: 0.868694]\n",
      "5750 [Discriminator loss: 0.718850, acc.: 58.59%] [Generator loss: 0.807172]\n",
      "5751 [Discriminator loss: 0.689202, acc.: 57.03%] [Generator loss: 1.147038]\n",
      "5752 [Discriminator loss: 0.954128, acc.: 37.50%] [Generator loss: 1.175482]\n",
      "5753 [Discriminator loss: 0.830899, acc.: 45.31%] [Generator loss: 1.020336]\n",
      "5754 [Discriminator loss: 0.711585, acc.: 61.72%] [Generator loss: 1.433331]\n",
      "5755 [Discriminator loss: 0.743210, acc.: 52.34%] [Generator loss: 1.047255]\n",
      "5756 [Discriminator loss: 0.744173, acc.: 53.12%] [Generator loss: 1.414232]\n",
      "5757 [Discriminator loss: 0.695567, acc.: 57.03%] [Generator loss: 1.591008]\n",
      "5758 [Discriminator loss: 0.659557, acc.: 66.41%] [Generator loss: 1.205176]\n",
      "5759 [Discriminator loss: 0.700532, acc.: 60.16%] [Generator loss: 0.901371]\n",
      "5760 [Discriminator loss: 0.686470, acc.: 64.84%] [Generator loss: 0.784225]\n",
      "5761 [Discriminator loss: 0.720079, acc.: 55.47%] [Generator loss: 0.766292]\n",
      "5762 [Discriminator loss: 0.607110, acc.: 69.53%] [Generator loss: 0.771864]\n",
      "5763 [Discriminator loss: 0.631358, acc.: 60.94%] [Generator loss: 0.868645]\n",
      "5764 [Discriminator loss: 0.677803, acc.: 58.59%] [Generator loss: 0.984418]\n",
      "5765 [Discriminator loss: 0.842398, acc.: 42.97%] [Generator loss: 0.882992]\n",
      "5766 [Discriminator loss: 0.599443, acc.: 66.41%] [Generator loss: 1.139105]\n",
      "5767 [Discriminator loss: 0.559414, acc.: 68.75%] [Generator loss: 0.948607]\n",
      "5768 [Discriminator loss: 0.610893, acc.: 64.06%] [Generator loss: 0.817391]\n",
      "5769 [Discriminator loss: 0.570573, acc.: 67.19%] [Generator loss: 1.014733]\n",
      "5770 [Discriminator loss: 0.607713, acc.: 71.88%] [Generator loss: 0.997872]\n",
      "5771 [Discriminator loss: 0.839008, acc.: 46.88%] [Generator loss: 1.547782]\n",
      "5772 [Discriminator loss: 0.715172, acc.: 57.81%] [Generator loss: 1.531141]\n",
      "5773 [Discriminator loss: 0.663796, acc.: 64.06%] [Generator loss: 0.954906]\n",
      "5774 [Discriminator loss: 0.826028, acc.: 48.44%] [Generator loss: 0.895615]\n",
      "5775 [Discriminator loss: 0.858111, acc.: 42.19%] [Generator loss: 1.048341]\n",
      "5776 [Discriminator loss: 0.761909, acc.: 53.12%] [Generator loss: 1.115926]\n",
      "5777 [Discriminator loss: 0.746520, acc.: 53.12%] [Generator loss: 0.963695]\n",
      "5778 [Discriminator loss: 0.741972, acc.: 55.47%] [Generator loss: 1.012116]\n",
      "5779 [Discriminator loss: 0.646309, acc.: 61.72%] [Generator loss: 1.043445]\n",
      "5780 [Discriminator loss: 0.696633, acc.: 55.47%] [Generator loss: 1.038316]\n",
      "5781 [Discriminator loss: 0.576604, acc.: 73.44%] [Generator loss: 1.011690]\n",
      "5782 [Discriminator loss: 0.621339, acc.: 67.19%] [Generator loss: 0.997682]\n",
      "5783 [Discriminator loss: 0.625531, acc.: 66.41%] [Generator loss: 1.045453]\n",
      "5784 [Discriminator loss: 0.589685, acc.: 75.00%] [Generator loss: 1.202404]\n",
      "5785 [Discriminator loss: 0.581785, acc.: 68.75%] [Generator loss: 1.003296]\n",
      "5786 [Discriminator loss: 0.653534, acc.: 60.94%] [Generator loss: 1.014338]\n",
      "5787 [Discriminator loss: 0.658694, acc.: 64.06%] [Generator loss: 1.288513]\n",
      "5788 [Discriminator loss: 0.503942, acc.: 84.38%] [Generator loss: 1.095440]\n",
      "5789 [Discriminator loss: 0.612648, acc.: 67.19%] [Generator loss: 1.278830]\n",
      "5790 [Discriminator loss: 0.606760, acc.: 66.41%] [Generator loss: 1.201835]\n",
      "5791 [Discriminator loss: 0.677791, acc.: 58.59%] [Generator loss: 0.993845]\n",
      "5792 [Discriminator loss: 0.681615, acc.: 58.59%] [Generator loss: 0.993259]\n",
      "5793 [Discriminator loss: 0.844643, acc.: 43.75%] [Generator loss: 1.041579]\n",
      "5794 [Discriminator loss: 0.720981, acc.: 56.25%] [Generator loss: 1.255042]\n",
      "5795 [Discriminator loss: 0.818486, acc.: 46.09%] [Generator loss: 1.210572]\n",
      "5796 [Discriminator loss: 0.861242, acc.: 41.41%] [Generator loss: 0.916249]\n",
      "5797 [Discriminator loss: 0.947164, acc.: 32.03%] [Generator loss: 0.718869]\n",
      "5798 [Discriminator loss: 0.708108, acc.: 55.47%] [Generator loss: 0.814105]\n",
      "5799 [Discriminator loss: 0.620190, acc.: 67.19%] [Generator loss: 0.984419]\n",
      "5800 [Discriminator loss: 0.525180, acc.: 75.78%] [Generator loss: 0.985851]\n",
      "5801 [Discriminator loss: 0.705838, acc.: 52.34%] [Generator loss: 0.926893]\n",
      "5802 [Discriminator loss: 0.600132, acc.: 64.06%] [Generator loss: 0.976125]\n",
      "5803 [Discriminator loss: 0.567067, acc.: 75.78%] [Generator loss: 0.835408]\n",
      "5804 [Discriminator loss: 0.652871, acc.: 56.25%] [Generator loss: 1.059880]\n",
      "5805 [Discriminator loss: 0.570560, acc.: 75.00%] [Generator loss: 1.086707]\n",
      "5806 [Discriminator loss: 0.571296, acc.: 67.19%] [Generator loss: 1.004819]\n",
      "5807 [Discriminator loss: 0.585870, acc.: 74.22%] [Generator loss: 1.197163]\n",
      "5808 [Discriminator loss: 0.583112, acc.: 71.88%] [Generator loss: 1.033361]\n",
      "5809 [Discriminator loss: 0.819250, acc.: 45.31%] [Generator loss: 1.271275]\n",
      "5810 [Discriminator loss: 0.664028, acc.: 58.59%] [Generator loss: 1.025578]\n",
      "5811 [Discriminator loss: 0.778936, acc.: 52.34%] [Generator loss: 1.031211]\n",
      "5812 [Discriminator loss: 0.850466, acc.: 41.41%] [Generator loss: 0.927072]\n",
      "5813 [Discriminator loss: 0.760832, acc.: 48.44%] [Generator loss: 1.043578]\n",
      "5814 [Discriminator loss: 0.711665, acc.: 57.81%] [Generator loss: 1.157628]\n",
      "5815 [Discriminator loss: 0.616740, acc.: 70.31%] [Generator loss: 1.147851]\n",
      "5816 [Discriminator loss: 0.610068, acc.: 69.53%] [Generator loss: 1.016255]\n",
      "5817 [Discriminator loss: 0.656884, acc.: 62.50%] [Generator loss: 1.146503]\n",
      "5818 [Discriminator loss: 0.576439, acc.: 67.97%] [Generator loss: 1.015165]\n",
      "5819 [Discriminator loss: 0.663692, acc.: 63.28%] [Generator loss: 1.289097]\n",
      "5820 [Discriminator loss: 0.632404, acc.: 58.59%] [Generator loss: 0.984972]\n",
      "5821 [Discriminator loss: 0.570075, acc.: 65.62%] [Generator loss: 0.924765]\n",
      "5822 [Discriminator loss: 0.472815, acc.: 81.25%] [Generator loss: 0.818758]\n",
      "5823 [Discriminator loss: 0.593846, acc.: 68.75%] [Generator loss: 0.721480]\n",
      "5824 [Discriminator loss: 0.600948, acc.: 65.62%] [Generator loss: 0.835203]\n",
      "5825 [Discriminator loss: 0.516702, acc.: 77.34%] [Generator loss: 0.960209]\n",
      "5826 [Discriminator loss: 0.659717, acc.: 59.38%] [Generator loss: 0.656894]\n",
      "5827 [Discriminator loss: 0.937519, acc.: 42.97%] [Generator loss: 0.988106]\n",
      "5828 [Discriminator loss: 0.638843, acc.: 64.06%] [Generator loss: 1.070207]\n",
      "5829 [Discriminator loss: 0.931141, acc.: 37.50%] [Generator loss: 1.187109]\n",
      "5830 [Discriminator loss: 0.756404, acc.: 56.25%] [Generator loss: 0.794718]\n",
      "5831 [Discriminator loss: 0.801000, acc.: 40.62%] [Generator loss: 0.736404]\n",
      "5832 [Discriminator loss: 0.777846, acc.: 50.78%] [Generator loss: 0.778272]\n",
      "5833 [Discriminator loss: 0.684102, acc.: 52.34%] [Generator loss: 0.825753]\n",
      "5834 [Discriminator loss: 0.673933, acc.: 59.38%] [Generator loss: 0.874274]\n",
      "5835 [Discriminator loss: 0.712067, acc.: 53.12%] [Generator loss: 1.012946]\n",
      "5836 [Discriminator loss: 0.659155, acc.: 64.06%] [Generator loss: 1.002177]\n",
      "5837 [Discriminator loss: 0.590621, acc.: 67.19%] [Generator loss: 1.045123]\n",
      "5838 [Discriminator loss: 0.543934, acc.: 75.78%] [Generator loss: 0.879468]\n",
      "5839 [Discriminator loss: 0.723719, acc.: 57.81%] [Generator loss: 0.942691]\n",
      "5840 [Discriminator loss: 0.584280, acc.: 71.09%] [Generator loss: 0.893548]\n",
      "5841 [Discriminator loss: 0.567870, acc.: 75.00%] [Generator loss: 0.841006]\n",
      "5842 [Discriminator loss: 0.695508, acc.: 57.81%] [Generator loss: 0.930326]\n",
      "5843 [Discriminator loss: 0.773311, acc.: 49.22%] [Generator loss: 0.867519]\n",
      "5844 [Discriminator loss: 0.665949, acc.: 67.19%] [Generator loss: 1.015028]\n",
      "5845 [Discriminator loss: 0.640717, acc.: 64.84%] [Generator loss: 0.958163]\n",
      "5846 [Discriminator loss: 0.725558, acc.: 53.91%] [Generator loss: 1.131288]\n",
      "5847 [Discriminator loss: 0.528041, acc.: 75.78%] [Generator loss: 0.958389]\n",
      "5848 [Discriminator loss: 0.720014, acc.: 55.47%] [Generator loss: 1.293967]\n",
      "5849 [Discriminator loss: 0.689167, acc.: 61.72%] [Generator loss: 0.966253]\n",
      "5850 [Discriminator loss: 0.748380, acc.: 50.78%] [Generator loss: 0.964613]\n",
      "5851 [Discriminator loss: 0.555438, acc.: 72.66%] [Generator loss: 1.217789]\n",
      "5852 [Discriminator loss: 0.654526, acc.: 60.94%] [Generator loss: 1.146792]\n",
      "5853 [Discriminator loss: 0.686251, acc.: 64.06%] [Generator loss: 1.016661]\n",
      "5854 [Discriminator loss: 0.580239, acc.: 69.53%] [Generator loss: 0.995554]\n",
      "5855 [Discriminator loss: 0.682726, acc.: 62.50%] [Generator loss: 0.853449]\n",
      "5856 [Discriminator loss: 0.621265, acc.: 64.84%] [Generator loss: 0.936674]\n",
      "5857 [Discriminator loss: 0.717422, acc.: 54.69%] [Generator loss: 1.091558]\n",
      "5858 [Discriminator loss: 0.639918, acc.: 68.75%] [Generator loss: 0.909388]\n",
      "5859 [Discriminator loss: 0.721247, acc.: 53.91%] [Generator loss: 0.769283]\n",
      "5860 [Discriminator loss: 0.555901, acc.: 73.44%] [Generator loss: 0.758059]\n",
      "5861 [Discriminator loss: 0.754998, acc.: 51.56%] [Generator loss: 0.921593]\n",
      "5862 [Discriminator loss: 0.568444, acc.: 74.22%] [Generator loss: 1.148738]\n",
      "5863 [Discriminator loss: 0.633838, acc.: 63.28%] [Generator loss: 1.041442]\n",
      "5864 [Discriminator loss: 0.659160, acc.: 66.41%] [Generator loss: 1.047847]\n",
      "5865 [Discriminator loss: 0.750294, acc.: 56.25%] [Generator loss: 0.811211]\n",
      "5866 [Discriminator loss: 0.586896, acc.: 71.09%] [Generator loss: 0.916199]\n",
      "5867 [Discriminator loss: 0.694506, acc.: 57.81%] [Generator loss: 1.120076]\n",
      "5868 [Discriminator loss: 0.550049, acc.: 74.22%] [Generator loss: 1.207951]\n",
      "5869 [Discriminator loss: 0.801461, acc.: 42.19%] [Generator loss: 0.947726]\n",
      "5870 [Discriminator loss: 0.516353, acc.: 76.56%] [Generator loss: 1.000484]\n",
      "5871 [Discriminator loss: 0.667250, acc.: 57.81%] [Generator loss: 0.926671]\n",
      "5872 [Discriminator loss: 0.838287, acc.: 44.53%] [Generator loss: 1.079944]\n",
      "5873 [Discriminator loss: 0.667500, acc.: 60.16%] [Generator loss: 0.978758]\n",
      "5874 [Discriminator loss: 0.662933, acc.: 60.94%] [Generator loss: 1.118311]\n",
      "5875 [Discriminator loss: 0.492851, acc.: 78.91%] [Generator loss: 1.051073]\n",
      "5876 [Discriminator loss: 0.485901, acc.: 81.25%] [Generator loss: 0.963534]\n",
      "5877 [Discriminator loss: 0.672778, acc.: 63.28%] [Generator loss: 0.867022]\n",
      "5878 [Discriminator loss: 0.519308, acc.: 76.56%] [Generator loss: 0.880234]\n",
      "5879 [Discriminator loss: 0.559725, acc.: 73.44%] [Generator loss: 0.919169]\n",
      "5880 [Discriminator loss: 0.718436, acc.: 55.47%] [Generator loss: 1.043890]\n",
      "5881 [Discriminator loss: 0.743608, acc.: 53.91%] [Generator loss: 1.213423]\n",
      "5882 [Discriminator loss: 0.600769, acc.: 66.41%] [Generator loss: 1.195373]\n",
      "5883 [Discriminator loss: 0.666040, acc.: 63.28%] [Generator loss: 1.190859]\n",
      "5884 [Discriminator loss: 0.679387, acc.: 64.84%] [Generator loss: 0.931258]\n",
      "5885 [Discriminator loss: 0.658855, acc.: 60.94%] [Generator loss: 0.961320]\n",
      "5886 [Discriminator loss: 0.651752, acc.: 65.62%] [Generator loss: 1.069055]\n",
      "5887 [Discriminator loss: 0.619109, acc.: 67.97%] [Generator loss: 1.157359]\n",
      "5888 [Discriminator loss: 0.700089, acc.: 60.94%] [Generator loss: 1.129980]\n",
      "5889 [Discriminator loss: 0.539188, acc.: 74.22%] [Generator loss: 1.143975]\n",
      "5890 [Discriminator loss: 0.724642, acc.: 57.81%] [Generator loss: 0.867074]\n",
      "5891 [Discriminator loss: 0.548226, acc.: 77.34%] [Generator loss: 1.036998]\n",
      "5892 [Discriminator loss: 0.574209, acc.: 71.09%] [Generator loss: 1.140327]\n",
      "5893 [Discriminator loss: 0.541935, acc.: 73.44%] [Generator loss: 1.050424]\n",
      "5894 [Discriminator loss: 0.652088, acc.: 64.06%] [Generator loss: 1.214474]\n",
      "5895 [Discriminator loss: 0.748517, acc.: 55.47%] [Generator loss: 1.262535]\n",
      "5896 [Discriminator loss: 0.536936, acc.: 72.66%] [Generator loss: 0.814770]\n",
      "5897 [Discriminator loss: 0.599634, acc.: 67.19%] [Generator loss: 1.116247]\n",
      "5898 [Discriminator loss: 0.626073, acc.: 76.56%] [Generator loss: 1.021939]\n",
      "5899 [Discriminator loss: 0.697604, acc.: 60.94%] [Generator loss: 0.922587]\n",
      "5900 [Discriminator loss: 0.649307, acc.: 58.59%] [Generator loss: 1.139264]\n",
      "5901 [Discriminator loss: 0.813895, acc.: 50.78%] [Generator loss: 1.493534]\n",
      "5902 [Discriminator loss: 1.037859, acc.: 39.06%] [Generator loss: 1.169447]\n",
      "5903 [Discriminator loss: 0.761895, acc.: 58.59%] [Generator loss: 0.970773]\n",
      "5904 [Discriminator loss: 0.685461, acc.: 57.03%] [Generator loss: 0.886154]\n",
      "5905 [Discriminator loss: 0.614141, acc.: 62.50%] [Generator loss: 1.042016]\n",
      "5906 [Discriminator loss: 0.474322, acc.: 78.12%] [Generator loss: 0.738486]\n",
      "5907 [Discriminator loss: 0.501537, acc.: 76.56%] [Generator loss: 0.612338]\n",
      "5908 [Discriminator loss: 0.657992, acc.: 61.72%] [Generator loss: 0.743647]\n",
      "5909 [Discriminator loss: 0.368216, acc.: 89.84%] [Generator loss: 1.046786]\n",
      "5910 [Discriminator loss: 0.810400, acc.: 52.34%] [Generator loss: 1.014449]\n",
      "5911 [Discriminator loss: 0.453005, acc.: 82.81%] [Generator loss: 1.301626]\n",
      "5912 [Discriminator loss: 0.717903, acc.: 57.81%] [Generator loss: 0.820515]\n",
      "5913 [Discriminator loss: 0.665925, acc.: 67.19%] [Generator loss: 0.685787]\n",
      "5914 [Discriminator loss: 1.002201, acc.: 39.06%] [Generator loss: 0.693492]\n",
      "5915 [Discriminator loss: 0.896934, acc.: 47.66%] [Generator loss: 1.255413]\n",
      "5916 [Discriminator loss: 0.623726, acc.: 67.97%] [Generator loss: 1.246392]\n",
      "5917 [Discriminator loss: 0.673750, acc.: 63.28%] [Generator loss: 1.026578]\n",
      "5918 [Discriminator loss: 0.802716, acc.: 46.88%] [Generator loss: 0.990395]\n",
      "5919 [Discriminator loss: 0.727455, acc.: 58.59%] [Generator loss: 0.979293]\n",
      "5920 [Discriminator loss: 0.670244, acc.: 60.94%] [Generator loss: 1.037866]\n",
      "5921 [Discriminator loss: 0.607841, acc.: 66.41%] [Generator loss: 1.150723]\n",
      "5922 [Discriminator loss: 0.717540, acc.: 55.47%] [Generator loss: 1.110549]\n",
      "5923 [Discriminator loss: 0.619268, acc.: 62.50%] [Generator loss: 1.011813]\n",
      "5924 [Discriminator loss: 0.648782, acc.: 57.81%] [Generator loss: 0.861729]\n",
      "5925 [Discriminator loss: 0.628887, acc.: 65.62%] [Generator loss: 1.166616]\n",
      "5926 [Discriminator loss: 0.548056, acc.: 73.44%] [Generator loss: 1.347452]\n",
      "5927 [Discriminator loss: 0.628927, acc.: 67.19%] [Generator loss: 1.306229]\n",
      "5928 [Discriminator loss: 0.671650, acc.: 59.38%] [Generator loss: 1.101757]\n",
      "5929 [Discriminator loss: 0.715429, acc.: 59.38%] [Generator loss: 0.963351]\n",
      "5930 [Discriminator loss: 0.602577, acc.: 66.41%] [Generator loss: 0.896128]\n",
      "5931 [Discriminator loss: 0.566174, acc.: 69.53%] [Generator loss: 1.054403]\n",
      "5932 [Discriminator loss: 0.690004, acc.: 50.00%] [Generator loss: 0.907388]\n",
      "5933 [Discriminator loss: 0.795197, acc.: 49.22%] [Generator loss: 0.991876]\n",
      "5934 [Discriminator loss: 0.684527, acc.: 57.03%] [Generator loss: 0.980187]\n",
      "5935 [Discriminator loss: 0.572257, acc.: 70.31%] [Generator loss: 0.866276]\n",
      "5936 [Discriminator loss: 0.764516, acc.: 56.25%] [Generator loss: 0.910594]\n",
      "5937 [Discriminator loss: 0.740948, acc.: 50.78%] [Generator loss: 0.757066]\n",
      "5938 [Discriminator loss: 0.824766, acc.: 50.00%] [Generator loss: 1.005660]\n",
      "5939 [Discriminator loss: 0.877450, acc.: 46.09%] [Generator loss: 1.004660]\n",
      "5940 [Discriminator loss: 0.728350, acc.: 58.59%] [Generator loss: 0.778199]\n",
      "5941 [Discriminator loss: 0.798791, acc.: 53.12%] [Generator loss: 0.966189]\n",
      "5942 [Discriminator loss: 0.650705, acc.: 59.38%] [Generator loss: 1.366515]\n",
      "5943 [Discriminator loss: 0.638093, acc.: 60.16%] [Generator loss: 1.274559]\n",
      "5944 [Discriminator loss: 0.845979, acc.: 38.28%] [Generator loss: 1.538527]\n",
      "5945 [Discriminator loss: 0.697349, acc.: 54.69%] [Generator loss: 1.216456]\n",
      "5946 [Discriminator loss: 0.768968, acc.: 51.56%] [Generator loss: 0.990586]\n",
      "5947 [Discriminator loss: 0.534382, acc.: 75.00%] [Generator loss: 0.888601]\n",
      "5948 [Discriminator loss: 0.735294, acc.: 56.25%] [Generator loss: 0.950994]\n",
      "5949 [Discriminator loss: 0.665832, acc.: 57.81%] [Generator loss: 1.049622]\n",
      "5950 [Discriminator loss: 0.437913, acc.: 88.28%] [Generator loss: 1.187183]\n",
      "5951 [Discriminator loss: 0.549653, acc.: 70.31%] [Generator loss: 1.238357]\n",
      "5952 [Discriminator loss: 0.615542, acc.: 69.53%] [Generator loss: 1.063094]\n",
      "5953 [Discriminator loss: 0.588513, acc.: 68.75%] [Generator loss: 1.007744]\n",
      "5954 [Discriminator loss: 0.396874, acc.: 85.94%] [Generator loss: 1.140063]\n",
      "5955 [Discriminator loss: 0.650996, acc.: 60.94%] [Generator loss: 1.246836]\n",
      "5956 [Discriminator loss: 0.525211, acc.: 75.00%] [Generator loss: 1.228620]\n",
      "5957 [Discriminator loss: 0.506176, acc.: 80.47%] [Generator loss: 1.182854]\n",
      "5958 [Discriminator loss: 0.407171, acc.: 85.94%] [Generator loss: 1.158835]\n",
      "5959 [Discriminator loss: 0.748287, acc.: 55.47%] [Generator loss: 1.099454]\n",
      "5960 [Discriminator loss: 0.563788, acc.: 75.00%] [Generator loss: 1.085970]\n",
      "5961 [Discriminator loss: 0.691561, acc.: 60.16%] [Generator loss: 0.999706]\n",
      "5962 [Discriminator loss: 0.752214, acc.: 50.78%] [Generator loss: 0.887814]\n",
      "5963 [Discriminator loss: 0.827666, acc.: 43.75%] [Generator loss: 0.928056]\n",
      "5964 [Discriminator loss: 0.945712, acc.: 42.19%] [Generator loss: 0.859178]\n",
      "5965 [Discriminator loss: 0.765795, acc.: 54.69%] [Generator loss: 0.912133]\n",
      "5966 [Discriminator loss: 0.910221, acc.: 50.00%] [Generator loss: 0.646625]\n",
      "5967 [Discriminator loss: 0.975372, acc.: 35.94%] [Generator loss: 0.972296]\n",
      "5968 [Discriminator loss: 0.813499, acc.: 46.88%] [Generator loss: 1.228123]\n",
      "5969 [Discriminator loss: 0.767488, acc.: 56.25%] [Generator loss: 1.072466]\n",
      "5970 [Discriminator loss: 0.659454, acc.: 64.84%] [Generator loss: 1.024780]\n",
      "5971 [Discriminator loss: 0.705383, acc.: 57.81%] [Generator loss: 1.216773]\n",
      "5972 [Discriminator loss: 0.612822, acc.: 66.41%] [Generator loss: 1.225980]\n",
      "5973 [Discriminator loss: 0.756891, acc.: 56.25%] [Generator loss: 1.450224]\n",
      "5974 [Discriminator loss: 0.606343, acc.: 63.28%] [Generator loss: 1.212392]\n",
      "5975 [Discriminator loss: 0.705533, acc.: 49.22%] [Generator loss: 1.144719]\n",
      "5976 [Discriminator loss: 0.724115, acc.: 56.25%] [Generator loss: 1.209778]\n",
      "5977 [Discriminator loss: 0.709348, acc.: 55.47%] [Generator loss: 0.997561]\n",
      "5978 [Discriminator loss: 0.704507, acc.: 56.25%] [Generator loss: 0.873913]\n",
      "5979 [Discriminator loss: 0.768102, acc.: 48.44%] [Generator loss: 0.886158]\n",
      "5980 [Discriminator loss: 0.617590, acc.: 66.41%] [Generator loss: 0.971871]\n",
      "5981 [Discriminator loss: 0.737633, acc.: 49.22%] [Generator loss: 0.873657]\n",
      "5982 [Discriminator loss: 0.617982, acc.: 67.19%] [Generator loss: 1.077215]\n",
      "5983 [Discriminator loss: 0.639343, acc.: 64.06%] [Generator loss: 0.974136]\n",
      "5984 [Discriminator loss: 0.684611, acc.: 57.03%] [Generator loss: 1.025628]\n",
      "5985 [Discriminator loss: 0.664725, acc.: 58.59%] [Generator loss: 1.006985]\n",
      "5986 [Discriminator loss: 0.593780, acc.: 71.88%] [Generator loss: 0.876420]\n",
      "5987 [Discriminator loss: 0.694507, acc.: 58.59%] [Generator loss: 0.868716]\n",
      "5988 [Discriminator loss: 0.636253, acc.: 61.72%] [Generator loss: 1.088271]\n",
      "5989 [Discriminator loss: 0.577529, acc.: 73.44%] [Generator loss: 1.016764]\n",
      "5990 [Discriminator loss: 0.682724, acc.: 54.69%] [Generator loss: 0.886049]\n",
      "5991 [Discriminator loss: 0.583571, acc.: 71.88%] [Generator loss: 1.051152]\n",
      "5992 [Discriminator loss: 0.681319, acc.: 58.59%] [Generator loss: 1.207212]\n",
      "5993 [Discriminator loss: 0.557773, acc.: 71.09%] [Generator loss: 1.337338]\n",
      "5994 [Discriminator loss: 0.546391, acc.: 75.78%] [Generator loss: 0.928077]\n",
      "5995 [Discriminator loss: 0.668415, acc.: 60.16%] [Generator loss: 0.903932]\n",
      "5996 [Discriminator loss: 0.597199, acc.: 68.75%] [Generator loss: 0.843899]\n",
      "5997 [Discriminator loss: 0.768412, acc.: 50.78%] [Generator loss: 0.769597]\n",
      "5998 [Discriminator loss: 0.748988, acc.: 50.78%] [Generator loss: 0.908494]\n",
      "5999 [Discriminator loss: 0.669767, acc.: 64.06%] [Generator loss: 0.965613]\n",
      "6000 [Discriminator loss: 0.825811, acc.: 44.53%] [Generator loss: 0.919690]\n",
      "6001 [Discriminator loss: 0.650528, acc.: 57.81%] [Generator loss: 0.907690]\n",
      "6002 [Discriminator loss: 0.644025, acc.: 62.50%] [Generator loss: 1.114230]\n",
      "6003 [Discriminator loss: 0.513597, acc.: 76.56%] [Generator loss: 1.223462]\n",
      "6004 [Discriminator loss: 0.690775, acc.: 64.84%] [Generator loss: 1.119881]\n",
      "6005 [Discriminator loss: 0.593268, acc.: 70.31%] [Generator loss: 1.032948]\n",
      "6006 [Discriminator loss: 0.621522, acc.: 66.41%] [Generator loss: 1.045408]\n",
      "6007 [Discriminator loss: 0.618631, acc.: 66.41%] [Generator loss: 1.017667]\n",
      "6008 [Discriminator loss: 0.719874, acc.: 54.69%] [Generator loss: 1.045544]\n",
      "6009 [Discriminator loss: 0.709072, acc.: 53.12%] [Generator loss: 0.853994]\n",
      "6010 [Discriminator loss: 0.751428, acc.: 54.69%] [Generator loss: 1.122375]\n",
      "6011 [Discriminator loss: 0.568174, acc.: 75.00%] [Generator loss: 1.105480]\n",
      "6012 [Discriminator loss: 0.664513, acc.: 64.84%] [Generator loss: 1.152406]\n",
      "6013 [Discriminator loss: 0.611382, acc.: 67.19%] [Generator loss: 1.082065]\n",
      "6014 [Discriminator loss: 0.574474, acc.: 75.00%] [Generator loss: 0.978515]\n",
      "6015 [Discriminator loss: 0.523188, acc.: 73.44%] [Generator loss: 0.897590]\n",
      "6016 [Discriminator loss: 0.603150, acc.: 70.31%] [Generator loss: 0.921477]\n",
      "6017 [Discriminator loss: 0.679149, acc.: 57.81%] [Generator loss: 0.839739]\n",
      "6018 [Discriminator loss: 0.550712, acc.: 74.22%] [Generator loss: 0.725713]\n",
      "6019 [Discriminator loss: 0.709197, acc.: 60.94%] [Generator loss: 1.351751]\n",
      "6020 [Discriminator loss: 0.776966, acc.: 53.12%] [Generator loss: 1.150193]\n",
      "6021 [Discriminator loss: 0.715189, acc.: 63.28%] [Generator loss: 1.137113]\n",
      "6022 [Discriminator loss: 0.720336, acc.: 56.25%] [Generator loss: 1.177476]\n",
      "6023 [Discriminator loss: 0.646620, acc.: 64.06%] [Generator loss: 1.603771]\n",
      "6024 [Discriminator loss: 0.775760, acc.: 47.66%] [Generator loss: 1.068452]\n",
      "6025 [Discriminator loss: 0.691001, acc.: 60.94%] [Generator loss: 0.925313]\n",
      "6026 [Discriminator loss: 0.641795, acc.: 60.94%] [Generator loss: 0.918647]\n",
      "6027 [Discriminator loss: 0.590775, acc.: 73.44%] [Generator loss: 0.981197]\n",
      "6028 [Discriminator loss: 0.554727, acc.: 69.53%] [Generator loss: 0.952286]\n",
      "6029 [Discriminator loss: 0.669697, acc.: 57.81%] [Generator loss: 1.216389]\n",
      "6030 [Discriminator loss: 0.508392, acc.: 78.12%] [Generator loss: 1.235665]\n",
      "6031 [Discriminator loss: 0.574901, acc.: 70.31%] [Generator loss: 1.180389]\n",
      "6032 [Discriminator loss: 0.598221, acc.: 64.84%] [Generator loss: 1.039950]\n",
      "6033 [Discriminator loss: 0.627018, acc.: 63.28%] [Generator loss: 1.006049]\n",
      "6034 [Discriminator loss: 0.703181, acc.: 60.94%] [Generator loss: 1.060343]\n",
      "6035 [Discriminator loss: 0.477461, acc.: 78.91%] [Generator loss: 1.068440]\n",
      "6036 [Discriminator loss: 0.792593, acc.: 51.56%] [Generator loss: 1.122799]\n",
      "6037 [Discriminator loss: 0.600819, acc.: 69.53%] [Generator loss: 1.018292]\n",
      "6038 [Discriminator loss: 0.851658, acc.: 46.09%] [Generator loss: 1.110210]\n",
      "6039 [Discriminator loss: 0.535500, acc.: 71.88%] [Generator loss: 1.273351]\n",
      "6040 [Discriminator loss: 0.624213, acc.: 69.53%] [Generator loss: 1.291835]\n",
      "6041 [Discriminator loss: 0.834197, acc.: 39.06%] [Generator loss: 1.265218]\n",
      "6042 [Discriminator loss: 0.862864, acc.: 42.19%] [Generator loss: 1.220132]\n",
      "6043 [Discriminator loss: 0.548157, acc.: 75.00%] [Generator loss: 1.385281]\n",
      "6044 [Discriminator loss: 0.713001, acc.: 55.47%] [Generator loss: 1.213918]\n",
      "6045 [Discriminator loss: 0.662010, acc.: 60.16%] [Generator loss: 1.144473]\n",
      "6046 [Discriminator loss: 0.661148, acc.: 60.16%] [Generator loss: 0.963820]\n",
      "6047 [Discriminator loss: 0.721962, acc.: 60.16%] [Generator loss: 1.333525]\n",
      "6048 [Discriminator loss: 0.701129, acc.: 58.59%] [Generator loss: 1.061204]\n",
      "6049 [Discriminator loss: 0.697863, acc.: 56.25%] [Generator loss: 1.127676]\n",
      "6050 [Discriminator loss: 0.501014, acc.: 80.47%] [Generator loss: 0.996549]\n",
      "6051 [Discriminator loss: 0.440566, acc.: 85.94%] [Generator loss: 0.938904]\n",
      "6052 [Discriminator loss: 0.700525, acc.: 55.47%] [Generator loss: 1.148391]\n",
      "6053 [Discriminator loss: 0.428177, acc.: 89.06%] [Generator loss: 1.115138]\n",
      "6054 [Discriminator loss: 0.487717, acc.: 84.38%] [Generator loss: 0.976018]\n",
      "6055 [Discriminator loss: 0.463862, acc.: 85.94%] [Generator loss: 0.977672]\n",
      "6056 [Discriminator loss: 0.397776, acc.: 89.06%] [Generator loss: 0.858762]\n",
      "6057 [Discriminator loss: 0.375804, acc.: 89.06%] [Generator loss: 0.866151]\n",
      "6058 [Discriminator loss: 0.471771, acc.: 81.25%] [Generator loss: 0.750730]\n",
      "6059 [Discriminator loss: 0.904783, acc.: 47.66%] [Generator loss: 1.473628]\n",
      "6060 [Discriminator loss: 0.655472, acc.: 66.41%] [Generator loss: 1.509879]\n",
      "6061 [Discriminator loss: 0.599148, acc.: 67.19%] [Generator loss: 1.201266]\n",
      "6062 [Discriminator loss: 0.851909, acc.: 48.44%] [Generator loss: 0.869785]\n",
      "6063 [Discriminator loss: 0.507170, acc.: 76.56%] [Generator loss: 0.812839]\n",
      "6064 [Discriminator loss: 0.703091, acc.: 58.59%] [Generator loss: 1.018981]\n",
      "6065 [Discriminator loss: 0.671403, acc.: 60.16%] [Generator loss: 0.986707]\n",
      "6066 [Discriminator loss: 0.606827, acc.: 67.19%] [Generator loss: 1.389830]\n",
      "6067 [Discriminator loss: 0.515516, acc.: 75.00%] [Generator loss: 1.258839]\n",
      "6068 [Discriminator loss: 0.562177, acc.: 74.22%] [Generator loss: 1.020769]\n",
      "6069 [Discriminator loss: 0.613388, acc.: 65.62%] [Generator loss: 0.753360]\n",
      "6070 [Discriminator loss: 0.542332, acc.: 72.66%] [Generator loss: 0.706189]\n",
      "6071 [Discriminator loss: 0.469844, acc.: 76.56%] [Generator loss: 0.849229]\n",
      "6072 [Discriminator loss: 0.555504, acc.: 71.09%] [Generator loss: 0.838270]\n",
      "6073 [Discriminator loss: 0.701778, acc.: 54.69%] [Generator loss: 0.980107]\n",
      "6074 [Discriminator loss: 0.641748, acc.: 64.84%] [Generator loss: 0.754900]\n",
      "6075 [Discriminator loss: 0.776630, acc.: 53.91%] [Generator loss: 1.243784]\n",
      "6076 [Discriminator loss: 0.589759, acc.: 65.62%] [Generator loss: 1.270404]\n",
      "6077 [Discriminator loss: 0.836198, acc.: 42.19%] [Generator loss: 1.015935]\n",
      "6078 [Discriminator loss: 0.804600, acc.: 53.91%] [Generator loss: 1.171260]\n",
      "6079 [Discriminator loss: 0.653807, acc.: 60.16%] [Generator loss: 1.176338]\n",
      "6080 [Discriminator loss: 0.728788, acc.: 48.44%] [Generator loss: 1.173048]\n",
      "6081 [Discriminator loss: 0.632107, acc.: 64.84%] [Generator loss: 1.313169]\n",
      "6082 [Discriminator loss: 0.731557, acc.: 53.12%] [Generator loss: 1.026805]\n",
      "6083 [Discriminator loss: 0.634714, acc.: 63.28%] [Generator loss: 1.036591]\n",
      "6084 [Discriminator loss: 0.585573, acc.: 70.31%] [Generator loss: 1.151714]\n",
      "6085 [Discriminator loss: 0.632438, acc.: 66.41%] [Generator loss: 1.274674]\n",
      "6086 [Discriminator loss: 0.678209, acc.: 60.94%] [Generator loss: 1.037498]\n",
      "6087 [Discriminator loss: 0.719865, acc.: 59.38%] [Generator loss: 1.111587]\n",
      "6088 [Discriminator loss: 0.650158, acc.: 62.50%] [Generator loss: 1.358118]\n",
      "6089 [Discriminator loss: 0.785930, acc.: 54.69%] [Generator loss: 0.773057]\n",
      "6090 [Discriminator loss: 0.765507, acc.: 47.66%] [Generator loss: 0.745851]\n",
      "6091 [Discriminator loss: 0.685482, acc.: 57.81%] [Generator loss: 0.876928]\n",
      "6092 [Discriminator loss: 0.833015, acc.: 45.31%] [Generator loss: 0.947389]\n",
      "6093 [Discriminator loss: 0.789696, acc.: 50.78%] [Generator loss: 1.065661]\n",
      "6094 [Discriminator loss: 0.873168, acc.: 43.75%] [Generator loss: 1.129256]\n",
      "6095 [Discriminator loss: 0.745694, acc.: 57.03%] [Generator loss: 1.256368]\n",
      "6096 [Discriminator loss: 0.593651, acc.: 71.88%] [Generator loss: 1.301940]\n",
      "6097 [Discriminator loss: 0.761432, acc.: 50.00%] [Generator loss: 1.064211]\n",
      "6098 [Discriminator loss: 0.703403, acc.: 57.81%] [Generator loss: 1.201924]\n",
      "6099 [Discriminator loss: 0.480208, acc.: 77.34%] [Generator loss: 1.012264]\n",
      "6100 [Discriminator loss: 0.655029, acc.: 61.72%] [Generator loss: 1.015409]\n",
      "6101 [Discriminator loss: 0.642222, acc.: 66.41%] [Generator loss: 1.090731]\n",
      "6102 [Discriminator loss: 0.692798, acc.: 61.72%] [Generator loss: 1.047927]\n",
      "6103 [Discriminator loss: 0.722523, acc.: 50.78%] [Generator loss: 0.890472]\n",
      "6104 [Discriminator loss: 0.595653, acc.: 64.06%] [Generator loss: 0.893751]\n",
      "6105 [Discriminator loss: 0.566755, acc.: 78.91%] [Generator loss: 0.965732]\n",
      "6106 [Discriminator loss: 0.648540, acc.: 66.41%] [Generator loss: 0.909643]\n",
      "6107 [Discriminator loss: 0.410070, acc.: 83.59%] [Generator loss: 0.883421]\n",
      "6108 [Discriminator loss: 0.660935, acc.: 59.38%] [Generator loss: 0.799142]\n",
      "6109 [Discriminator loss: 0.795100, acc.: 47.66%] [Generator loss: 0.942545]\n",
      "6110 [Discriminator loss: 0.641979, acc.: 65.62%] [Generator loss: 0.860691]\n",
      "6111 [Discriminator loss: 0.659245, acc.: 65.62%] [Generator loss: 0.823114]\n",
      "6112 [Discriminator loss: 0.762967, acc.: 52.34%] [Generator loss: 0.954983]\n",
      "6113 [Discriminator loss: 0.650267, acc.: 62.50%] [Generator loss: 1.057334]\n",
      "6114 [Discriminator loss: 0.580183, acc.: 69.53%] [Generator loss: 1.054591]\n",
      "6115 [Discriminator loss: 0.521184, acc.: 76.56%] [Generator loss: 0.887788]\n",
      "6116 [Discriminator loss: 0.610264, acc.: 63.28%] [Generator loss: 0.791458]\n",
      "6117 [Discriminator loss: 0.498632, acc.: 80.47%] [Generator loss: 0.886603]\n",
      "6118 [Discriminator loss: 0.685607, acc.: 55.47%] [Generator loss: 0.997598]\n",
      "6119 [Discriminator loss: 0.747503, acc.: 52.34%] [Generator loss: 1.248910]\n",
      "6120 [Discriminator loss: 1.095386, acc.: 38.28%] [Generator loss: 1.028750]\n",
      "6121 [Discriminator loss: 0.587657, acc.: 69.53%] [Generator loss: 1.181690]\n",
      "6122 [Discriminator loss: 0.720678, acc.: 53.91%] [Generator loss: 1.011185]\n",
      "6123 [Discriminator loss: 0.799904, acc.: 50.00%] [Generator loss: 1.216060]\n",
      "6124 [Discriminator loss: 0.852955, acc.: 42.97%] [Generator loss: 1.002160]\n",
      "6125 [Discriminator loss: 0.759751, acc.: 53.91%] [Generator loss: 0.956200]\n",
      "6126 [Discriminator loss: 0.562373, acc.: 72.66%] [Generator loss: 1.408594]\n",
      "6127 [Discriminator loss: 0.518875, acc.: 75.00%] [Generator loss: 0.838016]\n",
      "6128 [Discriminator loss: 0.562555, acc.: 70.31%] [Generator loss: 0.877095]\n",
      "6129 [Discriminator loss: 0.595255, acc.: 67.97%] [Generator loss: 0.983526]\n",
      "6130 [Discriminator loss: 0.531737, acc.: 74.22%] [Generator loss: 1.064263]\n",
      "6131 [Discriminator loss: 0.597421, acc.: 65.62%] [Generator loss: 1.051521]\n",
      "6132 [Discriminator loss: 0.726172, acc.: 54.69%] [Generator loss: 0.863335]\n",
      "6133 [Discriminator loss: 0.965470, acc.: 39.84%] [Generator loss: 0.795329]\n",
      "6134 [Discriminator loss: 0.674962, acc.: 59.38%] [Generator loss: 0.771044]\n",
      "6135 [Discriminator loss: 0.703866, acc.: 56.25%] [Generator loss: 0.931559]\n",
      "6136 [Discriminator loss: 0.670847, acc.: 54.69%] [Generator loss: 1.048923]\n",
      "6137 [Discriminator loss: 0.696399, acc.: 62.50%] [Generator loss: 0.960523]\n",
      "6138 [Discriminator loss: 0.728192, acc.: 51.56%] [Generator loss: 0.919378]\n",
      "6139 [Discriminator loss: 0.624774, acc.: 70.31%] [Generator loss: 1.223512]\n",
      "6140 [Discriminator loss: 0.682397, acc.: 59.38%] [Generator loss: 1.292119]\n",
      "6141 [Discriminator loss: 0.479607, acc.: 84.38%] [Generator loss: 1.218959]\n",
      "6142 [Discriminator loss: 0.506574, acc.: 82.81%] [Generator loss: 1.154194]\n",
      "6143 [Discriminator loss: 0.656537, acc.: 62.50%] [Generator loss: 0.955365]\n",
      "6144 [Discriminator loss: 0.576961, acc.: 74.22%] [Generator loss: 1.055433]\n",
      "6145 [Discriminator loss: 0.657182, acc.: 62.50%] [Generator loss: 0.945104]\n",
      "6146 [Discriminator loss: 0.562747, acc.: 75.78%] [Generator loss: 1.058317]\n",
      "6147 [Discriminator loss: 0.684561, acc.: 60.16%] [Generator loss: 0.903871]\n",
      "6148 [Discriminator loss: 0.536187, acc.: 73.44%] [Generator loss: 0.993267]\n",
      "6149 [Discriminator loss: 0.630266, acc.: 68.75%] [Generator loss: 1.210227]\n",
      "6150 [Discriminator loss: 0.649332, acc.: 66.41%] [Generator loss: 1.123065]\n",
      "6151 [Discriminator loss: 0.601439, acc.: 67.97%] [Generator loss: 0.941639]\n",
      "6152 [Discriminator loss: 0.687556, acc.: 56.25%] [Generator loss: 1.004082]\n",
      "6153 [Discriminator loss: 0.520098, acc.: 75.00%] [Generator loss: 0.894231]\n",
      "6154 [Discriminator loss: 0.700994, acc.: 60.94%] [Generator loss: 1.294061]\n",
      "6155 [Discriminator loss: 0.656074, acc.: 63.28%] [Generator loss: 1.375236]\n",
      "6156 [Discriminator loss: 0.741499, acc.: 54.69%] [Generator loss: 1.296014]\n",
      "6157 [Discriminator loss: 0.754185, acc.: 55.47%] [Generator loss: 1.266026]\n",
      "6158 [Discriminator loss: 0.735695, acc.: 58.59%] [Generator loss: 1.192157]\n",
      "6159 [Discriminator loss: 0.660613, acc.: 58.59%] [Generator loss: 1.059105]\n",
      "6160 [Discriminator loss: 0.847538, acc.: 49.22%] [Generator loss: 1.323622]\n",
      "6161 [Discriminator loss: 0.714230, acc.: 53.12%] [Generator loss: 1.235009]\n",
      "6162 [Discriminator loss: 0.808148, acc.: 46.88%] [Generator loss: 1.096190]\n",
      "6163 [Discriminator loss: 0.679471, acc.: 63.28%] [Generator loss: 0.854468]\n",
      "6164 [Discriminator loss: 0.696366, acc.: 53.12%] [Generator loss: 1.118946]\n",
      "6165 [Discriminator loss: 0.655370, acc.: 60.16%] [Generator loss: 1.016397]\n",
      "6166 [Discriminator loss: 0.560809, acc.: 71.88%] [Generator loss: 0.934641]\n",
      "6167 [Discriminator loss: 0.718072, acc.: 51.56%] [Generator loss: 1.011532]\n",
      "6168 [Discriminator loss: 0.721939, acc.: 54.69%] [Generator loss: 0.770616]\n",
      "6169 [Discriminator loss: 0.624591, acc.: 64.84%] [Generator loss: 0.986745]\n",
      "6170 [Discriminator loss: 0.607606, acc.: 67.19%] [Generator loss: 1.024174]\n",
      "6171 [Discriminator loss: 0.751911, acc.: 53.91%] [Generator loss: 1.034972]\n",
      "6172 [Discriminator loss: 0.561710, acc.: 71.09%] [Generator loss: 1.133741]\n",
      "6173 [Discriminator loss: 0.671155, acc.: 61.72%] [Generator loss: 0.882661]\n",
      "6174 [Discriminator loss: 0.670359, acc.: 62.50%] [Generator loss: 0.840819]\n",
      "6175 [Discriminator loss: 0.611763, acc.: 67.97%] [Generator loss: 0.899626]\n",
      "6176 [Discriminator loss: 0.490553, acc.: 74.22%] [Generator loss: 0.986731]\n",
      "6177 [Discriminator loss: 0.486330, acc.: 77.34%] [Generator loss: 1.213864]\n",
      "6178 [Discriminator loss: 0.788315, acc.: 52.34%] [Generator loss: 0.918633]\n",
      "6179 [Discriminator loss: 0.671603, acc.: 64.84%] [Generator loss: 1.065114]\n",
      "6180 [Discriminator loss: 0.605925, acc.: 69.53%] [Generator loss: 1.174225]\n",
      "6181 [Discriminator loss: 0.568567, acc.: 71.09%] [Generator loss: 1.009959]\n",
      "6182 [Discriminator loss: 0.562656, acc.: 69.53%] [Generator loss: 1.222873]\n",
      "6183 [Discriminator loss: 0.557465, acc.: 73.44%] [Generator loss: 0.956104]\n",
      "6184 [Discriminator loss: 0.762112, acc.: 56.25%] [Generator loss: 0.750222]\n",
      "6185 [Discriminator loss: 0.547219, acc.: 72.66%] [Generator loss: 0.836033]\n",
      "6186 [Discriminator loss: 0.720189, acc.: 58.59%] [Generator loss: 1.029050]\n",
      "6187 [Discriminator loss: 0.666729, acc.: 57.81%] [Generator loss: 1.040275]\n",
      "6188 [Discriminator loss: 0.675578, acc.: 65.62%] [Generator loss: 0.901735]\n",
      "6189 [Discriminator loss: 0.521481, acc.: 72.66%] [Generator loss: 0.903389]\n",
      "6190 [Discriminator loss: 0.730069, acc.: 53.12%] [Generator loss: 0.946020]\n",
      "6191 [Discriminator loss: 0.601133, acc.: 72.66%] [Generator loss: 1.182137]\n",
      "6192 [Discriminator loss: 0.542231, acc.: 73.44%] [Generator loss: 0.837572]\n",
      "6193 [Discriminator loss: 0.698795, acc.: 61.72%] [Generator loss: 0.717194]\n",
      "6194 [Discriminator loss: 0.943824, acc.: 41.41%] [Generator loss: 1.612203]\n",
      "6195 [Discriminator loss: 0.533837, acc.: 68.75%] [Generator loss: 1.967188]\n",
      "6196 [Discriminator loss: 0.542057, acc.: 67.19%] [Generator loss: 1.246434]\n",
      "6197 [Discriminator loss: 0.606921, acc.: 69.53%] [Generator loss: 0.953136]\n",
      "6198 [Discriminator loss: 0.592972, acc.: 69.53%] [Generator loss: 0.970853]\n",
      "6199 [Discriminator loss: 0.616247, acc.: 66.41%] [Generator loss: 0.979561]\n",
      "6200 [Discriminator loss: 0.417013, acc.: 86.72%] [Generator loss: 0.744375]\n",
      "6201 [Discriminator loss: 0.555362, acc.: 75.78%] [Generator loss: 0.599692]\n",
      "6202 [Discriminator loss: 0.851829, acc.: 42.97%] [Generator loss: 0.807122]\n",
      "6203 [Discriminator loss: 0.687300, acc.: 62.50%] [Generator loss: 1.363932]\n",
      "6204 [Discriminator loss: 0.926686, acc.: 46.09%] [Generator loss: 0.945583]\n",
      "6205 [Discriminator loss: 1.140144, acc.: 27.34%] [Generator loss: 0.777779]\n",
      "6206 [Discriminator loss: 0.629351, acc.: 62.50%] [Generator loss: 1.008113]\n",
      "6207 [Discriminator loss: 0.703560, acc.: 61.72%] [Generator loss: 0.930290]\n",
      "6208 [Discriminator loss: 0.636492, acc.: 58.59%] [Generator loss: 1.040684]\n",
      "6209 [Discriminator loss: 0.793328, acc.: 45.31%] [Generator loss: 1.109279]\n",
      "6210 [Discriminator loss: 0.581794, acc.: 69.53%] [Generator loss: 0.864192]\n",
      "6211 [Discriminator loss: 0.836553, acc.: 58.59%] [Generator loss: 0.719299]\n",
      "6212 [Discriminator loss: 0.557326, acc.: 69.53%] [Generator loss: 0.817551]\n",
      "6213 [Discriminator loss: 0.502565, acc.: 85.16%] [Generator loss: 0.998000]\n",
      "6214 [Discriminator loss: 0.747031, acc.: 40.62%] [Generator loss: 0.995969]\n",
      "6215 [Discriminator loss: 0.577333, acc.: 75.00%] [Generator loss: 0.816735]\n",
      "6216 [Discriminator loss: 0.366993, acc.: 89.84%] [Generator loss: 0.863299]\n",
      "6217 [Discriminator loss: 0.659082, acc.: 65.62%] [Generator loss: 0.710859]\n",
      "6218 [Discriminator loss: 0.802333, acc.: 53.12%] [Generator loss: 0.890365]\n",
      "6219 [Discriminator loss: 0.458060, acc.: 81.25%] [Generator loss: 1.004878]\n",
      "6220 [Discriminator loss: 0.729688, acc.: 57.03%] [Generator loss: 0.888653]\n",
      "6221 [Discriminator loss: 0.641808, acc.: 63.28%] [Generator loss: 0.775441]\n",
      "6222 [Discriminator loss: 0.709086, acc.: 53.91%] [Generator loss: 0.824792]\n",
      "6223 [Discriminator loss: 0.560270, acc.: 68.75%] [Generator loss: 0.900804]\n",
      "6224 [Discriminator loss: 0.628592, acc.: 67.97%] [Generator loss: 1.057227]\n",
      "6225 [Discriminator loss: 0.828346, acc.: 47.66%] [Generator loss: 1.191841]\n",
      "6226 [Discriminator loss: 0.673213, acc.: 54.69%] [Generator loss: 1.085928]\n",
      "6227 [Discriminator loss: 0.706111, acc.: 60.16%] [Generator loss: 1.121410]\n",
      "6228 [Discriminator loss: 0.671468, acc.: 56.25%] [Generator loss: 1.134982]\n",
      "6229 [Discriminator loss: 0.702966, acc.: 55.47%] [Generator loss: 1.047847]\n",
      "6230 [Discriminator loss: 0.932715, acc.: 37.50%] [Generator loss: 0.864461]\n",
      "6231 [Discriminator loss: 0.696063, acc.: 53.12%] [Generator loss: 1.095126]\n",
      "6232 [Discriminator loss: 0.589070, acc.: 67.97%] [Generator loss: 0.897859]\n",
      "6233 [Discriminator loss: 0.840268, acc.: 47.66%] [Generator loss: 1.314515]\n",
      "6234 [Discriminator loss: 0.747991, acc.: 57.81%] [Generator loss: 1.211393]\n",
      "6235 [Discriminator loss: 0.774899, acc.: 45.31%] [Generator loss: 1.077430]\n",
      "6236 [Discriminator loss: 0.693337, acc.: 57.03%] [Generator loss: 1.007636]\n",
      "6237 [Discriminator loss: 0.712358, acc.: 48.44%] [Generator loss: 0.979293]\n",
      "6238 [Discriminator loss: 0.685430, acc.: 62.50%] [Generator loss: 0.990808]\n",
      "6239 [Discriminator loss: 0.721152, acc.: 52.34%] [Generator loss: 0.895007]\n",
      "6240 [Discriminator loss: 0.640825, acc.: 66.41%] [Generator loss: 1.071614]\n",
      "6241 [Discriminator loss: 0.631370, acc.: 61.72%] [Generator loss: 1.022618]\n",
      "6242 [Discriminator loss: 0.641513, acc.: 63.28%] [Generator loss: 0.870928]\n",
      "6243 [Discriminator loss: 0.674070, acc.: 63.28%] [Generator loss: 0.896593]\n",
      "6244 [Discriminator loss: 0.607630, acc.: 66.41%] [Generator loss: 1.019760]\n",
      "6245 [Discriminator loss: 0.558891, acc.: 77.34%] [Generator loss: 0.919906]\n",
      "6246 [Discriminator loss: 0.558688, acc.: 71.09%] [Generator loss: 0.971422]\n",
      "6247 [Discriminator loss: 0.684173, acc.: 60.16%] [Generator loss: 1.171353]\n",
      "6248 [Discriminator loss: 0.710670, acc.: 60.16%] [Generator loss: 0.880170]\n",
      "6249 [Discriminator loss: 0.679962, acc.: 57.81%] [Generator loss: 0.814423]\n",
      "6250 [Discriminator loss: 0.626483, acc.: 64.06%] [Generator loss: 0.881797]\n",
      "6251 [Discriminator loss: 0.690876, acc.: 61.72%] [Generator loss: 1.190102]\n",
      "6252 [Discriminator loss: 0.600324, acc.: 65.62%] [Generator loss: 1.240837]\n",
      "6253 [Discriminator loss: 0.864297, acc.: 49.22%] [Generator loss: 0.913797]\n",
      "6254 [Discriminator loss: 0.804465, acc.: 50.00%] [Generator loss: 1.011158]\n",
      "6255 [Discriminator loss: 0.676392, acc.: 59.38%] [Generator loss: 1.130582]\n",
      "6256 [Discriminator loss: 0.646279, acc.: 61.72%] [Generator loss: 0.778155]\n",
      "6257 [Discriminator loss: 0.788047, acc.: 51.56%] [Generator loss: 0.820249]\n",
      "6258 [Discriminator loss: 0.677207, acc.: 57.03%] [Generator loss: 0.920974]\n",
      "6259 [Discriminator loss: 0.783799, acc.: 46.88%] [Generator loss: 1.157848]\n",
      "6260 [Discriminator loss: 0.789401, acc.: 56.25%] [Generator loss: 0.956891]\n",
      "6261 [Discriminator loss: 0.708902, acc.: 58.59%] [Generator loss: 0.961171]\n",
      "6262 [Discriminator loss: 0.696828, acc.: 64.84%] [Generator loss: 1.081725]\n",
      "6263 [Discriminator loss: 0.668176, acc.: 61.72%] [Generator loss: 1.156937]\n",
      "6264 [Discriminator loss: 0.551963, acc.: 75.00%] [Generator loss: 1.089242]\n",
      "6265 [Discriminator loss: 0.609724, acc.: 67.19%] [Generator loss: 0.888263]\n",
      "6266 [Discriminator loss: 0.592311, acc.: 66.41%] [Generator loss: 0.957796]\n",
      "6267 [Discriminator loss: 0.563503, acc.: 71.09%] [Generator loss: 0.908385]\n",
      "6268 [Discriminator loss: 0.653298, acc.: 66.41%] [Generator loss: 0.749826]\n",
      "6269 [Discriminator loss: 0.712427, acc.: 53.12%] [Generator loss: 0.876724]\n",
      "6270 [Discriminator loss: 0.639290, acc.: 65.62%] [Generator loss: 0.924986]\n",
      "6271 [Discriminator loss: 0.750931, acc.: 46.88%] [Generator loss: 1.106167]\n",
      "6272 [Discriminator loss: 0.774907, acc.: 52.34%] [Generator loss: 0.763402]\n",
      "6273 [Discriminator loss: 0.644306, acc.: 58.59%] [Generator loss: 0.797454]\n",
      "6274 [Discriminator loss: 0.605569, acc.: 68.75%] [Generator loss: 1.076957]\n",
      "6275 [Discriminator loss: 0.703683, acc.: 65.62%] [Generator loss: 0.876190]\n",
      "6276 [Discriminator loss: 0.775620, acc.: 53.12%] [Generator loss: 0.928302]\n",
      "6277 [Discriminator loss: 0.527122, acc.: 74.22%] [Generator loss: 1.184322]\n",
      "6278 [Discriminator loss: 0.663873, acc.: 59.38%] [Generator loss: 1.290967]\n",
      "6279 [Discriminator loss: 0.611061, acc.: 64.84%] [Generator loss: 1.338334]\n",
      "6280 [Discriminator loss: 0.792653, acc.: 50.78%] [Generator loss: 1.260547]\n",
      "6281 [Discriminator loss: 0.820729, acc.: 46.09%] [Generator loss: 1.008277]\n",
      "6282 [Discriminator loss: 0.880183, acc.: 47.66%] [Generator loss: 0.876384]\n",
      "6283 [Discriminator loss: 0.657654, acc.: 58.59%] [Generator loss: 0.987855]\n",
      "6284 [Discriminator loss: 0.636416, acc.: 67.97%] [Generator loss: 1.126825]\n",
      "6285 [Discriminator loss: 0.622421, acc.: 71.88%] [Generator loss: 1.062367]\n",
      "6286 [Discriminator loss: 0.742118, acc.: 56.25%] [Generator loss: 1.105545]\n",
      "6287 [Discriminator loss: 0.612897, acc.: 65.62%] [Generator loss: 1.082530]\n",
      "6288 [Discriminator loss: 0.550142, acc.: 75.78%] [Generator loss: 0.973917]\n",
      "6289 [Discriminator loss: 0.681309, acc.: 60.94%] [Generator loss: 1.081205]\n",
      "6290 [Discriminator loss: 0.596119, acc.: 70.31%] [Generator loss: 1.125283]\n",
      "6291 [Discriminator loss: 0.596306, acc.: 68.75%] [Generator loss: 1.029651]\n",
      "6292 [Discriminator loss: 0.727866, acc.: 54.69%] [Generator loss: 1.265493]\n",
      "6293 [Discriminator loss: 0.564433, acc.: 68.75%] [Generator loss: 1.140154]\n",
      "6294 [Discriminator loss: 0.655041, acc.: 61.72%] [Generator loss: 1.098372]\n",
      "6295 [Discriminator loss: 0.634072, acc.: 63.28%] [Generator loss: 1.039095]\n",
      "6296 [Discriminator loss: 0.720366, acc.: 55.47%] [Generator loss: 1.184602]\n",
      "6297 [Discriminator loss: 0.675042, acc.: 59.38%] [Generator loss: 1.076176]\n",
      "6298 [Discriminator loss: 0.618167, acc.: 64.84%] [Generator loss: 0.924514]\n",
      "6299 [Discriminator loss: 0.911088, acc.: 41.41%] [Generator loss: 0.526090]\n",
      "6300 [Discriminator loss: 0.607058, acc.: 64.06%] [Generator loss: 0.638101]\n",
      "6301 [Discriminator loss: 0.760460, acc.: 57.03%] [Generator loss: 0.928233]\n",
      "6302 [Discriminator loss: 0.655926, acc.: 60.94%] [Generator loss: 1.044374]\n",
      "6303 [Discriminator loss: 0.673229, acc.: 63.28%] [Generator loss: 1.240583]\n",
      "6304 [Discriminator loss: 0.575976, acc.: 71.09%] [Generator loss: 0.954262]\n",
      "6305 [Discriminator loss: 0.683484, acc.: 63.28%] [Generator loss: 1.142038]\n",
      "6306 [Discriminator loss: 0.594072, acc.: 69.53%] [Generator loss: 1.000138]\n",
      "6307 [Discriminator loss: 0.590448, acc.: 64.06%] [Generator loss: 1.106920]\n",
      "6308 [Discriminator loss: 0.522211, acc.: 75.78%] [Generator loss: 1.096439]\n",
      "6309 [Discriminator loss: 0.554971, acc.: 68.75%] [Generator loss: 1.171063]\n",
      "6310 [Discriminator loss: 0.882971, acc.: 39.84%] [Generator loss: 0.765143]\n",
      "6311 [Discriminator loss: 0.622446, acc.: 69.53%] [Generator loss: 1.174385]\n",
      "6312 [Discriminator loss: 0.509840, acc.: 78.12%] [Generator loss: 1.035190]\n",
      "6313 [Discriminator loss: 0.749711, acc.: 53.12%] [Generator loss: 1.267209]\n",
      "6314 [Discriminator loss: 0.589376, acc.: 71.09%] [Generator loss: 1.366244]\n",
      "6315 [Discriminator loss: 0.743119, acc.: 53.12%] [Generator loss: 1.193293]\n",
      "6316 [Discriminator loss: 0.781570, acc.: 53.12%] [Generator loss: 0.964933]\n",
      "6317 [Discriminator loss: 0.529204, acc.: 73.44%] [Generator loss: 1.038012]\n",
      "6318 [Discriminator loss: 0.667808, acc.: 61.72%] [Generator loss: 0.825194]\n",
      "6319 [Discriminator loss: 0.754305, acc.: 51.56%] [Generator loss: 1.058467]\n",
      "6320 [Discriminator loss: 0.694555, acc.: 60.16%] [Generator loss: 1.312833]\n",
      "6321 [Discriminator loss: 0.617721, acc.: 64.06%] [Generator loss: 1.414795]\n",
      "6322 [Discriminator loss: 0.672123, acc.: 64.06%] [Generator loss: 1.186421]\n",
      "6323 [Discriminator loss: 0.636784, acc.: 64.06%] [Generator loss: 1.127856]\n",
      "6324 [Discriminator loss: 0.819137, acc.: 48.44%] [Generator loss: 1.351549]\n",
      "6325 [Discriminator loss: 0.574850, acc.: 63.28%] [Generator loss: 1.396530]\n",
      "6326 [Discriminator loss: 0.629812, acc.: 64.06%] [Generator loss: 1.196980]\n",
      "6327 [Discriminator loss: 0.729873, acc.: 52.34%] [Generator loss: 0.732659]\n",
      "6328 [Discriminator loss: 0.693745, acc.: 55.47%] [Generator loss: 0.836600]\n",
      "6329 [Discriminator loss: 0.573086, acc.: 69.53%] [Generator loss: 0.829980]\n",
      "6330 [Discriminator loss: 0.589411, acc.: 70.31%] [Generator loss: 0.935818]\n",
      "6331 [Discriminator loss: 0.759727, acc.: 54.69%] [Generator loss: 0.975613]\n",
      "6332 [Discriminator loss: 0.735634, acc.: 55.47%] [Generator loss: 0.850745]\n",
      "6333 [Discriminator loss: 0.565941, acc.: 73.44%] [Generator loss: 0.950403]\n",
      "6334 [Discriminator loss: 0.650435, acc.: 66.41%] [Generator loss: 1.238696]\n",
      "6335 [Discriminator loss: 0.682274, acc.: 60.16%] [Generator loss: 1.168183]\n",
      "6336 [Discriminator loss: 0.690039, acc.: 57.81%] [Generator loss: 1.340877]\n",
      "6337 [Discriminator loss: 0.768753, acc.: 50.00%] [Generator loss: 1.108736]\n",
      "6338 [Discriminator loss: 0.594956, acc.: 71.09%] [Generator loss: 0.882541]\n",
      "6339 [Discriminator loss: 0.589965, acc.: 68.75%] [Generator loss: 0.769062]\n",
      "6340 [Discriminator loss: 0.811390, acc.: 46.09%] [Generator loss: 0.748473]\n",
      "6341 [Discriminator loss: 0.536117, acc.: 75.00%] [Generator loss: 0.825485]\n",
      "6342 [Discriminator loss: 0.819054, acc.: 47.66%] [Generator loss: 0.666915]\n",
      "6343 [Discriminator loss: 0.718548, acc.: 55.47%] [Generator loss: 1.160552]\n",
      "6344 [Discriminator loss: 0.547526, acc.: 73.44%] [Generator loss: 1.103331]\n",
      "6345 [Discriminator loss: 0.741915, acc.: 61.72%] [Generator loss: 0.880862]\n",
      "6346 [Discriminator loss: 0.643856, acc.: 58.59%] [Generator loss: 0.795124]\n",
      "6347 [Discriminator loss: 0.728779, acc.: 54.69%] [Generator loss: 0.817211]\n",
      "6348 [Discriminator loss: 0.601348, acc.: 66.41%] [Generator loss: 1.064522]\n",
      "6349 [Discriminator loss: 0.597086, acc.: 65.62%] [Generator loss: 1.061629]\n",
      "6350 [Discriminator loss: 0.598828, acc.: 73.44%] [Generator loss: 0.857090]\n",
      "6351 [Discriminator loss: 0.740946, acc.: 51.56%] [Generator loss: 0.960963]\n",
      "6352 [Discriminator loss: 0.623376, acc.: 70.31%] [Generator loss: 0.998827]\n",
      "6353 [Discriminator loss: 0.661543, acc.: 62.50%] [Generator loss: 0.847678]\n",
      "6354 [Discriminator loss: 0.668607, acc.: 57.81%] [Generator loss: 0.760803]\n",
      "6355 [Discriminator loss: 0.729424, acc.: 56.25%] [Generator loss: 0.734456]\n",
      "6356 [Discriminator loss: 0.682921, acc.: 57.81%] [Generator loss: 0.962517]\n",
      "6357 [Discriminator loss: 0.534302, acc.: 75.00%] [Generator loss: 0.968998]\n",
      "6358 [Discriminator loss: 0.600593, acc.: 69.53%] [Generator loss: 1.155607]\n",
      "6359 [Discriminator loss: 0.564578, acc.: 73.44%] [Generator loss: 0.969893]\n",
      "6360 [Discriminator loss: 0.543682, acc.: 75.00%] [Generator loss: 0.777746]\n",
      "6361 [Discriminator loss: 0.552115, acc.: 76.56%] [Generator loss: 0.608142]\n",
      "6362 [Discriminator loss: 0.607664, acc.: 75.78%] [Generator loss: 0.889370]\n",
      "6363 [Discriminator loss: 0.643482, acc.: 63.28%] [Generator loss: 1.251246]\n",
      "6364 [Discriminator loss: 0.653972, acc.: 61.72%] [Generator loss: 1.554669]\n",
      "6365 [Discriminator loss: 0.596087, acc.: 66.41%] [Generator loss: 1.100106]\n",
      "6366 [Discriminator loss: 1.135746, acc.: 32.03%] [Generator loss: 0.707529]\n",
      "6367 [Discriminator loss: 0.830059, acc.: 51.56%] [Generator loss: 0.916565]\n",
      "6368 [Discriminator loss: 0.645779, acc.: 62.50%] [Generator loss: 1.024898]\n",
      "6369 [Discriminator loss: 0.696522, acc.: 59.38%] [Generator loss: 1.024604]\n",
      "6370 [Discriminator loss: 0.858878, acc.: 46.88%] [Generator loss: 0.804460]\n",
      "6371 [Discriminator loss: 0.705312, acc.: 53.91%] [Generator loss: 0.774170]\n",
      "6372 [Discriminator loss: 0.593683, acc.: 70.31%] [Generator loss: 0.874331]\n",
      "6373 [Discriminator loss: 0.771181, acc.: 54.69%] [Generator loss: 1.051058]\n",
      "6374 [Discriminator loss: 0.563170, acc.: 66.41%] [Generator loss: 1.048666]\n",
      "6375 [Discriminator loss: 0.582948, acc.: 67.97%] [Generator loss: 0.940437]\n",
      "6376 [Discriminator loss: 0.628062, acc.: 60.94%] [Generator loss: 0.924007]\n",
      "6377 [Discriminator loss: 0.614512, acc.: 66.41%] [Generator loss: 0.940850]\n",
      "6378 [Discriminator loss: 0.677024, acc.: 63.28%] [Generator loss: 0.801100]\n",
      "6379 [Discriminator loss: 0.598867, acc.: 65.62%] [Generator loss: 0.979867]\n",
      "6380 [Discriminator loss: 0.510590, acc.: 73.44%] [Generator loss: 1.036740]\n",
      "6381 [Discriminator loss: 0.636058, acc.: 63.28%] [Generator loss: 1.030866]\n",
      "6382 [Discriminator loss: 0.568475, acc.: 68.75%] [Generator loss: 0.916909]\n",
      "6383 [Discriminator loss: 0.854478, acc.: 45.31%] [Generator loss: 0.827461]\n",
      "6384 [Discriminator loss: 0.864636, acc.: 44.53%] [Generator loss: 0.951219]\n",
      "6385 [Discriminator loss: 0.683568, acc.: 58.59%] [Generator loss: 0.902469]\n",
      "6386 [Discriminator loss: 0.537119, acc.: 71.88%] [Generator loss: 1.001428]\n",
      "6387 [Discriminator loss: 0.846783, acc.: 50.00%] [Generator loss: 1.144610]\n",
      "6388 [Discriminator loss: 0.850498, acc.: 46.88%] [Generator loss: 1.196940]\n",
      "6389 [Discriminator loss: 0.587586, acc.: 73.44%] [Generator loss: 1.071194]\n",
      "6390 [Discriminator loss: 0.729062, acc.: 57.81%] [Generator loss: 0.794004]\n",
      "6391 [Discriminator loss: 0.526437, acc.: 78.12%] [Generator loss: 0.975892]\n",
      "6392 [Discriminator loss: 0.539693, acc.: 71.88%] [Generator loss: 0.966750]\n",
      "6393 [Discriminator loss: 0.670736, acc.: 64.84%] [Generator loss: 0.979137]\n",
      "6394 [Discriminator loss: 0.546066, acc.: 69.53%] [Generator loss: 1.209848]\n",
      "6395 [Discriminator loss: 0.456345, acc.: 84.38%] [Generator loss: 0.988797]\n",
      "6396 [Discriminator loss: 0.602672, acc.: 69.53%] [Generator loss: 0.952828]\n",
      "6397 [Discriminator loss: 0.568288, acc.: 72.66%] [Generator loss: 0.721826]\n",
      "6398 [Discriminator loss: 0.710865, acc.: 60.16%] [Generator loss: 1.010113]\n",
      "6399 [Discriminator loss: 0.580982, acc.: 67.97%] [Generator loss: 1.180200]\n",
      "6400 [Discriminator loss: 0.551258, acc.: 72.66%] [Generator loss: 1.044706]\n",
      "6401 [Discriminator loss: 0.765139, acc.: 47.66%] [Generator loss: 1.099566]\n",
      "6402 [Discriminator loss: 0.834948, acc.: 50.00%] [Generator loss: 1.035008]\n",
      "6403 [Discriminator loss: 0.655033, acc.: 64.06%] [Generator loss: 1.045298]\n",
      "6404 [Discriminator loss: 0.698807, acc.: 50.78%] [Generator loss: 0.976646]\n",
      "6405 [Discriminator loss: 0.646004, acc.: 63.28%] [Generator loss: 1.438411]\n",
      "6406 [Discriminator loss: 0.663805, acc.: 60.16%] [Generator loss: 0.946905]\n",
      "6407 [Discriminator loss: 0.665366, acc.: 53.12%] [Generator loss: 0.895311]\n",
      "6408 [Discriminator loss: 0.561785, acc.: 73.44%] [Generator loss: 0.842260]\n",
      "6409 [Discriminator loss: 0.699414, acc.: 57.81%] [Generator loss: 0.758497]\n",
      "6410 [Discriminator loss: 0.616381, acc.: 67.97%] [Generator loss: 0.927191]\n",
      "6411 [Discriminator loss: 0.702281, acc.: 57.03%] [Generator loss: 0.883711]\n",
      "6412 [Discriminator loss: 0.460505, acc.: 82.03%] [Generator loss: 0.726600]\n",
      "6413 [Discriminator loss: 0.664394, acc.: 60.16%] [Generator loss: 0.799181]\n",
      "6414 [Discriminator loss: 0.744768, acc.: 55.47%] [Generator loss: 0.996718]\n",
      "6415 [Discriminator loss: 0.646285, acc.: 66.41%] [Generator loss: 1.099508]\n",
      "6416 [Discriminator loss: 0.604139, acc.: 69.53%] [Generator loss: 0.972806]\n",
      "6417 [Discriminator loss: 0.888778, acc.: 41.41%] [Generator loss: 0.835749]\n",
      "6418 [Discriminator loss: 0.777545, acc.: 44.53%] [Generator loss: 0.693959]\n",
      "6419 [Discriminator loss: 0.770926, acc.: 52.34%] [Generator loss: 1.209912]\n",
      "6420 [Discriminator loss: 0.829640, acc.: 48.44%] [Generator loss: 1.386608]\n",
      "6421 [Discriminator loss: 0.738553, acc.: 57.03%] [Generator loss: 1.196981]\n",
      "6422 [Discriminator loss: 0.622429, acc.: 65.62%] [Generator loss: 0.911245]\n",
      "6423 [Discriminator loss: 0.637223, acc.: 67.97%] [Generator loss: 0.894049]\n",
      "6424 [Discriminator loss: 0.537094, acc.: 76.56%] [Generator loss: 1.025994]\n",
      "6425 [Discriminator loss: 0.559298, acc.: 72.66%] [Generator loss: 1.083226]\n",
      "6426 [Discriminator loss: 0.596382, acc.: 70.31%] [Generator loss: 0.990084]\n",
      "6427 [Discriminator loss: 0.535727, acc.: 76.56%] [Generator loss: 0.902312]\n",
      "6428 [Discriminator loss: 0.584603, acc.: 71.88%] [Generator loss: 0.866280]\n",
      "6429 [Discriminator loss: 0.577027, acc.: 73.44%] [Generator loss: 1.119061]\n",
      "6430 [Discriminator loss: 0.694145, acc.: 57.81%] [Generator loss: 1.021433]\n",
      "6431 [Discriminator loss: 0.657040, acc.: 64.06%] [Generator loss: 0.991074]\n",
      "6432 [Discriminator loss: 0.546207, acc.: 83.59%] [Generator loss: 0.816165]\n",
      "6433 [Discriminator loss: 0.781087, acc.: 57.81%] [Generator loss: 1.172536]\n",
      "6434 [Discriminator loss: 0.693283, acc.: 59.38%] [Generator loss: 1.355596]\n",
      "6435 [Discriminator loss: 0.729917, acc.: 53.12%] [Generator loss: 1.151013]\n",
      "6436 [Discriminator loss: 1.015348, acc.: 35.16%] [Generator loss: 0.766394]\n",
      "6437 [Discriminator loss: 0.670186, acc.: 57.81%] [Generator loss: 0.956222]\n",
      "6438 [Discriminator loss: 0.709574, acc.: 52.34%] [Generator loss: 0.737644]\n",
      "6439 [Discriminator loss: 0.712505, acc.: 49.22%] [Generator loss: 0.984151]\n",
      "6440 [Discriminator loss: 0.706678, acc.: 49.22%] [Generator loss: 0.921739]\n",
      "6441 [Discriminator loss: 0.673143, acc.: 57.81%] [Generator loss: 1.005674]\n",
      "6442 [Discriminator loss: 0.713896, acc.: 51.56%] [Generator loss: 1.111089]\n",
      "6443 [Discriminator loss: 0.797368, acc.: 47.66%] [Generator loss: 0.914491]\n",
      "6444 [Discriminator loss: 0.725942, acc.: 49.22%] [Generator loss: 0.863578]\n",
      "6445 [Discriminator loss: 0.638404, acc.: 65.62%] [Generator loss: 0.948224]\n",
      "6446 [Discriminator loss: 0.599412, acc.: 71.09%] [Generator loss: 0.888952]\n",
      "6447 [Discriminator loss: 0.639037, acc.: 63.28%] [Generator loss: 0.933293]\n",
      "6448 [Discriminator loss: 0.735486, acc.: 57.03%] [Generator loss: 1.087487]\n",
      "6449 [Discriminator loss: 0.640208, acc.: 58.59%] [Generator loss: 1.337838]\n",
      "6450 [Discriminator loss: 0.927985, acc.: 38.28%] [Generator loss: 1.433537]\n",
      "6451 [Discriminator loss: 0.654477, acc.: 68.75%] [Generator loss: 0.817134]\n",
      "6452 [Discriminator loss: 0.631648, acc.: 60.94%] [Generator loss: 0.834762]\n",
      "6453 [Discriminator loss: 0.667964, acc.: 61.72%] [Generator loss: 0.778998]\n",
      "6454 [Discriminator loss: 0.611440, acc.: 64.84%] [Generator loss: 0.880435]\n",
      "6455 [Discriminator loss: 0.695485, acc.: 67.19%] [Generator loss: 0.786933]\n",
      "6456 [Discriminator loss: 0.698612, acc.: 55.47%] [Generator loss: 1.019237]\n",
      "6457 [Discriminator loss: 0.581516, acc.: 69.53%] [Generator loss: 1.019925]\n",
      "6458 [Discriminator loss: 0.717532, acc.: 57.03%] [Generator loss: 1.112159]\n",
      "6459 [Discriminator loss: 0.628197, acc.: 66.41%] [Generator loss: 1.141379]\n",
      "6460 [Discriminator loss: 0.555927, acc.: 75.78%] [Generator loss: 1.059983]\n",
      "6461 [Discriminator loss: 0.611502, acc.: 68.75%] [Generator loss: 1.109280]\n",
      "6462 [Discriminator loss: 0.605172, acc.: 69.53%] [Generator loss: 0.961916]\n",
      "6463 [Discriminator loss: 0.651723, acc.: 69.53%] [Generator loss: 0.766479]\n",
      "6464 [Discriminator loss: 0.564108, acc.: 72.66%] [Generator loss: 0.735630]\n",
      "6465 [Discriminator loss: 0.741210, acc.: 57.03%] [Generator loss: 0.984374]\n",
      "6466 [Discriminator loss: 0.503301, acc.: 85.16%] [Generator loss: 1.212036]\n",
      "6467 [Discriminator loss: 0.621427, acc.: 62.50%] [Generator loss: 1.009332]\n",
      "6468 [Discriminator loss: 0.687218, acc.: 60.94%] [Generator loss: 0.816301]\n",
      "6469 [Discriminator loss: 0.518252, acc.: 76.56%] [Generator loss: 0.752696]\n",
      "6470 [Discriminator loss: 0.833945, acc.: 37.50%] [Generator loss: 0.888867]\n",
      "6471 [Discriminator loss: 0.673820, acc.: 59.38%] [Generator loss: 1.121302]\n",
      "6472 [Discriminator loss: 0.715441, acc.: 61.72%] [Generator loss: 1.065271]\n",
      "6473 [Discriminator loss: 0.679703, acc.: 60.16%] [Generator loss: 1.285691]\n",
      "6474 [Discriminator loss: 0.735687, acc.: 58.59%] [Generator loss: 1.030366]\n",
      "6475 [Discriminator loss: 0.685383, acc.: 57.81%] [Generator loss: 1.159353]\n",
      "6476 [Discriminator loss: 0.649047, acc.: 62.50%] [Generator loss: 1.196038]\n",
      "6477 [Discriminator loss: 0.694529, acc.: 57.81%] [Generator loss: 0.940657]\n",
      "6478 [Discriminator loss: 0.589801, acc.: 65.62%] [Generator loss: 1.170155]\n",
      "6479 [Discriminator loss: 0.688569, acc.: 60.94%] [Generator loss: 0.887954]\n",
      "6480 [Discriminator loss: 0.521016, acc.: 78.12%] [Generator loss: 0.888622]\n",
      "6481 [Discriminator loss: 0.764717, acc.: 50.78%] [Generator loss: 0.674369]\n",
      "6482 [Discriminator loss: 1.070353, acc.: 33.59%] [Generator loss: 0.668920]\n",
      "6483 [Discriminator loss: 0.600226, acc.: 70.31%] [Generator loss: 0.984170]\n",
      "6484 [Discriminator loss: 0.631574, acc.: 65.62%] [Generator loss: 1.044581]\n",
      "6485 [Discriminator loss: 0.522538, acc.: 79.69%] [Generator loss: 1.076897]\n",
      "6486 [Discriminator loss: 0.590435, acc.: 67.19%] [Generator loss: 1.096901]\n",
      "6487 [Discriminator loss: 0.763395, acc.: 51.56%] [Generator loss: 0.936219]\n",
      "6488 [Discriminator loss: 0.859867, acc.: 43.75%] [Generator loss: 0.903719]\n",
      "6489 [Discriminator loss: 0.821984, acc.: 47.66%] [Generator loss: 1.059258]\n",
      "6490 [Discriminator loss: 0.561678, acc.: 71.09%] [Generator loss: 1.402945]\n",
      "6491 [Discriminator loss: 0.621646, acc.: 70.31%] [Generator loss: 1.113928]\n",
      "6492 [Discriminator loss: 0.570002, acc.: 72.66%] [Generator loss: 0.887172]\n",
      "6493 [Discriminator loss: 0.656424, acc.: 60.16%] [Generator loss: 0.879325]\n",
      "6494 [Discriminator loss: 0.410369, acc.: 89.06%] [Generator loss: 0.772203]\n",
      "6495 [Discriminator loss: 0.538921, acc.: 75.00%] [Generator loss: 0.887577]\n",
      "6496 [Discriminator loss: 0.622545, acc.: 68.75%] [Generator loss: 0.883024]\n",
      "6497 [Discriminator loss: 0.759172, acc.: 53.91%] [Generator loss: 0.935031]\n",
      "6498 [Discriminator loss: 0.598155, acc.: 72.66%] [Generator loss: 1.007010]\n",
      "6499 [Discriminator loss: 0.587897, acc.: 73.44%] [Generator loss: 0.982293]\n",
      "6500 [Discriminator loss: 0.567326, acc.: 73.44%] [Generator loss: 0.935531]\n",
      "6501 [Discriminator loss: 0.575574, acc.: 71.09%] [Generator loss: 0.767103]\n",
      "6502 [Discriminator loss: 0.926361, acc.: 43.75%] [Generator loss: 0.991701]\n",
      "6503 [Discriminator loss: 0.634915, acc.: 61.72%] [Generator loss: 1.407272]\n",
      "6504 [Discriminator loss: 0.879088, acc.: 45.31%] [Generator loss: 1.070248]\n",
      "6505 [Discriminator loss: 0.834743, acc.: 39.84%] [Generator loss: 0.886043]\n",
      "6506 [Discriminator loss: 0.715966, acc.: 53.91%] [Generator loss: 0.910356]\n",
      "6507 [Discriminator loss: 0.601173, acc.: 71.09%] [Generator loss: 0.954441]\n",
      "6508 [Discriminator loss: 0.441771, acc.: 86.72%] [Generator loss: 1.004222]\n",
      "6509 [Discriminator loss: 0.730902, acc.: 54.69%] [Generator loss: 0.980022]\n",
      "6510 [Discriminator loss: 0.516748, acc.: 74.22%] [Generator loss: 0.881306]\n",
      "6511 [Discriminator loss: 0.635903, acc.: 65.62%] [Generator loss: 0.742497]\n",
      "6512 [Discriminator loss: 0.803014, acc.: 44.53%] [Generator loss: 0.808284]\n",
      "6513 [Discriminator loss: 0.443509, acc.: 87.50%] [Generator loss: 0.971057]\n",
      "6514 [Discriminator loss: 0.650241, acc.: 56.25%] [Generator loss: 1.010679]\n",
      "6515 [Discriminator loss: 0.590428, acc.: 67.97%] [Generator loss: 1.005915]\n",
      "6516 [Discriminator loss: 0.590967, acc.: 71.09%] [Generator loss: 0.892359]\n",
      "6517 [Discriminator loss: 0.582022, acc.: 68.75%] [Generator loss: 1.089978]\n",
      "6518 [Discriminator loss: 0.549101, acc.: 74.22%] [Generator loss: 1.174679]\n",
      "6519 [Discriminator loss: 0.494099, acc.: 80.47%] [Generator loss: 1.056779]\n",
      "6520 [Discriminator loss: 0.709559, acc.: 54.69%] [Generator loss: 0.882193]\n",
      "6521 [Discriminator loss: 0.631153, acc.: 64.84%] [Generator loss: 0.922510]\n",
      "6522 [Discriminator loss: 0.598982, acc.: 73.44%] [Generator loss: 1.043202]\n",
      "6523 [Discriminator loss: 0.636486, acc.: 60.94%] [Generator loss: 0.940150]\n",
      "6524 [Discriminator loss: 0.766436, acc.: 54.69%] [Generator loss: 0.870361]\n",
      "6525 [Discriminator loss: 0.561026, acc.: 72.66%] [Generator loss: 0.881872]\n",
      "6526 [Discriminator loss: 0.635163, acc.: 60.16%] [Generator loss: 0.855576]\n",
      "6527 [Discriminator loss: 0.733846, acc.: 58.59%] [Generator loss: 0.714671]\n",
      "6528 [Discriminator loss: 0.648579, acc.: 62.50%] [Generator loss: 0.764353]\n",
      "6529 [Discriminator loss: 0.563063, acc.: 73.44%] [Generator loss: 0.942553]\n",
      "6530 [Discriminator loss: 0.508136, acc.: 75.00%] [Generator loss: 0.931353]\n",
      "6531 [Discriminator loss: 0.585855, acc.: 69.53%] [Generator loss: 0.934784]\n",
      "6532 [Discriminator loss: 0.563491, acc.: 71.09%] [Generator loss: 0.808002]\n",
      "6533 [Discriminator loss: 0.451099, acc.: 82.03%] [Generator loss: 0.827192]\n",
      "6534 [Discriminator loss: 0.571078, acc.: 77.34%] [Generator loss: 0.638535]\n",
      "6535 [Discriminator loss: 0.650059, acc.: 63.28%] [Generator loss: 0.896809]\n",
      "6536 [Discriminator loss: 0.787871, acc.: 52.34%] [Generator loss: 1.346749]\n",
      "6537 [Discriminator loss: 0.921208, acc.: 46.09%] [Generator loss: 1.162153]\n",
      "6538 [Discriminator loss: 0.779170, acc.: 47.66%] [Generator loss: 1.110969]\n",
      "6539 [Discriminator loss: 0.874126, acc.: 43.75%] [Generator loss: 0.996576]\n",
      "6540 [Discriminator loss: 0.752990, acc.: 50.78%] [Generator loss: 1.051544]\n",
      "6541 [Discriminator loss: 0.713070, acc.: 52.34%] [Generator loss: 1.120705]\n",
      "6542 [Discriminator loss: 0.695195, acc.: 59.38%] [Generator loss: 1.130581]\n",
      "6543 [Discriminator loss: 0.853967, acc.: 49.22%] [Generator loss: 1.178282]\n",
      "6544 [Discriminator loss: 0.708491, acc.: 60.16%] [Generator loss: 1.219929]\n",
      "6545 [Discriminator loss: 0.786101, acc.: 53.91%] [Generator loss: 0.776148]\n",
      "6546 [Discriminator loss: 0.879926, acc.: 39.84%] [Generator loss: 0.714477]\n",
      "6547 [Discriminator loss: 0.630664, acc.: 61.72%] [Generator loss: 0.863577]\n",
      "6548 [Discriminator loss: 0.618491, acc.: 69.53%] [Generator loss: 0.940435]\n",
      "6549 [Discriminator loss: 0.679726, acc.: 64.06%] [Generator loss: 0.767452]\n",
      "6550 [Discriminator loss: 0.645823, acc.: 63.28%] [Generator loss: 0.902109]\n",
      "6551 [Discriminator loss: 0.450604, acc.: 88.28%] [Generator loss: 0.814213]\n",
      "6552 [Discriminator loss: 0.672394, acc.: 60.16%] [Generator loss: 0.737145]\n",
      "6553 [Discriminator loss: 0.779450, acc.: 48.44%] [Generator loss: 0.883337]\n",
      "6554 [Discriminator loss: 0.765759, acc.: 46.88%] [Generator loss: 0.994568]\n",
      "6555 [Discriminator loss: 0.516620, acc.: 75.78%] [Generator loss: 1.145614]\n",
      "6556 [Discriminator loss: 0.478549, acc.: 80.47%] [Generator loss: 1.063336]\n",
      "6557 [Discriminator loss: 0.623035, acc.: 62.50%] [Generator loss: 0.976147]\n",
      "6558 [Discriminator loss: 0.645839, acc.: 61.72%] [Generator loss: 0.904895]\n",
      "6559 [Discriminator loss: 0.424879, acc.: 81.25%] [Generator loss: 0.878311]\n",
      "6560 [Discriminator loss: 0.449283, acc.: 85.16%] [Generator loss: 0.661688]\n",
      "6561 [Discriminator loss: 0.758047, acc.: 50.78%] [Generator loss: 0.690369]\n",
      "6562 [Discriminator loss: 0.493576, acc.: 80.47%] [Generator loss: 0.857139]\n",
      "6563 [Discriminator loss: 0.597511, acc.: 71.88%] [Generator loss: 0.964240]\n",
      "6564 [Discriminator loss: 0.734530, acc.: 51.56%] [Generator loss: 0.784791]\n",
      "6565 [Discriminator loss: 0.554845, acc.: 76.56%] [Generator loss: 0.822213]\n",
      "6566 [Discriminator loss: 0.652007, acc.: 61.72%] [Generator loss: 0.872328]\n",
      "6567 [Discriminator loss: 0.746905, acc.: 48.44%] [Generator loss: 0.732407]\n",
      "6568 [Discriminator loss: 0.726265, acc.: 54.69%] [Generator loss: 0.855546]\n",
      "6569 [Discriminator loss: 0.731121, acc.: 57.81%] [Generator loss: 1.046185]\n",
      "6570 [Discriminator loss: 0.572915, acc.: 77.34%] [Generator loss: 1.154739]\n",
      "6571 [Discriminator loss: 0.611712, acc.: 67.97%] [Generator loss: 1.100109]\n",
      "6572 [Discriminator loss: 0.416983, acc.: 82.81%] [Generator loss: 0.953308]\n",
      "6573 [Discriminator loss: 0.440356, acc.: 83.59%] [Generator loss: 0.909898]\n",
      "6574 [Discriminator loss: 0.647915, acc.: 63.28%] [Generator loss: 0.801132]\n",
      "6575 [Discriminator loss: 0.574165, acc.: 72.66%] [Generator loss: 0.900248]\n",
      "6576 [Discriminator loss: 0.503466, acc.: 81.25%] [Generator loss: 0.900833]\n",
      "6577 [Discriminator loss: 0.519748, acc.: 73.44%] [Generator loss: 0.717475]\n",
      "6578 [Discriminator loss: 0.552851, acc.: 74.22%] [Generator loss: 0.642139]\n",
      "6579 [Discriminator loss: 0.614075, acc.: 62.50%] [Generator loss: 0.734245]\n",
      "6580 [Discriminator loss: 0.504504, acc.: 74.22%] [Generator loss: 0.905999]\n",
      "6581 [Discriminator loss: 0.489391, acc.: 69.53%] [Generator loss: 0.671649]\n",
      "6582 [Discriminator loss: 0.827513, acc.: 53.91%] [Generator loss: 0.913356]\n",
      "6583 [Discriminator loss: 0.590462, acc.: 71.09%] [Generator loss: 0.753520]\n",
      "6584 [Discriminator loss: 0.559305, acc.: 69.53%] [Generator loss: 0.809890]\n",
      "6585 [Discriminator loss: 0.578988, acc.: 70.31%] [Generator loss: 0.696754]\n",
      "6586 [Discriminator loss: 0.521487, acc.: 74.22%] [Generator loss: 0.817460]\n",
      "6587 [Discriminator loss: 0.718978, acc.: 57.81%] [Generator loss: 1.032463]\n",
      "6588 [Discriminator loss: 0.563853, acc.: 71.09%] [Generator loss: 1.444517]\n",
      "6589 [Discriminator loss: 0.653635, acc.: 64.06%] [Generator loss: 1.350921]\n",
      "6590 [Discriminator loss: 0.674703, acc.: 62.50%] [Generator loss: 1.474867]\n",
      "6591 [Discriminator loss: 0.843309, acc.: 45.31%] [Generator loss: 0.825166]\n",
      "6592 [Discriminator loss: 0.752952, acc.: 48.44%] [Generator loss: 0.843789]\n",
      "6593 [Discriminator loss: 0.615863, acc.: 71.88%] [Generator loss: 0.963219]\n",
      "6594 [Discriminator loss: 0.559533, acc.: 68.75%] [Generator loss: 0.726878]\n",
      "6595 [Discriminator loss: 0.609408, acc.: 65.62%] [Generator loss: 0.721254]\n",
      "6596 [Discriminator loss: 0.592369, acc.: 68.75%] [Generator loss: 0.910089]\n",
      "6597 [Discriminator loss: 0.618710, acc.: 66.41%] [Generator loss: 1.059026]\n",
      "6598 [Discriminator loss: 0.519178, acc.: 76.56%] [Generator loss: 0.839931]\n",
      "6599 [Discriminator loss: 0.484111, acc.: 74.22%] [Generator loss: 0.860244]\n",
      "6600 [Discriminator loss: 0.574277, acc.: 67.97%] [Generator loss: 0.903759]\n",
      "6601 [Discriminator loss: 0.651211, acc.: 63.28%] [Generator loss: 0.637985]\n",
      "6602 [Discriminator loss: 0.625325, acc.: 65.62%] [Generator loss: 0.756747]\n",
      "6603 [Discriminator loss: 0.452128, acc.: 82.03%] [Generator loss: 1.027798]\n",
      "6604 [Discriminator loss: 0.541634, acc.: 71.09%] [Generator loss: 1.142249]\n",
      "6605 [Discriminator loss: 1.356217, acc.: 19.53%] [Generator loss: 1.015926]\n",
      "6606 [Discriminator loss: 1.003640, acc.: 41.41%] [Generator loss: 1.561327]\n",
      "6607 [Discriminator loss: 0.680050, acc.: 57.81%] [Generator loss: 1.562208]\n",
      "6608 [Discriminator loss: 0.506405, acc.: 78.12%] [Generator loss: 1.285982]\n",
      "6609 [Discriminator loss: 0.672122, acc.: 58.59%] [Generator loss: 1.171744]\n",
      "6610 [Discriminator loss: 0.735607, acc.: 57.03%] [Generator loss: 0.938707]\n",
      "6611 [Discriminator loss: 0.540273, acc.: 70.31%] [Generator loss: 0.973553]\n",
      "6612 [Discriminator loss: 0.610861, acc.: 65.62%] [Generator loss: 1.050232]\n",
      "6613 [Discriminator loss: 0.713688, acc.: 53.12%] [Generator loss: 1.357514]\n",
      "6614 [Discriminator loss: 0.986656, acc.: 35.94%] [Generator loss: 1.129912]\n",
      "6615 [Discriminator loss: 0.693500, acc.: 55.47%] [Generator loss: 1.331819]\n",
      "6616 [Discriminator loss: 0.599123, acc.: 71.88%] [Generator loss: 1.283903]\n",
      "6617 [Discriminator loss: 0.645923, acc.: 67.19%] [Generator loss: 1.219011]\n",
      "6618 [Discriminator loss: 0.602038, acc.: 61.72%] [Generator loss: 1.390848]\n",
      "6619 [Discriminator loss: 0.641719, acc.: 67.19%] [Generator loss: 1.183342]\n",
      "6620 [Discriminator loss: 0.599755, acc.: 66.41%] [Generator loss: 1.047766]\n",
      "6621 [Discriminator loss: 0.451201, acc.: 82.03%] [Generator loss: 0.887964]\n",
      "6622 [Discriminator loss: 0.433663, acc.: 82.81%] [Generator loss: 0.784231]\n",
      "6623 [Discriminator loss: 0.561329, acc.: 69.53%] [Generator loss: 0.716708]\n",
      "6624 [Discriminator loss: 0.522664, acc.: 78.12%] [Generator loss: 0.707977]\n",
      "6625 [Discriminator loss: 0.722007, acc.: 60.16%] [Generator loss: 0.658976]\n",
      "6626 [Discriminator loss: 0.661786, acc.: 56.25%] [Generator loss: 0.644784]\n",
      "6627 [Discriminator loss: 0.559080, acc.: 67.19%] [Generator loss: 0.687090]\n",
      "6628 [Discriminator loss: 0.714642, acc.: 51.56%] [Generator loss: 0.946438]\n",
      "6629 [Discriminator loss: 0.559472, acc.: 70.31%] [Generator loss: 0.723187]\n",
      "6630 [Discriminator loss: 0.607488, acc.: 66.41%] [Generator loss: 1.009522]\n",
      "6631 [Discriminator loss: 0.680416, acc.: 62.50%] [Generator loss: 1.306257]\n",
      "6632 [Discriminator loss: 0.766156, acc.: 57.81%] [Generator loss: 1.126561]\n",
      "6633 [Discriminator loss: 0.517395, acc.: 76.56%] [Generator loss: 0.923968]\n",
      "6634 [Discriminator loss: 0.711837, acc.: 63.28%] [Generator loss: 1.151405]\n",
      "6635 [Discriminator loss: 0.709318, acc.: 62.50%] [Generator loss: 0.920016]\n",
      "6636 [Discriminator loss: 0.597294, acc.: 70.31%] [Generator loss: 0.933830]\n",
      "6637 [Discriminator loss: 0.641283, acc.: 60.16%] [Generator loss: 1.161595]\n",
      "6638 [Discriminator loss: 0.577542, acc.: 73.44%] [Generator loss: 1.050089]\n",
      "6639 [Discriminator loss: 0.584587, acc.: 68.75%] [Generator loss: 0.869564]\n",
      "6640 [Discriminator loss: 0.521648, acc.: 75.00%] [Generator loss: 1.016745]\n",
      "6641 [Discriminator loss: 0.585258, acc.: 74.22%] [Generator loss: 0.989226]\n",
      "6642 [Discriminator loss: 0.587063, acc.: 71.88%] [Generator loss: 1.052169]\n",
      "6643 [Discriminator loss: 0.678921, acc.: 61.72%] [Generator loss: 0.702648]\n",
      "6644 [Discriminator loss: 0.779574, acc.: 57.03%] [Generator loss: 0.740180]\n",
      "6645 [Discriminator loss: 0.592128, acc.: 65.62%] [Generator loss: 0.689384]\n",
      "6646 [Discriminator loss: 0.740686, acc.: 58.59%] [Generator loss: 0.828867]\n",
      "6647 [Discriminator loss: 0.589467, acc.: 69.53%] [Generator loss: 0.689327]\n",
      "6648 [Discriminator loss: 0.666940, acc.: 57.81%] [Generator loss: 0.737369]\n",
      "6649 [Discriminator loss: 0.865714, acc.: 36.72%] [Generator loss: 0.879653]\n",
      "6650 [Discriminator loss: 0.704192, acc.: 55.47%] [Generator loss: 0.878347]\n",
      "6651 [Discriminator loss: 0.642391, acc.: 62.50%] [Generator loss: 1.076748]\n",
      "6652 [Discriminator loss: 0.690247, acc.: 58.59%] [Generator loss: 1.240196]\n",
      "6653 [Discriminator loss: 0.537976, acc.: 72.66%] [Generator loss: 0.903185]\n",
      "6654 [Discriminator loss: 0.401951, acc.: 86.72%] [Generator loss: 0.740129]\n",
      "6655 [Discriminator loss: 0.780630, acc.: 50.78%] [Generator loss: 0.920987]\n",
      "6656 [Discriminator loss: 0.705968, acc.: 60.94%] [Generator loss: 1.213920]\n",
      "6657 [Discriminator loss: 0.504728, acc.: 78.91%] [Generator loss: 1.425551]\n",
      "6658 [Discriminator loss: 0.508588, acc.: 75.00%] [Generator loss: 1.081606]\n",
      "6659 [Discriminator loss: 0.498631, acc.: 76.56%] [Generator loss: 1.120306]\n",
      "6660 [Discriminator loss: 0.732696, acc.: 55.47%] [Generator loss: 0.992411]\n",
      "6661 [Discriminator loss: 0.632142, acc.: 64.06%] [Generator loss: 0.720922]\n",
      "6662 [Discriminator loss: 0.580281, acc.: 68.75%] [Generator loss: 0.658438]\n",
      "6663 [Discriminator loss: 0.774287, acc.: 52.34%] [Generator loss: 0.699319]\n",
      "6664 [Discriminator loss: 0.666820, acc.: 60.94%] [Generator loss: 0.756346]\n",
      "6665 [Discriminator loss: 0.703100, acc.: 60.16%] [Generator loss: 0.748954]\n",
      "6666 [Discriminator loss: 0.605973, acc.: 65.62%] [Generator loss: 0.650614]\n",
      "6667 [Discriminator loss: 0.853363, acc.: 46.88%] [Generator loss: 0.818197]\n",
      "6668 [Discriminator loss: 0.760483, acc.: 52.34%] [Generator loss: 0.955453]\n",
      "6669 [Discriminator loss: 0.658110, acc.: 67.19%] [Generator loss: 0.993313]\n",
      "6670 [Discriminator loss: 0.668555, acc.: 64.84%] [Generator loss: 0.881412]\n",
      "6671 [Discriminator loss: 0.624715, acc.: 67.97%] [Generator loss: 0.963790]\n",
      "6672 [Discriminator loss: 0.573484, acc.: 70.31%] [Generator loss: 1.221456]\n",
      "6673 [Discriminator loss: 0.565998, acc.: 72.66%] [Generator loss: 1.497362]\n",
      "6674 [Discriminator loss: 0.625284, acc.: 63.28%] [Generator loss: 1.241465]\n",
      "6675 [Discriminator loss: 0.709273, acc.: 61.72%] [Generator loss: 1.224846]\n",
      "6676 [Discriminator loss: 0.929236, acc.: 39.06%] [Generator loss: 0.831353]\n",
      "6677 [Discriminator loss: 0.708954, acc.: 64.06%] [Generator loss: 0.967312]\n",
      "6678 [Discriminator loss: 0.462148, acc.: 82.03%] [Generator loss: 0.938993]\n",
      "6679 [Discriminator loss: 0.547489, acc.: 71.09%] [Generator loss: 0.558625]\n",
      "6680 [Discriminator loss: 0.526340, acc.: 73.44%] [Generator loss: 0.449256]\n",
      "6681 [Discriminator loss: 0.638778, acc.: 57.81%] [Generator loss: 0.667949]\n",
      "6682 [Discriminator loss: 0.545996, acc.: 71.88%] [Generator loss: 0.709165]\n",
      "6683 [Discriminator loss: 0.732875, acc.: 53.12%] [Generator loss: 0.600578]\n",
      "6684 [Discriminator loss: 0.604328, acc.: 67.97%] [Generator loss: 0.636368]\n",
      "6685 [Discriminator loss: 0.602962, acc.: 66.41%] [Generator loss: 0.962099]\n",
      "6686 [Discriminator loss: 0.913905, acc.: 41.41%] [Generator loss: 1.096243]\n",
      "6687 [Discriminator loss: 0.620385, acc.: 67.19%] [Generator loss: 1.225647]\n",
      "6688 [Discriminator loss: 0.748427, acc.: 49.22%] [Generator loss: 0.881139]\n",
      "6689 [Discriminator loss: 0.519562, acc.: 75.78%] [Generator loss: 1.080461]\n",
      "6690 [Discriminator loss: 0.579657, acc.: 75.00%] [Generator loss: 1.014424]\n",
      "6691 [Discriminator loss: 0.550998, acc.: 75.78%] [Generator loss: 0.920693]\n",
      "6692 [Discriminator loss: 0.875653, acc.: 36.72%] [Generator loss: 0.892194]\n",
      "6693 [Discriminator loss: 0.535409, acc.: 77.34%] [Generator loss: 1.051034]\n",
      "6694 [Discriminator loss: 0.586447, acc.: 61.72%] [Generator loss: 0.890125]\n",
      "6695 [Discriminator loss: 0.542973, acc.: 70.31%] [Generator loss: 0.800114]\n",
      "6696 [Discriminator loss: 0.481584, acc.: 79.69%] [Generator loss: 0.815853]\n",
      "6697 [Discriminator loss: 0.658073, acc.: 64.84%] [Generator loss: 0.851842]\n",
      "6698 [Discriminator loss: 0.575357, acc.: 71.09%] [Generator loss: 1.213245]\n",
      "6699 [Discriminator loss: 0.729455, acc.: 53.91%] [Generator loss: 1.135223]\n",
      "6700 [Discriminator loss: 0.747109, acc.: 59.38%] [Generator loss: 1.218606]\n",
      "6701 [Discriminator loss: 0.454373, acc.: 79.69%] [Generator loss: 1.293963]\n",
      "6702 [Discriminator loss: 0.505029, acc.: 82.03%] [Generator loss: 0.873513]\n",
      "6703 [Discriminator loss: 0.982736, acc.: 35.16%] [Generator loss: 0.953914]\n",
      "6704 [Discriminator loss: 0.642695, acc.: 65.62%] [Generator loss: 0.954192]\n",
      "6705 [Discriminator loss: 0.779515, acc.: 52.34%] [Generator loss: 1.026798]\n",
      "6706 [Discriminator loss: 0.734507, acc.: 52.34%] [Generator loss: 1.021919]\n",
      "6707 [Discriminator loss: 0.775065, acc.: 50.78%] [Generator loss: 1.065065]\n",
      "6708 [Discriminator loss: 0.566054, acc.: 67.19%] [Generator loss: 0.800160]\n",
      "6709 [Discriminator loss: 0.775425, acc.: 57.03%] [Generator loss: 0.961833]\n",
      "6710 [Discriminator loss: 0.667214, acc.: 60.94%] [Generator loss: 1.193777]\n",
      "6711 [Discriminator loss: 0.474298, acc.: 81.25%] [Generator loss: 1.051756]\n",
      "6712 [Discriminator loss: 0.510267, acc.: 77.34%] [Generator loss: 0.845999]\n",
      "6713 [Discriminator loss: 0.781456, acc.: 51.56%] [Generator loss: 0.907412]\n",
      "6714 [Discriminator loss: 0.584912, acc.: 69.53%] [Generator loss: 0.946200]\n",
      "6715 [Discriminator loss: 0.495450, acc.: 76.56%] [Generator loss: 0.903932]\n",
      "6716 [Discriminator loss: 0.628291, acc.: 64.06%] [Generator loss: 0.788044]\n",
      "6717 [Discriminator loss: 0.856581, acc.: 48.44%] [Generator loss: 0.684159]\n",
      "6718 [Discriminator loss: 0.550701, acc.: 72.66%] [Generator loss: 0.605449]\n",
      "6719 [Discriminator loss: 0.754802, acc.: 51.56%] [Generator loss: 0.956450]\n",
      "6720 [Discriminator loss: 0.548451, acc.: 76.56%] [Generator loss: 1.317780]\n",
      "6721 [Discriminator loss: 0.743911, acc.: 49.22%] [Generator loss: 1.231208]\n",
      "6722 [Discriminator loss: 0.607316, acc.: 66.41%] [Generator loss: 1.097978]\n",
      "6723 [Discriminator loss: 0.516697, acc.: 77.34%] [Generator loss: 0.933495]\n",
      "6724 [Discriminator loss: 0.651920, acc.: 60.16%] [Generator loss: 0.954037]\n",
      "6725 [Discriminator loss: 0.619965, acc.: 70.31%] [Generator loss: 0.898066]\n",
      "6726 [Discriminator loss: 0.750393, acc.: 53.91%] [Generator loss: 0.867299]\n",
      "6727 [Discriminator loss: 0.574565, acc.: 71.09%] [Generator loss: 0.859551]\n",
      "6728 [Discriminator loss: 0.628277, acc.: 66.41%] [Generator loss: 0.865775]\n",
      "6729 [Discriminator loss: 0.597308, acc.: 70.31%] [Generator loss: 1.011675]\n",
      "6730 [Discriminator loss: 0.925124, acc.: 41.41%] [Generator loss: 0.726801]\n",
      "6731 [Discriminator loss: 0.647568, acc.: 61.72%] [Generator loss: 0.898097]\n",
      "6732 [Discriminator loss: 0.568038, acc.: 70.31%] [Generator loss: 1.058985]\n",
      "6733 [Discriminator loss: 0.511777, acc.: 76.56%] [Generator loss: 0.932878]\n",
      "6734 [Discriminator loss: 0.617190, acc.: 66.41%] [Generator loss: 0.782039]\n",
      "6735 [Discriminator loss: 0.779318, acc.: 50.00%] [Generator loss: 0.905235]\n",
      "6736 [Discriminator loss: 0.610748, acc.: 66.41%] [Generator loss: 1.201614]\n",
      "6737 [Discriminator loss: 0.758761, acc.: 55.47%] [Generator loss: 1.381672]\n",
      "6738 [Discriminator loss: 0.686254, acc.: 59.38%] [Generator loss: 0.914297]\n",
      "6739 [Discriminator loss: 0.767748, acc.: 50.00%] [Generator loss: 1.158860]\n",
      "6740 [Discriminator loss: 0.730946, acc.: 55.47%] [Generator loss: 1.072512]\n",
      "6741 [Discriminator loss: 0.787948, acc.: 50.78%] [Generator loss: 0.941640]\n",
      "6742 [Discriminator loss: 0.578721, acc.: 76.56%] [Generator loss: 0.754084]\n",
      "6743 [Discriminator loss: 0.614007, acc.: 67.19%] [Generator loss: 0.787406]\n",
      "6744 [Discriminator loss: 0.684620, acc.: 60.16%] [Generator loss: 0.907913]\n",
      "6745 [Discriminator loss: 0.741850, acc.: 53.12%] [Generator loss: 1.545519]\n",
      "6746 [Discriminator loss: 0.662851, acc.: 57.81%] [Generator loss: 1.443139]\n",
      "6747 [Discriminator loss: 0.721949, acc.: 52.34%] [Generator loss: 1.270014]\n",
      "6748 [Discriminator loss: 0.561212, acc.: 69.53%] [Generator loss: 1.180121]\n",
      "6749 [Discriminator loss: 0.541585, acc.: 78.12%] [Generator loss: 0.861937]\n",
      "6750 [Discriminator loss: 0.743917, acc.: 48.44%] [Generator loss: 1.101520]\n",
      "6751 [Discriminator loss: 0.629677, acc.: 69.53%] [Generator loss: 1.162354]\n",
      "6752 [Discriminator loss: 0.735518, acc.: 54.69%] [Generator loss: 1.068589]\n",
      "6753 [Discriminator loss: 0.718110, acc.: 60.16%] [Generator loss: 1.173135]\n",
      "6754 [Discriminator loss: 0.594761, acc.: 71.09%] [Generator loss: 0.785695]\n",
      "6755 [Discriminator loss: 0.633057, acc.: 60.94%] [Generator loss: 1.007183]\n",
      "6756 [Discriminator loss: 0.628521, acc.: 64.84%] [Generator loss: 0.970094]\n",
      "6757 [Discriminator loss: 0.644862, acc.: 63.28%] [Generator loss: 1.021387]\n",
      "6758 [Discriminator loss: 0.620228, acc.: 71.09%] [Generator loss: 0.960367]\n",
      "6759 [Discriminator loss: 0.672890, acc.: 57.03%] [Generator loss: 0.928738]\n",
      "6760 [Discriminator loss: 0.664909, acc.: 61.72%] [Generator loss: 0.796297]\n",
      "6761 [Discriminator loss: 0.687438, acc.: 55.47%] [Generator loss: 0.780316]\n",
      "6762 [Discriminator loss: 0.657751, acc.: 60.94%] [Generator loss: 0.916700]\n",
      "6763 [Discriminator loss: 0.888756, acc.: 34.38%] [Generator loss: 1.088801]\n",
      "6764 [Discriminator loss: 0.704340, acc.: 57.81%] [Generator loss: 1.224419]\n",
      "6765 [Discriminator loss: 0.632931, acc.: 65.62%] [Generator loss: 1.445961]\n",
      "6766 [Discriminator loss: 0.666280, acc.: 62.50%] [Generator loss: 0.938951]\n",
      "6767 [Discriminator loss: 0.734521, acc.: 46.88%] [Generator loss: 0.909258]\n",
      "6768 [Discriminator loss: 0.641267, acc.: 61.72%] [Generator loss: 0.994260]\n",
      "6769 [Discriminator loss: 0.805747, acc.: 50.78%] [Generator loss: 0.995582]\n",
      "6770 [Discriminator loss: 0.571350, acc.: 70.31%] [Generator loss: 1.072664]\n",
      "6771 [Discriminator loss: 0.614675, acc.: 65.62%] [Generator loss: 1.086287]\n",
      "6772 [Discriminator loss: 0.493431, acc.: 76.56%] [Generator loss: 1.236509]\n",
      "6773 [Discriminator loss: 0.567455, acc.: 68.75%] [Generator loss: 0.997806]\n",
      "6774 [Discriminator loss: 0.462799, acc.: 83.59%] [Generator loss: 1.088677]\n",
      "6775 [Discriminator loss: 0.660069, acc.: 62.50%] [Generator loss: 1.191834]\n",
      "6776 [Discriminator loss: 0.654998, acc.: 60.94%] [Generator loss: 0.997685]\n",
      "6777 [Discriminator loss: 0.501210, acc.: 79.69%] [Generator loss: 0.992759]\n",
      "6778 [Discriminator loss: 0.752178, acc.: 51.56%] [Generator loss: 0.964338]\n",
      "6779 [Discriminator loss: 0.659051, acc.: 62.50%] [Generator loss: 1.193305]\n",
      "6780 [Discriminator loss: 0.636235, acc.: 66.41%] [Generator loss: 1.403970]\n",
      "6781 [Discriminator loss: 0.872574, acc.: 43.75%] [Generator loss: 1.696241]\n",
      "6782 [Discriminator loss: 0.618965, acc.: 67.97%] [Generator loss: 1.218537]\n",
      "6783 [Discriminator loss: 0.743332, acc.: 55.47%] [Generator loss: 0.757845]\n",
      "6784 [Discriminator loss: 0.728375, acc.: 58.59%] [Generator loss: 1.131127]\n",
      "6785 [Discriminator loss: 0.693620, acc.: 56.25%] [Generator loss: 0.917990]\n",
      "6786 [Discriminator loss: 0.822646, acc.: 51.56%] [Generator loss: 0.746750]\n",
      "6787 [Discriminator loss: 0.859768, acc.: 46.88%] [Generator loss: 0.767327]\n",
      "6788 [Discriminator loss: 0.660840, acc.: 61.72%] [Generator loss: 1.040469]\n",
      "6789 [Discriminator loss: 0.827860, acc.: 48.44%] [Generator loss: 0.872701]\n",
      "6790 [Discriminator loss: 0.620267, acc.: 68.75%] [Generator loss: 0.859984]\n",
      "6791 [Discriminator loss: 0.635451, acc.: 64.84%] [Generator loss: 0.816672]\n",
      "6792 [Discriminator loss: 0.624416, acc.: 65.62%] [Generator loss: 0.851399]\n",
      "6793 [Discriminator loss: 0.569566, acc.: 72.66%] [Generator loss: 1.007648]\n",
      "6794 [Discriminator loss: 0.638707, acc.: 61.72%] [Generator loss: 0.884496]\n",
      "6795 [Discriminator loss: 0.772288, acc.: 53.12%] [Generator loss: 0.871831]\n",
      "6796 [Discriminator loss: 0.754817, acc.: 50.00%] [Generator loss: 0.852190]\n",
      "6797 [Discriminator loss: 0.487082, acc.: 77.34%] [Generator loss: 0.942462]\n",
      "6798 [Discriminator loss: 0.611658, acc.: 70.31%] [Generator loss: 0.919631]\n",
      "6799 [Discriminator loss: 0.659924, acc.: 63.28%] [Generator loss: 0.861691]\n",
      "6800 [Discriminator loss: 0.627793, acc.: 66.41%] [Generator loss: 0.977377]\n",
      "6801 [Discriminator loss: 0.602013, acc.: 69.53%] [Generator loss: 1.018148]\n",
      "6802 [Discriminator loss: 0.705319, acc.: 57.81%] [Generator loss: 1.275107]\n",
      "6803 [Discriminator loss: 0.580254, acc.: 67.19%] [Generator loss: 0.988954]\n",
      "6804 [Discriminator loss: 0.688674, acc.: 59.38%] [Generator loss: 1.025499]\n",
      "6805 [Discriminator loss: 0.673709, acc.: 64.84%] [Generator loss: 0.952711]\n",
      "6806 [Discriminator loss: 0.699050, acc.: 56.25%] [Generator loss: 1.026329]\n",
      "6807 [Discriminator loss: 0.752381, acc.: 54.69%] [Generator loss: 0.900905]\n",
      "6808 [Discriminator loss: 0.648630, acc.: 60.94%] [Generator loss: 0.788967]\n",
      "6809 [Discriminator loss: 0.632862, acc.: 66.41%] [Generator loss: 1.006533]\n",
      "6810 [Discriminator loss: 0.504845, acc.: 74.22%] [Generator loss: 1.070438]\n",
      "6811 [Discriminator loss: 0.727020, acc.: 57.81%] [Generator loss: 1.001003]\n",
      "6812 [Discriminator loss: 0.743637, acc.: 53.91%] [Generator loss: 1.138148]\n",
      "6813 [Discriminator loss: 0.752124, acc.: 55.47%] [Generator loss: 0.835593]\n",
      "6814 [Discriminator loss: 0.804635, acc.: 47.66%] [Generator loss: 0.836575]\n",
      "6815 [Discriminator loss: 0.783742, acc.: 51.56%] [Generator loss: 0.849128]\n",
      "6816 [Discriminator loss: 0.472408, acc.: 81.25%] [Generator loss: 0.918867]\n",
      "6817 [Discriminator loss: 0.601546, acc.: 72.66%] [Generator loss: 0.757988]\n",
      "6818 [Discriminator loss: 0.856802, acc.: 46.88%] [Generator loss: 0.937919]\n",
      "6819 [Discriminator loss: 0.643502, acc.: 66.41%] [Generator loss: 1.054638]\n",
      "6820 [Discriminator loss: 0.715436, acc.: 56.25%] [Generator loss: 0.864647]\n",
      "6821 [Discriminator loss: 0.981252, acc.: 25.78%] [Generator loss: 0.906443]\n",
      "6822 [Discriminator loss: 0.593155, acc.: 69.53%] [Generator loss: 0.835614]\n",
      "6823 [Discriminator loss: 0.846341, acc.: 56.25%] [Generator loss: 1.207043]\n",
      "6824 [Discriminator loss: 0.612031, acc.: 69.53%] [Generator loss: 1.154079]\n",
      "6825 [Discriminator loss: 0.758092, acc.: 52.34%] [Generator loss: 1.103286]\n",
      "6826 [Discriminator loss: 0.656918, acc.: 62.50%] [Generator loss: 0.766276]\n",
      "6827 [Discriminator loss: 0.634601, acc.: 64.84%] [Generator loss: 0.890241]\n",
      "6828 [Discriminator loss: 0.457919, acc.: 82.03%] [Generator loss: 0.890416]\n",
      "6829 [Discriminator loss: 0.889037, acc.: 38.28%] [Generator loss: 0.785218]\n",
      "6830 [Discriminator loss: 0.579132, acc.: 74.22%] [Generator loss: 0.791043]\n",
      "6831 [Discriminator loss: 0.662998, acc.: 58.59%] [Generator loss: 1.129597]\n",
      "6832 [Discriminator loss: 0.618429, acc.: 67.97%] [Generator loss: 1.175503]\n",
      "6833 [Discriminator loss: 0.690625, acc.: 54.69%] [Generator loss: 1.324650]\n",
      "6834 [Discriminator loss: 0.664904, acc.: 62.50%] [Generator loss: 1.046376]\n",
      "6835 [Discriminator loss: 0.854781, acc.: 45.31%] [Generator loss: 0.825838]\n",
      "6836 [Discriminator loss: 0.740337, acc.: 51.56%] [Generator loss: 1.242408]\n",
      "6837 [Discriminator loss: 0.654662, acc.: 60.94%] [Generator loss: 1.225129]\n",
      "6838 [Discriminator loss: 0.639117, acc.: 63.28%] [Generator loss: 1.190915]\n",
      "6839 [Discriminator loss: 0.511119, acc.: 74.22%] [Generator loss: 0.946046]\n",
      "6840 [Discriminator loss: 0.442272, acc.: 85.16%] [Generator loss: 0.665631]\n",
      "6841 [Discriminator loss: 0.825626, acc.: 44.53%] [Generator loss: 0.662221]\n",
      "6842 [Discriminator loss: 0.507200, acc.: 74.22%] [Generator loss: 0.792534]\n",
      "6843 [Discriminator loss: 0.697432, acc.: 60.16%] [Generator loss: 0.850745]\n",
      "6844 [Discriminator loss: 0.538173, acc.: 74.22%] [Generator loss: 0.837237]\n",
      "6845 [Discriminator loss: 0.307882, acc.: 94.53%] [Generator loss: 0.698706]\n",
      "6846 [Discriminator loss: 0.538588, acc.: 73.44%] [Generator loss: 0.829663]\n",
      "6847 [Discriminator loss: 0.565170, acc.: 67.97%] [Generator loss: 0.895070]\n",
      "6848 [Discriminator loss: 0.708530, acc.: 64.06%] [Generator loss: 0.691133]\n",
      "6849 [Discriminator loss: 0.447044, acc.: 83.59%] [Generator loss: 1.088203]\n",
      "6850 [Discriminator loss: 0.756192, acc.: 53.91%] [Generator loss: 0.968596]\n",
      "6851 [Discriminator loss: 0.869147, acc.: 37.50%] [Generator loss: 0.995308]\n",
      "6852 [Discriminator loss: 0.981597, acc.: 29.69%] [Generator loss: 1.200568]\n",
      "6853 [Discriminator loss: 0.544624, acc.: 70.31%] [Generator loss: 1.017691]\n",
      "6854 [Discriminator loss: 0.852198, acc.: 42.97%] [Generator loss: 0.755911]\n",
      "6855 [Discriminator loss: 0.610731, acc.: 68.75%] [Generator loss: 0.835626]\n",
      "6856 [Discriminator loss: 0.809817, acc.: 46.09%] [Generator loss: 0.919283]\n",
      "6857 [Discriminator loss: 0.376446, acc.: 89.84%] [Generator loss: 0.876026]\n",
      "6858 [Discriminator loss: 0.500532, acc.: 80.47%] [Generator loss: 0.616329]\n",
      "6859 [Discriminator loss: 0.555474, acc.: 71.88%] [Generator loss: 0.591463]\n",
      "6860 [Discriminator loss: 0.491867, acc.: 83.59%] [Generator loss: 0.567304]\n",
      "6861 [Discriminator loss: 0.591496, acc.: 66.41%] [Generator loss: 0.612643]\n",
      "6862 [Discriminator loss: 0.416312, acc.: 89.06%] [Generator loss: 0.620031]\n",
      "6863 [Discriminator loss: 0.534625, acc.: 74.22%] [Generator loss: 0.605377]\n",
      "6864 [Discriminator loss: 0.882158, acc.: 56.25%] [Generator loss: 1.339199]\n",
      "6865 [Discriminator loss: 0.560153, acc.: 72.66%] [Generator loss: 1.311710]\n",
      "6866 [Discriminator loss: 1.035368, acc.: 39.06%] [Generator loss: 0.722906]\n",
      "6867 [Discriminator loss: 0.451287, acc.: 82.03%] [Generator loss: 0.725469]\n",
      "6868 [Discriminator loss: 0.631646, acc.: 66.41%] [Generator loss: 0.755856]\n",
      "6869 [Discriminator loss: 0.726079, acc.: 50.78%] [Generator loss: 0.886396]\n",
      "6870 [Discriminator loss: 0.610596, acc.: 70.31%] [Generator loss: 1.068505]\n",
      "6871 [Discriminator loss: 0.629045, acc.: 66.41%] [Generator loss: 1.090438]\n",
      "6872 [Discriminator loss: 0.712039, acc.: 53.91%] [Generator loss: 1.132229]\n",
      "6873 [Discriminator loss: 0.562826, acc.: 75.00%] [Generator loss: 1.092299]\n",
      "6874 [Discriminator loss: 0.813378, acc.: 44.53%] [Generator loss: 0.852069]\n",
      "6875 [Discriminator loss: 0.555585, acc.: 70.31%] [Generator loss: 0.947083]\n",
      "6876 [Discriminator loss: 0.602781, acc.: 66.41%] [Generator loss: 0.986853]\n",
      "6877 [Discriminator loss: 0.553961, acc.: 71.88%] [Generator loss: 0.870011]\n",
      "6878 [Discriminator loss: 0.534379, acc.: 79.69%] [Generator loss: 0.870520]\n",
      "6879 [Discriminator loss: 0.634077, acc.: 68.75%] [Generator loss: 0.793657]\n",
      "6880 [Discriminator loss: 0.591176, acc.: 66.41%] [Generator loss: 0.723232]\n",
      "6881 [Discriminator loss: 0.783120, acc.: 55.47%] [Generator loss: 0.953806]\n",
      "6882 [Discriminator loss: 0.764854, acc.: 48.44%] [Generator loss: 1.124123]\n",
      "6883 [Discriminator loss: 0.664008, acc.: 60.16%] [Generator loss: 0.972233]\n",
      "6884 [Discriminator loss: 0.517212, acc.: 73.44%] [Generator loss: 0.832974]\n",
      "6885 [Discriminator loss: 0.682823, acc.: 61.72%] [Generator loss: 0.935483]\n",
      "6886 [Discriminator loss: 0.610426, acc.: 66.41%] [Generator loss: 1.051638]\n",
      "6887 [Discriminator loss: 0.621059, acc.: 67.19%] [Generator loss: 1.116240]\n",
      "6888 [Discriminator loss: 0.693438, acc.: 58.59%] [Generator loss: 0.982279]\n",
      "6889 [Discriminator loss: 0.473794, acc.: 82.03%] [Generator loss: 1.064443]\n",
      "6890 [Discriminator loss: 0.591960, acc.: 68.75%] [Generator loss: 1.041690]\n",
      "6891 [Discriminator loss: 0.673706, acc.: 67.19%] [Generator loss: 0.992368]\n",
      "6892 [Discriminator loss: 0.727006, acc.: 53.12%] [Generator loss: 1.097016]\n",
      "6893 [Discriminator loss: 0.490566, acc.: 75.78%] [Generator loss: 1.042186]\n",
      "6894 [Discriminator loss: 0.688090, acc.: 58.59%] [Generator loss: 0.746328]\n",
      "6895 [Discriminator loss: 0.589796, acc.: 70.31%] [Generator loss: 0.868245]\n",
      "6896 [Discriminator loss: 0.535369, acc.: 71.09%] [Generator loss: 0.937539]\n",
      "6897 [Discriminator loss: 0.585778, acc.: 71.09%] [Generator loss: 0.629770]\n",
      "6898 [Discriminator loss: 0.589314, acc.: 67.97%] [Generator loss: 0.762590]\n",
      "6899 [Discriminator loss: 0.735591, acc.: 57.81%] [Generator loss: 1.161467]\n",
      "6900 [Discriminator loss: 0.691805, acc.: 62.50%] [Generator loss: 1.130998]\n",
      "6901 [Discriminator loss: 0.634741, acc.: 67.97%] [Generator loss: 0.842315]\n",
      "6902 [Discriminator loss: 0.675509, acc.: 61.72%] [Generator loss: 0.832801]\n",
      "6903 [Discriminator loss: 0.641633, acc.: 65.62%] [Generator loss: 0.986430]\n",
      "6904 [Discriminator loss: 0.792376, acc.: 53.12%] [Generator loss: 1.039958]\n",
      "6905 [Discriminator loss: 0.579540, acc.: 69.53%] [Generator loss: 1.072000]\n",
      "6906 [Discriminator loss: 0.621118, acc.: 70.31%] [Generator loss: 1.440233]\n",
      "6907 [Discriminator loss: 0.607003, acc.: 68.75%] [Generator loss: 1.098326]\n",
      "6908 [Discriminator loss: 0.516864, acc.: 80.47%] [Generator loss: 0.972419]\n",
      "6909 [Discriminator loss: 0.455778, acc.: 78.91%] [Generator loss: 0.809191]\n",
      "6910 [Discriminator loss: 0.627555, acc.: 60.94%] [Generator loss: 1.011053]\n",
      "6911 [Discriminator loss: 0.590777, acc.: 67.19%] [Generator loss: 1.012083]\n",
      "6912 [Discriminator loss: 0.566401, acc.: 69.53%] [Generator loss: 0.867850]\n",
      "6913 [Discriminator loss: 0.483746, acc.: 75.00%] [Generator loss: 0.693561]\n",
      "6914 [Discriminator loss: 0.803871, acc.: 56.25%] [Generator loss: 0.756601]\n",
      "6915 [Discriminator loss: 0.518058, acc.: 73.44%] [Generator loss: 0.873224]\n",
      "6916 [Discriminator loss: 0.852245, acc.: 45.31%] [Generator loss: 0.601712]\n",
      "6917 [Discriminator loss: 0.643315, acc.: 66.41%] [Generator loss: 0.766043]\n",
      "6918 [Discriminator loss: 1.186486, acc.: 24.22%] [Generator loss: 0.982348]\n",
      "6919 [Discriminator loss: 0.491454, acc.: 75.00%] [Generator loss: 1.311144]\n",
      "6920 [Discriminator loss: 0.631620, acc.: 66.41%] [Generator loss: 1.033782]\n",
      "6921 [Discriminator loss: 0.547130, acc.: 73.44%] [Generator loss: 1.030421]\n",
      "6922 [Discriminator loss: 0.655313, acc.: 57.81%] [Generator loss: 0.959593]\n",
      "6923 [Discriminator loss: 0.537825, acc.: 71.88%] [Generator loss: 1.079719]\n",
      "6924 [Discriminator loss: 0.522389, acc.: 74.22%] [Generator loss: 1.180506]\n",
      "6925 [Discriminator loss: 0.668370, acc.: 60.94%] [Generator loss: 1.263138]\n",
      "6926 [Discriminator loss: 0.725453, acc.: 52.34%] [Generator loss: 1.024740]\n",
      "6927 [Discriminator loss: 0.611148, acc.: 60.16%] [Generator loss: 1.204805]\n",
      "6928 [Discriminator loss: 0.541656, acc.: 71.09%] [Generator loss: 0.988581]\n",
      "6929 [Discriminator loss: 0.712966, acc.: 59.38%] [Generator loss: 0.899842]\n",
      "6930 [Discriminator loss: 0.587250, acc.: 77.34%] [Generator loss: 0.827679]\n",
      "6931 [Discriminator loss: 0.636820, acc.: 62.50%] [Generator loss: 0.661649]\n",
      "6932 [Discriminator loss: 0.906937, acc.: 43.75%] [Generator loss: 0.722667]\n",
      "6933 [Discriminator loss: 0.620341, acc.: 63.28%] [Generator loss: 0.832479]\n",
      "6934 [Discriminator loss: 0.573181, acc.: 68.75%] [Generator loss: 0.992729]\n",
      "6935 [Discriminator loss: 0.620034, acc.: 71.09%] [Generator loss: 0.763308]\n",
      "6936 [Discriminator loss: 0.598182, acc.: 74.22%] [Generator loss: 0.838523]\n",
      "6937 [Discriminator loss: 0.907975, acc.: 40.62%] [Generator loss: 0.807606]\n",
      "6938 [Discriminator loss: 0.509160, acc.: 82.03%] [Generator loss: 0.754089]\n",
      "6939 [Discriminator loss: 0.726134, acc.: 57.03%] [Generator loss: 0.701850]\n",
      "6940 [Discriminator loss: 0.681616, acc.: 58.59%] [Generator loss: 1.189085]\n",
      "6941 [Discriminator loss: 0.651292, acc.: 65.62%] [Generator loss: 1.400931]\n",
      "6942 [Discriminator loss: 0.710513, acc.: 61.72%] [Generator loss: 1.004395]\n",
      "6943 [Discriminator loss: 0.945568, acc.: 32.03%] [Generator loss: 0.844338]\n",
      "6944 [Discriminator loss: 0.792684, acc.: 41.41%] [Generator loss: 1.246139]\n",
      "6945 [Discriminator loss: 0.879462, acc.: 50.78%] [Generator loss: 0.956946]\n",
      "6946 [Discriminator loss: 0.801014, acc.: 52.34%] [Generator loss: 0.860011]\n",
      "6947 [Discriminator loss: 0.701284, acc.: 63.28%] [Generator loss: 0.701191]\n",
      "6948 [Discriminator loss: 0.705685, acc.: 58.59%] [Generator loss: 0.878867]\n",
      "6949 [Discriminator loss: 0.737708, acc.: 58.59%] [Generator loss: 0.935252]\n",
      "6950 [Discriminator loss: 0.665200, acc.: 56.25%] [Generator loss: 1.006489]\n",
      "6951 [Discriminator loss: 0.652927, acc.: 61.72%] [Generator loss: 1.097503]\n",
      "6952 [Discriminator loss: 0.703432, acc.: 57.81%] [Generator loss: 1.055793]\n",
      "6953 [Discriminator loss: 0.482611, acc.: 78.91%] [Generator loss: 1.061529]\n",
      "6954 [Discriminator loss: 0.606509, acc.: 67.97%] [Generator loss: 0.890955]\n",
      "6955 [Discriminator loss: 0.663212, acc.: 64.06%] [Generator loss: 0.744612]\n",
      "6956 [Discriminator loss: 0.798590, acc.: 50.78%] [Generator loss: 0.911768]\n",
      "6957 [Discriminator loss: 0.617934, acc.: 64.06%] [Generator loss: 0.968068]\n",
      "6958 [Discriminator loss: 0.631521, acc.: 67.97%] [Generator loss: 1.073585]\n",
      "6959 [Discriminator loss: 1.296762, acc.: 32.81%] [Generator loss: 1.577617]\n",
      "6960 [Discriminator loss: 0.628633, acc.: 60.16%] [Generator loss: 1.462561]\n",
      "6961 [Discriminator loss: 1.133479, acc.: 37.50%] [Generator loss: 0.905145]\n",
      "6962 [Discriminator loss: 0.741812, acc.: 60.16%] [Generator loss: 0.793070]\n",
      "6963 [Discriminator loss: 0.545667, acc.: 80.47%] [Generator loss: 0.794313]\n",
      "6964 [Discriminator loss: 0.713909, acc.: 49.22%] [Generator loss: 0.884569]\n",
      "6965 [Discriminator loss: 0.662562, acc.: 61.72%] [Generator loss: 0.711729]\n",
      "6966 [Discriminator loss: 0.814401, acc.: 47.66%] [Generator loss: 1.210548]\n",
      "6967 [Discriminator loss: 0.787677, acc.: 51.56%] [Generator loss: 1.121342]\n",
      "6968 [Discriminator loss: 0.654681, acc.: 58.59%] [Generator loss: 1.041258]\n",
      "6969 [Discriminator loss: 0.618289, acc.: 63.28%] [Generator loss: 0.966545]\n",
      "6970 [Discriminator loss: 0.601561, acc.: 66.41%] [Generator loss: 0.984219]\n",
      "6971 [Discriminator loss: 0.786156, acc.: 46.09%] [Generator loss: 0.840865]\n",
      "6972 [Discriminator loss: 0.666185, acc.: 57.81%] [Generator loss: 0.929731]\n",
      "6973 [Discriminator loss: 0.627691, acc.: 64.84%] [Generator loss: 0.964433]\n",
      "6974 [Discriminator loss: 0.622421, acc.: 69.53%] [Generator loss: 0.861745]\n",
      "6975 [Discriminator loss: 0.614956, acc.: 70.31%] [Generator loss: 0.952905]\n",
      "6976 [Discriminator loss: 0.609537, acc.: 66.41%] [Generator loss: 0.928798]\n",
      "6977 [Discriminator loss: 0.599595, acc.: 75.78%] [Generator loss: 0.861417]\n",
      "6978 [Discriminator loss: 0.512002, acc.: 77.34%] [Generator loss: 0.873363]\n",
      "6979 [Discriminator loss: 0.633072, acc.: 64.06%] [Generator loss: 0.907772]\n",
      "6980 [Discriminator loss: 0.822671, acc.: 50.00%] [Generator loss: 0.830626]\n",
      "6981 [Discriminator loss: 0.613451, acc.: 66.41%] [Generator loss: 0.978794]\n",
      "6982 [Discriminator loss: 0.660732, acc.: 63.28%] [Generator loss: 0.879003]\n",
      "6983 [Discriminator loss: 0.542310, acc.: 78.91%] [Generator loss: 0.757873]\n",
      "6984 [Discriminator loss: 0.695191, acc.: 58.59%] [Generator loss: 1.027947]\n",
      "6985 [Discriminator loss: 0.635735, acc.: 66.41%] [Generator loss: 0.879907]\n",
      "6986 [Discriminator loss: 0.649318, acc.: 67.19%] [Generator loss: 0.848159]\n",
      "6987 [Discriminator loss: 0.743610, acc.: 50.78%] [Generator loss: 0.869062]\n",
      "6988 [Discriminator loss: 0.721753, acc.: 56.25%] [Generator loss: 1.063093]\n",
      "6989 [Discriminator loss: 0.612454, acc.: 62.50%] [Generator loss: 1.051202]\n",
      "6990 [Discriminator loss: 0.729384, acc.: 54.69%] [Generator loss: 1.336676]\n",
      "6991 [Discriminator loss: 0.679245, acc.: 58.59%] [Generator loss: 1.625144]\n",
      "6992 [Discriminator loss: 0.707733, acc.: 54.69%] [Generator loss: 1.321368]\n",
      "6993 [Discriminator loss: 0.606100, acc.: 71.88%] [Generator loss: 0.860397]\n",
      "6994 [Discriminator loss: 0.682589, acc.: 53.91%] [Generator loss: 0.795310]\n",
      "6995 [Discriminator loss: 0.662459, acc.: 58.59%] [Generator loss: 0.727353]\n",
      "6996 [Discriminator loss: 0.517287, acc.: 74.22%] [Generator loss: 0.790913]\n",
      "6997 [Discriminator loss: 0.678681, acc.: 59.38%] [Generator loss: 0.662440]\n",
      "6998 [Discriminator loss: 0.616109, acc.: 67.19%] [Generator loss: 0.713907]\n",
      "6999 [Discriminator loss: 0.597689, acc.: 64.06%] [Generator loss: 0.856476]\n",
      "7000 [Discriminator loss: 0.640247, acc.: 65.62%] [Generator loss: 0.789399]\n",
      "7001 [Discriminator loss: 0.911558, acc.: 41.41%] [Generator loss: 1.015684]\n",
      "7002 [Discriminator loss: 0.617365, acc.: 66.41%] [Generator loss: 1.077406]\n",
      "7003 [Discriminator loss: 0.610799, acc.: 67.19%] [Generator loss: 0.987887]\n",
      "7004 [Discriminator loss: 0.718989, acc.: 53.12%] [Generator loss: 0.859943]\n",
      "7005 [Discriminator loss: 0.763239, acc.: 55.47%] [Generator loss: 0.746381]\n",
      "7006 [Discriminator loss: 0.844015, acc.: 46.09%] [Generator loss: 0.967082]\n",
      "7007 [Discriminator loss: 0.560742, acc.: 78.12%] [Generator loss: 1.185971]\n",
      "7008 [Discriminator loss: 0.679814, acc.: 59.38%] [Generator loss: 1.236292]\n",
      "7009 [Discriminator loss: 0.601143, acc.: 67.19%] [Generator loss: 1.170269]\n",
      "7010 [Discriminator loss: 0.709709, acc.: 62.50%] [Generator loss: 0.966091]\n",
      "7011 [Discriminator loss: 0.615428, acc.: 64.06%] [Generator loss: 0.887227]\n",
      "7012 [Discriminator loss: 0.785397, acc.: 42.97%] [Generator loss: 0.874576]\n",
      "7013 [Discriminator loss: 0.611175, acc.: 62.50%] [Generator loss: 1.061063]\n",
      "7014 [Discriminator loss: 0.674132, acc.: 64.84%] [Generator loss: 0.895590]\n",
      "7015 [Discriminator loss: 0.679384, acc.: 60.16%] [Generator loss: 0.821420]\n",
      "7016 [Discriminator loss: 0.560714, acc.: 75.00%] [Generator loss: 0.896936]\n",
      "7017 [Discriminator loss: 0.599636, acc.: 67.19%] [Generator loss: 0.924792]\n",
      "7018 [Discriminator loss: 0.648809, acc.: 62.50%] [Generator loss: 1.039565]\n",
      "7019 [Discriminator loss: 0.603635, acc.: 67.19%] [Generator loss: 1.006069]\n",
      "7020 [Discriminator loss: 0.691548, acc.: 58.59%] [Generator loss: 0.887970]\n",
      "7021 [Discriminator loss: 0.700130, acc.: 61.72%] [Generator loss: 0.833323]\n",
      "7022 [Discriminator loss: 0.579666, acc.: 69.53%] [Generator loss: 0.939662]\n",
      "7023 [Discriminator loss: 0.549844, acc.: 78.12%] [Generator loss: 1.087943]\n",
      "7024 [Discriminator loss: 0.690331, acc.: 58.59%] [Generator loss: 1.087770]\n",
      "7025 [Discriminator loss: 0.640246, acc.: 64.06%] [Generator loss: 0.950654]\n",
      "7026 [Discriminator loss: 0.750303, acc.: 57.03%] [Generator loss: 0.931960]\n",
      "7027 [Discriminator loss: 0.837733, acc.: 43.75%] [Generator loss: 1.178555]\n",
      "7028 [Discriminator loss: 0.515329, acc.: 73.44%] [Generator loss: 1.118534]\n",
      "7029 [Discriminator loss: 0.759360, acc.: 50.00%] [Generator loss: 0.965933]\n",
      "7030 [Discriminator loss: 0.778244, acc.: 53.12%] [Generator loss: 0.950400]\n",
      "7031 [Discriminator loss: 0.804544, acc.: 53.12%] [Generator loss: 1.029428]\n",
      "7032 [Discriminator loss: 0.624898, acc.: 62.50%] [Generator loss: 0.784364]\n",
      "7033 [Discriminator loss: 0.761576, acc.: 44.53%] [Generator loss: 0.824692]\n",
      "7034 [Discriminator loss: 0.835625, acc.: 51.56%] [Generator loss: 0.852108]\n",
      "7035 [Discriminator loss: 0.709373, acc.: 58.59%] [Generator loss: 1.230608]\n",
      "7036 [Discriminator loss: 0.732752, acc.: 50.00%] [Generator loss: 1.083138]\n",
      "7037 [Discriminator loss: 0.640496, acc.: 61.72%] [Generator loss: 0.862624]\n",
      "7038 [Discriminator loss: 0.720919, acc.: 55.47%] [Generator loss: 0.803342]\n",
      "7039 [Discriminator loss: 0.758814, acc.: 50.78%] [Generator loss: 0.926066]\n",
      "7040 [Discriminator loss: 0.722770, acc.: 57.03%] [Generator loss: 1.019967]\n",
      "7041 [Discriminator loss: 0.643755, acc.: 67.19%] [Generator loss: 1.034763]\n",
      "7042 [Discriminator loss: 0.671734, acc.: 60.94%] [Generator loss: 0.966282]\n",
      "7043 [Discriminator loss: 0.597813, acc.: 70.31%] [Generator loss: 1.009638]\n",
      "7044 [Discriminator loss: 0.567413, acc.: 71.88%] [Generator loss: 0.849392]\n",
      "7045 [Discriminator loss: 0.756785, acc.: 58.59%] [Generator loss: 0.981628]\n",
      "7046 [Discriminator loss: 0.439988, acc.: 79.69%] [Generator loss: 1.032451]\n",
      "7047 [Discriminator loss: 0.697939, acc.: 53.91%] [Generator loss: 0.912329]\n",
      "7048 [Discriminator loss: 0.704369, acc.: 57.03%] [Generator loss: 1.021253]\n",
      "7049 [Discriminator loss: 0.661240, acc.: 65.62%] [Generator loss: 0.876233]\n",
      "7050 [Discriminator loss: 0.772864, acc.: 50.78%] [Generator loss: 0.900372]\n",
      "7051 [Discriminator loss: 0.561105, acc.: 71.88%] [Generator loss: 0.849105]\n",
      "7052 [Discriminator loss: 0.828742, acc.: 43.75%] [Generator loss: 0.822292]\n",
      "7053 [Discriminator loss: 0.677308, acc.: 62.50%] [Generator loss: 0.983355]\n",
      "7054 [Discriminator loss: 0.899562, acc.: 46.09%] [Generator loss: 1.033380]\n",
      "7055 [Discriminator loss: 0.641442, acc.: 60.94%] [Generator loss: 0.962636]\n",
      "7056 [Discriminator loss: 0.794053, acc.: 49.22%] [Generator loss: 0.860428]\n",
      "7057 [Discriminator loss: 0.615445, acc.: 64.84%] [Generator loss: 0.886419]\n",
      "7058 [Discriminator loss: 0.799062, acc.: 53.12%] [Generator loss: 1.031402]\n",
      "7059 [Discriminator loss: 0.551672, acc.: 74.22%] [Generator loss: 1.095313]\n",
      "7060 [Discriminator loss: 0.601596, acc.: 64.06%] [Generator loss: 1.347881]\n",
      "7061 [Discriminator loss: 0.570218, acc.: 74.22%] [Generator loss: 1.262306]\n",
      "7062 [Discriminator loss: 0.465801, acc.: 78.91%] [Generator loss: 1.276137]\n",
      "7063 [Discriminator loss: 0.500980, acc.: 79.69%] [Generator loss: 1.178556]\n",
      "7064 [Discriminator loss: 0.610321, acc.: 70.31%] [Generator loss: 1.023060]\n",
      "7065 [Discriminator loss: 0.658179, acc.: 59.38%] [Generator loss: 0.811276]\n",
      "7066 [Discriminator loss: 0.583113, acc.: 71.88%] [Generator loss: 0.881717]\n",
      "7067 [Discriminator loss: 0.781534, acc.: 49.22%] [Generator loss: 0.765537]\n",
      "7068 [Discriminator loss: 0.600042, acc.: 71.88%] [Generator loss: 0.828578]\n",
      "7069 [Discriminator loss: 0.547373, acc.: 74.22%] [Generator loss: 0.814343]\n",
      "7070 [Discriminator loss: 0.497129, acc.: 78.12%] [Generator loss: 0.881250]\n",
      "7071 [Discriminator loss: 0.667689, acc.: 66.41%] [Generator loss: 0.859489]\n",
      "7072 [Discriminator loss: 0.701040, acc.: 59.38%] [Generator loss: 0.760406]\n",
      "7073 [Discriminator loss: 0.618414, acc.: 62.50%] [Generator loss: 0.973161]\n",
      "7074 [Discriminator loss: 0.635938, acc.: 65.62%] [Generator loss: 1.085932]\n",
      "7075 [Discriminator loss: 0.766312, acc.: 48.44%] [Generator loss: 0.917094]\n",
      "7076 [Discriminator loss: 0.715673, acc.: 55.47%] [Generator loss: 1.179106]\n",
      "7077 [Discriminator loss: 0.621047, acc.: 65.62%] [Generator loss: 1.192782]\n",
      "7078 [Discriminator loss: 0.640809, acc.: 65.62%] [Generator loss: 1.515343]\n",
      "7079 [Discriminator loss: 0.461474, acc.: 77.34%] [Generator loss: 1.306305]\n",
      "7080 [Discriminator loss: 0.549786, acc.: 69.53%] [Generator loss: 1.021912]\n",
      "7081 [Discriminator loss: 0.482722, acc.: 78.91%] [Generator loss: 0.923644]\n",
      "7082 [Discriminator loss: 0.712476, acc.: 57.03%] [Generator loss: 0.883334]\n",
      "7083 [Discriminator loss: 0.681157, acc.: 60.94%] [Generator loss: 0.850533]\n",
      "7084 [Discriminator loss: 0.607088, acc.: 73.44%] [Generator loss: 0.906648]\n",
      "7085 [Discriminator loss: 0.853824, acc.: 47.66%] [Generator loss: 0.866667]\n",
      "7086 [Discriminator loss: 0.578013, acc.: 67.97%] [Generator loss: 1.172706]\n",
      "7087 [Discriminator loss: 0.682906, acc.: 65.62%] [Generator loss: 0.785020]\n",
      "7088 [Discriminator loss: 0.810775, acc.: 45.31%] [Generator loss: 0.856977]\n",
      "7089 [Discriminator loss: 0.706906, acc.: 60.16%] [Generator loss: 1.057324]\n",
      "7090 [Discriminator loss: 0.562513, acc.: 72.66%] [Generator loss: 1.166436]\n",
      "7091 [Discriminator loss: 0.683387, acc.: 56.25%] [Generator loss: 1.371293]\n",
      "7092 [Discriminator loss: 0.643167, acc.: 60.94%] [Generator loss: 1.144115]\n",
      "7093 [Discriminator loss: 0.684536, acc.: 61.72%] [Generator loss: 0.876048]\n",
      "7094 [Discriminator loss: 0.599138, acc.: 71.88%] [Generator loss: 1.004241]\n",
      "7095 [Discriminator loss: 0.556009, acc.: 70.31%] [Generator loss: 0.903803]\n",
      "7096 [Discriminator loss: 0.641711, acc.: 65.62%] [Generator loss: 1.206407]\n",
      "7097 [Discriminator loss: 0.696707, acc.: 57.03%] [Generator loss: 0.872447]\n",
      "7098 [Discriminator loss: 0.646534, acc.: 60.94%] [Generator loss: 0.836777]\n",
      "7099 [Discriminator loss: 0.709528, acc.: 55.47%] [Generator loss: 0.779083]\n",
      "7100 [Discriminator loss: 0.783953, acc.: 50.00%] [Generator loss: 1.001839]\n",
      "7101 [Discriminator loss: 0.632531, acc.: 66.41%] [Generator loss: 0.781065]\n",
      "7102 [Discriminator loss: 0.528334, acc.: 75.00%] [Generator loss: 0.821060]\n",
      "7103 [Discriminator loss: 0.666034, acc.: 57.81%] [Generator loss: 0.911467]\n",
      "7104 [Discriminator loss: 0.546071, acc.: 70.31%] [Generator loss: 0.999827]\n",
      "7105 [Discriminator loss: 0.674670, acc.: 61.72%] [Generator loss: 1.028480]\n",
      "7106 [Discriminator loss: 0.575501, acc.: 73.44%] [Generator loss: 0.897417]\n",
      "7107 [Discriminator loss: 0.611528, acc.: 66.41%] [Generator loss: 0.880787]\n",
      "7108 [Discriminator loss: 0.562166, acc.: 71.09%] [Generator loss: 1.072655]\n",
      "7109 [Discriminator loss: 0.782060, acc.: 53.91%] [Generator loss: 0.927273]\n",
      "7110 [Discriminator loss: 0.625384, acc.: 60.94%] [Generator loss: 1.203205]\n",
      "7111 [Discriminator loss: 0.694633, acc.: 57.81%] [Generator loss: 1.031724]\n",
      "7112 [Discriminator loss: 0.980138, acc.: 39.84%] [Generator loss: 0.862355]\n",
      "7113 [Discriminator loss: 0.714706, acc.: 53.12%] [Generator loss: 0.987506]\n",
      "7114 [Discriminator loss: 0.621266, acc.: 65.62%] [Generator loss: 1.206532]\n",
      "7115 [Discriminator loss: 0.644276, acc.: 64.84%] [Generator loss: 1.226613]\n",
      "7116 [Discriminator loss: 0.589552, acc.: 64.06%] [Generator loss: 1.234008]\n",
      "7117 [Discriminator loss: 0.649053, acc.: 63.28%] [Generator loss: 0.939419]\n",
      "7118 [Discriminator loss: 0.586677, acc.: 73.44%] [Generator loss: 0.695187]\n",
      "7119 [Discriminator loss: 0.706421, acc.: 62.50%] [Generator loss: 1.101527]\n",
      "7120 [Discriminator loss: 0.590415, acc.: 64.84%] [Generator loss: 1.022886]\n",
      "7121 [Discriminator loss: 0.719107, acc.: 57.03%] [Generator loss: 1.049663]\n",
      "7122 [Discriminator loss: 0.748107, acc.: 56.25%] [Generator loss: 1.142815]\n",
      "7123 [Discriminator loss: 0.667498, acc.: 55.47%] [Generator loss: 1.184903]\n",
      "7124 [Discriminator loss: 0.590742, acc.: 68.75%] [Generator loss: 1.092544]\n",
      "7125 [Discriminator loss: 0.612711, acc.: 67.97%] [Generator loss: 1.130864]\n",
      "7126 [Discriminator loss: 0.678813, acc.: 60.16%] [Generator loss: 0.962713]\n",
      "7127 [Discriminator loss: 0.664396, acc.: 64.84%] [Generator loss: 0.786113]\n",
      "7128 [Discriminator loss: 0.590699, acc.: 74.22%] [Generator loss: 0.963398]\n",
      "7129 [Discriminator loss: 0.616770, acc.: 69.53%] [Generator loss: 1.241332]\n",
      "7130 [Discriminator loss: 0.608213, acc.: 65.62%] [Generator loss: 1.069464]\n",
      "7131 [Discriminator loss: 0.854823, acc.: 44.53%] [Generator loss: 0.946271]\n",
      "7132 [Discriminator loss: 0.591598, acc.: 67.97%] [Generator loss: 1.019678]\n",
      "7133 [Discriminator loss: 0.588208, acc.: 69.53%] [Generator loss: 1.148018]\n",
      "7134 [Discriminator loss: 0.573360, acc.: 68.75%] [Generator loss: 1.211866]\n",
      "7135 [Discriminator loss: 0.563707, acc.: 71.09%] [Generator loss: 0.952339]\n",
      "7136 [Discriminator loss: 0.676576, acc.: 60.94%] [Generator loss: 0.890721]\n",
      "7137 [Discriminator loss: 0.913381, acc.: 42.19%] [Generator loss: 1.063559]\n",
      "7138 [Discriminator loss: 0.881526, acc.: 39.84%] [Generator loss: 1.022591]\n",
      "7139 [Discriminator loss: 0.662976, acc.: 63.28%] [Generator loss: 1.162238]\n",
      "7140 [Discriminator loss: 0.714487, acc.: 51.56%] [Generator loss: 0.984621]\n",
      "7141 [Discriminator loss: 0.691695, acc.: 57.03%] [Generator loss: 0.972009]\n",
      "7142 [Discriminator loss: 0.558159, acc.: 73.44%] [Generator loss: 1.111694]\n",
      "7143 [Discriminator loss: 0.684314, acc.: 58.59%] [Generator loss: 1.013191]\n",
      "7144 [Discriminator loss: 0.576424, acc.: 71.09%] [Generator loss: 1.038049]\n",
      "7145 [Discriminator loss: 0.675858, acc.: 62.50%] [Generator loss: 0.971583]\n",
      "7146 [Discriminator loss: 0.495459, acc.: 78.91%] [Generator loss: 0.913313]\n",
      "7147 [Discriminator loss: 0.745798, acc.: 60.16%] [Generator loss: 0.827505]\n",
      "7148 [Discriminator loss: 0.550727, acc.: 69.53%] [Generator loss: 0.932403]\n",
      "7149 [Discriminator loss: 0.543961, acc.: 75.78%] [Generator loss: 0.993541]\n",
      "7150 [Discriminator loss: 0.605221, acc.: 68.75%] [Generator loss: 1.090649]\n",
      "7151 [Discriminator loss: 0.595088, acc.: 66.41%] [Generator loss: 1.017259]\n",
      "7152 [Discriminator loss: 0.904417, acc.: 40.62%] [Generator loss: 0.786444]\n",
      "7153 [Discriminator loss: 0.694266, acc.: 55.47%] [Generator loss: 0.938391]\n",
      "7154 [Discriminator loss: 0.782048, acc.: 51.56%] [Generator loss: 1.012753]\n",
      "7155 [Discriminator loss: 0.700605, acc.: 56.25%] [Generator loss: 1.221883]\n",
      "7156 [Discriminator loss: 0.468755, acc.: 78.12%] [Generator loss: 1.389795]\n",
      "7157 [Discriminator loss: 0.478935, acc.: 80.47%] [Generator loss: 1.248577]\n",
      "7158 [Discriminator loss: 0.563606, acc.: 71.88%] [Generator loss: 0.908780]\n",
      "7159 [Discriminator loss: 0.713897, acc.: 57.03%] [Generator loss: 1.018099]\n",
      "7160 [Discriminator loss: 0.772440, acc.: 50.00%] [Generator loss: 0.953123]\n",
      "7161 [Discriminator loss: 0.741467, acc.: 52.34%] [Generator loss: 0.796702]\n",
      "7162 [Discriminator loss: 0.466919, acc.: 82.81%] [Generator loss: 0.642960]\n",
      "7163 [Discriminator loss: 0.716117, acc.: 59.38%] [Generator loss: 0.805570]\n",
      "7164 [Discriminator loss: 0.457642, acc.: 79.69%] [Generator loss: 0.778479]\n",
      "7165 [Discriminator loss: 0.654178, acc.: 65.62%] [Generator loss: 1.005550]\n",
      "7166 [Discriminator loss: 0.590593, acc.: 67.97%] [Generator loss: 1.240668]\n",
      "7167 [Discriminator loss: 0.738399, acc.: 48.44%] [Generator loss: 0.918473]\n",
      "7168 [Discriminator loss: 0.677154, acc.: 57.03%] [Generator loss: 1.040880]\n",
      "7169 [Discriminator loss: 0.524797, acc.: 79.69%] [Generator loss: 1.074204]\n",
      "7170 [Discriminator loss: 0.532552, acc.: 74.22%] [Generator loss: 0.609888]\n",
      "7171 [Discriminator loss: 0.924744, acc.: 42.19%] [Generator loss: 0.750929]\n",
      "7172 [Discriminator loss: 0.536866, acc.: 75.00%] [Generator loss: 1.156565]\n",
      "7173 [Discriminator loss: 0.663830, acc.: 60.16%] [Generator loss: 1.433757]\n",
      "7174 [Discriminator loss: 0.596785, acc.: 61.72%] [Generator loss: 1.083141]\n",
      "7175 [Discriminator loss: 0.586358, acc.: 67.97%] [Generator loss: 0.880857]\n",
      "7176 [Discriminator loss: 0.564721, acc.: 71.88%] [Generator loss: 1.076666]\n",
      "7177 [Discriminator loss: 0.707098, acc.: 61.72%] [Generator loss: 0.975816]\n",
      "7178 [Discriminator loss: 0.569330, acc.: 71.88%] [Generator loss: 0.818350]\n",
      "7179 [Discriminator loss: 0.687329, acc.: 60.16%] [Generator loss: 0.712577]\n",
      "7180 [Discriminator loss: 0.639388, acc.: 59.38%] [Generator loss: 0.701587]\n",
      "7181 [Discriminator loss: 0.576276, acc.: 68.75%] [Generator loss: 0.898486]\n",
      "7182 [Discriminator loss: 0.434323, acc.: 82.81%] [Generator loss: 0.954445]\n",
      "7183 [Discriminator loss: 0.528508, acc.: 79.69%] [Generator loss: 0.907130]\n",
      "7184 [Discriminator loss: 0.582035, acc.: 71.09%] [Generator loss: 0.862773]\n",
      "7185 [Discriminator loss: 0.605224, acc.: 62.50%] [Generator loss: 1.008189]\n",
      "7186 [Discriminator loss: 0.459567, acc.: 81.25%] [Generator loss: 0.977723]\n",
      "7187 [Discriminator loss: 0.647333, acc.: 61.72%] [Generator loss: 0.882574]\n",
      "7188 [Discriminator loss: 0.629761, acc.: 67.97%] [Generator loss: 0.799236]\n",
      "7189 [Discriminator loss: 0.506286, acc.: 74.22%] [Generator loss: 0.899534]\n",
      "7190 [Discriminator loss: 0.518392, acc.: 75.78%] [Generator loss: 0.864357]\n",
      "7191 [Discriminator loss: 1.037247, acc.: 38.28%] [Generator loss: 1.245473]\n",
      "7192 [Discriminator loss: 0.769150, acc.: 52.34%] [Generator loss: 1.513103]\n",
      "7193 [Discriminator loss: 0.630760, acc.: 64.06%] [Generator loss: 0.981230]\n",
      "7194 [Discriminator loss: 0.736072, acc.: 49.22%] [Generator loss: 1.023482]\n",
      "7195 [Discriminator loss: 0.623054, acc.: 67.19%] [Generator loss: 1.268521]\n",
      "7196 [Discriminator loss: 0.737575, acc.: 55.47%] [Generator loss: 0.951471]\n",
      "7197 [Discriminator loss: 0.609134, acc.: 66.41%] [Generator loss: 1.163980]\n",
      "7198 [Discriminator loss: 0.723548, acc.: 52.34%] [Generator loss: 1.039271]\n",
      "7199 [Discriminator loss: 0.652656, acc.: 60.16%] [Generator loss: 0.855427]\n",
      "7200 [Discriminator loss: 0.812677, acc.: 45.31%] [Generator loss: 1.173066]\n",
      "7201 [Discriminator loss: 0.533623, acc.: 72.66%] [Generator loss: 1.641432]\n",
      "7202 [Discriminator loss: 0.721181, acc.: 64.06%] [Generator loss: 0.791393]\n",
      "7203 [Discriminator loss: 0.633523, acc.: 63.28%] [Generator loss: 0.651448]\n",
      "7204 [Discriminator loss: 0.558363, acc.: 75.00%] [Generator loss: 0.802601]\n",
      "7205 [Discriminator loss: 0.565985, acc.: 71.88%] [Generator loss: 0.914732]\n",
      "7206 [Discriminator loss: 0.636670, acc.: 66.41%] [Generator loss: 0.949198]\n",
      "7207 [Discriminator loss: 0.809247, acc.: 43.75%] [Generator loss: 0.838639]\n",
      "7208 [Discriminator loss: 0.798861, acc.: 53.12%] [Generator loss: 0.861345]\n",
      "7209 [Discriminator loss: 0.609569, acc.: 62.50%] [Generator loss: 0.859961]\n",
      "7210 [Discriminator loss: 0.838002, acc.: 50.00%] [Generator loss: 0.966754]\n",
      "7211 [Discriminator loss: 0.555462, acc.: 71.09%] [Generator loss: 1.042312]\n",
      "7212 [Discriminator loss: 0.864403, acc.: 42.97%] [Generator loss: 1.012902]\n",
      "7213 [Discriminator loss: 0.969003, acc.: 45.31%] [Generator loss: 0.855702]\n",
      "7214 [Discriminator loss: 0.483904, acc.: 77.34%] [Generator loss: 0.993219]\n",
      "7215 [Discriminator loss: 0.502795, acc.: 82.81%] [Generator loss: 1.094422]\n",
      "7216 [Discriminator loss: 0.505301, acc.: 79.69%] [Generator loss: 1.114430]\n",
      "7217 [Discriminator loss: 0.639779, acc.: 62.50%] [Generator loss: 0.872117]\n",
      "7218 [Discriminator loss: 0.493572, acc.: 79.69%] [Generator loss: 0.899776]\n",
      "7219 [Discriminator loss: 0.541744, acc.: 75.78%] [Generator loss: 0.854700]\n",
      "7220 [Discriminator loss: 0.724679, acc.: 55.47%] [Generator loss: 0.729744]\n",
      "7221 [Discriminator loss: 0.503064, acc.: 78.91%] [Generator loss: 0.923036]\n",
      "7222 [Discriminator loss: 0.611051, acc.: 68.75%] [Generator loss: 0.925032]\n",
      "7223 [Discriminator loss: 0.936288, acc.: 35.94%] [Generator loss: 0.709459]\n",
      "7224 [Discriminator loss: 0.636228, acc.: 64.06%] [Generator loss: 0.954368]\n",
      "7225 [Discriminator loss: 0.494657, acc.: 76.56%] [Generator loss: 0.828919]\n",
      "7226 [Discriminator loss: 0.720248, acc.: 59.38%] [Generator loss: 0.713060]\n",
      "7227 [Discriminator loss: 0.611842, acc.: 66.41%] [Generator loss: 0.852458]\n",
      "7228 [Discriminator loss: 0.510290, acc.: 78.12%] [Generator loss: 1.003653]\n",
      "7229 [Discriminator loss: 0.819474, acc.: 50.78%] [Generator loss: 0.643675]\n",
      "7230 [Discriminator loss: 0.688873, acc.: 58.59%] [Generator loss: 0.984437]\n",
      "7231 [Discriminator loss: 0.828590, acc.: 45.31%] [Generator loss: 1.095631]\n",
      "7232 [Discriminator loss: 0.777313, acc.: 50.00%] [Generator loss: 1.299212]\n",
      "7233 [Discriminator loss: 1.029631, acc.: 35.94%] [Generator loss: 0.953506]\n",
      "7234 [Discriminator loss: 0.666254, acc.: 64.84%] [Generator loss: 1.012395]\n",
      "7235 [Discriminator loss: 0.656255, acc.: 65.62%] [Generator loss: 0.877706]\n",
      "7236 [Discriminator loss: 0.375015, acc.: 85.94%] [Generator loss: 0.757425]\n",
      "7237 [Discriminator loss: 0.696958, acc.: 53.91%] [Generator loss: 0.892370]\n",
      "7238 [Discriminator loss: 0.504945, acc.: 82.03%] [Generator loss: 1.007516]\n",
      "7239 [Discriminator loss: 0.575635, acc.: 75.78%] [Generator loss: 0.960442]\n",
      "7240 [Discriminator loss: 0.632083, acc.: 65.62%] [Generator loss: 0.958143]\n",
      "7241 [Discriminator loss: 0.482042, acc.: 80.47%] [Generator loss: 0.858689]\n",
      "7242 [Discriminator loss: 0.640079, acc.: 64.84%] [Generator loss: 1.147865]\n",
      "7243 [Discriminator loss: 0.846515, acc.: 38.28%] [Generator loss: 0.864659]\n",
      "7244 [Discriminator loss: 0.747835, acc.: 57.03%] [Generator loss: 0.937466]\n",
      "7245 [Discriminator loss: 0.551740, acc.: 75.00%] [Generator loss: 0.889334]\n",
      "7246 [Discriminator loss: 0.649550, acc.: 61.72%] [Generator loss: 1.168404]\n",
      "7247 [Discriminator loss: 0.462568, acc.: 80.47%] [Generator loss: 1.015422]\n",
      "7248 [Discriminator loss: 0.673737, acc.: 52.34%] [Generator loss: 0.882258]\n",
      "7249 [Discriminator loss: 0.594799, acc.: 69.53%] [Generator loss: 0.886142]\n",
      "7250 [Discriminator loss: 0.790007, acc.: 60.94%] [Generator loss: 1.024326]\n",
      "7251 [Discriminator loss: 0.480437, acc.: 77.34%] [Generator loss: 0.901688]\n",
      "7252 [Discriminator loss: 0.666988, acc.: 58.59%] [Generator loss: 0.859189]\n",
      "7253 [Discriminator loss: 0.554901, acc.: 75.00%] [Generator loss: 0.821354]\n",
      "7254 [Discriminator loss: 0.677709, acc.: 60.94%] [Generator loss: 0.806188]\n",
      "7255 [Discriminator loss: 0.842573, acc.: 41.41%] [Generator loss: 1.007385]\n",
      "7256 [Discriminator loss: 0.535049, acc.: 74.22%] [Generator loss: 1.072978]\n",
      "7257 [Discriminator loss: 0.589039, acc.: 65.62%] [Generator loss: 0.856064]\n",
      "7258 [Discriminator loss: 0.664749, acc.: 61.72%] [Generator loss: 0.766359]\n",
      "7259 [Discriminator loss: 0.554255, acc.: 68.75%] [Generator loss: 0.915885]\n",
      "7260 [Discriminator loss: 0.622117, acc.: 64.06%] [Generator loss: 0.850017]\n",
      "7261 [Discriminator loss: 0.564216, acc.: 71.09%] [Generator loss: 0.929151]\n",
      "7262 [Discriminator loss: 0.667432, acc.: 63.28%] [Generator loss: 1.028383]\n",
      "7263 [Discriminator loss: 0.516711, acc.: 77.34%] [Generator loss: 0.728488]\n",
      "7264 [Discriminator loss: 0.525019, acc.: 75.78%] [Generator loss: 0.819743]\n",
      "7265 [Discriminator loss: 0.437112, acc.: 87.50%] [Generator loss: 0.747261]\n",
      "7266 [Discriminator loss: 0.726296, acc.: 56.25%] [Generator loss: 0.676787]\n",
      "7267 [Discriminator loss: 0.569129, acc.: 71.88%] [Generator loss: 1.493279]\n",
      "7268 [Discriminator loss: 0.745666, acc.: 66.41%] [Generator loss: 0.941779]\n",
      "7269 [Discriminator loss: 0.637332, acc.: 69.53%] [Generator loss: 0.520113]\n",
      "7270 [Discriminator loss: 0.585931, acc.: 67.97%] [Generator loss: 0.641183]\n",
      "7271 [Discriminator loss: 0.687158, acc.: 53.91%] [Generator loss: 0.651410]\n",
      "7272 [Discriminator loss: 0.857789, acc.: 39.84%] [Generator loss: 0.722010]\n",
      "7273 [Discriminator loss: 0.670036, acc.: 58.59%] [Generator loss: 0.876646]\n",
      "7274 [Discriminator loss: 0.511726, acc.: 77.34%] [Generator loss: 0.844300]\n",
      "7275 [Discriminator loss: 0.800015, acc.: 46.88%] [Generator loss: 1.407941]\n",
      "7276 [Discriminator loss: 0.739722, acc.: 56.25%] [Generator loss: 1.422433]\n",
      "7277 [Discriminator loss: 0.653715, acc.: 60.94%] [Generator loss: 1.279403]\n",
      "7278 [Discriminator loss: 0.575337, acc.: 72.66%] [Generator loss: 1.204244]\n",
      "7279 [Discriminator loss: 0.492088, acc.: 82.03%] [Generator loss: 0.986525]\n",
      "7280 [Discriminator loss: 0.465657, acc.: 83.59%] [Generator loss: 0.827990]\n",
      "7281 [Discriminator loss: 0.562543, acc.: 75.78%] [Generator loss: 0.825105]\n",
      "7282 [Discriminator loss: 0.488271, acc.: 81.25%] [Generator loss: 0.707566]\n",
      "7283 [Discriminator loss: 0.925510, acc.: 37.50%] [Generator loss: 0.607012]\n",
      "7284 [Discriminator loss: 0.406045, acc.: 89.06%] [Generator loss: 0.823899]\n",
      "7285 [Discriminator loss: 0.623299, acc.: 68.75%] [Generator loss: 0.859600]\n",
      "7286 [Discriminator loss: 0.715612, acc.: 56.25%] [Generator loss: 0.816802]\n",
      "7287 [Discriminator loss: 0.431392, acc.: 83.59%] [Generator loss: 1.078647]\n",
      "7288 [Discriminator loss: 0.685848, acc.: 61.72%] [Generator loss: 0.772798]\n",
      "7289 [Discriminator loss: 0.469939, acc.: 78.91%] [Generator loss: 0.790829]\n",
      "7290 [Discriminator loss: 0.631890, acc.: 63.28%] [Generator loss: 0.978117]\n",
      "7291 [Discriminator loss: 0.840741, acc.: 48.44%] [Generator loss: 1.032524]\n",
      "7292 [Discriminator loss: 0.705577, acc.: 63.28%] [Generator loss: 0.884621]\n",
      "7293 [Discriminator loss: 0.795934, acc.: 49.22%] [Generator loss: 0.812561]\n",
      "7294 [Discriminator loss: 0.912425, acc.: 36.72%] [Generator loss: 0.936623]\n",
      "7295 [Discriminator loss: 0.426699, acc.: 87.50%] [Generator loss: 1.095616]\n",
      "7296 [Discriminator loss: 0.582402, acc.: 70.31%] [Generator loss: 1.207125]\n",
      "7297 [Discriminator loss: 0.610730, acc.: 66.41%] [Generator loss: 1.130836]\n",
      "7298 [Discriminator loss: 0.665848, acc.: 65.62%] [Generator loss: 1.233055]\n",
      "7299 [Discriminator loss: 0.777043, acc.: 56.25%] [Generator loss: 1.016571]\n",
      "7300 [Discriminator loss: 0.657410, acc.: 64.84%] [Generator loss: 1.155769]\n",
      "7301 [Discriminator loss: 0.840179, acc.: 49.22%] [Generator loss: 0.910996]\n",
      "7302 [Discriminator loss: 0.556133, acc.: 75.78%] [Generator loss: 1.055416]\n",
      "7303 [Discriminator loss: 0.564541, acc.: 73.44%] [Generator loss: 0.813871]\n",
      "7304 [Discriminator loss: 0.806080, acc.: 47.66%] [Generator loss: 0.684342]\n",
      "7305 [Discriminator loss: 0.693518, acc.: 64.06%] [Generator loss: 0.901261]\n",
      "7306 [Discriminator loss: 0.725526, acc.: 53.91%] [Generator loss: 1.122108]\n",
      "7307 [Discriminator loss: 0.486475, acc.: 76.56%] [Generator loss: 1.106265]\n",
      "7308 [Discriminator loss: 0.782305, acc.: 46.09%] [Generator loss: 0.805226]\n",
      "7309 [Discriminator loss: 0.686836, acc.: 57.03%] [Generator loss: 1.058417]\n",
      "7310 [Discriminator loss: 0.557475, acc.: 72.66%] [Generator loss: 1.097226]\n",
      "7311 [Discriminator loss: 0.610473, acc.: 69.53%] [Generator loss: 1.174008]\n",
      "7312 [Discriminator loss: 0.559326, acc.: 69.53%] [Generator loss: 1.249528]\n",
      "7313 [Discriminator loss: 0.627783, acc.: 68.75%] [Generator loss: 1.319127]\n",
      "7314 [Discriminator loss: 0.683083, acc.: 62.50%] [Generator loss: 1.151538]\n",
      "7315 [Discriminator loss: 0.642022, acc.: 64.06%] [Generator loss: 1.162544]\n",
      "7316 [Discriminator loss: 0.599928, acc.: 70.31%] [Generator loss: 0.973571]\n",
      "7317 [Discriminator loss: 0.505502, acc.: 73.44%] [Generator loss: 0.985096]\n",
      "7318 [Discriminator loss: 0.656069, acc.: 60.94%] [Generator loss: 0.766692]\n",
      "7319 [Discriminator loss: 0.870861, acc.: 37.50%] [Generator loss: 0.704263]\n",
      "7320 [Discriminator loss: 0.570790, acc.: 74.22%] [Generator loss: 0.692760]\n",
      "7321 [Discriminator loss: 0.769883, acc.: 56.25%] [Generator loss: 0.801124]\n",
      "7322 [Discriminator loss: 0.949761, acc.: 41.41%] [Generator loss: 0.667892]\n",
      "7323 [Discriminator loss: 0.602611, acc.: 69.53%] [Generator loss: 0.823539]\n",
      "7324 [Discriminator loss: 0.708970, acc.: 53.12%] [Generator loss: 1.030689]\n",
      "7325 [Discriminator loss: 0.608944, acc.: 73.44%] [Generator loss: 1.147431]\n",
      "7326 [Discriminator loss: 0.729450, acc.: 53.12%] [Generator loss: 1.057797]\n",
      "7327 [Discriminator loss: 0.685934, acc.: 61.72%] [Generator loss: 0.961120]\n",
      "7328 [Discriminator loss: 0.603070, acc.: 67.97%] [Generator loss: 0.980127]\n",
      "7329 [Discriminator loss: 0.536900, acc.: 75.78%] [Generator loss: 0.925259]\n",
      "7330 [Discriminator loss: 0.576188, acc.: 74.22%] [Generator loss: 1.071227]\n",
      "7331 [Discriminator loss: 0.625930, acc.: 64.06%] [Generator loss: 1.035507]\n",
      "7332 [Discriminator loss: 0.592974, acc.: 67.19%] [Generator loss: 1.252336]\n",
      "7333 [Discriminator loss: 0.446822, acc.: 84.38%] [Generator loss: 0.965606]\n",
      "7334 [Discriminator loss: 0.547376, acc.: 73.44%] [Generator loss: 0.732970]\n",
      "7335 [Discriminator loss: 0.645051, acc.: 64.84%] [Generator loss: 0.653835]\n",
      "7336 [Discriminator loss: 0.515764, acc.: 71.09%] [Generator loss: 0.934806]\n",
      "7337 [Discriminator loss: 0.712551, acc.: 57.81%] [Generator loss: 0.769831]\n",
      "7338 [Discriminator loss: 0.634220, acc.: 59.38%] [Generator loss: 0.774104]\n",
      "7339 [Discriminator loss: 0.667291, acc.: 62.50%] [Generator loss: 0.841574]\n",
      "7340 [Discriminator loss: 0.721404, acc.: 56.25%] [Generator loss: 0.938372]\n",
      "7341 [Discriminator loss: 0.809296, acc.: 50.78%] [Generator loss: 1.093108]\n",
      "7342 [Discriminator loss: 0.770013, acc.: 49.22%] [Generator loss: 0.814265]\n",
      "7343 [Discriminator loss: 0.871280, acc.: 41.41%] [Generator loss: 1.283243]\n",
      "7344 [Discriminator loss: 0.526183, acc.: 72.66%] [Generator loss: 1.159708]\n",
      "7345 [Discriminator loss: 0.569898, acc.: 74.22%] [Generator loss: 1.008769]\n",
      "7346 [Discriminator loss: 0.599417, acc.: 69.53%] [Generator loss: 0.831892]\n",
      "7347 [Discriminator loss: 0.624419, acc.: 66.41%] [Generator loss: 0.905434]\n",
      "7348 [Discriminator loss: 0.528706, acc.: 75.78%] [Generator loss: 0.872820]\n",
      "7349 [Discriminator loss: 0.689550, acc.: 60.16%] [Generator loss: 0.881163]\n",
      "7350 [Discriminator loss: 0.606286, acc.: 71.09%] [Generator loss: 0.770962]\n",
      "7351 [Discriminator loss: 0.746391, acc.: 62.50%] [Generator loss: 1.178481]\n",
      "7352 [Discriminator loss: 0.701416, acc.: 64.06%] [Generator loss: 1.269213]\n",
      "7353 [Discriminator loss: 0.712322, acc.: 58.59%] [Generator loss: 0.836538]\n",
      "7354 [Discriminator loss: 0.876530, acc.: 39.84%] [Generator loss: 0.787070]\n",
      "7355 [Discriminator loss: 0.582877, acc.: 71.09%] [Generator loss: 1.002107]\n",
      "7356 [Discriminator loss: 0.739132, acc.: 50.78%] [Generator loss: 1.224270]\n",
      "7357 [Discriminator loss: 0.973296, acc.: 36.72%] [Generator loss: 1.124142]\n",
      "7358 [Discriminator loss: 0.859300, acc.: 45.31%] [Generator loss: 0.908810]\n",
      "7359 [Discriminator loss: 0.775762, acc.: 51.56%] [Generator loss: 1.090145]\n",
      "7360 [Discriminator loss: 0.999343, acc.: 35.94%] [Generator loss: 1.058809]\n",
      "7361 [Discriminator loss: 0.784077, acc.: 41.41%] [Generator loss: 1.289125]\n",
      "7362 [Discriminator loss: 0.669384, acc.: 59.38%] [Generator loss: 1.218121]\n",
      "7363 [Discriminator loss: 0.673459, acc.: 63.28%] [Generator loss: 1.075000]\n",
      "7364 [Discriminator loss: 0.726953, acc.: 50.00%] [Generator loss: 0.890875]\n",
      "7365 [Discriminator loss: 0.530924, acc.: 74.22%] [Generator loss: 0.955619]\n",
      "7366 [Discriminator loss: 0.697648, acc.: 53.91%] [Generator loss: 0.882670]\n",
      "7367 [Discriminator loss: 0.634662, acc.: 68.75%] [Generator loss: 0.988750]\n",
      "7368 [Discriminator loss: 0.657054, acc.: 57.81%] [Generator loss: 0.862067]\n",
      "7369 [Discriminator loss: 0.624688, acc.: 65.62%] [Generator loss: 1.045791]\n",
      "7370 [Discriminator loss: 0.570285, acc.: 72.66%] [Generator loss: 0.986891]\n",
      "7371 [Discriminator loss: 0.523026, acc.: 78.91%] [Generator loss: 0.813074]\n",
      "7372 [Discriminator loss: 0.790625, acc.: 47.66%] [Generator loss: 0.792999]\n",
      "7373 [Discriminator loss: 0.658197, acc.: 60.16%] [Generator loss: 0.920834]\n",
      "7374 [Discriminator loss: 0.606218, acc.: 70.31%] [Generator loss: 0.876881]\n",
      "7375 [Discriminator loss: 0.687518, acc.: 57.81%] [Generator loss: 0.902224]\n",
      "7376 [Discriminator loss: 0.652816, acc.: 67.19%] [Generator loss: 1.227855]\n",
      "7377 [Discriminator loss: 0.948818, acc.: 30.47%] [Generator loss: 0.930080]\n",
      "7378 [Discriminator loss: 0.819468, acc.: 52.34%] [Generator loss: 1.106205]\n",
      "7379 [Discriminator loss: 0.611584, acc.: 62.50%] [Generator loss: 1.189389]\n",
      "7380 [Discriminator loss: 0.736443, acc.: 52.34%] [Generator loss: 1.224485]\n",
      "7381 [Discriminator loss: 0.663259, acc.: 64.84%] [Generator loss: 0.916822]\n",
      "7382 [Discriminator loss: 0.664142, acc.: 60.16%] [Generator loss: 0.862725]\n",
      "7383 [Discriminator loss: 0.704105, acc.: 56.25%] [Generator loss: 0.854438]\n",
      "7384 [Discriminator loss: 0.623870, acc.: 68.75%] [Generator loss: 0.913690]\n",
      "7385 [Discriminator loss: 0.685046, acc.: 62.50%] [Generator loss: 0.850688]\n",
      "7386 [Discriminator loss: 0.956515, acc.: 41.41%] [Generator loss: 0.760390]\n",
      "7387 [Discriminator loss: 0.564154, acc.: 76.56%] [Generator loss: 0.917835]\n",
      "7388 [Discriminator loss: 0.581548, acc.: 69.53%] [Generator loss: 0.955507]\n",
      "7389 [Discriminator loss: 0.739715, acc.: 57.81%] [Generator loss: 0.740811]\n",
      "7390 [Discriminator loss: 0.667023, acc.: 66.41%] [Generator loss: 0.943786]\n",
      "7391 [Discriminator loss: 0.720546, acc.: 56.25%] [Generator loss: 0.960572]\n",
      "7392 [Discriminator loss: 0.908099, acc.: 35.94%] [Generator loss: 1.020573]\n",
      "7393 [Discriminator loss: 0.597456, acc.: 68.75%] [Generator loss: 0.855581]\n",
      "7394 [Discriminator loss: 0.610435, acc.: 69.53%] [Generator loss: 1.059070]\n",
      "7395 [Discriminator loss: 0.651392, acc.: 60.94%] [Generator loss: 0.877906]\n",
      "7396 [Discriminator loss: 0.751312, acc.: 41.41%] [Generator loss: 0.868304]\n",
      "7397 [Discriminator loss: 0.462113, acc.: 81.25%] [Generator loss: 0.973377]\n",
      "7398 [Discriminator loss: 0.735236, acc.: 56.25%] [Generator loss: 0.900825]\n",
      "7399 [Discriminator loss: 0.510840, acc.: 75.78%] [Generator loss: 0.936892]\n",
      "7400 [Discriminator loss: 0.742027, acc.: 55.47%] [Generator loss: 1.136873]\n",
      "7401 [Discriminator loss: 0.592614, acc.: 72.66%] [Generator loss: 1.235390]\n",
      "7402 [Discriminator loss: 0.578210, acc.: 69.53%] [Generator loss: 0.964579]\n",
      "7403 [Discriminator loss: 0.560339, acc.: 72.66%] [Generator loss: 0.801459]\n",
      "7404 [Discriminator loss: 0.459270, acc.: 85.16%] [Generator loss: 0.612010]\n",
      "7405 [Discriminator loss: 0.774960, acc.: 46.09%] [Generator loss: 0.670307]\n",
      "7406 [Discriminator loss: 0.777500, acc.: 53.91%] [Generator loss: 0.733183]\n",
      "7407 [Discriminator loss: 0.594015, acc.: 73.44%] [Generator loss: 1.079215]\n",
      "7408 [Discriminator loss: 0.643453, acc.: 61.72%] [Generator loss: 1.284447]\n",
      "7409 [Discriminator loss: 0.828384, acc.: 46.09%] [Generator loss: 1.352338]\n",
      "7410 [Discriminator loss: 0.665390, acc.: 63.28%] [Generator loss: 1.092769]\n",
      "7411 [Discriminator loss: 0.623978, acc.: 64.84%] [Generator loss: 1.004119]\n",
      "7412 [Discriminator loss: 0.588225, acc.: 64.06%] [Generator loss: 0.978762]\n",
      "7413 [Discriminator loss: 0.683012, acc.: 53.91%] [Generator loss: 1.183864]\n",
      "7414 [Discriminator loss: 0.611337, acc.: 63.28%] [Generator loss: 1.220318]\n",
      "7415 [Discriminator loss: 0.744988, acc.: 50.78%] [Generator loss: 1.036351]\n",
      "7416 [Discriminator loss: 0.421863, acc.: 86.72%] [Generator loss: 0.836119]\n",
      "7417 [Discriminator loss: 0.709248, acc.: 52.34%] [Generator loss: 0.913791]\n",
      "7418 [Discriminator loss: 0.505481, acc.: 78.12%] [Generator loss: 0.879000]\n",
      "7419 [Discriminator loss: 0.875178, acc.: 49.22%] [Generator loss: 0.986493]\n",
      "7420 [Discriminator loss: 0.540442, acc.: 75.78%] [Generator loss: 1.396498]\n",
      "7421 [Discriminator loss: 0.686098, acc.: 60.94%] [Generator loss: 1.185867]\n",
      "7422 [Discriminator loss: 0.618900, acc.: 65.62%] [Generator loss: 1.138858]\n",
      "7423 [Discriminator loss: 0.882370, acc.: 39.84%] [Generator loss: 0.860533]\n",
      "7424 [Discriminator loss: 0.665484, acc.: 58.59%] [Generator loss: 0.863129]\n",
      "7425 [Discriminator loss: 0.710513, acc.: 53.91%] [Generator loss: 0.918994]\n",
      "7426 [Discriminator loss: 0.653829, acc.: 55.47%] [Generator loss: 0.907092]\n",
      "7427 [Discriminator loss: 0.664377, acc.: 67.19%] [Generator loss: 1.007069]\n",
      "7428 [Discriminator loss: 0.651209, acc.: 63.28%] [Generator loss: 0.955800]\n",
      "7429 [Discriminator loss: 0.537536, acc.: 77.34%] [Generator loss: 0.989091]\n",
      "7430 [Discriminator loss: 0.662925, acc.: 60.16%] [Generator loss: 0.970318]\n",
      "7431 [Discriminator loss: 0.623524, acc.: 67.97%] [Generator loss: 0.985289]\n",
      "7432 [Discriminator loss: 0.512770, acc.: 74.22%] [Generator loss: 1.028987]\n",
      "7433 [Discriminator loss: 0.530047, acc.: 71.09%] [Generator loss: 0.874229]\n",
      "7434 [Discriminator loss: 0.706455, acc.: 64.06%] [Generator loss: 0.889736]\n",
      "7435 [Discriminator loss: 0.639155, acc.: 64.84%] [Generator loss: 1.077106]\n",
      "7436 [Discriminator loss: 0.487702, acc.: 77.34%] [Generator loss: 1.015474]\n",
      "7437 [Discriminator loss: 0.850564, acc.: 40.62%] [Generator loss: 0.843948]\n",
      "7438 [Discriminator loss: 0.685644, acc.: 60.16%] [Generator loss: 1.011997]\n",
      "7439 [Discriminator loss: 0.827312, acc.: 48.44%] [Generator loss: 0.854982]\n",
      "7440 [Discriminator loss: 0.732984, acc.: 50.78%] [Generator loss: 0.741881]\n",
      "7441 [Discriminator loss: 0.658307, acc.: 60.16%] [Generator loss: 0.831664]\n",
      "7442 [Discriminator loss: 0.541869, acc.: 82.03%] [Generator loss: 0.891238]\n",
      "7443 [Discriminator loss: 0.723115, acc.: 56.25%] [Generator loss: 0.841694]\n",
      "7444 [Discriminator loss: 0.681004, acc.: 61.72%] [Generator loss: 0.909029]\n",
      "7445 [Discriminator loss: 0.611909, acc.: 69.53%] [Generator loss: 0.931759]\n",
      "7446 [Discriminator loss: 0.446515, acc.: 83.59%] [Generator loss: 0.919441]\n",
      "7447 [Discriminator loss: 0.574187, acc.: 71.09%] [Generator loss: 1.147033]\n",
      "7448 [Discriminator loss: 0.673472, acc.: 63.28%] [Generator loss: 0.969300]\n",
      "7449 [Discriminator loss: 0.493912, acc.: 80.47%] [Generator loss: 0.993779]\n",
      "7450 [Discriminator loss: 0.633130, acc.: 63.28%] [Generator loss: 0.846946]\n",
      "7451 [Discriminator loss: 0.530544, acc.: 75.00%] [Generator loss: 0.849085]\n",
      "7452 [Discriminator loss: 0.544979, acc.: 75.78%] [Generator loss: 0.831993]\n",
      "7453 [Discriminator loss: 0.635467, acc.: 64.06%] [Generator loss: 0.761186]\n",
      "7454 [Discriminator loss: 0.944836, acc.: 41.41%] [Generator loss: 0.840888]\n",
      "7455 [Discriminator loss: 0.670842, acc.: 58.59%] [Generator loss: 0.818319]\n",
      "7456 [Discriminator loss: 0.603828, acc.: 69.53%] [Generator loss: 1.047741]\n",
      "7457 [Discriminator loss: 0.755764, acc.: 46.88%] [Generator loss: 1.014970]\n",
      "7458 [Discriminator loss: 0.584222, acc.: 72.66%] [Generator loss: 0.924398]\n",
      "7459 [Discriminator loss: 0.715553, acc.: 54.69%] [Generator loss: 0.892060]\n",
      "7460 [Discriminator loss: 0.707068, acc.: 63.28%] [Generator loss: 0.911377]\n",
      "7461 [Discriminator loss: 0.501214, acc.: 77.34%] [Generator loss: 0.947909]\n",
      "7462 [Discriminator loss: 0.703488, acc.: 57.03%] [Generator loss: 1.427675]\n",
      "7463 [Discriminator loss: 0.668805, acc.: 57.03%] [Generator loss: 1.406609]\n",
      "7464 [Discriminator loss: 0.699483, acc.: 61.72%] [Generator loss: 1.347706]\n",
      "7465 [Discriminator loss: 0.991564, acc.: 34.38%] [Generator loss: 0.994038]\n",
      "7466 [Discriminator loss: 0.579764, acc.: 66.41%] [Generator loss: 0.918040]\n",
      "7467 [Discriminator loss: 0.487348, acc.: 80.47%] [Generator loss: 0.886280]\n",
      "7468 [Discriminator loss: 0.486907, acc.: 82.81%] [Generator loss: 0.904337]\n",
      "7469 [Discriminator loss: 0.588877, acc.: 71.88%] [Generator loss: 0.883355]\n",
      "7470 [Discriminator loss: 0.546072, acc.: 72.66%] [Generator loss: 0.847819]\n",
      "7471 [Discriminator loss: 0.526779, acc.: 73.44%] [Generator loss: 1.002312]\n",
      "7472 [Discriminator loss: 0.819447, acc.: 42.97%] [Generator loss: 0.787883]\n",
      "7473 [Discriminator loss: 0.601711, acc.: 67.97%] [Generator loss: 1.149190]\n",
      "7474 [Discriminator loss: 0.560879, acc.: 69.53%] [Generator loss: 1.361821]\n",
      "7475 [Discriminator loss: 0.647822, acc.: 62.50%] [Generator loss: 1.131834]\n",
      "7476 [Discriminator loss: 0.670917, acc.: 57.81%] [Generator loss: 1.096542]\n",
      "7477 [Discriminator loss: 0.633689, acc.: 59.38%] [Generator loss: 0.951668]\n",
      "7478 [Discriminator loss: 0.647102, acc.: 58.59%] [Generator loss: 0.867821]\n",
      "7479 [Discriminator loss: 0.643521, acc.: 64.84%] [Generator loss: 0.922282]\n",
      "7480 [Discriminator loss: 0.556749, acc.: 70.31%] [Generator loss: 0.926226]\n",
      "7481 [Discriminator loss: 0.632775, acc.: 64.06%] [Generator loss: 1.410521]\n",
      "7482 [Discriminator loss: 0.719151, acc.: 61.72%] [Generator loss: 0.965954]\n",
      "7483 [Discriminator loss: 0.930040, acc.: 39.84%] [Generator loss: 0.860426]\n",
      "7484 [Discriminator loss: 0.693466, acc.: 60.16%] [Generator loss: 1.357677]\n",
      "7485 [Discriminator loss: 0.780529, acc.: 52.34%] [Generator loss: 1.226399]\n",
      "7486 [Discriminator loss: 0.598176, acc.: 71.88%] [Generator loss: 0.921634]\n",
      "7487 [Discriminator loss: 0.882496, acc.: 37.50%] [Generator loss: 0.931976]\n",
      "7488 [Discriminator loss: 0.566623, acc.: 67.97%] [Generator loss: 1.294688]\n",
      "7489 [Discriminator loss: 0.605797, acc.: 67.97%] [Generator loss: 1.110543]\n",
      "7490 [Discriminator loss: 0.860622, acc.: 44.53%] [Generator loss: 1.027005]\n",
      "7491 [Discriminator loss: 0.536695, acc.: 71.88%] [Generator loss: 1.060423]\n",
      "7492 [Discriminator loss: 0.674244, acc.: 56.25%] [Generator loss: 0.807149]\n",
      "7493 [Discriminator loss: 0.542831, acc.: 73.44%] [Generator loss: 0.611599]\n",
      "7494 [Discriminator loss: 0.792438, acc.: 47.66%] [Generator loss: 0.997400]\n",
      "7495 [Discriminator loss: 0.616524, acc.: 63.28%] [Generator loss: 0.902253]\n",
      "7496 [Discriminator loss: 0.638656, acc.: 67.19%] [Generator loss: 1.043855]\n",
      "7497 [Discriminator loss: 0.692292, acc.: 53.91%] [Generator loss: 1.017706]\n",
      "7498 [Discriminator loss: 0.543604, acc.: 71.88%] [Generator loss: 0.868666]\n",
      "7499 [Discriminator loss: 0.964330, acc.: 39.84%] [Generator loss: 1.205670]\n",
      "7500 [Discriminator loss: 0.547550, acc.: 82.03%] [Generator loss: 1.185548]\n",
      "7501 [Discriminator loss: 0.541032, acc.: 73.44%] [Generator loss: 1.217821]\n",
      "7502 [Discriminator loss: 0.615030, acc.: 68.75%] [Generator loss: 0.846225]\n",
      "7503 [Discriminator loss: 0.450741, acc.: 81.25%] [Generator loss: 0.795728]\n",
      "7504 [Discriminator loss: 0.470440, acc.: 81.25%] [Generator loss: 0.717570]\n",
      "7505 [Discriminator loss: 0.666103, acc.: 62.50%] [Generator loss: 1.063786]\n",
      "7506 [Discriminator loss: 0.429402, acc.: 85.16%] [Generator loss: 1.148820]\n",
      "7507 [Discriminator loss: 0.543564, acc.: 73.44%] [Generator loss: 0.944567]\n",
      "7508 [Discriminator loss: 0.607942, acc.: 62.50%] [Generator loss: 0.927413]\n",
      "7509 [Discriminator loss: 0.509443, acc.: 78.12%] [Generator loss: 0.848108]\n",
      "7510 [Discriminator loss: 0.731531, acc.: 58.59%] [Generator loss: 0.892644]\n",
      "7511 [Discriminator loss: 0.608195, acc.: 69.53%] [Generator loss: 1.418798]\n",
      "7512 [Discriminator loss: 0.635439, acc.: 66.41%] [Generator loss: 1.321508]\n",
      "7513 [Discriminator loss: 0.703414, acc.: 56.25%] [Generator loss: 1.149830]\n",
      "7514 [Discriminator loss: 0.523007, acc.: 71.09%] [Generator loss: 0.953524]\n",
      "7515 [Discriminator loss: 0.661517, acc.: 55.47%] [Generator loss: 0.853396]\n",
      "7516 [Discriminator loss: 0.558355, acc.: 72.66%] [Generator loss: 1.159428]\n",
      "7517 [Discriminator loss: 0.486953, acc.: 80.47%] [Generator loss: 1.135803]\n",
      "7518 [Discriminator loss: 0.610554, acc.: 64.84%] [Generator loss: 1.410640]\n",
      "7519 [Discriminator loss: 0.591082, acc.: 75.78%] [Generator loss: 1.089256]\n",
      "7520 [Discriminator loss: 0.689825, acc.: 60.16%] [Generator loss: 1.064546]\n",
      "7521 [Discriminator loss: 0.976963, acc.: 40.62%] [Generator loss: 0.957161]\n",
      "7522 [Discriminator loss: 0.724375, acc.: 66.41%] [Generator loss: 1.055207]\n",
      "7523 [Discriminator loss: 0.710348, acc.: 56.25%] [Generator loss: 0.931846]\n",
      "7524 [Discriminator loss: 0.887929, acc.: 50.78%] [Generator loss: 1.247485]\n",
      "7525 [Discriminator loss: 0.627169, acc.: 66.41%] [Generator loss: 1.167171]\n",
      "7526 [Discriminator loss: 0.688538, acc.: 54.69%] [Generator loss: 1.016218]\n",
      "7527 [Discriminator loss: 0.746658, acc.: 57.03%] [Generator loss: 0.855920]\n",
      "7528 [Discriminator loss: 0.628854, acc.: 67.97%] [Generator loss: 0.984928]\n",
      "7529 [Discriminator loss: 0.649585, acc.: 64.84%] [Generator loss: 1.304171]\n",
      "7530 [Discriminator loss: 0.537408, acc.: 75.00%] [Generator loss: 1.055753]\n",
      "7531 [Discriminator loss: 0.537962, acc.: 75.00%] [Generator loss: 0.841610]\n",
      "7532 [Discriminator loss: 0.575198, acc.: 69.53%] [Generator loss: 0.868904]\n",
      "7533 [Discriminator loss: 0.804629, acc.: 43.75%] [Generator loss: 0.799637]\n",
      "7534 [Discriminator loss: 0.749453, acc.: 48.44%] [Generator loss: 0.942400]\n",
      "7535 [Discriminator loss: 0.776937, acc.: 50.78%] [Generator loss: 0.961687]\n",
      "7536 [Discriminator loss: 0.676438, acc.: 57.81%] [Generator loss: 0.990212]\n",
      "7537 [Discriminator loss: 0.653240, acc.: 65.62%] [Generator loss: 0.932389]\n",
      "7538 [Discriminator loss: 0.569373, acc.: 72.66%] [Generator loss: 0.925601]\n",
      "7539 [Discriminator loss: 0.509873, acc.: 79.69%] [Generator loss: 1.049692]\n",
      "7540 [Discriminator loss: 0.703166, acc.: 60.16%] [Generator loss: 1.177814]\n",
      "7541 [Discriminator loss: 0.591221, acc.: 74.22%] [Generator loss: 1.034121]\n",
      "7542 [Discriminator loss: 0.741643, acc.: 53.12%] [Generator loss: 0.941000]\n",
      "7543 [Discriminator loss: 0.558011, acc.: 75.00%] [Generator loss: 0.998966]\n",
      "7544 [Discriminator loss: 0.740425, acc.: 57.81%] [Generator loss: 1.029649]\n",
      "7545 [Discriminator loss: 0.757114, acc.: 53.12%] [Generator loss: 1.045867]\n",
      "7546 [Discriminator loss: 0.722079, acc.: 60.16%] [Generator loss: 1.044976]\n",
      "7547 [Discriminator loss: 0.459782, acc.: 85.16%] [Generator loss: 1.049080]\n",
      "7548 [Discriminator loss: 0.605434, acc.: 67.19%] [Generator loss: 1.021114]\n",
      "7549 [Discriminator loss: 0.562192, acc.: 69.53%] [Generator loss: 1.294346]\n",
      "7550 [Discriminator loss: 0.573675, acc.: 74.22%] [Generator loss: 1.245566]\n",
      "7551 [Discriminator loss: 0.725782, acc.: 56.25%] [Generator loss: 1.226172]\n",
      "7552 [Discriminator loss: 0.831458, acc.: 53.91%] [Generator loss: 0.965236]\n",
      "7553 [Discriminator loss: 0.681406, acc.: 61.72%] [Generator loss: 1.065720]\n",
      "7554 [Discriminator loss: 0.717976, acc.: 57.03%] [Generator loss: 0.923990]\n",
      "7555 [Discriminator loss: 0.670777, acc.: 61.72%] [Generator loss: 1.038509]\n",
      "7556 [Discriminator loss: 0.496427, acc.: 78.12%] [Generator loss: 0.928373]\n",
      "7557 [Discriminator loss: 0.679418, acc.: 64.06%] [Generator loss: 1.105367]\n",
      "7558 [Discriminator loss: 0.653503, acc.: 65.62%] [Generator loss: 1.089750]\n",
      "7559 [Discriminator loss: 0.719563, acc.: 58.59%] [Generator loss: 0.809616]\n",
      "7560 [Discriminator loss: 0.866473, acc.: 49.22%] [Generator loss: 1.018057]\n",
      "7561 [Discriminator loss: 0.523446, acc.: 77.34%] [Generator loss: 1.414824]\n",
      "7562 [Discriminator loss: 0.729764, acc.: 55.47%] [Generator loss: 1.050611]\n",
      "7563 [Discriminator loss: 0.747600, acc.: 50.00%] [Generator loss: 0.973232]\n",
      "7564 [Discriminator loss: 0.833516, acc.: 48.44%] [Generator loss: 0.999858]\n",
      "7565 [Discriminator loss: 0.708709, acc.: 60.16%] [Generator loss: 0.938874]\n",
      "7566 [Discriminator loss: 0.720573, acc.: 53.91%] [Generator loss: 1.032507]\n",
      "7567 [Discriminator loss: 0.658072, acc.: 60.16%] [Generator loss: 1.208008]\n",
      "7568 [Discriminator loss: 0.620950, acc.: 66.41%] [Generator loss: 0.993584]\n",
      "7569 [Discriminator loss: 0.748524, acc.: 49.22%] [Generator loss: 1.094869]\n",
      "7570 [Discriminator loss: 0.583284, acc.: 68.75%] [Generator loss: 1.003431]\n",
      "7571 [Discriminator loss: 0.925957, acc.: 35.94%] [Generator loss: 0.909863]\n",
      "7572 [Discriminator loss: 0.679484, acc.: 60.16%] [Generator loss: 0.914127]\n",
      "7573 [Discriminator loss: 0.613729, acc.: 66.41%] [Generator loss: 1.187064]\n",
      "7574 [Discriminator loss: 0.568904, acc.: 71.88%] [Generator loss: 0.954671]\n",
      "7575 [Discriminator loss: 0.646356, acc.: 70.31%] [Generator loss: 0.900296]\n",
      "7576 [Discriminator loss: 0.706279, acc.: 54.69%] [Generator loss: 0.858974]\n",
      "7577 [Discriminator loss: 0.668918, acc.: 57.81%] [Generator loss: 1.021783]\n",
      "7578 [Discriminator loss: 0.556908, acc.: 75.00%] [Generator loss: 1.118504]\n",
      "7579 [Discriminator loss: 0.626395, acc.: 66.41%] [Generator loss: 0.993283]\n",
      "7580 [Discriminator loss: 0.621723, acc.: 67.19%] [Generator loss: 0.939802]\n",
      "7581 [Discriminator loss: 0.553955, acc.: 71.09%] [Generator loss: 0.884980]\n",
      "7582 [Discriminator loss: 0.636882, acc.: 61.72%] [Generator loss: 0.932433]\n",
      "7583 [Discriminator loss: 0.677999, acc.: 60.16%] [Generator loss: 1.065351]\n",
      "7584 [Discriminator loss: 0.753689, acc.: 54.69%] [Generator loss: 1.128383]\n",
      "7585 [Discriminator loss: 0.676392, acc.: 61.72%] [Generator loss: 1.351313]\n",
      "7586 [Discriminator loss: 0.589066, acc.: 68.75%] [Generator loss: 0.997150]\n",
      "7587 [Discriminator loss: 0.790692, acc.: 46.88%] [Generator loss: 0.914978]\n",
      "7588 [Discriminator loss: 0.701795, acc.: 60.16%] [Generator loss: 1.329645]\n",
      "7589 [Discriminator loss: 0.524220, acc.: 73.44%] [Generator loss: 1.344549]\n",
      "7590 [Discriminator loss: 0.564275, acc.: 75.78%] [Generator loss: 0.979754]\n",
      "7591 [Discriminator loss: 0.494796, acc.: 80.47%] [Generator loss: 0.844579]\n",
      "7592 [Discriminator loss: 0.671966, acc.: 63.28%] [Generator loss: 0.777608]\n",
      "7593 [Discriminator loss: 0.653879, acc.: 62.50%] [Generator loss: 0.806679]\n",
      "7594 [Discriminator loss: 0.517418, acc.: 74.22%] [Generator loss: 0.773305]\n",
      "7595 [Discriminator loss: 0.670228, acc.: 64.84%] [Generator loss: 0.822489]\n",
      "7596 [Discriminator loss: 0.740374, acc.: 55.47%] [Generator loss: 1.204446]\n",
      "7597 [Discriminator loss: 0.666634, acc.: 66.41%] [Generator loss: 1.042808]\n",
      "7598 [Discriminator loss: 0.691279, acc.: 58.59%] [Generator loss: 0.772965]\n",
      "7599 [Discriminator loss: 0.592401, acc.: 70.31%] [Generator loss: 0.827410]\n",
      "7600 [Discriminator loss: 0.583481, acc.: 69.53%] [Generator loss: 0.769239]\n",
      "7601 [Discriminator loss: 0.660307, acc.: 64.84%] [Generator loss: 0.802950]\n",
      "7602 [Discriminator loss: 0.721933, acc.: 55.47%] [Generator loss: 1.118232]\n",
      "7603 [Discriminator loss: 0.711254, acc.: 56.25%] [Generator loss: 1.243604]\n",
      "7604 [Discriminator loss: 0.723503, acc.: 59.38%] [Generator loss: 1.240518]\n",
      "7605 [Discriminator loss: 0.593733, acc.: 66.41%] [Generator loss: 1.188079]\n",
      "7606 [Discriminator loss: 0.598541, acc.: 67.97%] [Generator loss: 1.157028]\n",
      "7607 [Discriminator loss: 0.444971, acc.: 84.38%] [Generator loss: 1.025803]\n",
      "7608 [Discriminator loss: 0.468466, acc.: 83.59%] [Generator loss: 0.921828]\n",
      "7609 [Discriminator loss: 0.461983, acc.: 81.25%] [Generator loss: 0.786338]\n",
      "7610 [Discriminator loss: 0.488289, acc.: 78.91%] [Generator loss: 0.693035]\n",
      "7611 [Discriminator loss: 0.566932, acc.: 72.66%] [Generator loss: 0.677202]\n",
      "7612 [Discriminator loss: 0.729220, acc.: 59.38%] [Generator loss: 0.729817]\n",
      "7613 [Discriminator loss: 0.552328, acc.: 76.56%] [Generator loss: 0.978786]\n",
      "7614 [Discriminator loss: 0.553530, acc.: 66.41%] [Generator loss: 0.761546]\n",
      "7615 [Discriminator loss: 0.625442, acc.: 70.31%] [Generator loss: 0.648162]\n",
      "7616 [Discriminator loss: 0.667907, acc.: 55.47%] [Generator loss: 0.828009]\n",
      "7617 [Discriminator loss: 0.545856, acc.: 65.62%] [Generator loss: 0.919971]\n",
      "7618 [Discriminator loss: 0.615400, acc.: 67.97%] [Generator loss: 1.192693]\n",
      "7619 [Discriminator loss: 0.592965, acc.: 72.66%] [Generator loss: 0.777464]\n",
      "7620 [Discriminator loss: 0.560455, acc.: 71.88%] [Generator loss: 0.944644]\n",
      "7621 [Discriminator loss: 0.748771, acc.: 53.91%] [Generator loss: 0.938550]\n",
      "7622 [Discriminator loss: 0.694349, acc.: 60.16%] [Generator loss: 1.035711]\n",
      "7623 [Discriminator loss: 0.639620, acc.: 64.06%] [Generator loss: 0.831286]\n",
      "7624 [Discriminator loss: 0.519222, acc.: 78.12%] [Generator loss: 0.790077]\n",
      "7625 [Discriminator loss: 0.912189, acc.: 33.59%] [Generator loss: 0.700064]\n",
      "7626 [Discriminator loss: 0.407447, acc.: 85.16%] [Generator loss: 0.838307]\n",
      "7627 [Discriminator loss: 0.416280, acc.: 85.16%] [Generator loss: 0.713748]\n",
      "7628 [Discriminator loss: 0.638568, acc.: 62.50%] [Generator loss: 0.722627]\n",
      "7629 [Discriminator loss: 0.446146, acc.: 80.47%] [Generator loss: 1.009184]\n",
      "7630 [Discriminator loss: 0.618825, acc.: 63.28%] [Generator loss: 0.920653]\n",
      "7631 [Discriminator loss: 0.434054, acc.: 82.03%] [Generator loss: 0.711186]\n",
      "7632 [Discriminator loss: 0.456156, acc.: 82.03%] [Generator loss: 0.885030]\n",
      "7633 [Discriminator loss: 0.638983, acc.: 64.84%] [Generator loss: 0.863226]\n",
      "7634 [Discriminator loss: 0.839148, acc.: 50.00%] [Generator loss: 0.771080]\n",
      "7635 [Discriminator loss: 0.768049, acc.: 49.22%] [Generator loss: 0.779149]\n",
      "7636 [Discriminator loss: 0.675497, acc.: 59.38%] [Generator loss: 1.077879]\n",
      "7637 [Discriminator loss: 0.784826, acc.: 57.81%] [Generator loss: 1.269552]\n",
      "7638 [Discriminator loss: 0.675623, acc.: 67.97%] [Generator loss: 0.945867]\n",
      "7639 [Discriminator loss: 1.015780, acc.: 31.25%] [Generator loss: 0.752238]\n",
      "7640 [Discriminator loss: 0.742601, acc.: 57.03%] [Generator loss: 0.971916]\n",
      "7641 [Discriminator loss: 0.720207, acc.: 56.25%] [Generator loss: 0.975925]\n",
      "7642 [Discriminator loss: 0.607350, acc.: 69.53%] [Generator loss: 1.070765]\n",
      "7643 [Discriminator loss: 0.667027, acc.: 60.16%] [Generator loss: 1.011929]\n",
      "7644 [Discriminator loss: 0.645442, acc.: 64.06%] [Generator loss: 0.960019]\n",
      "7645 [Discriminator loss: 0.683489, acc.: 57.81%] [Generator loss: 1.026649]\n",
      "7646 [Discriminator loss: 0.564753, acc.: 67.97%] [Generator loss: 1.032575]\n",
      "7647 [Discriminator loss: 0.802738, acc.: 45.31%] [Generator loss: 0.978103]\n",
      "7648 [Discriminator loss: 0.641925, acc.: 64.06%] [Generator loss: 1.569670]\n",
      "7649 [Discriminator loss: 0.765315, acc.: 55.47%] [Generator loss: 1.103668]\n",
      "7650 [Discriminator loss: 0.628234, acc.: 64.84%] [Generator loss: 1.130157]\n",
      "7651 [Discriminator loss: 0.803531, acc.: 43.75%] [Generator loss: 0.833511]\n",
      "7652 [Discriminator loss: 0.635063, acc.: 67.19%] [Generator loss: 1.206917]\n",
      "7653 [Discriminator loss: 0.709234, acc.: 57.81%] [Generator loss: 1.166928]\n",
      "7654 [Discriminator loss: 0.756000, acc.: 46.09%] [Generator loss: 0.835564]\n",
      "7655 [Discriminator loss: 0.735714, acc.: 49.22%] [Generator loss: 1.076294]\n",
      "7656 [Discriminator loss: 0.611246, acc.: 65.62%] [Generator loss: 1.013887]\n",
      "7657 [Discriminator loss: 0.666140, acc.: 60.94%] [Generator loss: 0.899049]\n",
      "7658 [Discriminator loss: 0.667223, acc.: 61.72%] [Generator loss: 0.780137]\n",
      "7659 [Discriminator loss: 0.474269, acc.: 80.47%] [Generator loss: 0.816972]\n",
      "7660 [Discriminator loss: 0.650658, acc.: 61.72%] [Generator loss: 0.930390]\n",
      "7661 [Discriminator loss: 0.648603, acc.: 60.16%] [Generator loss: 0.914789]\n",
      "7662 [Discriminator loss: 0.632379, acc.: 63.28%] [Generator loss: 0.881424]\n",
      "7663 [Discriminator loss: 0.548520, acc.: 78.12%] [Generator loss: 0.906710]\n",
      "7664 [Discriminator loss: 0.827661, acc.: 50.00%] [Generator loss: 0.761687]\n",
      "7665 [Discriminator loss: 0.664953, acc.: 57.81%] [Generator loss: 0.778102]\n",
      "7666 [Discriminator loss: 0.733829, acc.: 52.34%] [Generator loss: 0.975976]\n",
      "7667 [Discriminator loss: 0.601559, acc.: 73.44%] [Generator loss: 1.089159]\n",
      "7668 [Discriminator loss: 0.744351, acc.: 48.44%] [Generator loss: 1.100552]\n",
      "7669 [Discriminator loss: 0.738777, acc.: 53.12%] [Generator loss: 1.176708]\n",
      "7670 [Discriminator loss: 0.596881, acc.: 73.44%] [Generator loss: 1.023013]\n",
      "7671 [Discriminator loss: 0.612907, acc.: 70.31%] [Generator loss: 1.096121]\n",
      "7672 [Discriminator loss: 0.614847, acc.: 71.09%] [Generator loss: 1.124048]\n",
      "7673 [Discriminator loss: 0.551295, acc.: 77.34%] [Generator loss: 1.086594]\n",
      "7674 [Discriminator loss: 0.600978, acc.: 67.19%] [Generator loss: 1.067682]\n",
      "7675 [Discriminator loss: 0.513725, acc.: 71.88%] [Generator loss: 1.030163]\n",
      "7676 [Discriminator loss: 0.532066, acc.: 79.69%] [Generator loss: 0.926055]\n",
      "7677 [Discriminator loss: 0.749025, acc.: 50.00%] [Generator loss: 1.042502]\n",
      "7678 [Discriminator loss: 0.630500, acc.: 61.72%] [Generator loss: 0.743024]\n",
      "7679 [Discriminator loss: 0.641814, acc.: 62.50%] [Generator loss: 1.002344]\n",
      "7680 [Discriminator loss: 0.663748, acc.: 56.25%] [Generator loss: 1.454833]\n",
      "7681 [Discriminator loss: 0.657146, acc.: 64.84%] [Generator loss: 1.075840]\n",
      "7682 [Discriminator loss: 0.503076, acc.: 75.78%] [Generator loss: 0.726347]\n",
      "7683 [Discriminator loss: 0.733283, acc.: 54.69%] [Generator loss: 0.745789]\n",
      "7684 [Discriminator loss: 0.406434, acc.: 83.59%] [Generator loss: 0.780913]\n",
      "7685 [Discriminator loss: 1.055903, acc.: 32.03%] [Generator loss: 0.798999]\n",
      "7686 [Discriminator loss: 0.627934, acc.: 65.62%] [Generator loss: 1.243163]\n",
      "7687 [Discriminator loss: 0.913331, acc.: 43.75%] [Generator loss: 1.273414]\n",
      "7688 [Discriminator loss: 0.680564, acc.: 61.72%] [Generator loss: 1.313719]\n",
      "7689 [Discriminator loss: 0.473450, acc.: 84.38%] [Generator loss: 1.223381]\n",
      "7690 [Discriminator loss: 0.590521, acc.: 67.19%] [Generator loss: 1.095334]\n",
      "7691 [Discriminator loss: 0.646326, acc.: 67.97%] [Generator loss: 1.004452]\n",
      "7692 [Discriminator loss: 0.686165, acc.: 63.28%] [Generator loss: 0.912140]\n",
      "7693 [Discriminator loss: 0.727433, acc.: 52.34%] [Generator loss: 0.782649]\n",
      "7694 [Discriminator loss: 0.849744, acc.: 48.44%] [Generator loss: 0.758669]\n",
      "7695 [Discriminator loss: 0.734007, acc.: 57.03%] [Generator loss: 0.909232]\n",
      "7696 [Discriminator loss: 0.678805, acc.: 57.03%] [Generator loss: 0.907765]\n",
      "7697 [Discriminator loss: 0.453523, acc.: 80.47%] [Generator loss: 0.919336]\n",
      "7698 [Discriminator loss: 0.613745, acc.: 66.41%] [Generator loss: 0.834185]\n",
      "7699 [Discriminator loss: 0.660521, acc.: 65.62%] [Generator loss: 0.911318]\n",
      "7700 [Discriminator loss: 0.602284, acc.: 64.84%] [Generator loss: 0.849219]\n",
      "7701 [Discriminator loss: 0.878198, acc.: 42.97%] [Generator loss: 0.772013]\n",
      "7702 [Discriminator loss: 0.662110, acc.: 59.38%] [Generator loss: 0.824706]\n",
      "7703 [Discriminator loss: 0.658589, acc.: 57.03%] [Generator loss: 0.895538]\n",
      "7704 [Discriminator loss: 0.511071, acc.: 78.12%] [Generator loss: 1.042061]\n",
      "7705 [Discriminator loss: 0.561622, acc.: 76.56%] [Generator loss: 1.007682]\n",
      "7706 [Discriminator loss: 0.665433, acc.: 61.72%] [Generator loss: 0.939348]\n",
      "7707 [Discriminator loss: 0.706384, acc.: 52.34%] [Generator loss: 0.917996]\n",
      "7708 [Discriminator loss: 0.658376, acc.: 60.16%] [Generator loss: 1.148085]\n",
      "7709 [Discriminator loss: 0.604073, acc.: 72.66%] [Generator loss: 1.052039]\n",
      "7710 [Discriminator loss: 0.569820, acc.: 70.31%] [Generator loss: 0.853113]\n",
      "7711 [Discriminator loss: 0.746606, acc.: 53.91%] [Generator loss: 0.963902]\n",
      "7712 [Discriminator loss: 0.644474, acc.: 60.16%] [Generator loss: 0.943191]\n",
      "7713 [Discriminator loss: 0.579402, acc.: 75.78%] [Generator loss: 0.920540]\n",
      "7714 [Discriminator loss: 0.782499, acc.: 48.44%] [Generator loss: 0.896828]\n",
      "7715 [Discriminator loss: 0.724907, acc.: 57.03%] [Generator loss: 0.955072]\n",
      "7716 [Discriminator loss: 0.615432, acc.: 68.75%] [Generator loss: 1.140620]\n",
      "7717 [Discriminator loss: 0.658998, acc.: 62.50%] [Generator loss: 1.102176]\n",
      "7718 [Discriminator loss: 0.574395, acc.: 74.22%] [Generator loss: 1.038962]\n",
      "7719 [Discriminator loss: 0.569465, acc.: 71.88%] [Generator loss: 0.827328]\n",
      "7720 [Discriminator loss: 0.573926, acc.: 69.53%] [Generator loss: 0.986881]\n",
      "7721 [Discriminator loss: 0.663294, acc.: 60.16%] [Generator loss: 1.143896]\n",
      "7722 [Discriminator loss: 0.724972, acc.: 56.25%] [Generator loss: 1.090675]\n",
      "7723 [Discriminator loss: 0.703633, acc.: 59.38%] [Generator loss: 0.999956]\n",
      "7724 [Discriminator loss: 0.692254, acc.: 53.91%] [Generator loss: 0.957097]\n",
      "7725 [Discriminator loss: 0.763694, acc.: 49.22%] [Generator loss: 1.150708]\n",
      "7726 [Discriminator loss: 0.628088, acc.: 67.97%] [Generator loss: 1.067133]\n",
      "7727 [Discriminator loss: 0.754509, acc.: 50.00%] [Generator loss: 1.037133]\n",
      "7728 [Discriminator loss: 0.665704, acc.: 59.38%] [Generator loss: 0.789436]\n",
      "7729 [Discriminator loss: 0.777032, acc.: 53.12%] [Generator loss: 0.975129]\n",
      "7730 [Discriminator loss: 0.676337, acc.: 59.38%] [Generator loss: 1.035083]\n",
      "7731 [Discriminator loss: 0.756141, acc.: 49.22%] [Generator loss: 1.002028]\n",
      "7732 [Discriminator loss: 0.679270, acc.: 64.06%] [Generator loss: 1.012046]\n",
      "7733 [Discriminator loss: 0.920073, acc.: 42.97%] [Generator loss: 0.729091]\n",
      "7734 [Discriminator loss: 0.593677, acc.: 64.84%] [Generator loss: 0.927862]\n",
      "7735 [Discriminator loss: 0.506697, acc.: 81.25%] [Generator loss: 0.825977]\n",
      "7736 [Discriminator loss: 0.704590, acc.: 56.25%] [Generator loss: 0.739389]\n",
      "7737 [Discriminator loss: 0.691013, acc.: 55.47%] [Generator loss: 0.847486]\n",
      "7738 [Discriminator loss: 0.524831, acc.: 78.91%] [Generator loss: 0.913387]\n",
      "7739 [Discriminator loss: 0.843988, acc.: 49.22%] [Generator loss: 1.119431]\n",
      "7740 [Discriminator loss: 0.621786, acc.: 62.50%] [Generator loss: 1.299439]\n",
      "7741 [Discriminator loss: 0.708245, acc.: 60.16%] [Generator loss: 1.119384]\n",
      "7742 [Discriminator loss: 0.647802, acc.: 62.50%] [Generator loss: 1.047289]\n",
      "7743 [Discriminator loss: 0.641945, acc.: 63.28%] [Generator loss: 0.844661]\n",
      "7744 [Discriminator loss: 0.681988, acc.: 66.41%] [Generator loss: 0.813840]\n",
      "7745 [Discriminator loss: 0.749780, acc.: 50.78%] [Generator loss: 0.877197]\n",
      "7746 [Discriminator loss: 0.670549, acc.: 55.47%] [Generator loss: 1.081274]\n",
      "7747 [Discriminator loss: 0.742026, acc.: 52.34%] [Generator loss: 0.973625]\n",
      "7748 [Discriminator loss: 0.759098, acc.: 51.56%] [Generator loss: 1.489495]\n",
      "7749 [Discriminator loss: 0.606452, acc.: 68.75%] [Generator loss: 1.533067]\n",
      "7750 [Discriminator loss: 0.719465, acc.: 55.47%] [Generator loss: 1.028188]\n",
      "7751 [Discriminator loss: 0.654913, acc.: 65.62%] [Generator loss: 0.861457]\n",
      "7752 [Discriminator loss: 0.838010, acc.: 42.19%] [Generator loss: 0.842443]\n",
      "7753 [Discriminator loss: 0.673057, acc.: 60.16%] [Generator loss: 0.897138]\n",
      "7754 [Discriminator loss: 0.591219, acc.: 71.09%] [Generator loss: 1.130557]\n",
      "7755 [Discriminator loss: 0.736543, acc.: 50.00%] [Generator loss: 1.119189]\n",
      "7756 [Discriminator loss: 0.662570, acc.: 60.16%] [Generator loss: 1.029971]\n",
      "7757 [Discriminator loss: 0.561648, acc.: 72.66%] [Generator loss: 1.017564]\n",
      "7758 [Discriminator loss: 0.624904, acc.: 68.75%] [Generator loss: 1.085379]\n",
      "7759 [Discriminator loss: 0.637236, acc.: 61.72%] [Generator loss: 1.194301]\n",
      "7760 [Discriminator loss: 0.710756, acc.: 61.72%] [Generator loss: 1.227055]\n",
      "7761 [Discriminator loss: 0.611894, acc.: 67.19%] [Generator loss: 0.909720]\n",
      "7762 [Discriminator loss: 0.691432, acc.: 58.59%] [Generator loss: 0.762584]\n",
      "7763 [Discriminator loss: 0.621717, acc.: 57.03%] [Generator loss: 0.960859]\n",
      "7764 [Discriminator loss: 0.617492, acc.: 69.53%] [Generator loss: 0.895675]\n",
      "7765 [Discriminator loss: 0.550527, acc.: 74.22%] [Generator loss: 0.942770]\n",
      "7766 [Discriminator loss: 0.529143, acc.: 73.44%] [Generator loss: 1.072351]\n",
      "7767 [Discriminator loss: 0.762738, acc.: 52.34%] [Generator loss: 1.007303]\n",
      "7768 [Discriminator loss: 0.606061, acc.: 71.09%] [Generator loss: 1.046556]\n",
      "7769 [Discriminator loss: 0.669144, acc.: 60.94%] [Generator loss: 1.275467]\n",
      "7770 [Discriminator loss: 0.740634, acc.: 54.69%] [Generator loss: 1.331193]\n",
      "7771 [Discriminator loss: 0.491845, acc.: 80.47%] [Generator loss: 0.982759]\n",
      "7772 [Discriminator loss: 0.538916, acc.: 73.44%] [Generator loss: 0.628100]\n",
      "7773 [Discriminator loss: 0.808571, acc.: 46.88%] [Generator loss: 0.799663]\n",
      "7774 [Discriminator loss: 0.933302, acc.: 36.72%] [Generator loss: 1.164607]\n",
      "7775 [Discriminator loss: 0.589306, acc.: 67.97%] [Generator loss: 1.183585]\n",
      "7776 [Discriminator loss: 0.722060, acc.: 55.47%] [Generator loss: 1.110081]\n",
      "7777 [Discriminator loss: 0.597695, acc.: 68.75%] [Generator loss: 1.115012]\n",
      "7778 [Discriminator loss: 0.598769, acc.: 70.31%] [Generator loss: 0.980546]\n",
      "7779 [Discriminator loss: 0.590631, acc.: 74.22%] [Generator loss: 1.032766]\n",
      "7780 [Discriminator loss: 0.641419, acc.: 62.50%] [Generator loss: 1.054239]\n",
      "7781 [Discriminator loss: 0.582977, acc.: 70.31%] [Generator loss: 1.156166]\n",
      "7782 [Discriminator loss: 0.471701, acc.: 77.34%] [Generator loss: 1.308599]\n",
      "7783 [Discriminator loss: 0.620227, acc.: 67.19%] [Generator loss: 1.039227]\n",
      "7784 [Discriminator loss: 0.627050, acc.: 64.84%] [Generator loss: 0.988916]\n",
      "7785 [Discriminator loss: 0.619537, acc.: 63.28%] [Generator loss: 0.842875]\n",
      "7786 [Discriminator loss: 0.665002, acc.: 56.25%] [Generator loss: 0.857942]\n",
      "7787 [Discriminator loss: 0.709957, acc.: 53.12%] [Generator loss: 0.770276]\n",
      "7788 [Discriminator loss: 0.616412, acc.: 63.28%] [Generator loss: 0.844555]\n",
      "7789 [Discriminator loss: 0.783544, acc.: 48.44%] [Generator loss: 0.870143]\n",
      "7790 [Discriminator loss: 0.872466, acc.: 40.62%] [Generator loss: 0.939708]\n",
      "7791 [Discriminator loss: 0.670678, acc.: 53.12%] [Generator loss: 1.270787]\n",
      "7792 [Discriminator loss: 0.700958, acc.: 57.81%] [Generator loss: 1.270144]\n",
      "7793 [Discriminator loss: 0.697698, acc.: 60.94%] [Generator loss: 1.106637]\n",
      "7794 [Discriminator loss: 0.552657, acc.: 72.66%] [Generator loss: 0.975485]\n",
      "7795 [Discriminator loss: 0.690142, acc.: 60.16%] [Generator loss: 1.073290]\n",
      "7796 [Discriminator loss: 0.512300, acc.: 78.91%] [Generator loss: 1.188966]\n",
      "7797 [Discriminator loss: 0.736611, acc.: 48.44%] [Generator loss: 0.903849]\n",
      "7798 [Discriminator loss: 0.716559, acc.: 50.78%] [Generator loss: 0.803409]\n",
      "7799 [Discriminator loss: 0.586594, acc.: 70.31%] [Generator loss: 0.848303]\n",
      "7800 [Discriminator loss: 0.658922, acc.: 62.50%] [Generator loss: 0.838385]\n",
      "7801 [Discriminator loss: 0.575028, acc.: 72.66%] [Generator loss: 0.895149]\n",
      "7802 [Discriminator loss: 0.641984, acc.: 64.84%] [Generator loss: 1.002295]\n",
      "7803 [Discriminator loss: 0.556538, acc.: 71.88%] [Generator loss: 0.992711]\n",
      "7804 [Discriminator loss: 0.710044, acc.: 53.12%] [Generator loss: 0.868106]\n",
      "7805 [Discriminator loss: 0.710191, acc.: 52.34%] [Generator loss: 1.182066]\n",
      "7806 [Discriminator loss: 0.570990, acc.: 68.75%] [Generator loss: 0.980760]\n",
      "7807 [Discriminator loss: 0.593533, acc.: 70.31%] [Generator loss: 1.091073]\n",
      "7808 [Discriminator loss: 0.659554, acc.: 60.16%] [Generator loss: 0.907593]\n",
      "7809 [Discriminator loss: 0.658498, acc.: 58.59%] [Generator loss: 0.926268]\n",
      "7810 [Discriminator loss: 0.777701, acc.: 50.78%] [Generator loss: 1.106425]\n",
      "7811 [Discriminator loss: 0.523800, acc.: 71.88%] [Generator loss: 1.184496]\n",
      "7812 [Discriminator loss: 0.731197, acc.: 57.03%] [Generator loss: 0.865945]\n",
      "7813 [Discriminator loss: 0.676246, acc.: 57.03%] [Generator loss: 0.848081]\n",
      "7814 [Discriminator loss: 0.574136, acc.: 74.22%] [Generator loss: 0.871502]\n",
      "7815 [Discriminator loss: 0.940440, acc.: 34.38%] [Generator loss: 0.719950]\n",
      "7816 [Discriminator loss: 0.603418, acc.: 64.84%] [Generator loss: 1.041202]\n",
      "7817 [Discriminator loss: 0.731998, acc.: 57.03%] [Generator loss: 1.047295]\n",
      "7818 [Discriminator loss: 0.700283, acc.: 59.38%] [Generator loss: 0.848475]\n",
      "7819 [Discriminator loss: 0.659827, acc.: 59.38%] [Generator loss: 1.107578]\n",
      "7820 [Discriminator loss: 0.660423, acc.: 58.59%] [Generator loss: 1.095538]\n",
      "7821 [Discriminator loss: 0.483299, acc.: 78.12%] [Generator loss: 1.096715]\n",
      "7822 [Discriminator loss: 0.640795, acc.: 65.62%] [Generator loss: 1.196525]\n",
      "7823 [Discriminator loss: 0.618446, acc.: 65.62%] [Generator loss: 1.104583]\n",
      "7824 [Discriminator loss: 0.778411, acc.: 49.22%] [Generator loss: 1.158508]\n",
      "7825 [Discriminator loss: 0.608559, acc.: 66.41%] [Generator loss: 1.352235]\n",
      "7826 [Discriminator loss: 0.490239, acc.: 80.47%] [Generator loss: 1.273591]\n",
      "7827 [Discriminator loss: 0.594407, acc.: 69.53%] [Generator loss: 0.990051]\n",
      "7828 [Discriminator loss: 0.604551, acc.: 70.31%] [Generator loss: 0.868073]\n",
      "7829 [Discriminator loss: 0.509358, acc.: 79.69%] [Generator loss: 0.954081]\n",
      "7830 [Discriminator loss: 0.641003, acc.: 65.62%] [Generator loss: 1.147573]\n",
      "7831 [Discriminator loss: 0.717068, acc.: 57.03%] [Generator loss: 0.851288]\n",
      "7832 [Discriminator loss: 0.660239, acc.: 53.12%] [Generator loss: 0.891796]\n",
      "7833 [Discriminator loss: 0.673590, acc.: 65.62%] [Generator loss: 1.036506]\n",
      "7834 [Discriminator loss: 0.508392, acc.: 77.34%] [Generator loss: 1.208042]\n",
      "7835 [Discriminator loss: 0.592933, acc.: 68.75%] [Generator loss: 1.102417]\n",
      "7836 [Discriminator loss: 0.698548, acc.: 63.28%] [Generator loss: 0.942896]\n",
      "7837 [Discriminator loss: 0.676920, acc.: 59.38%] [Generator loss: 0.921017]\n",
      "7838 [Discriminator loss: 0.638521, acc.: 67.97%] [Generator loss: 0.851399]\n",
      "7839 [Discriminator loss: 0.680718, acc.: 54.69%] [Generator loss: 0.702380]\n",
      "7840 [Discriminator loss: 0.700765, acc.: 54.69%] [Generator loss: 0.886557]\n",
      "7841 [Discriminator loss: 0.700361, acc.: 57.03%] [Generator loss: 0.997649]\n",
      "7842 [Discriminator loss: 0.580063, acc.: 71.88%] [Generator loss: 0.765273]\n",
      "7843 [Discriminator loss: 0.734331, acc.: 59.38%] [Generator loss: 0.682653]\n",
      "7844 [Discriminator loss: 0.636297, acc.: 63.28%] [Generator loss: 0.877740]\n",
      "7845 [Discriminator loss: 0.609780, acc.: 71.09%] [Generator loss: 1.152843]\n",
      "7846 [Discriminator loss: 0.675575, acc.: 60.16%] [Generator loss: 1.015323]\n",
      "7847 [Discriminator loss: 0.948563, acc.: 39.06%] [Generator loss: 1.032145]\n",
      "7848 [Discriminator loss: 0.562105, acc.: 75.78%] [Generator loss: 1.012415]\n",
      "7849 [Discriminator loss: 0.653894, acc.: 67.97%] [Generator loss: 0.891969]\n",
      "7850 [Discriminator loss: 0.502977, acc.: 81.25%] [Generator loss: 0.846574]\n",
      "7851 [Discriminator loss: 0.905436, acc.: 46.88%] [Generator loss: 0.663771]\n",
      "7852 [Discriminator loss: 0.755553, acc.: 53.91%] [Generator loss: 1.082186]\n",
      "7853 [Discriminator loss: 0.652428, acc.: 69.53%] [Generator loss: 1.027337]\n",
      "7854 [Discriminator loss: 0.615414, acc.: 64.84%] [Generator loss: 1.254645]\n",
      "7855 [Discriminator loss: 0.599788, acc.: 64.06%] [Generator loss: 1.190024]\n",
      "7856 [Discriminator loss: 0.643083, acc.: 66.41%] [Generator loss: 1.053687]\n",
      "7857 [Discriminator loss: 0.584283, acc.: 71.88%] [Generator loss: 0.992597]\n",
      "7858 [Discriminator loss: 0.565865, acc.: 78.12%] [Generator loss: 0.955653]\n",
      "7859 [Discriminator loss: 0.698283, acc.: 57.03%] [Generator loss: 0.999670]\n",
      "7860 [Discriminator loss: 0.662701, acc.: 62.50%] [Generator loss: 0.975342]\n",
      "7861 [Discriminator loss: 0.586681, acc.: 66.41%] [Generator loss: 1.001863]\n",
      "7862 [Discriminator loss: 0.602406, acc.: 64.84%] [Generator loss: 0.906853]\n",
      "7863 [Discriminator loss: 0.533845, acc.: 75.00%] [Generator loss: 0.814808]\n",
      "7864 [Discriminator loss: 0.497151, acc.: 78.91%] [Generator loss: 0.705897]\n",
      "7865 [Discriminator loss: 0.655470, acc.: 64.06%] [Generator loss: 0.631463]\n",
      "7866 [Discriminator loss: 0.980774, acc.: 40.62%] [Generator loss: 1.302340]\n",
      "7867 [Discriminator loss: 0.572554, acc.: 68.75%] [Generator loss: 1.294959]\n",
      "7868 [Discriminator loss: 0.590716, acc.: 69.53%] [Generator loss: 1.023585]\n",
      "7869 [Discriminator loss: 0.911269, acc.: 43.75%] [Generator loss: 0.770578]\n",
      "7870 [Discriminator loss: 0.766054, acc.: 58.59%] [Generator loss: 0.977931]\n",
      "7871 [Discriminator loss: 0.681006, acc.: 58.59%] [Generator loss: 1.347123]\n",
      "7872 [Discriminator loss: 0.639899, acc.: 67.19%] [Generator loss: 1.202924]\n",
      "7873 [Discriminator loss: 0.643396, acc.: 64.84%] [Generator loss: 1.214043]\n",
      "7874 [Discriminator loss: 0.623726, acc.: 67.19%] [Generator loss: 1.206451]\n",
      "7875 [Discriminator loss: 0.570784, acc.: 78.12%] [Generator loss: 1.161041]\n",
      "7876 [Discriminator loss: 0.810281, acc.: 48.44%] [Generator loss: 0.870474]\n",
      "7877 [Discriminator loss: 0.649408, acc.: 63.28%] [Generator loss: 0.745290]\n",
      "7878 [Discriminator loss: 0.584209, acc.: 71.88%] [Generator loss: 0.877077]\n",
      "7879 [Discriminator loss: 0.782406, acc.: 56.25%] [Generator loss: 1.080331]\n",
      "7880 [Discriminator loss: 0.514344, acc.: 81.25%] [Generator loss: 0.993291]\n",
      "7881 [Discriminator loss: 0.581612, acc.: 71.09%] [Generator loss: 0.803101]\n",
      "7882 [Discriminator loss: 0.602781, acc.: 67.19%] [Generator loss: 0.779824]\n",
      "7883 [Discriminator loss: 0.666575, acc.: 60.94%] [Generator loss: 0.839113]\n",
      "7884 [Discriminator loss: 0.511966, acc.: 78.91%] [Generator loss: 1.044130]\n",
      "7885 [Discriminator loss: 0.694181, acc.: 54.69%] [Generator loss: 1.029714]\n",
      "7886 [Discriminator loss: 0.565199, acc.: 72.66%] [Generator loss: 1.149312]\n",
      "7887 [Discriminator loss: 0.532067, acc.: 73.44%] [Generator loss: 0.999050]\n",
      "7888 [Discriminator loss: 0.808693, acc.: 46.88%] [Generator loss: 1.219769]\n",
      "7889 [Discriminator loss: 0.613484, acc.: 66.41%] [Generator loss: 1.043855]\n",
      "7890 [Discriminator loss: 0.732748, acc.: 52.34%] [Generator loss: 1.119218]\n",
      "7891 [Discriminator loss: 0.728012, acc.: 54.69%] [Generator loss: 1.032944]\n",
      "7892 [Discriminator loss: 0.503131, acc.: 76.56%] [Generator loss: 1.031571]\n",
      "7893 [Discriminator loss: 0.634525, acc.: 60.94%] [Generator loss: 1.113628]\n",
      "7894 [Discriminator loss: 0.607796, acc.: 67.19%] [Generator loss: 1.033760]\n",
      "7895 [Discriminator loss: 0.580454, acc.: 69.53%] [Generator loss: 0.894418]\n",
      "7896 [Discriminator loss: 0.823757, acc.: 50.78%] [Generator loss: 0.845002]\n",
      "7897 [Discriminator loss: 0.614637, acc.: 66.41%] [Generator loss: 0.934627]\n",
      "7898 [Discriminator loss: 0.829914, acc.: 45.31%] [Generator loss: 1.064297]\n",
      "7899 [Discriminator loss: 0.694657, acc.: 57.03%] [Generator loss: 1.151021]\n",
      "7900 [Discriminator loss: 0.624703, acc.: 67.19%] [Generator loss: 1.055941]\n",
      "7901 [Discriminator loss: 0.559326, acc.: 72.66%] [Generator loss: 0.997195]\n",
      "7902 [Discriminator loss: 0.584443, acc.: 67.97%] [Generator loss: 1.170877]\n",
      "7903 [Discriminator loss: 0.699487, acc.: 58.59%] [Generator loss: 0.968133]\n",
      "7904 [Discriminator loss: 0.505577, acc.: 78.12%] [Generator loss: 1.117038]\n",
      "7905 [Discriminator loss: 0.716206, acc.: 56.25%] [Generator loss: 1.112340]\n",
      "7906 [Discriminator loss: 0.629339, acc.: 68.75%] [Generator loss: 1.070438]\n",
      "7907 [Discriminator loss: 0.789711, acc.: 52.34%] [Generator loss: 0.950922]\n",
      "7908 [Discriminator loss: 0.667210, acc.: 64.84%] [Generator loss: 0.879464]\n",
      "7909 [Discriminator loss: 0.729634, acc.: 50.78%] [Generator loss: 0.808768]\n",
      "7910 [Discriminator loss: 0.673575, acc.: 59.38%] [Generator loss: 1.111310]\n",
      "7911 [Discriminator loss: 0.646824, acc.: 70.31%] [Generator loss: 1.142609]\n",
      "7912 [Discriminator loss: 0.516292, acc.: 76.56%] [Generator loss: 0.878623]\n",
      "7913 [Discriminator loss: 0.602679, acc.: 68.75%] [Generator loss: 0.789481]\n",
      "7914 [Discriminator loss: 0.642563, acc.: 63.28%] [Generator loss: 0.788129]\n",
      "7915 [Discriminator loss: 0.792642, acc.: 50.78%] [Generator loss: 0.686408]\n",
      "7916 [Discriminator loss: 0.718954, acc.: 56.25%] [Generator loss: 0.790641]\n",
      "7917 [Discriminator loss: 0.634070, acc.: 62.50%] [Generator loss: 0.846399]\n",
      "7918 [Discriminator loss: 0.697450, acc.: 59.38%] [Generator loss: 0.746936]\n",
      "7919 [Discriminator loss: 0.570126, acc.: 69.53%] [Generator loss: 0.949560]\n",
      "7920 [Discriminator loss: 0.801216, acc.: 46.09%] [Generator loss: 0.979292]\n",
      "7921 [Discriminator loss: 0.636774, acc.: 64.84%] [Generator loss: 0.861100]\n",
      "7922 [Discriminator loss: 0.842458, acc.: 48.44%] [Generator loss: 1.043913]\n",
      "7923 [Discriminator loss: 0.805232, acc.: 47.66%] [Generator loss: 1.011407]\n",
      "7924 [Discriminator loss: 0.601332, acc.: 63.28%] [Generator loss: 1.187247]\n",
      "7925 [Discriminator loss: 0.848659, acc.: 40.62%] [Generator loss: 1.112844]\n",
      "7926 [Discriminator loss: 0.683342, acc.: 57.81%] [Generator loss: 1.216190]\n",
      "7927 [Discriminator loss: 0.719151, acc.: 57.03%] [Generator loss: 1.172406]\n",
      "7928 [Discriminator loss: 0.674168, acc.: 60.94%] [Generator loss: 1.181394]\n",
      "7929 [Discriminator loss: 0.680958, acc.: 61.72%] [Generator loss: 0.828934]\n",
      "7930 [Discriminator loss: 0.753914, acc.: 49.22%] [Generator loss: 0.691051]\n",
      "7931 [Discriminator loss: 0.807087, acc.: 36.72%] [Generator loss: 0.737869]\n",
      "7932 [Discriminator loss: 0.760614, acc.: 54.69%] [Generator loss: 1.180181]\n",
      "7933 [Discriminator loss: 0.650985, acc.: 67.19%] [Generator loss: 1.039222]\n",
      "7934 [Discriminator loss: 0.537263, acc.: 73.44%] [Generator loss: 1.027881]\n",
      "7935 [Discriminator loss: 0.569948, acc.: 72.66%] [Generator loss: 1.129951]\n",
      "7936 [Discriminator loss: 0.671429, acc.: 64.84%] [Generator loss: 0.814154]\n",
      "7937 [Discriminator loss: 0.634224, acc.: 57.81%] [Generator loss: 0.688702]\n",
      "7938 [Discriminator loss: 0.639144, acc.: 64.84%] [Generator loss: 0.806851]\n",
      "7939 [Discriminator loss: 0.565930, acc.: 72.66%] [Generator loss: 0.991782]\n",
      "7940 [Discriminator loss: 0.529757, acc.: 72.66%] [Generator loss: 1.155118]\n",
      "7941 [Discriminator loss: 0.515349, acc.: 78.91%] [Generator loss: 0.958212]\n",
      "7942 [Discriminator loss: 0.518390, acc.: 74.22%] [Generator loss: 1.268982]\n",
      "7943 [Discriminator loss: 0.620420, acc.: 63.28%] [Generator loss: 1.260152]\n",
      "7944 [Discriminator loss: 0.754044, acc.: 46.88%] [Generator loss: 0.789122]\n",
      "7945 [Discriminator loss: 0.593903, acc.: 71.88%] [Generator loss: 0.881729]\n",
      "7946 [Discriminator loss: 0.649470, acc.: 61.72%] [Generator loss: 0.853142]\n",
      "7947 [Discriminator loss: 0.704295, acc.: 56.25%] [Generator loss: 0.848884]\n",
      "7948 [Discriminator loss: 0.690694, acc.: 63.28%] [Generator loss: 0.849707]\n",
      "7949 [Discriminator loss: 0.562107, acc.: 77.34%] [Generator loss: 0.895885]\n",
      "7950 [Discriminator loss: 0.741463, acc.: 50.78%] [Generator loss: 0.945170]\n",
      "7951 [Discriminator loss: 0.660741, acc.: 57.81%] [Generator loss: 1.084389]\n",
      "7952 [Discriminator loss: 0.521572, acc.: 82.81%] [Generator loss: 0.886098]\n",
      "7953 [Discriminator loss: 0.655369, acc.: 60.16%] [Generator loss: 1.084854]\n",
      "7954 [Discriminator loss: 0.890492, acc.: 35.94%] [Generator loss: 1.114955]\n",
      "7955 [Discriminator loss: 0.531303, acc.: 75.00%] [Generator loss: 1.233488]\n",
      "7956 [Discriminator loss: 0.931362, acc.: 38.28%] [Generator loss: 1.057831]\n",
      "7957 [Discriminator loss: 0.851310, acc.: 55.47%] [Generator loss: 1.456670]\n",
      "7958 [Discriminator loss: 0.693617, acc.: 62.50%] [Generator loss: 1.505898]\n",
      "7959 [Discriminator loss: 0.671094, acc.: 59.38%] [Generator loss: 1.212956]\n",
      "7960 [Discriminator loss: 0.793919, acc.: 42.97%] [Generator loss: 0.984799]\n",
      "7961 [Discriminator loss: 0.598331, acc.: 65.62%] [Generator loss: 1.225335]\n",
      "7962 [Discriminator loss: 0.637152, acc.: 63.28%] [Generator loss: 0.992866]\n",
      "7963 [Discriminator loss: 0.720889, acc.: 49.22%] [Generator loss: 0.814595]\n",
      "7964 [Discriminator loss: 0.586169, acc.: 72.66%] [Generator loss: 0.914292]\n",
      "7965 [Discriminator loss: 0.664639, acc.: 65.62%] [Generator loss: 0.769243]\n",
      "7966 [Discriminator loss: 0.547403, acc.: 75.78%] [Generator loss: 1.008660]\n",
      "7967 [Discriminator loss: 0.582558, acc.: 63.28%] [Generator loss: 1.008155]\n",
      "7968 [Discriminator loss: 0.841208, acc.: 45.31%] [Generator loss: 0.979438]\n",
      "7969 [Discriminator loss: 0.497072, acc.: 77.34%] [Generator loss: 1.040450]\n",
      "7970 [Discriminator loss: 0.674328, acc.: 58.59%] [Generator loss: 1.087608]\n",
      "7971 [Discriminator loss: 0.667254, acc.: 60.16%] [Generator loss: 1.053663]\n",
      "7972 [Discriminator loss: 0.579327, acc.: 72.66%] [Generator loss: 0.963689]\n",
      "7973 [Discriminator loss: 0.810365, acc.: 45.31%] [Generator loss: 0.928282]\n",
      "7974 [Discriminator loss: 0.698251, acc.: 56.25%] [Generator loss: 0.929396]\n",
      "7975 [Discriminator loss: 0.751795, acc.: 54.69%] [Generator loss: 1.021361]\n",
      "7976 [Discriminator loss: 0.442457, acc.: 86.72%] [Generator loss: 0.962842]\n",
      "7977 [Discriminator loss: 0.559414, acc.: 70.31%] [Generator loss: 1.022166]\n",
      "7978 [Discriminator loss: 0.660214, acc.: 64.06%] [Generator loss: 0.914563]\n",
      "7979 [Discriminator loss: 0.509166, acc.: 80.47%] [Generator loss: 0.748013]\n",
      "7980 [Discriminator loss: 0.518484, acc.: 76.56%] [Generator loss: 0.724251]\n",
      "7981 [Discriminator loss: 0.650103, acc.: 61.72%] [Generator loss: 0.767240]\n",
      "7982 [Discriminator loss: 0.519665, acc.: 78.91%] [Generator loss: 1.148294]\n",
      "7983 [Discriminator loss: 0.538745, acc.: 74.22%] [Generator loss: 0.908205]\n",
      "7984 [Discriminator loss: 0.763029, acc.: 59.38%] [Generator loss: 1.013727]\n",
      "7985 [Discriminator loss: 0.806005, acc.: 53.12%] [Generator loss: 1.157128]\n",
      "7986 [Discriminator loss: 0.548933, acc.: 74.22%] [Generator loss: 0.931778]\n",
      "7987 [Discriminator loss: 0.637884, acc.: 67.19%] [Generator loss: 0.790705]\n",
      "7988 [Discriminator loss: 0.740389, acc.: 51.56%] [Generator loss: 0.909413]\n",
      "7989 [Discriminator loss: 0.615466, acc.: 57.81%] [Generator loss: 0.896748]\n",
      "7990 [Discriminator loss: 0.527200, acc.: 77.34%] [Generator loss: 0.928243]\n",
      "7991 [Discriminator loss: 0.635994, acc.: 66.41%] [Generator loss: 0.752581]\n",
      "7992 [Discriminator loss: 0.626558, acc.: 64.06%] [Generator loss: 0.905385]\n",
      "7993 [Discriminator loss: 0.679778, acc.: 60.16%] [Generator loss: 0.995008]\n",
      "7994 [Discriminator loss: 0.587240, acc.: 67.97%] [Generator loss: 0.949979]\n",
      "7995 [Discriminator loss: 0.609013, acc.: 64.84%] [Generator loss: 0.945183]\n",
      "7996 [Discriminator loss: 0.858159, acc.: 41.41%] [Generator loss: 1.056410]\n",
      "7997 [Discriminator loss: 0.746352, acc.: 58.59%] [Generator loss: 1.311270]\n",
      "7998 [Discriminator loss: 0.783232, acc.: 51.56%] [Generator loss: 1.163448]\n",
      "7999 [Discriminator loss: 0.651835, acc.: 59.38%] [Generator loss: 1.016905]\n",
      "8000 [Discriminator loss: 0.588278, acc.: 70.31%] [Generator loss: 1.023756]\n",
      "8001 [Discriminator loss: 0.655354, acc.: 68.75%] [Generator loss: 0.873267]\n",
      "8002 [Discriminator loss: 0.578022, acc.: 68.75%] [Generator loss: 0.830831]\n",
      "8003 [Discriminator loss: 0.711165, acc.: 53.91%] [Generator loss: 0.890238]\n",
      "8004 [Discriminator loss: 0.723787, acc.: 53.12%] [Generator loss: 0.888330]\n",
      "8005 [Discriminator loss: 0.709689, acc.: 60.94%] [Generator loss: 0.921765]\n",
      "8006 [Discriminator loss: 0.582615, acc.: 70.31%] [Generator loss: 0.939286]\n",
      "8007 [Discriminator loss: 0.622017, acc.: 64.84%] [Generator loss: 1.066576]\n",
      "8008 [Discriminator loss: 0.756638, acc.: 50.78%] [Generator loss: 1.009576]\n",
      "8009 [Discriminator loss: 0.697331, acc.: 55.47%] [Generator loss: 0.828521]\n",
      "8010 [Discriminator loss: 0.807151, acc.: 50.00%] [Generator loss: 0.816558]\n",
      "8011 [Discriminator loss: 0.603392, acc.: 67.19%] [Generator loss: 0.825407]\n",
      "8012 [Discriminator loss: 0.714719, acc.: 50.78%] [Generator loss: 0.941322]\n",
      "8013 [Discriminator loss: 0.645489, acc.: 63.28%] [Generator loss: 1.014575]\n",
      "8014 [Discriminator loss: 0.704768, acc.: 53.12%] [Generator loss: 0.965462]\n",
      "8015 [Discriminator loss: 0.676588, acc.: 66.41%] [Generator loss: 0.950618]\n",
      "8016 [Discriminator loss: 0.669916, acc.: 61.72%] [Generator loss: 0.890556]\n",
      "8017 [Discriminator loss: 0.666084, acc.: 60.94%] [Generator loss: 0.850884]\n",
      "8018 [Discriminator loss: 0.657328, acc.: 64.84%] [Generator loss: 0.909315]\n",
      "8019 [Discriminator loss: 0.640041, acc.: 58.59%] [Generator loss: 1.153958]\n",
      "8020 [Discriminator loss: 0.644675, acc.: 65.62%] [Generator loss: 1.145012]\n",
      "8021 [Discriminator loss: 0.608396, acc.: 65.62%] [Generator loss: 1.116423]\n",
      "8022 [Discriminator loss: 0.575820, acc.: 69.53%] [Generator loss: 0.992140]\n",
      "8023 [Discriminator loss: 0.550615, acc.: 67.19%] [Generator loss: 0.955399]\n",
      "8024 [Discriminator loss: 0.736847, acc.: 54.69%] [Generator loss: 1.055375]\n",
      "8025 [Discriminator loss: 0.487806, acc.: 78.12%] [Generator loss: 0.631409]\n",
      "8026 [Discriminator loss: 0.647619, acc.: 65.62%] [Generator loss: 0.708360]\n",
      "8027 [Discriminator loss: 0.701479, acc.: 60.16%] [Generator loss: 0.790975]\n",
      "8028 [Discriminator loss: 0.583214, acc.: 71.88%] [Generator loss: 0.992774]\n",
      "8029 [Discriminator loss: 0.444619, acc.: 80.47%] [Generator loss: 0.892034]\n",
      "8030 [Discriminator loss: 0.617489, acc.: 69.53%] [Generator loss: 0.737495]\n",
      "8031 [Discriminator loss: 0.617102, acc.: 60.94%] [Generator loss: 0.868633]\n",
      "8032 [Discriminator loss: 0.628202, acc.: 60.94%] [Generator loss: 1.023791]\n",
      "8033 [Discriminator loss: 0.556193, acc.: 75.00%] [Generator loss: 0.997746]\n",
      "8034 [Discriminator loss: 0.673902, acc.: 57.03%] [Generator loss: 1.049439]\n",
      "8035 [Discriminator loss: 0.625253, acc.: 62.50%] [Generator loss: 1.062357]\n",
      "8036 [Discriminator loss: 0.747238, acc.: 47.66%] [Generator loss: 0.715266]\n",
      "8037 [Discriminator loss: 0.792395, acc.: 56.25%] [Generator loss: 1.616745]\n",
      "8038 [Discriminator loss: 0.725922, acc.: 63.28%] [Generator loss: 1.548235]\n",
      "8039 [Discriminator loss: 0.699140, acc.: 58.59%] [Generator loss: 0.883361]\n",
      "8040 [Discriminator loss: 0.730729, acc.: 58.59%] [Generator loss: 0.937345]\n",
      "8041 [Discriminator loss: 0.662300, acc.: 62.50%] [Generator loss: 0.900423]\n",
      "8042 [Discriminator loss: 0.728092, acc.: 48.44%] [Generator loss: 1.048259]\n",
      "8043 [Discriminator loss: 0.639291, acc.: 61.72%] [Generator loss: 1.239576]\n",
      "8044 [Discriminator loss: 0.719848, acc.: 56.25%] [Generator loss: 1.039033]\n",
      "8045 [Discriminator loss: 0.636399, acc.: 63.28%] [Generator loss: 0.997413]\n",
      "8046 [Discriminator loss: 0.604171, acc.: 69.53%] [Generator loss: 1.013846]\n",
      "8047 [Discriminator loss: 0.630326, acc.: 63.28%] [Generator loss: 0.904617]\n",
      "8048 [Discriminator loss: 0.602069, acc.: 64.84%] [Generator loss: 0.885293]\n",
      "8049 [Discriminator loss: 0.624555, acc.: 68.75%] [Generator loss: 0.933202]\n",
      "8050 [Discriminator loss: 0.630024, acc.: 63.28%] [Generator loss: 1.000677]\n",
      "8051 [Discriminator loss: 0.725239, acc.: 50.78%] [Generator loss: 0.892863]\n",
      "8052 [Discriminator loss: 0.626973, acc.: 62.50%] [Generator loss: 0.814659]\n",
      "8053 [Discriminator loss: 0.802945, acc.: 50.00%] [Generator loss: 0.816254]\n",
      "8054 [Discriminator loss: 0.722663, acc.: 54.69%] [Generator loss: 1.193115]\n",
      "8055 [Discriminator loss: 0.800628, acc.: 51.56%] [Generator loss: 0.814959]\n",
      "8056 [Discriminator loss: 0.706646, acc.: 52.34%] [Generator loss: 0.944516]\n",
      "8057 [Discriminator loss: 0.630313, acc.: 66.41%] [Generator loss: 0.932646]\n",
      "8058 [Discriminator loss: 0.550104, acc.: 77.34%] [Generator loss: 0.824140]\n",
      "8059 [Discriminator loss: 0.805308, acc.: 49.22%] [Generator loss: 0.963106]\n",
      "8060 [Discriminator loss: 0.668396, acc.: 57.81%] [Generator loss: 1.050356]\n",
      "8061 [Discriminator loss: 0.578849, acc.: 71.88%] [Generator loss: 1.125543]\n",
      "8062 [Discriminator loss: 0.533696, acc.: 67.97%] [Generator loss: 1.260896]\n",
      "8063 [Discriminator loss: 0.528726, acc.: 78.12%] [Generator loss: 1.041181]\n",
      "8064 [Discriminator loss: 0.695169, acc.: 53.91%] [Generator loss: 0.847013]\n",
      "8065 [Discriminator loss: 0.673585, acc.: 53.91%] [Generator loss: 0.958315]\n",
      "8066 [Discriminator loss: 0.652082, acc.: 66.41%] [Generator loss: 1.029275]\n",
      "8067 [Discriminator loss: 0.931002, acc.: 36.72%] [Generator loss: 0.907050]\n",
      "8068 [Discriminator loss: 0.638536, acc.: 66.41%] [Generator loss: 0.974264]\n",
      "8069 [Discriminator loss: 0.554367, acc.: 73.44%] [Generator loss: 0.939753]\n",
      "8070 [Discriminator loss: 0.563675, acc.: 71.09%] [Generator loss: 0.905048]\n",
      "8071 [Discriminator loss: 0.781925, acc.: 46.88%] [Generator loss: 0.869449]\n",
      "8072 [Discriminator loss: 0.663741, acc.: 66.41%] [Generator loss: 0.811721]\n",
      "8073 [Discriminator loss: 0.515893, acc.: 76.56%] [Generator loss: 0.857825]\n",
      "8074 [Discriminator loss: 0.556558, acc.: 73.44%] [Generator loss: 0.811891]\n",
      "8075 [Discriminator loss: 0.765369, acc.: 51.56%] [Generator loss: 0.996376]\n",
      "8076 [Discriminator loss: 0.596433, acc.: 66.41%] [Generator loss: 1.039839]\n",
      "8077 [Discriminator loss: 0.688188, acc.: 63.28%] [Generator loss: 0.868444]\n",
      "8078 [Discriminator loss: 0.888820, acc.: 44.53%] [Generator loss: 0.685867]\n",
      "8079 [Discriminator loss: 0.629316, acc.: 58.59%] [Generator loss: 0.888892]\n",
      "8080 [Discriminator loss: 0.962465, acc.: 39.06%] [Generator loss: 1.100271]\n",
      "8081 [Discriminator loss: 0.617777, acc.: 70.31%] [Generator loss: 1.187510]\n",
      "8082 [Discriminator loss: 0.648589, acc.: 63.28%] [Generator loss: 1.232559]\n",
      "8083 [Discriminator loss: 0.671987, acc.: 60.94%] [Generator loss: 1.104816]\n",
      "8084 [Discriminator loss: 0.730367, acc.: 53.91%] [Generator loss: 0.989975]\n",
      "8085 [Discriminator loss: 0.766886, acc.: 51.56%] [Generator loss: 1.023629]\n",
      "8086 [Discriminator loss: 0.540366, acc.: 73.44%] [Generator loss: 0.920695]\n",
      "8087 [Discriminator loss: 0.720358, acc.: 56.25%] [Generator loss: 0.912704]\n",
      "8088 [Discriminator loss: 0.671158, acc.: 62.50%] [Generator loss: 0.992038]\n",
      "8089 [Discriminator loss: 0.514724, acc.: 75.78%] [Generator loss: 0.937875]\n",
      "8090 [Discriminator loss: 0.664646, acc.: 57.81%] [Generator loss: 0.867911]\n",
      "8091 [Discriminator loss: 0.640582, acc.: 66.41%] [Generator loss: 0.856331]\n",
      "8092 [Discriminator loss: 0.626246, acc.: 71.88%] [Generator loss: 0.830621]\n",
      "8093 [Discriminator loss: 0.698382, acc.: 55.47%] [Generator loss: 0.805077]\n",
      "8094 [Discriminator loss: 0.661599, acc.: 58.59%] [Generator loss: 0.908203]\n",
      "8095 [Discriminator loss: 0.672769, acc.: 62.50%] [Generator loss: 0.980574]\n",
      "8096 [Discriminator loss: 0.734040, acc.: 50.78%] [Generator loss: 1.177501]\n",
      "8097 [Discriminator loss: 0.802698, acc.: 48.44%] [Generator loss: 0.949106]\n",
      "8098 [Discriminator loss: 0.660946, acc.: 62.50%] [Generator loss: 0.836170]\n",
      "8099 [Discriminator loss: 0.732592, acc.: 51.56%] [Generator loss: 0.932623]\n",
      "8100 [Discriminator loss: 0.764997, acc.: 51.56%] [Generator loss: 0.924357]\n",
      "8101 [Discriminator loss: 0.819051, acc.: 45.31%] [Generator loss: 0.930889]\n",
      "8102 [Discriminator loss: 0.753660, acc.: 51.56%] [Generator loss: 1.244432]\n",
      "8103 [Discriminator loss: 0.645741, acc.: 63.28%] [Generator loss: 1.256785]\n",
      "8104 [Discriminator loss: 0.669805, acc.: 64.06%] [Generator loss: 0.944825]\n",
      "8105 [Discriminator loss: 0.631245, acc.: 69.53%] [Generator loss: 1.054040]\n",
      "8106 [Discriminator loss: 0.593493, acc.: 69.53%] [Generator loss: 0.746423]\n",
      "8107 [Discriminator loss: 0.686372, acc.: 58.59%] [Generator loss: 0.856259]\n",
      "8108 [Discriminator loss: 0.509547, acc.: 82.03%] [Generator loss: 0.933568]\n",
      "8109 [Discriminator loss: 0.571224, acc.: 67.97%] [Generator loss: 0.970025]\n",
      "8110 [Discriminator loss: 0.539953, acc.: 78.12%] [Generator loss: 0.973223]\n",
      "8111 [Discriminator loss: 0.553087, acc.: 76.56%] [Generator loss: 1.116820]\n",
      "8112 [Discriminator loss: 0.657391, acc.: 69.53%] [Generator loss: 0.951354]\n",
      "8113 [Discriminator loss: 0.616434, acc.: 61.72%] [Generator loss: 1.130470]\n",
      "8114 [Discriminator loss: 0.832744, acc.: 49.22%] [Generator loss: 0.754236]\n",
      "8115 [Discriminator loss: 0.807519, acc.: 42.19%] [Generator loss: 0.748260]\n",
      "8116 [Discriminator loss: 0.670370, acc.: 57.03%] [Generator loss: 0.950269]\n",
      "8117 [Discriminator loss: 0.777751, acc.: 46.88%] [Generator loss: 0.792168]\n",
      "8118 [Discriminator loss: 0.670764, acc.: 60.16%] [Generator loss: 0.847410]\n",
      "8119 [Discriminator loss: 0.830071, acc.: 43.75%] [Generator loss: 0.876905]\n",
      "8120 [Discriminator loss: 0.630043, acc.: 67.97%] [Generator loss: 0.838755]\n",
      "8121 [Discriminator loss: 0.692646, acc.: 56.25%] [Generator loss: 1.067466]\n",
      "8122 [Discriminator loss: 0.597179, acc.: 68.75%] [Generator loss: 1.243881]\n",
      "8123 [Discriminator loss: 0.761912, acc.: 56.25%] [Generator loss: 1.404373]\n",
      "8124 [Discriminator loss: 0.849648, acc.: 48.44%] [Generator loss: 0.980893]\n",
      "8125 [Discriminator loss: 0.698287, acc.: 54.69%] [Generator loss: 0.972017]\n",
      "8126 [Discriminator loss: 0.681923, acc.: 66.41%] [Generator loss: 0.876183]\n",
      "8127 [Discriminator loss: 0.608626, acc.: 65.62%] [Generator loss: 0.878440]\n",
      "8128 [Discriminator loss: 0.819570, acc.: 50.00%] [Generator loss: 0.806062]\n",
      "8129 [Discriminator loss: 0.768050, acc.: 50.78%] [Generator loss: 0.696701]\n",
      "8130 [Discriminator loss: 0.593876, acc.: 66.41%] [Generator loss: 0.810525]\n",
      "8131 [Discriminator loss: 0.722232, acc.: 56.25%] [Generator loss: 0.814438]\n",
      "8132 [Discriminator loss: 0.645571, acc.: 66.41%] [Generator loss: 0.964338]\n",
      "8133 [Discriminator loss: 0.713161, acc.: 59.38%] [Generator loss: 1.107581]\n",
      "8134 [Discriminator loss: 0.602254, acc.: 64.84%] [Generator loss: 0.962906]\n",
      "8135 [Discriminator loss: 0.803449, acc.: 42.19%] [Generator loss: 0.835148]\n",
      "8136 [Discriminator loss: 0.627095, acc.: 65.62%] [Generator loss: 0.918465]\n",
      "8137 [Discriminator loss: 0.640720, acc.: 64.06%] [Generator loss: 0.908728]\n",
      "8138 [Discriminator loss: 0.637646, acc.: 64.84%] [Generator loss: 1.070330]\n",
      "8139 [Discriminator loss: 0.744175, acc.: 60.16%] [Generator loss: 1.049696]\n",
      "8140 [Discriminator loss: 0.602197, acc.: 62.50%] [Generator loss: 1.169627]\n",
      "8141 [Discriminator loss: 0.712374, acc.: 54.69%] [Generator loss: 0.919477]\n",
      "8142 [Discriminator loss: 0.732206, acc.: 57.81%] [Generator loss: 0.826900]\n",
      "8143 [Discriminator loss: 0.898255, acc.: 37.50%] [Generator loss: 0.692072]\n",
      "8144 [Discriminator loss: 0.800945, acc.: 46.88%] [Generator loss: 0.706553]\n",
      "8145 [Discriminator loss: 0.663324, acc.: 59.38%] [Generator loss: 0.804346]\n",
      "8146 [Discriminator loss: 0.599521, acc.: 68.75%] [Generator loss: 0.915278]\n",
      "8147 [Discriminator loss: 0.610718, acc.: 69.53%] [Generator loss: 0.748578]\n",
      "8148 [Discriminator loss: 0.796158, acc.: 46.09%] [Generator loss: 0.821196]\n",
      "8149 [Discriminator loss: 0.832987, acc.: 41.41%] [Generator loss: 0.822414]\n",
      "8150 [Discriminator loss: 0.662259, acc.: 61.72%] [Generator loss: 0.927581]\n",
      "8151 [Discriminator loss: 0.638637, acc.: 61.72%] [Generator loss: 1.044963]\n",
      "8152 [Discriminator loss: 0.682389, acc.: 64.06%] [Generator loss: 1.112033]\n",
      "8153 [Discriminator loss: 0.669218, acc.: 62.50%] [Generator loss: 0.911802]\n",
      "8154 [Discriminator loss: 0.669748, acc.: 55.47%] [Generator loss: 0.889737]\n",
      "8155 [Discriminator loss: 0.551071, acc.: 75.78%] [Generator loss: 1.034068]\n",
      "8156 [Discriminator loss: 0.595549, acc.: 72.66%] [Generator loss: 1.007010]\n",
      "8157 [Discriminator loss: 0.578036, acc.: 73.44%] [Generator loss: 0.887985]\n",
      "8158 [Discriminator loss: 0.459290, acc.: 82.81%] [Generator loss: 0.910620]\n",
      "8159 [Discriminator loss: 0.660713, acc.: 61.72%] [Generator loss: 0.868132]\n",
      "8160 [Discriminator loss: 0.532832, acc.: 82.81%] [Generator loss: 0.854046]\n",
      "8161 [Discriminator loss: 0.595707, acc.: 66.41%] [Generator loss: 0.864206]\n",
      "8162 [Discriminator loss: 0.709398, acc.: 59.38%] [Generator loss: 0.844487]\n",
      "8163 [Discriminator loss: 0.806506, acc.: 48.44%] [Generator loss: 0.774948]\n",
      "8164 [Discriminator loss: 0.639089, acc.: 59.38%] [Generator loss: 0.845853]\n",
      "8165 [Discriminator loss: 0.617065, acc.: 69.53%] [Generator loss: 1.025370]\n",
      "8166 [Discriminator loss: 0.683639, acc.: 56.25%] [Generator loss: 1.027386]\n",
      "8167 [Discriminator loss: 0.704728, acc.: 49.22%] [Generator loss: 0.926269]\n",
      "8168 [Discriminator loss: 0.717819, acc.: 54.69%] [Generator loss: 0.940253]\n",
      "8169 [Discriminator loss: 0.692614, acc.: 60.16%] [Generator loss: 1.150890]\n",
      "8170 [Discriminator loss: 0.593308, acc.: 67.19%] [Generator loss: 1.182893]\n",
      "8171 [Discriminator loss: 0.639297, acc.: 64.84%] [Generator loss: 0.943714]\n",
      "8172 [Discriminator loss: 0.601074, acc.: 69.53%] [Generator loss: 0.674720]\n",
      "8173 [Discriminator loss: 0.596230, acc.: 71.88%] [Generator loss: 0.631507]\n",
      "8174 [Discriminator loss: 0.700770, acc.: 63.28%] [Generator loss: 0.588708]\n",
      "8175 [Discriminator loss: 0.680159, acc.: 63.28%] [Generator loss: 0.813942]\n",
      "8176 [Discriminator loss: 0.519417, acc.: 76.56%] [Generator loss: 0.828358]\n",
      "8177 [Discriminator loss: 0.613211, acc.: 67.97%] [Generator loss: 0.849112]\n",
      "8178 [Discriminator loss: 0.806724, acc.: 41.41%] [Generator loss: 0.611068]\n",
      "8179 [Discriminator loss: 0.481709, acc.: 80.47%] [Generator loss: 0.658775]\n",
      "8180 [Discriminator loss: 0.737466, acc.: 57.03%] [Generator loss: 1.100996]\n",
      "8181 [Discriminator loss: 0.597552, acc.: 64.84%] [Generator loss: 1.072160]\n",
      "8182 [Discriminator loss: 0.562468, acc.: 69.53%] [Generator loss: 0.721009]\n",
      "8183 [Discriminator loss: 1.200273, acc.: 21.09%] [Generator loss: 0.984065]\n",
      "8184 [Discriminator loss: 0.532424, acc.: 73.44%] [Generator loss: 1.007211]\n",
      "8185 [Discriminator loss: 0.688064, acc.: 62.50%] [Generator loss: 0.891411]\n",
      "8186 [Discriminator loss: 0.588903, acc.: 71.88%] [Generator loss: 0.955629]\n",
      "8187 [Discriminator loss: 0.602228, acc.: 64.06%] [Generator loss: 0.848374]\n",
      "8188 [Discriminator loss: 0.517728, acc.: 82.03%] [Generator loss: 0.912932]\n",
      "8189 [Discriminator loss: 0.624115, acc.: 65.62%] [Generator loss: 0.768685]\n",
      "8190 [Discriminator loss: 0.617835, acc.: 70.31%] [Generator loss: 0.848731]\n",
      "8191 [Discriminator loss: 0.586533, acc.: 71.09%] [Generator loss: 0.870247]\n",
      "8192 [Discriminator loss: 0.546935, acc.: 74.22%] [Generator loss: 0.780451]\n",
      "8193 [Discriminator loss: 0.583704, acc.: 71.09%] [Generator loss: 0.773970]\n",
      "8194 [Discriminator loss: 0.585361, acc.: 69.53%] [Generator loss: 0.830189]\n",
      "8195 [Discriminator loss: 0.665278, acc.: 64.06%] [Generator loss: 0.897492]\n",
      "8196 [Discriminator loss: 0.732549, acc.: 50.00%] [Generator loss: 0.852078]\n",
      "8197 [Discriminator loss: 0.625201, acc.: 64.06%] [Generator loss: 0.726484]\n",
      "8198 [Discriminator loss: 0.751713, acc.: 53.91%] [Generator loss: 0.916304]\n",
      "8199 [Discriminator loss: 0.705162, acc.: 57.81%] [Generator loss: 1.032235]\n",
      "8200 [Discriminator loss: 0.638876, acc.: 60.16%] [Generator loss: 0.844123]\n",
      "8201 [Discriminator loss: 0.619264, acc.: 66.41%] [Generator loss: 0.979105]\n",
      "8202 [Discriminator loss: 0.666786, acc.: 58.59%] [Generator loss: 0.993818]\n",
      "8203 [Discriminator loss: 0.623597, acc.: 60.94%] [Generator loss: 0.795990]\n",
      "8204 [Discriminator loss: 0.643178, acc.: 66.41%] [Generator loss: 0.913436]\n",
      "8205 [Discriminator loss: 0.587115, acc.: 70.31%] [Generator loss: 0.866107]\n",
      "8206 [Discriminator loss: 0.659365, acc.: 58.59%] [Generator loss: 0.957328]\n",
      "8207 [Discriminator loss: 0.712311, acc.: 56.25%] [Generator loss: 0.816574]\n",
      "8208 [Discriminator loss: 0.645149, acc.: 65.62%] [Generator loss: 0.904182]\n",
      "8209 [Discriminator loss: 0.599486, acc.: 70.31%] [Generator loss: 0.865711]\n",
      "8210 [Discriminator loss: 0.491527, acc.: 78.12%] [Generator loss: 0.840587]\n",
      "8211 [Discriminator loss: 0.756098, acc.: 50.00%] [Generator loss: 1.151410]\n",
      "8212 [Discriminator loss: 0.705394, acc.: 55.47%] [Generator loss: 1.042997]\n",
      "8213 [Discriminator loss: 0.801273, acc.: 41.41%] [Generator loss: 1.210174]\n",
      "8214 [Discriminator loss: 0.640340, acc.: 65.62%] [Generator loss: 1.064680]\n",
      "8215 [Discriminator loss: 0.723052, acc.: 52.34%] [Generator loss: 1.148256]\n",
      "8216 [Discriminator loss: 0.583598, acc.: 73.44%] [Generator loss: 1.210811]\n",
      "8217 [Discriminator loss: 0.591113, acc.: 72.66%] [Generator loss: 0.947364]\n",
      "8218 [Discriminator loss: 0.674412, acc.: 56.25%] [Generator loss: 0.989853]\n",
      "8219 [Discriminator loss: 0.703120, acc.: 58.59%] [Generator loss: 0.962660]\n",
      "8220 [Discriminator loss: 0.678721, acc.: 67.19%] [Generator loss: 0.772254]\n",
      "8221 [Discriminator loss: 0.709658, acc.: 57.81%] [Generator loss: 0.807903]\n",
      "8222 [Discriminator loss: 0.792966, acc.: 50.00%] [Generator loss: 1.032000]\n",
      "8223 [Discriminator loss: 0.696321, acc.: 61.72%] [Generator loss: 1.172938]\n",
      "8224 [Discriminator loss: 0.774012, acc.: 49.22%] [Generator loss: 1.268323]\n",
      "8225 [Discriminator loss: 0.786379, acc.: 52.34%] [Generator loss: 1.092028]\n",
      "8226 [Discriminator loss: 0.819490, acc.: 44.53%] [Generator loss: 1.061157]\n",
      "8227 [Discriminator loss: 0.829509, acc.: 42.19%] [Generator loss: 1.130217]\n",
      "8228 [Discriminator loss: 0.531968, acc.: 71.09%] [Generator loss: 0.997785]\n",
      "8229 [Discriminator loss: 0.854116, acc.: 50.00%] [Generator loss: 1.072462]\n",
      "8230 [Discriminator loss: 0.620730, acc.: 71.09%] [Generator loss: 0.937509]\n",
      "8231 [Discriminator loss: 0.739030, acc.: 59.38%] [Generator loss: 1.120200]\n",
      "8232 [Discriminator loss: 0.572926, acc.: 68.75%] [Generator loss: 1.030921]\n",
      "8233 [Discriminator loss: 0.757918, acc.: 52.34%] [Generator loss: 0.948595]\n",
      "8234 [Discriminator loss: 0.751631, acc.: 51.56%] [Generator loss: 0.805835]\n",
      "8235 [Discriminator loss: 0.638858, acc.: 65.62%] [Generator loss: 0.947861]\n",
      "8236 [Discriminator loss: 0.617473, acc.: 71.09%] [Generator loss: 0.936649]\n",
      "8237 [Discriminator loss: 0.892257, acc.: 41.41%] [Generator loss: 0.835547]\n",
      "8238 [Discriminator loss: 0.565917, acc.: 71.88%] [Generator loss: 0.804327]\n",
      "8239 [Discriminator loss: 0.723057, acc.: 53.12%] [Generator loss: 0.924065]\n",
      "8240 [Discriminator loss: 0.569438, acc.: 73.44%] [Generator loss: 0.931638]\n",
      "8241 [Discriminator loss: 0.550303, acc.: 80.47%] [Generator loss: 1.063637]\n",
      "8242 [Discriminator loss: 0.754351, acc.: 51.56%] [Generator loss: 0.780719]\n",
      "8243 [Discriminator loss: 0.651973, acc.: 60.94%] [Generator loss: 0.824693]\n",
      "8244 [Discriminator loss: 0.658761, acc.: 65.62%] [Generator loss: 0.765459]\n",
      "8245 [Discriminator loss: 0.772052, acc.: 50.78%] [Generator loss: 1.018818]\n",
      "8246 [Discriminator loss: 0.538725, acc.: 75.00%] [Generator loss: 1.332179]\n",
      "8247 [Discriminator loss: 0.522812, acc.: 77.34%] [Generator loss: 0.964701]\n",
      "8248 [Discriminator loss: 0.560215, acc.: 75.78%] [Generator loss: 0.724642]\n",
      "8249 [Discriminator loss: 0.531113, acc.: 71.09%] [Generator loss: 0.566517]\n",
      "8250 [Discriminator loss: 0.523859, acc.: 79.69%] [Generator loss: 0.515591]\n",
      "8251 [Discriminator loss: 0.546972, acc.: 75.78%] [Generator loss: 0.666934]\n",
      "8252 [Discriminator loss: 0.696267, acc.: 64.06%] [Generator loss: 0.774346]\n",
      "8253 [Discriminator loss: 0.727132, acc.: 53.12%] [Generator loss: 0.863536]\n",
      "8254 [Discriminator loss: 0.717345, acc.: 57.03%] [Generator loss: 0.834693]\n",
      "8255 [Discriminator loss: 0.615220, acc.: 69.53%] [Generator loss: 1.123857]\n",
      "8256 [Discriminator loss: 0.691632, acc.: 56.25%] [Generator loss: 1.150901]\n",
      "8257 [Discriminator loss: 0.595342, acc.: 72.66%] [Generator loss: 1.221246]\n",
      "8258 [Discriminator loss: 0.874225, acc.: 43.75%] [Generator loss: 0.997317]\n",
      "8259 [Discriminator loss: 0.772338, acc.: 51.56%] [Generator loss: 0.906502]\n",
      "8260 [Discriminator loss: 0.560125, acc.: 75.00%] [Generator loss: 0.953804]\n",
      "8261 [Discriminator loss: 0.487784, acc.: 83.59%] [Generator loss: 0.743436]\n",
      "8262 [Discriminator loss: 0.541436, acc.: 77.34%] [Generator loss: 0.636274]\n",
      "8263 [Discriminator loss: 0.452095, acc.: 83.59%] [Generator loss: 0.543059]\n",
      "8264 [Discriminator loss: 0.503409, acc.: 79.69%] [Generator loss: 0.622948]\n",
      "8265 [Discriminator loss: 0.522512, acc.: 82.81%] [Generator loss: 0.534084]\n",
      "8266 [Discriminator loss: 0.423573, acc.: 87.50%] [Generator loss: 0.591507]\n",
      "8267 [Discriminator loss: 0.456308, acc.: 80.47%] [Generator loss: 0.596518]\n",
      "8268 [Discriminator loss: 0.874133, acc.: 44.53%] [Generator loss: 0.655601]\n",
      "8269 [Discriminator loss: 0.544603, acc.: 73.44%] [Generator loss: 0.643521]\n",
      "8270 [Discriminator loss: 0.776796, acc.: 48.44%] [Generator loss: 0.820006]\n",
      "8271 [Discriminator loss: 0.584821, acc.: 69.53%] [Generator loss: 0.953267]\n",
      "8272 [Discriminator loss: 0.630760, acc.: 69.53%] [Generator loss: 1.011678]\n",
      "8273 [Discriminator loss: 0.675524, acc.: 63.28%] [Generator loss: 1.014329]\n",
      "8274 [Discriminator loss: 0.718090, acc.: 57.81%] [Generator loss: 1.472583]\n",
      "8275 [Discriminator loss: 0.499126, acc.: 67.19%] [Generator loss: 1.222580]\n",
      "8276 [Discriminator loss: 0.910679, acc.: 37.50%] [Generator loss: 0.909496]\n",
      "8277 [Discriminator loss: 0.678795, acc.: 57.03%] [Generator loss: 0.752758]\n",
      "8278 [Discriminator loss: 0.649090, acc.: 63.28%] [Generator loss: 0.921853]\n",
      "8279 [Discriminator loss: 0.529258, acc.: 77.34%] [Generator loss: 1.047200]\n",
      "8280 [Discriminator loss: 0.615750, acc.: 69.53%] [Generator loss: 1.025284]\n",
      "8281 [Discriminator loss: 0.753906, acc.: 49.22%] [Generator loss: 0.986675]\n",
      "8282 [Discriminator loss: 0.600991, acc.: 63.28%] [Generator loss: 0.988674]\n",
      "8283 [Discriminator loss: 0.727749, acc.: 54.69%] [Generator loss: 0.919375]\n",
      "8284 [Discriminator loss: 0.594526, acc.: 72.66%] [Generator loss: 1.045397]\n",
      "8285 [Discriminator loss: 0.614801, acc.: 63.28%] [Generator loss: 1.034030]\n",
      "8286 [Discriminator loss: 0.735488, acc.: 53.12%] [Generator loss: 0.810781]\n",
      "8287 [Discriminator loss: 0.811891, acc.: 50.78%] [Generator loss: 0.831731]\n",
      "8288 [Discriminator loss: 0.741144, acc.: 53.12%] [Generator loss: 0.915258]\n",
      "8289 [Discriminator loss: 0.633020, acc.: 67.97%] [Generator loss: 0.987034]\n",
      "8290 [Discriminator loss: 0.704270, acc.: 60.16%] [Generator loss: 0.812154]\n",
      "8291 [Discriminator loss: 0.710938, acc.: 52.34%] [Generator loss: 0.839373]\n",
      "8292 [Discriminator loss: 0.651905, acc.: 67.19%] [Generator loss: 0.910282]\n",
      "8293 [Discriminator loss: 0.649964, acc.: 64.06%] [Generator loss: 1.033459]\n",
      "8294 [Discriminator loss: 0.578152, acc.: 74.22%] [Generator loss: 0.983571]\n",
      "8295 [Discriminator loss: 0.659757, acc.: 59.38%] [Generator loss: 1.092234]\n",
      "8296 [Discriminator loss: 0.756351, acc.: 58.59%] [Generator loss: 1.169113]\n",
      "8297 [Discriminator loss: 0.738636, acc.: 55.47%] [Generator loss: 0.896947]\n",
      "8298 [Discriminator loss: 0.776314, acc.: 54.69%] [Generator loss: 0.957319]\n",
      "8299 [Discriminator loss: 0.562601, acc.: 71.88%] [Generator loss: 1.077622]\n",
      "8300 [Discriminator loss: 0.908259, acc.: 43.75%] [Generator loss: 1.380478]\n",
      "8301 [Discriminator loss: 0.900677, acc.: 52.34%] [Generator loss: 1.081343]\n",
      "8302 [Discriminator loss: 1.423861, acc.: 21.09%] [Generator loss: 0.699257]\n",
      "8303 [Discriminator loss: 0.576522, acc.: 70.31%] [Generator loss: 0.960346]\n",
      "8304 [Discriminator loss: 0.725736, acc.: 44.53%] [Generator loss: 0.787144]\n",
      "8305 [Discriminator loss: 0.853000, acc.: 33.59%] [Generator loss: 1.090828]\n",
      "8306 [Discriminator loss: 0.611179, acc.: 67.19%] [Generator loss: 1.377168]\n",
      "8307 [Discriminator loss: 0.666110, acc.: 61.72%] [Generator loss: 1.069991]\n",
      "8308 [Discriminator loss: 0.601809, acc.: 72.66%] [Generator loss: 0.907411]\n",
      "8309 [Discriminator loss: 0.715832, acc.: 57.81%] [Generator loss: 0.797638]\n",
      "8310 [Discriminator loss: 0.667300, acc.: 63.28%] [Generator loss: 0.765569]\n",
      "8311 [Discriminator loss: 0.722412, acc.: 56.25%] [Generator loss: 0.779398]\n",
      "8312 [Discriminator loss: 0.638413, acc.: 60.16%] [Generator loss: 0.891887]\n",
      "8313 [Discriminator loss: 0.809263, acc.: 41.41%] [Generator loss: 0.955132]\n",
      "8314 [Discriminator loss: 0.773330, acc.: 50.78%] [Generator loss: 0.898037]\n",
      "8315 [Discriminator loss: 0.761883, acc.: 50.78%] [Generator loss: 0.825439]\n",
      "8316 [Discriminator loss: 0.748535, acc.: 51.56%] [Generator loss: 0.871179]\n",
      "8317 [Discriminator loss: 0.745601, acc.: 50.00%] [Generator loss: 0.786643]\n",
      "8318 [Discriminator loss: 0.640147, acc.: 57.03%] [Generator loss: 0.995957]\n",
      "8319 [Discriminator loss: 0.706427, acc.: 62.50%] [Generator loss: 0.949064]\n",
      "8320 [Discriminator loss: 0.630304, acc.: 67.97%] [Generator loss: 0.979735]\n",
      "8321 [Discriminator loss: 0.634482, acc.: 61.72%] [Generator loss: 0.934270]\n",
      "8322 [Discriminator loss: 0.631646, acc.: 64.06%] [Generator loss: 0.808299]\n",
      "8323 [Discriminator loss: 0.554859, acc.: 75.78%] [Generator loss: 0.758817]\n",
      "8324 [Discriminator loss: 0.688361, acc.: 57.03%] [Generator loss: 0.965343]\n",
      "8325 [Discriminator loss: 0.808301, acc.: 49.22%] [Generator loss: 0.823891]\n",
      "8326 [Discriminator loss: 0.621487, acc.: 67.97%] [Generator loss: 0.745760]\n",
      "8327 [Discriminator loss: 0.782835, acc.: 49.22%] [Generator loss: 0.867969]\n",
      "8328 [Discriminator loss: 0.692525, acc.: 56.25%] [Generator loss: 0.920392]\n",
      "8329 [Discriminator loss: 0.691962, acc.: 57.03%] [Generator loss: 0.970231]\n",
      "8330 [Discriminator loss: 0.825807, acc.: 35.94%] [Generator loss: 0.904687]\n",
      "8331 [Discriminator loss: 0.576431, acc.: 78.91%] [Generator loss: 1.144431]\n",
      "8332 [Discriminator loss: 0.545882, acc.: 75.00%] [Generator loss: 1.038687]\n",
      "8333 [Discriminator loss: 0.667078, acc.: 57.81%] [Generator loss: 1.115004]\n",
      "8334 [Discriminator loss: 0.506480, acc.: 76.56%] [Generator loss: 0.861108]\n",
      "8335 [Discriminator loss: 0.652664, acc.: 62.50%] [Generator loss: 0.766494]\n",
      "8336 [Discriminator loss: 0.499550, acc.: 79.69%] [Generator loss: 0.821523]\n",
      "8337 [Discriminator loss: 0.615428, acc.: 68.75%] [Generator loss: 0.836359]\n",
      "8338 [Discriminator loss: 0.755777, acc.: 48.44%] [Generator loss: 0.782063]\n",
      "8339 [Discriminator loss: 0.755532, acc.: 52.34%] [Generator loss: 0.729953]\n",
      "8340 [Discriminator loss: 0.742638, acc.: 46.88%] [Generator loss: 0.808747]\n",
      "8341 [Discriminator loss: 0.758840, acc.: 51.56%] [Generator loss: 0.863182]\n",
      "8342 [Discriminator loss: 0.774387, acc.: 52.34%] [Generator loss: 0.802306]\n",
      "8343 [Discriminator loss: 0.572877, acc.: 66.41%] [Generator loss: 0.982475]\n",
      "8344 [Discriminator loss: 0.660766, acc.: 64.06%] [Generator loss: 0.942937]\n",
      "8345 [Discriminator loss: 0.644000, acc.: 69.53%] [Generator loss: 0.961143]\n",
      "8346 [Discriminator loss: 0.574730, acc.: 68.75%] [Generator loss: 1.010310]\n",
      "8347 [Discriminator loss: 0.888594, acc.: 43.75%] [Generator loss: 1.065507]\n",
      "8348 [Discriminator loss: 0.623217, acc.: 66.41%] [Generator loss: 1.209141]\n",
      "8349 [Discriminator loss: 0.630412, acc.: 59.38%] [Generator loss: 1.136483]\n",
      "8350 [Discriminator loss: 0.679188, acc.: 57.03%] [Generator loss: 1.002500]\n",
      "8351 [Discriminator loss: 0.650150, acc.: 64.06%] [Generator loss: 1.047455]\n",
      "8352 [Discriminator loss: 0.627197, acc.: 66.41%] [Generator loss: 0.891512]\n",
      "8353 [Discriminator loss: 0.612638, acc.: 70.31%] [Generator loss: 0.895011]\n",
      "8354 [Discriminator loss: 0.563554, acc.: 75.00%] [Generator loss: 1.004789]\n",
      "8355 [Discriminator loss: 0.660480, acc.: 59.38%] [Generator loss: 0.877783]\n",
      "8356 [Discriminator loss: 0.619731, acc.: 63.28%] [Generator loss: 0.779896]\n",
      "8357 [Discriminator loss: 0.621324, acc.: 63.28%] [Generator loss: 0.760712]\n",
      "8358 [Discriminator loss: 0.755339, acc.: 50.78%] [Generator loss: 0.986827]\n",
      "8359 [Discriminator loss: 0.742144, acc.: 53.91%] [Generator loss: 0.934201]\n",
      "8360 [Discriminator loss: 0.654696, acc.: 63.28%] [Generator loss: 0.850315]\n",
      "8361 [Discriminator loss: 0.748553, acc.: 52.34%] [Generator loss: 0.945563]\n",
      "8362 [Discriminator loss: 0.602971, acc.: 71.09%] [Generator loss: 0.806358]\n",
      "8363 [Discriminator loss: 0.696504, acc.: 60.94%] [Generator loss: 1.048642]\n",
      "8364 [Discriminator loss: 0.792612, acc.: 49.22%] [Generator loss: 0.870376]\n",
      "8365 [Discriminator loss: 0.740714, acc.: 55.47%] [Generator loss: 1.001007]\n",
      "8366 [Discriminator loss: 0.629522, acc.: 64.84%] [Generator loss: 0.947140]\n",
      "8367 [Discriminator loss: 0.607121, acc.: 71.88%] [Generator loss: 1.066272]\n",
      "8368 [Discriminator loss: 0.695037, acc.: 53.12%] [Generator loss: 0.909105]\n",
      "8369 [Discriminator loss: 0.560057, acc.: 71.88%] [Generator loss: 0.902485]\n",
      "8370 [Discriminator loss: 0.482086, acc.: 82.03%] [Generator loss: 0.844042]\n",
      "8371 [Discriminator loss: 0.702361, acc.: 61.72%] [Generator loss: 0.704268]\n",
      "8372 [Discriminator loss: 0.637252, acc.: 63.28%] [Generator loss: 0.808204]\n",
      "8373 [Discriminator loss: 0.698145, acc.: 51.56%] [Generator loss: 0.945405]\n",
      "8374 [Discriminator loss: 0.601000, acc.: 69.53%] [Generator loss: 1.134016]\n",
      "8375 [Discriminator loss: 0.626052, acc.: 67.97%] [Generator loss: 0.981765]\n",
      "8376 [Discriminator loss: 0.585071, acc.: 65.62%] [Generator loss: 0.840214]\n",
      "8377 [Discriminator loss: 0.530907, acc.: 76.56%] [Generator loss: 0.737250]\n",
      "8378 [Discriminator loss: 0.584082, acc.: 67.97%] [Generator loss: 0.804292]\n",
      "8379 [Discriminator loss: 0.682372, acc.: 59.38%] [Generator loss: 1.243474]\n",
      "8380 [Discriminator loss: 0.496146, acc.: 75.00%] [Generator loss: 1.000601]\n",
      "8381 [Discriminator loss: 0.584661, acc.: 71.09%] [Generator loss: 0.859346]\n",
      "8382 [Discriminator loss: 0.976597, acc.: 36.72%] [Generator loss: 1.240973]\n",
      "8383 [Discriminator loss: 0.666841, acc.: 65.62%] [Generator loss: 1.218634]\n",
      "8384 [Discriminator loss: 0.609811, acc.: 64.84%] [Generator loss: 1.002580]\n",
      "8385 [Discriminator loss: 0.713193, acc.: 60.16%] [Generator loss: 0.906114]\n",
      "8386 [Discriminator loss: 0.594378, acc.: 71.88%] [Generator loss: 0.847637]\n",
      "8387 [Discriminator loss: 0.538974, acc.: 78.12%] [Generator loss: 0.785140]\n",
      "8388 [Discriminator loss: 0.928330, acc.: 36.72%] [Generator loss: 0.760771]\n",
      "8389 [Discriminator loss: 0.576904, acc.: 76.56%] [Generator loss: 0.848612]\n",
      "8390 [Discriminator loss: 0.885841, acc.: 39.84%] [Generator loss: 0.891161]\n",
      "8391 [Discriminator loss: 0.657435, acc.: 59.38%] [Generator loss: 0.967098]\n",
      "8392 [Discriminator loss: 0.759388, acc.: 49.22%] [Generator loss: 1.036715]\n",
      "8393 [Discriminator loss: 0.486959, acc.: 81.25%] [Generator loss: 0.921549]\n",
      "8394 [Discriminator loss: 0.781133, acc.: 45.31%] [Generator loss: 0.816521]\n",
      "8395 [Discriminator loss: 0.643561, acc.: 60.94%] [Generator loss: 0.853712]\n",
      "8396 [Discriminator loss: 0.693032, acc.: 56.25%] [Generator loss: 0.814628]\n",
      "8397 [Discriminator loss: 0.637205, acc.: 69.53%] [Generator loss: 0.950005]\n",
      "8398 [Discriminator loss: 0.551547, acc.: 71.09%] [Generator loss: 0.932674]\n",
      "8399 [Discriminator loss: 0.791547, acc.: 52.34%] [Generator loss: 0.852049]\n",
      "8400 [Discriminator loss: 0.506575, acc.: 77.34%] [Generator loss: 0.898439]\n",
      "8401 [Discriminator loss: 0.662830, acc.: 57.81%] [Generator loss: 0.852204]\n",
      "8402 [Discriminator loss: 0.638421, acc.: 64.84%] [Generator loss: 0.699929]\n",
      "8403 [Discriminator loss: 0.663568, acc.: 55.47%] [Generator loss: 0.593503]\n",
      "8404 [Discriminator loss: 0.706255, acc.: 62.50%] [Generator loss: 0.767475]\n",
      "8405 [Discriminator loss: 0.662575, acc.: 64.06%] [Generator loss: 0.848439]\n",
      "8406 [Discriminator loss: 0.643509, acc.: 58.59%] [Generator loss: 1.056665]\n",
      "8407 [Discriminator loss: 0.643230, acc.: 67.97%] [Generator loss: 0.953354]\n",
      "8408 [Discriminator loss: 0.686190, acc.: 60.94%] [Generator loss: 0.937964]\n",
      "8409 [Discriminator loss: 0.829623, acc.: 42.97%] [Generator loss: 0.892252]\n",
      "8410 [Discriminator loss: 0.732582, acc.: 51.56%] [Generator loss: 0.868726]\n",
      "8411 [Discriminator loss: 0.557445, acc.: 68.75%] [Generator loss: 0.899481]\n",
      "8412 [Discriminator loss: 0.751046, acc.: 55.47%] [Generator loss: 0.732210]\n",
      "8413 [Discriminator loss: 0.621375, acc.: 62.50%] [Generator loss: 0.925619]\n",
      "8414 [Discriminator loss: 0.656972, acc.: 63.28%] [Generator loss: 0.944893]\n",
      "8415 [Discriminator loss: 0.659468, acc.: 64.84%] [Generator loss: 0.867605]\n",
      "8416 [Discriminator loss: 0.638351, acc.: 61.72%] [Generator loss: 0.889034]\n",
      "8417 [Discriminator loss: 0.682674, acc.: 60.16%] [Generator loss: 0.914431]\n",
      "8418 [Discriminator loss: 0.730922, acc.: 54.69%] [Generator loss: 1.045245]\n",
      "8419 [Discriminator loss: 0.778072, acc.: 53.12%] [Generator loss: 0.804162]\n",
      "8420 [Discriminator loss: 0.822444, acc.: 45.31%] [Generator loss: 0.747015]\n",
      "8421 [Discriminator loss: 0.713965, acc.: 54.69%] [Generator loss: 0.734278]\n",
      "8422 [Discriminator loss: 0.805435, acc.: 44.53%] [Generator loss: 1.182927]\n",
      "8423 [Discriminator loss: 0.696357, acc.: 60.16%] [Generator loss: 1.030148]\n",
      "8424 [Discriminator loss: 0.740342, acc.: 52.34%] [Generator loss: 0.914938]\n",
      "8425 [Discriminator loss: 0.711079, acc.: 60.16%] [Generator loss: 0.930343]\n",
      "8426 [Discriminator loss: 0.574107, acc.: 69.53%] [Generator loss: 0.898669]\n",
      "8427 [Discriminator loss: 0.557570, acc.: 78.12%] [Generator loss: 0.921177]\n",
      "8428 [Discriminator loss: 0.711161, acc.: 56.25%] [Generator loss: 0.931453]\n",
      "8429 [Discriminator loss: 0.731091, acc.: 51.56%] [Generator loss: 0.836856]\n",
      "8430 [Discriminator loss: 0.615485, acc.: 63.28%] [Generator loss: 0.876960]\n",
      "8431 [Discriminator loss: 0.713751, acc.: 58.59%] [Generator loss: 0.908674]\n",
      "8432 [Discriminator loss: 0.714998, acc.: 53.12%] [Generator loss: 0.962559]\n",
      "8433 [Discriminator loss: 0.941372, acc.: 36.72%] [Generator loss: 1.137050]\n",
      "8434 [Discriminator loss: 0.622588, acc.: 63.28%] [Generator loss: 1.219039]\n",
      "8435 [Discriminator loss: 0.597662, acc.: 64.84%] [Generator loss: 0.844471]\n",
      "8436 [Discriminator loss: 0.626769, acc.: 63.28%] [Generator loss: 0.976360]\n",
      "8437 [Discriminator loss: 0.687810, acc.: 57.81%] [Generator loss: 0.863630]\n",
      "8438 [Discriminator loss: 0.523830, acc.: 77.34%] [Generator loss: 0.845529]\n",
      "8439 [Discriminator loss: 0.704809, acc.: 49.22%] [Generator loss: 0.812085]\n",
      "8440 [Discriminator loss: 0.584503, acc.: 69.53%] [Generator loss: 0.667005]\n",
      "8441 [Discriminator loss: 0.621496, acc.: 65.62%] [Generator loss: 0.781004]\n",
      "8442 [Discriminator loss: 0.799912, acc.: 46.09%] [Generator loss: 0.822514]\n",
      "8443 [Discriminator loss: 0.603468, acc.: 67.19%] [Generator loss: 0.700708]\n",
      "8444 [Discriminator loss: 0.551586, acc.: 72.66%] [Generator loss: 0.733963]\n",
      "8445 [Discriminator loss: 0.708826, acc.: 58.59%] [Generator loss: 0.887560]\n",
      "8446 [Discriminator loss: 0.625855, acc.: 67.19%] [Generator loss: 1.277295]\n",
      "8447 [Discriminator loss: 0.631868, acc.: 65.62%] [Generator loss: 0.981719]\n",
      "8448 [Discriminator loss: 0.923010, acc.: 43.75%] [Generator loss: 0.963568]\n",
      "8449 [Discriminator loss: 0.548705, acc.: 73.44%] [Generator loss: 0.896805]\n",
      "8450 [Discriminator loss: 0.642568, acc.: 71.09%] [Generator loss: 0.982935]\n",
      "8451 [Discriminator loss: 0.679013, acc.: 52.34%] [Generator loss: 0.943856]\n",
      "8452 [Discriminator loss: 0.594316, acc.: 68.75%] [Generator loss: 1.007939]\n",
      "8453 [Discriminator loss: 0.582006, acc.: 67.97%] [Generator loss: 0.932131]\n",
      "8454 [Discriminator loss: 0.440502, acc.: 85.94%] [Generator loss: 0.866443]\n",
      "8455 [Discriminator loss: 0.594037, acc.: 72.66%] [Generator loss: 0.814314]\n",
      "8456 [Discriminator loss: 0.689755, acc.: 61.72%] [Generator loss: 0.899412]\n",
      "8457 [Discriminator loss: 0.688930, acc.: 60.94%] [Generator loss: 0.944004]\n",
      "8458 [Discriminator loss: 0.720403, acc.: 57.81%] [Generator loss: 0.891975]\n",
      "8459 [Discriminator loss: 0.596307, acc.: 70.31%] [Generator loss: 0.919350]\n",
      "8460 [Discriminator loss: 0.774610, acc.: 59.38%] [Generator loss: 0.860734]\n",
      "8461 [Discriminator loss: 0.684199, acc.: 58.59%] [Generator loss: 0.966361]\n",
      "8462 [Discriminator loss: 0.743836, acc.: 53.91%] [Generator loss: 0.873162]\n",
      "8463 [Discriminator loss: 0.646201, acc.: 60.94%] [Generator loss: 0.936937]\n",
      "8464 [Discriminator loss: 0.656877, acc.: 65.62%] [Generator loss: 0.816927]\n",
      "8465 [Discriminator loss: 0.586935, acc.: 66.41%] [Generator loss: 0.644869]\n",
      "8466 [Discriminator loss: 0.727154, acc.: 57.03%] [Generator loss: 0.613622]\n",
      "8467 [Discriminator loss: 0.957065, acc.: 37.50%] [Generator loss: 0.656325]\n",
      "8468 [Discriminator loss: 0.652280, acc.: 55.47%] [Generator loss: 0.826758]\n",
      "8469 [Discriminator loss: 0.914350, acc.: 41.41%] [Generator loss: 0.787730]\n",
      "8470 [Discriminator loss: 0.615321, acc.: 70.31%] [Generator loss: 1.102574]\n",
      "8471 [Discriminator loss: 0.652416, acc.: 60.16%] [Generator loss: 1.185986]\n",
      "8472 [Discriminator loss: 0.742682, acc.: 52.34%] [Generator loss: 0.949321]\n",
      "8473 [Discriminator loss: 0.783554, acc.: 50.00%] [Generator loss: 1.010476]\n",
      "8474 [Discriminator loss: 0.646704, acc.: 62.50%] [Generator loss: 1.195963]\n",
      "8475 [Discriminator loss: 0.763376, acc.: 51.56%] [Generator loss: 0.907909]\n",
      "8476 [Discriminator loss: 0.642728, acc.: 57.03%] [Generator loss: 0.857649]\n",
      "8477 [Discriminator loss: 0.747620, acc.: 57.81%] [Generator loss: 0.935020]\n",
      "8478 [Discriminator loss: 0.923438, acc.: 37.50%] [Generator loss: 0.972068]\n",
      "8479 [Discriminator loss: 0.718109, acc.: 56.25%] [Generator loss: 0.846472]\n",
      "8480 [Discriminator loss: 0.619820, acc.: 66.41%] [Generator loss: 0.937019]\n",
      "8481 [Discriminator loss: 0.566319, acc.: 70.31%] [Generator loss: 0.845444]\n",
      "8482 [Discriminator loss: 0.682484, acc.: 57.03%] [Generator loss: 0.832123]\n",
      "8483 [Discriminator loss: 0.585887, acc.: 71.88%] [Generator loss: 0.876765]\n",
      "8484 [Discriminator loss: 0.700807, acc.: 58.59%] [Generator loss: 0.745127]\n",
      "8485 [Discriminator loss: 0.629064, acc.: 70.31%] [Generator loss: 0.761426]\n",
      "8486 [Discriminator loss: 0.441075, acc.: 86.72%] [Generator loss: 0.794118]\n",
      "8487 [Discriminator loss: 0.724626, acc.: 57.03%] [Generator loss: 0.628535]\n",
      "8488 [Discriminator loss: 0.984092, acc.: 28.91%] [Generator loss: 0.679250]\n",
      "8489 [Discriminator loss: 0.698533, acc.: 57.03%] [Generator loss: 0.841660]\n",
      "8490 [Discriminator loss: 0.506245, acc.: 78.12%] [Generator loss: 0.896515]\n",
      "8491 [Discriminator loss: 0.573578, acc.: 75.00%] [Generator loss: 0.964245]\n",
      "8492 [Discriminator loss: 0.607216, acc.: 68.75%] [Generator loss: 0.780510]\n",
      "8493 [Discriminator loss: 0.649701, acc.: 63.28%] [Generator loss: 0.723086]\n",
      "8494 [Discriminator loss: 0.586410, acc.: 70.31%] [Generator loss: 0.900321]\n",
      "8495 [Discriminator loss: 0.617580, acc.: 66.41%] [Generator loss: 0.924847]\n",
      "8496 [Discriminator loss: 0.670532, acc.: 55.47%] [Generator loss: 0.924422]\n",
      "8497 [Discriminator loss: 0.573536, acc.: 72.66%] [Generator loss: 0.968062]\n",
      "8498 [Discriminator loss: 0.753287, acc.: 53.91%] [Generator loss: 0.920613]\n",
      "8499 [Discriminator loss: 0.794831, acc.: 47.66%] [Generator loss: 0.792220]\n",
      "8500 [Discriminator loss: 0.810390, acc.: 41.41%] [Generator loss: 1.074753]\n",
      "8501 [Discriminator loss: 0.820813, acc.: 53.12%] [Generator loss: 1.158685]\n",
      "8502 [Discriminator loss: 0.682803, acc.: 59.38%] [Generator loss: 1.262000]\n",
      "8503 [Discriminator loss: 0.728266, acc.: 52.34%] [Generator loss: 1.140178]\n",
      "8504 [Discriminator loss: 0.544854, acc.: 69.53%] [Generator loss: 1.009317]\n",
      "8505 [Discriminator loss: 0.726727, acc.: 54.69%] [Generator loss: 0.991465]\n",
      "8506 [Discriminator loss: 0.540071, acc.: 72.66%] [Generator loss: 0.935711]\n",
      "8507 [Discriminator loss: 0.595940, acc.: 67.19%] [Generator loss: 1.004442]\n",
      "8508 [Discriminator loss: 0.650596, acc.: 58.59%] [Generator loss: 0.847475]\n",
      "8509 [Discriminator loss: 0.615585, acc.: 68.75%] [Generator loss: 0.873757]\n",
      "8510 [Discriminator loss: 0.743708, acc.: 49.22%] [Generator loss: 0.763144]\n",
      "8511 [Discriminator loss: 0.821176, acc.: 50.78%] [Generator loss: 0.804221]\n",
      "8512 [Discriminator loss: 0.648836, acc.: 60.94%] [Generator loss: 1.199113]\n",
      "8513 [Discriminator loss: 0.677058, acc.: 66.41%] [Generator loss: 1.008312]\n",
      "8514 [Discriminator loss: 0.712510, acc.: 57.81%] [Generator loss: 0.833817]\n",
      "8515 [Discriminator loss: 0.720633, acc.: 57.03%] [Generator loss: 0.738147]\n",
      "8516 [Discriminator loss: 0.646480, acc.: 59.38%] [Generator loss: 0.942287]\n",
      "8517 [Discriminator loss: 0.739904, acc.: 51.56%] [Generator loss: 0.885430]\n",
      "8518 [Discriminator loss: 0.830287, acc.: 37.50%] [Generator loss: 0.940171]\n",
      "8519 [Discriminator loss: 0.752397, acc.: 48.44%] [Generator loss: 0.784070]\n",
      "8520 [Discriminator loss: 0.533331, acc.: 80.47%] [Generator loss: 0.782690]\n",
      "8521 [Discriminator loss: 0.798309, acc.: 41.41%] [Generator loss: 0.769576]\n",
      "8522 [Discriminator loss: 0.532311, acc.: 70.31%] [Generator loss: 0.852614]\n",
      "8523 [Discriminator loss: 0.658942, acc.: 62.50%] [Generator loss: 0.924771]\n",
      "8524 [Discriminator loss: 0.520766, acc.: 75.78%] [Generator loss: 0.787615]\n",
      "8525 [Discriminator loss: 0.524693, acc.: 79.69%] [Generator loss: 0.971090]\n",
      "8526 [Discriminator loss: 0.585528, acc.: 80.47%] [Generator loss: 0.784323]\n",
      "8527 [Discriminator loss: 0.652480, acc.: 61.72%] [Generator loss: 0.976951]\n",
      "8528 [Discriminator loss: 0.684382, acc.: 61.72%] [Generator loss: 0.831007]\n",
      "8529 [Discriminator loss: 0.587351, acc.: 72.66%] [Generator loss: 0.813518]\n",
      "8530 [Discriminator loss: 0.621276, acc.: 62.50%] [Generator loss: 0.839280]\n",
      "8531 [Discriminator loss: 0.722824, acc.: 52.34%] [Generator loss: 0.898677]\n",
      "8532 [Discriminator loss: 0.634627, acc.: 70.31%] [Generator loss: 0.739040]\n",
      "8533 [Discriminator loss: 0.824831, acc.: 42.97%] [Generator loss: 0.667094]\n",
      "8534 [Discriminator loss: 0.632513, acc.: 62.50%] [Generator loss: 0.707487]\n",
      "8535 [Discriminator loss: 0.487612, acc.: 81.25%] [Generator loss: 0.651313]\n",
      "8536 [Discriminator loss: 0.675038, acc.: 58.59%] [Generator loss: 0.797439]\n",
      "8537 [Discriminator loss: 0.662922, acc.: 58.59%] [Generator loss: 0.834912]\n",
      "8538 [Discriminator loss: 0.480969, acc.: 81.25%] [Generator loss: 0.852299]\n",
      "8539 [Discriminator loss: 0.804938, acc.: 48.44%] [Generator loss: 1.442407]\n",
      "8540 [Discriminator loss: 0.857610, acc.: 44.53%] [Generator loss: 1.312342]\n",
      "8541 [Discriminator loss: 0.665601, acc.: 64.84%] [Generator loss: 1.055142]\n",
      "8542 [Discriminator loss: 0.646149, acc.: 58.59%] [Generator loss: 0.957053]\n",
      "8543 [Discriminator loss: 0.625176, acc.: 68.75%] [Generator loss: 0.761426]\n",
      "8544 [Discriminator loss: 0.701414, acc.: 65.62%] [Generator loss: 0.786161]\n",
      "8545 [Discriminator loss: 0.737577, acc.: 46.88%] [Generator loss: 0.945388]\n",
      "8546 [Discriminator loss: 0.599475, acc.: 62.50%] [Generator loss: 0.881656]\n",
      "8547 [Discriminator loss: 0.747861, acc.: 50.78%] [Generator loss: 1.101533]\n",
      "8548 [Discriminator loss: 0.664303, acc.: 58.59%] [Generator loss: 0.972121]\n",
      "8549 [Discriminator loss: 0.840057, acc.: 39.84%] [Generator loss: 0.862079]\n",
      "8550 [Discriminator loss: 0.717901, acc.: 51.56%] [Generator loss: 0.990165]\n",
      "8551 [Discriminator loss: 0.821876, acc.: 42.97%] [Generator loss: 0.921359]\n",
      "8552 [Discriminator loss: 0.768973, acc.: 50.00%] [Generator loss: 0.943177]\n",
      "8553 [Discriminator loss: 0.686963, acc.: 57.81%] [Generator loss: 0.930137]\n",
      "8554 [Discriminator loss: 0.732319, acc.: 57.03%] [Generator loss: 1.031768]\n",
      "8555 [Discriminator loss: 0.645124, acc.: 68.75%] [Generator loss: 0.937318]\n",
      "8556 [Discriminator loss: 0.704179, acc.: 57.81%] [Generator loss: 1.140788]\n",
      "8557 [Discriminator loss: 0.794565, acc.: 42.97%] [Generator loss: 1.018536]\n",
      "8558 [Discriminator loss: 0.670982, acc.: 64.84%] [Generator loss: 0.918120]\n",
      "8559 [Discriminator loss: 0.613294, acc.: 65.62%] [Generator loss: 0.880586]\n",
      "8560 [Discriminator loss: 0.627499, acc.: 58.59%] [Generator loss: 0.793513]\n",
      "8561 [Discriminator loss: 0.654389, acc.: 64.06%] [Generator loss: 0.770247]\n",
      "8562 [Discriminator loss: 0.717540, acc.: 57.81%] [Generator loss: 0.916055]\n",
      "8563 [Discriminator loss: 0.769758, acc.: 42.97%] [Generator loss: 1.029624]\n",
      "8564 [Discriminator loss: 0.582440, acc.: 71.09%] [Generator loss: 0.976210]\n",
      "8565 [Discriminator loss: 0.560880, acc.: 73.44%] [Generator loss: 0.836661]\n",
      "8566 [Discriminator loss: 0.534559, acc.: 74.22%] [Generator loss: 0.881722]\n",
      "8567 [Discriminator loss: 0.698713, acc.: 57.81%] [Generator loss: 0.823228]\n",
      "8568 [Discriminator loss: 0.751320, acc.: 51.56%] [Generator loss: 0.943437]\n",
      "8569 [Discriminator loss: 0.806745, acc.: 47.66%] [Generator loss: 0.682056]\n",
      "8570 [Discriminator loss: 0.807847, acc.: 43.75%] [Generator loss: 0.686709]\n",
      "8571 [Discriminator loss: 0.559708, acc.: 78.12%] [Generator loss: 0.710938]\n",
      "8572 [Discriminator loss: 0.958762, acc.: 42.97%] [Generator loss: 1.289666]\n",
      "8573 [Discriminator loss: 0.709861, acc.: 58.59%] [Generator loss: 1.299056]\n",
      "8574 [Discriminator loss: 0.676276, acc.: 57.81%] [Generator loss: 1.210840]\n",
      "8575 [Discriminator loss: 0.799894, acc.: 47.66%] [Generator loss: 1.046841]\n",
      "8576 [Discriminator loss: 0.578660, acc.: 67.97%] [Generator loss: 0.911460]\n",
      "8577 [Discriminator loss: 0.723811, acc.: 54.69%] [Generator loss: 0.924450]\n",
      "8578 [Discriminator loss: 0.659156, acc.: 65.62%] [Generator loss: 0.926795]\n",
      "8579 [Discriminator loss: 0.604887, acc.: 70.31%] [Generator loss: 0.993263]\n",
      "8580 [Discriminator loss: 0.499649, acc.: 77.34%] [Generator loss: 0.991168]\n",
      "8581 [Discriminator loss: 0.619401, acc.: 65.62%] [Generator loss: 0.969276]\n",
      "8582 [Discriminator loss: 0.689039, acc.: 66.41%] [Generator loss: 1.047287]\n",
      "8583 [Discriminator loss: 0.634898, acc.: 64.06%] [Generator loss: 0.954555]\n",
      "8584 [Discriminator loss: 0.607942, acc.: 72.66%] [Generator loss: 0.840469]\n",
      "8585 [Discriminator loss: 0.679792, acc.: 62.50%] [Generator loss: 0.895230]\n",
      "8586 [Discriminator loss: 0.589429, acc.: 67.19%] [Generator loss: 0.937969]\n",
      "8587 [Discriminator loss: 0.694631, acc.: 63.28%] [Generator loss: 0.893115]\n",
      "8588 [Discriminator loss: 0.723963, acc.: 51.56%] [Generator loss: 0.772321]\n",
      "8589 [Discriminator loss: 0.650291, acc.: 58.59%] [Generator loss: 0.851863]\n",
      "8590 [Discriminator loss: 0.715505, acc.: 57.81%] [Generator loss: 0.832744]\n",
      "8591 [Discriminator loss: 0.720905, acc.: 57.81%] [Generator loss: 0.840341]\n",
      "8592 [Discriminator loss: 0.772830, acc.: 49.22%] [Generator loss: 0.777353]\n",
      "8593 [Discriminator loss: 0.692510, acc.: 60.16%] [Generator loss: 1.021449]\n",
      "8594 [Discriminator loss: 0.737735, acc.: 59.38%] [Generator loss: 1.155207]\n",
      "8595 [Discriminator loss: 0.667056, acc.: 64.84%] [Generator loss: 0.908456]\n",
      "8596 [Discriminator loss: 0.761018, acc.: 50.00%] [Generator loss: 0.903147]\n",
      "8597 [Discriminator loss: 0.774966, acc.: 51.56%] [Generator loss: 0.905039]\n",
      "8598 [Discriminator loss: 0.706439, acc.: 56.25%] [Generator loss: 0.966078]\n",
      "8599 [Discriminator loss: 0.753241, acc.: 52.34%] [Generator loss: 1.082529]\n",
      "8600 [Discriminator loss: 0.610013, acc.: 67.19%] [Generator loss: 1.032889]\n",
      "8601 [Discriminator loss: 0.914897, acc.: 45.31%] [Generator loss: 1.009851]\n",
      "8602 [Discriminator loss: 0.701667, acc.: 56.25%] [Generator loss: 1.003804]\n",
      "8603 [Discriminator loss: 0.660511, acc.: 61.72%] [Generator loss: 1.182996]\n",
      "8604 [Discriminator loss: 0.676941, acc.: 60.16%] [Generator loss: 1.128290]\n",
      "8605 [Discriminator loss: 0.758591, acc.: 50.00%] [Generator loss: 0.897633]\n",
      "8606 [Discriminator loss: 0.678426, acc.: 60.16%] [Generator loss: 0.829051]\n",
      "8607 [Discriminator loss: 0.589034, acc.: 72.66%] [Generator loss: 0.821156]\n",
      "8608 [Discriminator loss: 0.599656, acc.: 75.00%] [Generator loss: 0.971062]\n",
      "8609 [Discriminator loss: 0.804234, acc.: 46.09%] [Generator loss: 0.806484]\n",
      "8610 [Discriminator loss: 0.716653, acc.: 52.34%] [Generator loss: 0.835958]\n",
      "8611 [Discriminator loss: 0.693767, acc.: 59.38%] [Generator loss: 0.953843]\n",
      "8612 [Discriminator loss: 0.801505, acc.: 40.62%] [Generator loss: 0.867515]\n",
      "8613 [Discriminator loss: 0.637113, acc.: 64.84%] [Generator loss: 0.816504]\n",
      "8614 [Discriminator loss: 0.512277, acc.: 76.56%] [Generator loss: 0.889928]\n",
      "8615 [Discriminator loss: 0.770673, acc.: 45.31%] [Generator loss: 0.861886]\n",
      "8616 [Discriminator loss: 0.534309, acc.: 75.00%] [Generator loss: 0.834417]\n",
      "8617 [Discriminator loss: 0.530134, acc.: 77.34%] [Generator loss: 0.869458]\n",
      "8618 [Discriminator loss: 0.686263, acc.: 60.94%] [Generator loss: 1.114217]\n",
      "8619 [Discriminator loss: 0.661056, acc.: 60.94%] [Generator loss: 0.916900]\n",
      "8620 [Discriminator loss: 0.598481, acc.: 68.75%] [Generator loss: 0.902548]\n",
      "8621 [Discriminator loss: 0.747055, acc.: 49.22%] [Generator loss: 0.793812]\n",
      "8622 [Discriminator loss: 0.726898, acc.: 51.56%] [Generator loss: 0.737440]\n",
      "8623 [Discriminator loss: 0.767162, acc.: 49.22%] [Generator loss: 0.783990]\n",
      "8624 [Discriminator loss: 0.773448, acc.: 53.91%] [Generator loss: 0.861623]\n",
      "8625 [Discriminator loss: 0.641565, acc.: 63.28%] [Generator loss: 0.892599]\n",
      "8626 [Discriminator loss: 0.636673, acc.: 70.31%] [Generator loss: 0.840553]\n",
      "8627 [Discriminator loss: 0.687616, acc.: 57.81%] [Generator loss: 1.000616]\n",
      "8628 [Discriminator loss: 0.622509, acc.: 70.31%] [Generator loss: 0.880112]\n",
      "8629 [Discriminator loss: 0.619414, acc.: 73.44%] [Generator loss: 0.795478]\n",
      "8630 [Discriminator loss: 0.719110, acc.: 57.81%] [Generator loss: 0.907703]\n",
      "8631 [Discriminator loss: 0.615650, acc.: 65.62%] [Generator loss: 0.989134]\n",
      "8632 [Discriminator loss: 0.598846, acc.: 67.97%] [Generator loss: 1.159740]\n",
      "8633 [Discriminator loss: 0.611910, acc.: 63.28%] [Generator loss: 1.260962]\n",
      "8634 [Discriminator loss: 0.653398, acc.: 63.28%] [Generator loss: 0.851372]\n",
      "8635 [Discriminator loss: 0.598395, acc.: 69.53%] [Generator loss: 0.943838]\n",
      "8636 [Discriminator loss: 0.509316, acc.: 81.25%] [Generator loss: 0.810205]\n",
      "8637 [Discriminator loss: 0.561354, acc.: 75.78%] [Generator loss: 0.749356]\n",
      "8638 [Discriminator loss: 0.724986, acc.: 52.34%] [Generator loss: 0.750029]\n",
      "8639 [Discriminator loss: 0.745313, acc.: 47.66%] [Generator loss: 0.823059]\n",
      "8640 [Discriminator loss: 0.736787, acc.: 50.00%] [Generator loss: 0.702275]\n",
      "8641 [Discriminator loss: 0.642848, acc.: 61.72%] [Generator loss: 0.811370]\n",
      "8642 [Discriminator loss: 0.751640, acc.: 48.44%] [Generator loss: 1.008674]\n",
      "8643 [Discriminator loss: 0.750822, acc.: 50.78%] [Generator loss: 0.809651]\n",
      "8644 [Discriminator loss: 0.745617, acc.: 50.78%] [Generator loss: 0.687947]\n",
      "8645 [Discriminator loss: 0.569935, acc.: 67.97%] [Generator loss: 0.720970]\n",
      "8646 [Discriminator loss: 0.765974, acc.: 42.97%] [Generator loss: 0.819268]\n",
      "8647 [Discriminator loss: 0.758665, acc.: 56.25%] [Generator loss: 0.839045]\n",
      "8648 [Discriminator loss: 0.798195, acc.: 40.62%] [Generator loss: 0.815296]\n",
      "8649 [Discriminator loss: 0.591262, acc.: 70.31%] [Generator loss: 1.016182]\n",
      "8650 [Discriminator loss: 0.667105, acc.: 61.72%] [Generator loss: 0.880613]\n",
      "8651 [Discriminator loss: 0.621189, acc.: 67.19%] [Generator loss: 1.055601]\n",
      "8652 [Discriminator loss: 0.695303, acc.: 55.47%] [Generator loss: 1.017843]\n",
      "8653 [Discriminator loss: 0.688680, acc.: 66.41%] [Generator loss: 0.863103]\n",
      "8654 [Discriminator loss: 0.746736, acc.: 50.78%] [Generator loss: 0.990901]\n",
      "8655 [Discriminator loss: 0.635776, acc.: 69.53%] [Generator loss: 0.988967]\n",
      "8656 [Discriminator loss: 0.807732, acc.: 46.88%] [Generator loss: 0.831183]\n",
      "8657 [Discriminator loss: 0.791971, acc.: 39.06%] [Generator loss: 0.814994]\n",
      "8658 [Discriminator loss: 0.680887, acc.: 58.59%] [Generator loss: 1.099968]\n",
      "8659 [Discriminator loss: 0.819597, acc.: 46.88%] [Generator loss: 0.858670]\n",
      "8660 [Discriminator loss: 0.760525, acc.: 53.91%] [Generator loss: 0.958459]\n",
      "8661 [Discriminator loss: 0.479848, acc.: 81.25%] [Generator loss: 0.992584]\n",
      "8662 [Discriminator loss: 0.617061, acc.: 63.28%] [Generator loss: 0.937324]\n",
      "8663 [Discriminator loss: 0.896930, acc.: 42.97%] [Generator loss: 0.686834]\n",
      "8664 [Discriminator loss: 0.778493, acc.: 42.97%] [Generator loss: 0.707096]\n",
      "8665 [Discriminator loss: 0.563067, acc.: 71.09%] [Generator loss: 0.886732]\n",
      "8666 [Discriminator loss: 0.768098, acc.: 52.34%] [Generator loss: 0.777488]\n",
      "8667 [Discriminator loss: 0.615070, acc.: 64.06%] [Generator loss: 0.921440]\n",
      "8668 [Discriminator loss: 0.550943, acc.: 77.34%] [Generator loss: 1.084321]\n",
      "8669 [Discriminator loss: 0.780219, acc.: 49.22%] [Generator loss: 0.815538]\n",
      "8670 [Discriminator loss: 0.573613, acc.: 71.09%] [Generator loss: 0.920537]\n",
      "8671 [Discriminator loss: 0.620088, acc.: 69.53%] [Generator loss: 0.917032]\n",
      "8672 [Discriminator loss: 0.538986, acc.: 78.91%] [Generator loss: 0.849349]\n",
      "8673 [Discriminator loss: 0.675174, acc.: 61.72%] [Generator loss: 0.810950]\n",
      "8674 [Discriminator loss: 0.727481, acc.: 59.38%] [Generator loss: 0.905114]\n",
      "8675 [Discriminator loss: 0.695545, acc.: 53.91%] [Generator loss: 1.122771]\n",
      "8676 [Discriminator loss: 0.594774, acc.: 72.66%] [Generator loss: 1.088429]\n",
      "8677 [Discriminator loss: 0.613167, acc.: 67.19%] [Generator loss: 0.896413]\n",
      "8678 [Discriminator loss: 0.671265, acc.: 55.47%] [Generator loss: 0.936042]\n",
      "8679 [Discriminator loss: 0.604909, acc.: 65.62%] [Generator loss: 1.208999]\n",
      "8680 [Discriminator loss: 0.604832, acc.: 64.84%] [Generator loss: 0.935394]\n",
      "8681 [Discriminator loss: 0.601465, acc.: 69.53%] [Generator loss: 0.858383]\n",
      "8682 [Discriminator loss: 0.788258, acc.: 50.00%] [Generator loss: 0.804087]\n",
      "8683 [Discriminator loss: 0.749229, acc.: 50.78%] [Generator loss: 0.795900]\n",
      "8684 [Discriminator loss: 0.725487, acc.: 55.47%] [Generator loss: 0.894594]\n",
      "8685 [Discriminator loss: 0.534049, acc.: 77.34%] [Generator loss: 0.841016]\n",
      "8686 [Discriminator loss: 0.574510, acc.: 69.53%] [Generator loss: 0.909496]\n",
      "8687 [Discriminator loss: 0.637005, acc.: 68.75%] [Generator loss: 0.839735]\n",
      "8688 [Discriminator loss: 0.776358, acc.: 46.88%] [Generator loss: 0.886558]\n",
      "8689 [Discriminator loss: 0.660825, acc.: 64.84%] [Generator loss: 0.761567]\n",
      "8690 [Discriminator loss: 0.682587, acc.: 60.94%] [Generator loss: 0.749044]\n",
      "8691 [Discriminator loss: 0.571138, acc.: 71.09%] [Generator loss: 0.798087]\n",
      "8692 [Discriminator loss: 0.721688, acc.: 49.22%] [Generator loss: 0.788876]\n",
      "8693 [Discriminator loss: 0.745264, acc.: 57.81%] [Generator loss: 0.687894]\n",
      "8694 [Discriminator loss: 0.697431, acc.: 60.16%] [Generator loss: 0.817801]\n",
      "8695 [Discriminator loss: 0.619016, acc.: 68.75%] [Generator loss: 1.013092]\n",
      "8696 [Discriminator loss: 0.704775, acc.: 53.12%] [Generator loss: 1.032050]\n",
      "8697 [Discriminator loss: 0.570100, acc.: 72.66%] [Generator loss: 0.949309]\n",
      "8698 [Discriminator loss: 0.515765, acc.: 82.03%] [Generator loss: 0.797449]\n",
      "8699 [Discriminator loss: 0.625712, acc.: 62.50%] [Generator loss: 0.738037]\n",
      "8700 [Discriminator loss: 0.571293, acc.: 71.09%] [Generator loss: 0.755604]\n",
      "8701 [Discriminator loss: 0.703270, acc.: 64.84%] [Generator loss: 0.715827]\n",
      "8702 [Discriminator loss: 0.606129, acc.: 61.72%] [Generator loss: 0.755824]\n",
      "8703 [Discriminator loss: 0.711416, acc.: 60.16%] [Generator loss: 0.771732]\n",
      "8704 [Discriminator loss: 0.694605, acc.: 62.50%] [Generator loss: 0.654683]\n",
      "8705 [Discriminator loss: 0.673917, acc.: 60.16%] [Generator loss: 0.845035]\n",
      "8706 [Discriminator loss: 0.552287, acc.: 72.66%] [Generator loss: 0.932566]\n",
      "8707 [Discriminator loss: 0.565840, acc.: 75.78%] [Generator loss: 0.818801]\n",
      "8708 [Discriminator loss: 0.558307, acc.: 73.44%] [Generator loss: 0.708735]\n",
      "8709 [Discriminator loss: 0.990643, acc.: 36.72%] [Generator loss: 0.682708]\n",
      "8710 [Discriminator loss: 0.705195, acc.: 56.25%] [Generator loss: 0.792372]\n",
      "8711 [Discriminator loss: 0.718905, acc.: 54.69%] [Generator loss: 1.354312]\n",
      "8712 [Discriminator loss: 0.805581, acc.: 49.22%] [Generator loss: 1.291199]\n",
      "8713 [Discriminator loss: 0.647744, acc.: 60.16%] [Generator loss: 1.036953]\n",
      "8714 [Discriminator loss: 0.789603, acc.: 46.09%] [Generator loss: 0.947094]\n",
      "8715 [Discriminator loss: 0.554412, acc.: 68.75%] [Generator loss: 0.870980]\n",
      "8716 [Discriminator loss: 0.596414, acc.: 68.75%] [Generator loss: 0.767701]\n",
      "8717 [Discriminator loss: 0.640111, acc.: 67.19%] [Generator loss: 0.872561]\n",
      "8718 [Discriminator loss: 0.753135, acc.: 54.69%] [Generator loss: 0.851663]\n",
      "8719 [Discriminator loss: 0.626588, acc.: 61.72%] [Generator loss: 0.985505]\n",
      "8720 [Discriminator loss: 0.629684, acc.: 61.72%] [Generator loss: 0.902608]\n",
      "8721 [Discriminator loss: 0.732984, acc.: 57.81%] [Generator loss: 0.751228]\n",
      "8722 [Discriminator loss: 0.714147, acc.: 61.72%] [Generator loss: 0.707936]\n",
      "8723 [Discriminator loss: 0.612864, acc.: 71.88%] [Generator loss: 0.758741]\n",
      "8724 [Discriminator loss: 0.626802, acc.: 64.84%] [Generator loss: 0.911182]\n",
      "8725 [Discriminator loss: 0.588897, acc.: 71.09%] [Generator loss: 0.989005]\n",
      "8726 [Discriminator loss: 0.568741, acc.: 68.75%] [Generator loss: 0.955354]\n",
      "8727 [Discriminator loss: 0.555290, acc.: 72.66%] [Generator loss: 0.844907]\n",
      "8728 [Discriminator loss: 0.725849, acc.: 55.47%] [Generator loss: 0.893452]\n",
      "8729 [Discriminator loss: 0.890843, acc.: 44.53%] [Generator loss: 1.085346]\n",
      "8730 [Discriminator loss: 0.703248, acc.: 58.59%] [Generator loss: 0.955182]\n",
      "8731 [Discriminator loss: 0.658277, acc.: 65.62%] [Generator loss: 0.827442]\n",
      "8732 [Discriminator loss: 0.642292, acc.: 62.50%] [Generator loss: 0.767387]\n",
      "8733 [Discriminator loss: 0.752910, acc.: 55.47%] [Generator loss: 0.858199]\n",
      "8734 [Discriminator loss: 0.740695, acc.: 56.25%] [Generator loss: 0.741593]\n",
      "8735 [Discriminator loss: 0.614723, acc.: 72.66%] [Generator loss: 0.861704]\n",
      "8736 [Discriminator loss: 0.694871, acc.: 56.25%] [Generator loss: 0.778455]\n",
      "8737 [Discriminator loss: 0.592113, acc.: 65.62%] [Generator loss: 1.136289]\n",
      "8738 [Discriminator loss: 0.629710, acc.: 62.50%] [Generator loss: 0.839250]\n",
      "8739 [Discriminator loss: 0.487169, acc.: 82.03%] [Generator loss: 0.774123]\n",
      "8740 [Discriminator loss: 0.683283, acc.: 58.59%] [Generator loss: 0.790403]\n",
      "8741 [Discriminator loss: 0.597247, acc.: 67.97%] [Generator loss: 0.834293]\n",
      "8742 [Discriminator loss: 0.707099, acc.: 60.16%] [Generator loss: 0.828874]\n",
      "8743 [Discriminator loss: 0.763326, acc.: 46.09%] [Generator loss: 0.835697]\n",
      "8744 [Discriminator loss: 0.648514, acc.: 66.41%] [Generator loss: 0.824204]\n",
      "8745 [Discriminator loss: 0.691913, acc.: 57.03%] [Generator loss: 0.956097]\n",
      "8746 [Discriminator loss: 0.746547, acc.: 54.69%] [Generator loss: 0.841358]\n",
      "8747 [Discriminator loss: 0.689634, acc.: 57.81%] [Generator loss: 0.719137]\n",
      "8748 [Discriminator loss: 0.598940, acc.: 71.88%] [Generator loss: 0.604786]\n",
      "8749 [Discriminator loss: 0.577584, acc.: 66.41%] [Generator loss: 0.789160]\n",
      "8750 [Discriminator loss: 0.613510, acc.: 70.31%] [Generator loss: 0.849178]\n",
      "8751 [Discriminator loss: 0.515059, acc.: 75.78%] [Generator loss: 0.887882]\n",
      "8752 [Discriminator loss: 0.548664, acc.: 75.00%] [Generator loss: 0.909913]\n",
      "8753 [Discriminator loss: 0.548154, acc.: 71.88%] [Generator loss: 0.947269]\n",
      "8754 [Discriminator loss: 0.758419, acc.: 49.22%] [Generator loss: 0.968406]\n",
      "8755 [Discriminator loss: 0.598273, acc.: 67.19%] [Generator loss: 1.040528]\n",
      "8756 [Discriminator loss: 0.539579, acc.: 76.56%] [Generator loss: 0.905735]\n",
      "8757 [Discriminator loss: 0.604795, acc.: 67.19%] [Generator loss: 0.967384]\n",
      "8758 [Discriminator loss: 0.503827, acc.: 75.00%] [Generator loss: 0.948803]\n",
      "8759 [Discriminator loss: 0.520703, acc.: 72.66%] [Generator loss: 0.966713]\n",
      "8760 [Discriminator loss: 0.620999, acc.: 61.72%] [Generator loss: 0.680509]\n",
      "8761 [Discriminator loss: 0.663083, acc.: 56.25%] [Generator loss: 0.811148]\n",
      "8762 [Discriminator loss: 0.650150, acc.: 57.81%] [Generator loss: 0.922393]\n",
      "8763 [Discriminator loss: 0.547843, acc.: 71.88%] [Generator loss: 0.826911]\n",
      "8764 [Discriminator loss: 0.613622, acc.: 70.31%] [Generator loss: 0.773277]\n",
      "8765 [Discriminator loss: 0.558611, acc.: 68.75%] [Generator loss: 0.914178]\n",
      "8766 [Discriminator loss: 0.679098, acc.: 59.38%] [Generator loss: 1.070503]\n",
      "8767 [Discriminator loss: 0.718611, acc.: 55.47%] [Generator loss: 0.766073]\n",
      "8768 [Discriminator loss: 0.674841, acc.: 60.16%] [Generator loss: 1.024767]\n",
      "8769 [Discriminator loss: 0.789104, acc.: 46.88%] [Generator loss: 1.054937]\n",
      "8770 [Discriminator loss: 0.670743, acc.: 57.81%] [Generator loss: 1.127761]\n",
      "8771 [Discriminator loss: 0.528213, acc.: 69.53%] [Generator loss: 1.089629]\n",
      "8772 [Discriminator loss: 0.640727, acc.: 60.94%] [Generator loss: 0.973000]\n",
      "8773 [Discriminator loss: 0.935992, acc.: 39.06%] [Generator loss: 1.308021]\n",
      "8774 [Discriminator loss: 0.716232, acc.: 62.50%] [Generator loss: 1.103868]\n",
      "8775 [Discriminator loss: 0.958490, acc.: 39.06%] [Generator loss: 0.824441]\n",
      "8776 [Discriminator loss: 0.692999, acc.: 60.16%] [Generator loss: 1.147751]\n",
      "8777 [Discriminator loss: 0.567907, acc.: 73.44%] [Generator loss: 1.159472]\n",
      "8778 [Discriminator loss: 0.602283, acc.: 65.62%] [Generator loss: 1.123803]\n",
      "8779 [Discriminator loss: 0.520315, acc.: 79.69%] [Generator loss: 1.014217]\n",
      "8780 [Discriminator loss: 0.558629, acc.: 75.78%] [Generator loss: 0.775725]\n",
      "8781 [Discriminator loss: 0.597448, acc.: 71.09%] [Generator loss: 0.679060]\n",
      "8782 [Discriminator loss: 0.503777, acc.: 75.78%] [Generator loss: 0.682899]\n",
      "8783 [Discriminator loss: 0.678215, acc.: 61.72%] [Generator loss: 0.678451]\n",
      "8784 [Discriminator loss: 0.531005, acc.: 75.78%] [Generator loss: 0.873607]\n",
      "8785 [Discriminator loss: 0.640112, acc.: 60.94%] [Generator loss: 0.930901]\n",
      "8786 [Discriminator loss: 0.605497, acc.: 64.06%] [Generator loss: 0.983845]\n",
      "8787 [Discriminator loss: 0.710354, acc.: 53.91%] [Generator loss: 0.948424]\n",
      "8788 [Discriminator loss: 0.661438, acc.: 67.97%] [Generator loss: 1.136329]\n",
      "8789 [Discriminator loss: 0.572548, acc.: 73.44%] [Generator loss: 0.914555]\n",
      "8790 [Discriminator loss: 0.458525, acc.: 80.47%] [Generator loss: 0.755945]\n",
      "8791 [Discriminator loss: 0.628138, acc.: 65.62%] [Generator loss: 0.694481]\n",
      "8792 [Discriminator loss: 0.642184, acc.: 66.41%] [Generator loss: 0.673902]\n",
      "8793 [Discriminator loss: 0.768240, acc.: 50.00%] [Generator loss: 0.719475]\n",
      "8794 [Discriminator loss: 0.631408, acc.: 64.84%] [Generator loss: 0.870157]\n",
      "8795 [Discriminator loss: 0.714126, acc.: 55.47%] [Generator loss: 0.784038]\n",
      "8796 [Discriminator loss: 0.686794, acc.: 62.50%] [Generator loss: 0.894026]\n",
      "8797 [Discriminator loss: 0.658024, acc.: 63.28%] [Generator loss: 0.948397]\n",
      "8798 [Discriminator loss: 0.647081, acc.: 64.06%] [Generator loss: 1.001096]\n",
      "8799 [Discriminator loss: 0.612224, acc.: 68.75%] [Generator loss: 1.054209]\n",
      "8800 [Discriminator loss: 0.626885, acc.: 64.06%] [Generator loss: 1.203389]\n",
      "8801 [Discriminator loss: 0.644265, acc.: 71.88%] [Generator loss: 1.041533]\n",
      "8802 [Discriminator loss: 0.726965, acc.: 62.50%] [Generator loss: 1.087188]\n",
      "8803 [Discriminator loss: 0.563917, acc.: 71.09%] [Generator loss: 0.963794]\n",
      "8804 [Discriminator loss: 0.792880, acc.: 48.44%] [Generator loss: 0.933169]\n",
      "8805 [Discriminator loss: 0.646001, acc.: 59.38%] [Generator loss: 0.979372]\n",
      "8806 [Discriminator loss: 0.630602, acc.: 64.84%] [Generator loss: 0.908182]\n",
      "8807 [Discriminator loss: 0.557906, acc.: 71.88%] [Generator loss: 0.915404]\n",
      "8808 [Discriminator loss: 0.979963, acc.: 35.16%] [Generator loss: 0.762933]\n",
      "8809 [Discriminator loss: 0.820277, acc.: 49.22%] [Generator loss: 0.984474]\n",
      "8810 [Discriminator loss: 0.638697, acc.: 64.06%] [Generator loss: 1.113113]\n",
      "8811 [Discriminator loss: 0.594694, acc.: 68.75%] [Generator loss: 1.060889]\n",
      "8812 [Discriminator loss: 0.610244, acc.: 73.44%] [Generator loss: 0.880215]\n",
      "8813 [Discriminator loss: 0.747573, acc.: 52.34%] [Generator loss: 0.950020]\n",
      "8814 [Discriminator loss: 0.785017, acc.: 49.22%] [Generator loss: 1.058222]\n",
      "8815 [Discriminator loss: 0.723876, acc.: 55.47%] [Generator loss: 1.198707]\n",
      "8816 [Discriminator loss: 0.597529, acc.: 71.88%] [Generator loss: 1.114795]\n",
      "8817 [Discriminator loss: 0.616759, acc.: 62.50%] [Generator loss: 0.995869]\n",
      "8818 [Discriminator loss: 0.676923, acc.: 60.16%] [Generator loss: 0.743234]\n",
      "8819 [Discriminator loss: 0.586628, acc.: 71.09%] [Generator loss: 1.030801]\n",
      "8820 [Discriminator loss: 0.482821, acc.: 70.31%] [Generator loss: 1.140277]\n",
      "8821 [Discriminator loss: 0.537170, acc.: 77.34%] [Generator loss: 0.899070]\n",
      "8822 [Discriminator loss: 0.622602, acc.: 64.84%] [Generator loss: 0.943817]\n",
      "8823 [Discriminator loss: 0.674093, acc.: 59.38%] [Generator loss: 0.996661]\n",
      "8824 [Discriminator loss: 0.557672, acc.: 70.31%] [Generator loss: 0.905121]\n",
      "8825 [Discriminator loss: 0.707164, acc.: 50.00%] [Generator loss: 0.684016]\n",
      "8826 [Discriminator loss: 0.720851, acc.: 57.03%] [Generator loss: 0.753779]\n",
      "8827 [Discriminator loss: 0.672952, acc.: 56.25%] [Generator loss: 0.865400]\n",
      "8828 [Discriminator loss: 0.970050, acc.: 30.47%] [Generator loss: 0.782387]\n",
      "8829 [Discriminator loss: 0.784444, acc.: 48.44%] [Generator loss: 0.928956]\n",
      "8830 [Discriminator loss: 0.668822, acc.: 67.19%] [Generator loss: 0.929230]\n",
      "8831 [Discriminator loss: 0.747413, acc.: 57.03%] [Generator loss: 0.893991]\n",
      "8832 [Discriminator loss: 0.621775, acc.: 60.16%] [Generator loss: 0.863874]\n",
      "8833 [Discriminator loss: 0.624820, acc.: 64.84%] [Generator loss: 0.856133]\n",
      "8834 [Discriminator loss: 0.716916, acc.: 50.00%] [Generator loss: 0.709782]\n",
      "8835 [Discriminator loss: 0.705168, acc.: 60.94%] [Generator loss: 0.852542]\n",
      "8836 [Discriminator loss: 0.501574, acc.: 78.91%] [Generator loss: 1.044728]\n",
      "8837 [Discriminator loss: 0.684068, acc.: 59.38%] [Generator loss: 0.770430]\n",
      "8838 [Discriminator loss: 0.607980, acc.: 67.19%] [Generator loss: 0.785214]\n",
      "8839 [Discriminator loss: 0.611107, acc.: 64.84%] [Generator loss: 0.776647]\n",
      "8840 [Discriminator loss: 0.599902, acc.: 60.16%] [Generator loss: 0.833791]\n",
      "8841 [Discriminator loss: 0.707733, acc.: 57.81%] [Generator loss: 0.703313]\n",
      "8842 [Discriminator loss: 0.638765, acc.: 67.19%] [Generator loss: 0.859493]\n",
      "8843 [Discriminator loss: 0.519823, acc.: 73.44%] [Generator loss: 0.928713]\n",
      "8844 [Discriminator loss: 0.622193, acc.: 61.72%] [Generator loss: 0.978884]\n",
      "8845 [Discriminator loss: 0.611796, acc.: 67.19%] [Generator loss: 1.040240]\n",
      "8846 [Discriminator loss: 0.534420, acc.: 73.44%] [Generator loss: 0.901500]\n",
      "8847 [Discriminator loss: 0.661305, acc.: 60.94%] [Generator loss: 0.951858]\n",
      "8848 [Discriminator loss: 0.738811, acc.: 53.91%] [Generator loss: 1.146372]\n",
      "8849 [Discriminator loss: 0.558343, acc.: 68.75%] [Generator loss: 1.526068]\n",
      "8850 [Discriminator loss: 0.733920, acc.: 50.78%] [Generator loss: 1.030903]\n",
      "8851 [Discriminator loss: 0.666881, acc.: 63.28%] [Generator loss: 0.784414]\n",
      "8852 [Discriminator loss: 0.619984, acc.: 67.97%] [Generator loss: 0.774005]\n",
      "8853 [Discriminator loss: 0.641455, acc.: 61.72%] [Generator loss: 0.664777]\n",
      "8854 [Discriminator loss: 0.655217, acc.: 59.38%] [Generator loss: 0.702660]\n",
      "8855 [Discriminator loss: 0.641098, acc.: 61.72%] [Generator loss: 0.918022]\n",
      "8856 [Discriminator loss: 0.690463, acc.: 60.16%] [Generator loss: 0.978858]\n",
      "8857 [Discriminator loss: 0.655520, acc.: 64.06%] [Generator loss: 1.054299]\n",
      "8858 [Discriminator loss: 0.744975, acc.: 50.78%] [Generator loss: 0.909908]\n",
      "8859 [Discriminator loss: 0.609065, acc.: 68.75%] [Generator loss: 0.982956]\n",
      "8860 [Discriminator loss: 1.016906, acc.: 27.34%] [Generator loss: 0.721043]\n",
      "8861 [Discriminator loss: 0.634208, acc.: 60.94%] [Generator loss: 0.996409]\n",
      "8862 [Discriminator loss: 0.784976, acc.: 45.31%] [Generator loss: 1.110558]\n",
      "8863 [Discriminator loss: 0.489384, acc.: 83.59%] [Generator loss: 1.158353]\n",
      "8864 [Discriminator loss: 0.501931, acc.: 81.25%] [Generator loss: 0.950683]\n",
      "8865 [Discriminator loss: 0.585950, acc.: 75.00%] [Generator loss: 0.691710]\n",
      "8866 [Discriminator loss: 0.668128, acc.: 57.03%] [Generator loss: 0.726315]\n",
      "8867 [Discriminator loss: 0.747487, acc.: 48.44%] [Generator loss: 0.934137]\n",
      "8868 [Discriminator loss: 0.933885, acc.: 42.19%] [Generator loss: 0.887020]\n",
      "8869 [Discriminator loss: 0.630880, acc.: 60.16%] [Generator loss: 0.890018]\n",
      "8870 [Discriminator loss: 0.625772, acc.: 67.97%] [Generator loss: 0.955852]\n",
      "8871 [Discriminator loss: 0.639998, acc.: 60.16%] [Generator loss: 0.969710]\n",
      "8872 [Discriminator loss: 0.686675, acc.: 59.38%] [Generator loss: 0.821989]\n",
      "8873 [Discriminator loss: 0.799197, acc.: 48.44%] [Generator loss: 0.893830]\n",
      "8874 [Discriminator loss: 0.582584, acc.: 69.53%] [Generator loss: 0.928306]\n",
      "8875 [Discriminator loss: 0.749426, acc.: 46.88%] [Generator loss: 0.902066]\n",
      "8876 [Discriminator loss: 0.681867, acc.: 63.28%] [Generator loss: 1.010107]\n",
      "8877 [Discriminator loss: 0.568213, acc.: 76.56%] [Generator loss: 1.134758]\n",
      "8878 [Discriminator loss: 0.706114, acc.: 57.03%] [Generator loss: 1.044863]\n",
      "8879 [Discriminator loss: 0.786054, acc.: 46.09%] [Generator loss: 0.917049]\n",
      "8880 [Discriminator loss: 0.616894, acc.: 66.41%] [Generator loss: 1.010114]\n",
      "8881 [Discriminator loss: 0.722619, acc.: 49.22%] [Generator loss: 1.094172]\n",
      "8882 [Discriminator loss: 0.766294, acc.: 52.34%] [Generator loss: 0.971216]\n",
      "8883 [Discriminator loss: 0.615194, acc.: 67.19%] [Generator loss: 0.924073]\n",
      "8884 [Discriminator loss: 0.605338, acc.: 68.75%] [Generator loss: 0.895431]\n",
      "8885 [Discriminator loss: 0.507546, acc.: 76.56%] [Generator loss: 0.897940]\n",
      "8886 [Discriminator loss: 0.737184, acc.: 53.12%] [Generator loss: 0.878120]\n",
      "8887 [Discriminator loss: 0.775970, acc.: 50.00%] [Generator loss: 0.853472]\n",
      "8888 [Discriminator loss: 0.666335, acc.: 60.94%] [Generator loss: 0.916730]\n",
      "8889 [Discriminator loss: 0.564607, acc.: 72.66%] [Generator loss: 0.834531]\n",
      "8890 [Discriminator loss: 0.588037, acc.: 69.53%] [Generator loss: 0.940636]\n",
      "8891 [Discriminator loss: 0.605399, acc.: 68.75%] [Generator loss: 0.868329]\n",
      "8892 [Discriminator loss: 0.637077, acc.: 63.28%] [Generator loss: 0.873038]\n",
      "8893 [Discriminator loss: 0.671170, acc.: 61.72%] [Generator loss: 0.863331]\n",
      "8894 [Discriminator loss: 0.666204, acc.: 57.81%] [Generator loss: 0.981405]\n",
      "8895 [Discriminator loss: 0.586460, acc.: 74.22%] [Generator loss: 0.802671]\n",
      "8896 [Discriminator loss: 0.752569, acc.: 53.12%] [Generator loss: 0.729266]\n",
      "8897 [Discriminator loss: 0.756551, acc.: 47.66%] [Generator loss: 0.979875]\n",
      "8898 [Discriminator loss: 0.667311, acc.: 64.06%] [Generator loss: 1.061153]\n",
      "8899 [Discriminator loss: 0.559403, acc.: 73.44%] [Generator loss: 0.785130]\n",
      "8900 [Discriminator loss: 0.698474, acc.: 60.94%] [Generator loss: 0.760072]\n",
      "8901 [Discriminator loss: 0.636468, acc.: 66.41%] [Generator loss: 0.768447]\n",
      "8902 [Discriminator loss: 0.775123, acc.: 45.31%] [Generator loss: 0.882693]\n",
      "8903 [Discriminator loss: 0.638357, acc.: 63.28%] [Generator loss: 0.971207]\n",
      "8904 [Discriminator loss: 0.569783, acc.: 73.44%] [Generator loss: 1.027459]\n",
      "8905 [Discriminator loss: 0.665575, acc.: 67.19%] [Generator loss: 0.802963]\n",
      "8906 [Discriminator loss: 0.635674, acc.: 63.28%] [Generator loss: 0.886364]\n",
      "8907 [Discriminator loss: 0.698286, acc.: 59.38%] [Generator loss: 0.895345]\n",
      "8908 [Discriminator loss: 0.654037, acc.: 55.47%] [Generator loss: 0.940020]\n",
      "8909 [Discriminator loss: 0.622931, acc.: 65.62%] [Generator loss: 0.910279]\n",
      "8910 [Discriminator loss: 0.633857, acc.: 62.50%] [Generator loss: 0.788572]\n",
      "8911 [Discriminator loss: 0.778729, acc.: 52.34%] [Generator loss: 0.705568]\n",
      "8912 [Discriminator loss: 0.664802, acc.: 61.72%] [Generator loss: 0.846135]\n",
      "8913 [Discriminator loss: 0.590600, acc.: 65.62%] [Generator loss: 0.860488]\n",
      "8914 [Discriminator loss: 0.606717, acc.: 69.53%] [Generator loss: 0.950690]\n",
      "8915 [Discriminator loss: 0.629868, acc.: 68.75%] [Generator loss: 0.827871]\n",
      "8916 [Discriminator loss: 0.779042, acc.: 50.00%] [Generator loss: 0.811378]\n",
      "8917 [Discriminator loss: 0.694347, acc.: 62.50%] [Generator loss: 0.927201]\n",
      "8918 [Discriminator loss: 0.692222, acc.: 57.81%] [Generator loss: 0.915123]\n",
      "8919 [Discriminator loss: 0.553989, acc.: 76.56%] [Generator loss: 0.938744]\n",
      "8920 [Discriminator loss: 0.677230, acc.: 58.59%] [Generator loss: 0.937082]\n",
      "8921 [Discriminator loss: 0.750239, acc.: 51.56%] [Generator loss: 0.836196]\n",
      "8922 [Discriminator loss: 0.693941, acc.: 54.69%] [Generator loss: 0.832878]\n",
      "8923 [Discriminator loss: 0.679069, acc.: 60.16%] [Generator loss: 0.984143]\n",
      "8924 [Discriminator loss: 0.593732, acc.: 66.41%] [Generator loss: 1.082568]\n",
      "8925 [Discriminator loss: 0.843376, acc.: 45.31%] [Generator loss: 1.274431]\n",
      "8926 [Discriminator loss: 0.708583, acc.: 52.34%] [Generator loss: 1.044799]\n",
      "8927 [Discriminator loss: 0.733750, acc.: 53.91%] [Generator loss: 1.052942]\n",
      "8928 [Discriminator loss: 0.777506, acc.: 53.91%] [Generator loss: 0.978675]\n",
      "8929 [Discriminator loss: 0.731308, acc.: 50.00%] [Generator loss: 0.818964]\n",
      "8930 [Discriminator loss: 0.823532, acc.: 48.44%] [Generator loss: 0.781302]\n",
      "8931 [Discriminator loss: 0.697369, acc.: 53.91%] [Generator loss: 1.019318]\n",
      "8932 [Discriminator loss: 0.849068, acc.: 39.84%] [Generator loss: 0.865297]\n",
      "8933 [Discriminator loss: 0.653550, acc.: 63.28%] [Generator loss: 0.925410]\n",
      "8934 [Discriminator loss: 0.589490, acc.: 72.66%] [Generator loss: 1.044698]\n",
      "8935 [Discriminator loss: 0.634264, acc.: 60.94%] [Generator loss: 1.147607]\n",
      "8936 [Discriminator loss: 0.653739, acc.: 57.81%] [Generator loss: 1.143860]\n",
      "8937 [Discriminator loss: 0.577631, acc.: 66.41%] [Generator loss: 1.118697]\n",
      "8938 [Discriminator loss: 0.571454, acc.: 67.97%] [Generator loss: 1.246347]\n",
      "8939 [Discriminator loss: 0.623725, acc.: 69.53%] [Generator loss: 1.032897]\n",
      "8940 [Discriminator loss: 0.533898, acc.: 75.78%] [Generator loss: 1.119172]\n",
      "8941 [Discriminator loss: 0.638676, acc.: 71.09%] [Generator loss: 0.840914]\n",
      "8942 [Discriminator loss: 0.667547, acc.: 58.59%] [Generator loss: 0.743550]\n",
      "8943 [Discriminator loss: 0.680542, acc.: 60.16%] [Generator loss: 0.757777]\n",
      "8944 [Discriminator loss: 0.911413, acc.: 47.66%] [Generator loss: 0.869092]\n",
      "8945 [Discriminator loss: 0.572698, acc.: 74.22%] [Generator loss: 0.783906]\n",
      "8946 [Discriminator loss: 0.481696, acc.: 82.03%] [Generator loss: 0.951971]\n",
      "8947 [Discriminator loss: 0.856121, acc.: 41.41%] [Generator loss: 0.991890]\n",
      "8948 [Discriminator loss: 0.679636, acc.: 60.94%] [Generator loss: 0.930763]\n",
      "8949 [Discriminator loss: 0.721666, acc.: 54.69%] [Generator loss: 0.966705]\n",
      "8950 [Discriminator loss: 0.658718, acc.: 67.19%] [Generator loss: 0.902984]\n",
      "8951 [Discriminator loss: 0.686516, acc.: 54.69%] [Generator loss: 0.959471]\n",
      "8952 [Discriminator loss: 0.735557, acc.: 50.78%] [Generator loss: 0.856012]\n",
      "8953 [Discriminator loss: 0.745205, acc.: 50.00%] [Generator loss: 0.805040]\n",
      "8954 [Discriminator loss: 0.647104, acc.: 60.94%] [Generator loss: 0.969504]\n",
      "8955 [Discriminator loss: 0.582892, acc.: 71.88%] [Generator loss: 0.865625]\n",
      "8956 [Discriminator loss: 0.773493, acc.: 51.56%] [Generator loss: 1.071935]\n",
      "8957 [Discriminator loss: 0.571243, acc.: 71.88%] [Generator loss: 1.098034]\n",
      "8958 [Discriminator loss: 0.664725, acc.: 59.38%] [Generator loss: 0.990509]\n",
      "8959 [Discriminator loss: 0.680395, acc.: 58.59%] [Generator loss: 0.842037]\n",
      "8960 [Discriminator loss: 1.062742, acc.: 42.97%] [Generator loss: 1.287181]\n",
      "8961 [Discriminator loss: 0.662084, acc.: 57.03%] [Generator loss: 1.594802]\n",
      "8962 [Discriminator loss: 0.569602, acc.: 61.72%] [Generator loss: 1.458408]\n",
      "8963 [Discriminator loss: 0.578162, acc.: 66.41%] [Generator loss: 0.873892]\n",
      "8964 [Discriminator loss: 0.612176, acc.: 65.62%] [Generator loss: 0.795112]\n",
      "8965 [Discriminator loss: 0.667841, acc.: 66.41%] [Generator loss: 0.721818]\n",
      "8966 [Discriminator loss: 0.529013, acc.: 76.56%] [Generator loss: 0.802532]\n",
      "8967 [Discriminator loss: 0.623515, acc.: 67.19%] [Generator loss: 0.703075]\n",
      "8968 [Discriminator loss: 0.715591, acc.: 57.03%] [Generator loss: 0.735633]\n",
      "8969 [Discriminator loss: 0.632261, acc.: 67.19%] [Generator loss: 0.780870]\n",
      "8970 [Discriminator loss: 0.705368, acc.: 64.84%] [Generator loss: 0.694277]\n",
      "8971 [Discriminator loss: 0.685101, acc.: 58.59%] [Generator loss: 0.641701]\n",
      "8972 [Discriminator loss: 0.628332, acc.: 71.88%] [Generator loss: 0.930445]\n",
      "8973 [Discriminator loss: 0.589261, acc.: 72.66%] [Generator loss: 1.296741]\n",
      "8974 [Discriminator loss: 0.790972, acc.: 45.31%] [Generator loss: 0.870966]\n",
      "8975 [Discriminator loss: 0.597180, acc.: 66.41%] [Generator loss: 0.831532]\n",
      "8976 [Discriminator loss: 0.679163, acc.: 55.47%] [Generator loss: 0.767817]\n",
      "8977 [Discriminator loss: 0.603132, acc.: 68.75%] [Generator loss: 1.045612]\n",
      "8978 [Discriminator loss: 0.754853, acc.: 55.47%] [Generator loss: 0.877524]\n",
      "8979 [Discriminator loss: 0.648500, acc.: 57.03%] [Generator loss: 0.883731]\n",
      "8980 [Discriminator loss: 0.627431, acc.: 67.19%] [Generator loss: 0.805537]\n",
      "8981 [Discriminator loss: 0.673422, acc.: 62.50%] [Generator loss: 0.947984]\n",
      "8982 [Discriminator loss: 0.629566, acc.: 57.03%] [Generator loss: 0.769118]\n",
      "8983 [Discriminator loss: 0.637656, acc.: 62.50%] [Generator loss: 0.849871]\n",
      "8984 [Discriminator loss: 0.753383, acc.: 57.81%] [Generator loss: 0.902629]\n",
      "8985 [Discriminator loss: 0.572319, acc.: 69.53%] [Generator loss: 0.998252]\n",
      "8986 [Discriminator loss: 0.760983, acc.: 46.88%] [Generator loss: 0.821932]\n",
      "8987 [Discriminator loss: 0.758227, acc.: 50.78%] [Generator loss: 0.770486]\n",
      "8988 [Discriminator loss: 0.719338, acc.: 51.56%] [Generator loss: 0.819312]\n",
      "8989 [Discriminator loss: 0.669635, acc.: 59.38%] [Generator loss: 0.905967]\n",
      "8990 [Discriminator loss: 0.739617, acc.: 61.72%] [Generator loss: 0.761354]\n",
      "8991 [Discriminator loss: 0.622518, acc.: 54.69%] [Generator loss: 0.770319]\n",
      "8992 [Discriminator loss: 0.817006, acc.: 44.53%] [Generator loss: 0.824809]\n",
      "8993 [Discriminator loss: 0.606264, acc.: 70.31%] [Generator loss: 0.856760]\n",
      "8994 [Discriminator loss: 0.705311, acc.: 53.91%] [Generator loss: 0.828293]\n",
      "8995 [Discriminator loss: 0.591471, acc.: 67.97%] [Generator loss: 0.913657]\n",
      "8996 [Discriminator loss: 0.492624, acc.: 80.47%] [Generator loss: 0.998542]\n",
      "8997 [Discriminator loss: 0.489338, acc.: 80.47%] [Generator loss: 0.875139]\n",
      "8998 [Discriminator loss: 0.740862, acc.: 57.03%] [Generator loss: 0.897259]\n",
      "8999 [Discriminator loss: 0.628744, acc.: 68.75%] [Generator loss: 1.158447]\n",
      "9000 [Discriminator loss: 0.695372, acc.: 60.94%] [Generator loss: 1.063092]\n",
      "9001 [Discriminator loss: 0.740774, acc.: 53.91%] [Generator loss: 0.867197]\n",
      "9002 [Discriminator loss: 0.669265, acc.: 65.62%] [Generator loss: 0.795221]\n",
      "9003 [Discriminator loss: 0.744955, acc.: 50.00%] [Generator loss: 0.683914]\n",
      "9004 [Discriminator loss: 0.594693, acc.: 70.31%] [Generator loss: 0.713713]\n",
      "9005 [Discriminator loss: 0.746216, acc.: 53.91%] [Generator loss: 0.789200]\n",
      "9006 [Discriminator loss: 0.596414, acc.: 71.88%] [Generator loss: 1.027459]\n",
      "9007 [Discriminator loss: 0.742779, acc.: 53.91%] [Generator loss: 0.804993]\n",
      "9008 [Discriminator loss: 0.651406, acc.: 61.72%] [Generator loss: 0.832337]\n",
      "9009 [Discriminator loss: 0.586471, acc.: 71.88%] [Generator loss: 0.795377]\n",
      "9010 [Discriminator loss: 0.546698, acc.: 77.34%] [Generator loss: 0.780069]\n",
      "9011 [Discriminator loss: 0.801599, acc.: 39.84%] [Generator loss: 0.825635]\n",
      "9012 [Discriminator loss: 0.663729, acc.: 61.72%] [Generator loss: 0.919264]\n",
      "9013 [Discriminator loss: 0.717224, acc.: 56.25%] [Generator loss: 1.115612]\n",
      "9014 [Discriminator loss: 0.713929, acc.: 57.81%] [Generator loss: 1.051879]\n",
      "9015 [Discriminator loss: 0.613078, acc.: 67.97%] [Generator loss: 0.783805]\n",
      "9016 [Discriminator loss: 0.715174, acc.: 56.25%] [Generator loss: 0.876921]\n",
      "9017 [Discriminator loss: 0.597152, acc.: 75.00%] [Generator loss: 0.961075]\n",
      "9018 [Discriminator loss: 0.528567, acc.: 76.56%] [Generator loss: 0.923373]\n",
      "9019 [Discriminator loss: 0.712395, acc.: 64.06%] [Generator loss: 0.887810]\n",
      "9020 [Discriminator loss: 0.610362, acc.: 61.72%] [Generator loss: 0.891296]\n",
      "9021 [Discriminator loss: 0.616336, acc.: 65.62%] [Generator loss: 0.907075]\n",
      "9022 [Discriminator loss: 0.670719, acc.: 63.28%] [Generator loss: 0.965127]\n",
      "9023 [Discriminator loss: 0.813498, acc.: 38.28%] [Generator loss: 0.797458]\n",
      "9024 [Discriminator loss: 0.686932, acc.: 61.72%] [Generator loss: 0.892104]\n",
      "9025 [Discriminator loss: 0.757939, acc.: 52.34%] [Generator loss: 1.139785]\n",
      "9026 [Discriminator loss: 0.647811, acc.: 61.72%] [Generator loss: 1.116521]\n",
      "9027 [Discriminator loss: 0.657026, acc.: 59.38%] [Generator loss: 1.255052]\n",
      "9028 [Discriminator loss: 0.888611, acc.: 42.19%] [Generator loss: 0.859001]\n",
      "9029 [Discriminator loss: 0.724652, acc.: 54.69%] [Generator loss: 0.987790]\n",
      "9030 [Discriminator loss: 0.849854, acc.: 38.28%] [Generator loss: 0.955290]\n",
      "9031 [Discriminator loss: 0.718545, acc.: 57.03%] [Generator loss: 0.911570]\n",
      "9032 [Discriminator loss: 0.671158, acc.: 62.50%] [Generator loss: 0.959540]\n",
      "9033 [Discriminator loss: 0.683978, acc.: 59.38%] [Generator loss: 0.889573]\n",
      "9034 [Discriminator loss: 0.714605, acc.: 59.38%] [Generator loss: 0.887443]\n",
      "9035 [Discriminator loss: 0.719959, acc.: 56.25%] [Generator loss: 0.867641]\n",
      "9036 [Discriminator loss: 0.665623, acc.: 62.50%] [Generator loss: 0.841793]\n",
      "9037 [Discriminator loss: 0.635855, acc.: 64.84%] [Generator loss: 0.786363]\n",
      "9038 [Discriminator loss: 0.626982, acc.: 65.62%] [Generator loss: 0.777934]\n",
      "9039 [Discriminator loss: 0.774657, acc.: 49.22%] [Generator loss: 0.804783]\n",
      "9040 [Discriminator loss: 0.551773, acc.: 75.78%] [Generator loss: 0.923236]\n",
      "9041 [Discriminator loss: 0.535531, acc.: 77.34%] [Generator loss: 0.861069]\n",
      "9042 [Discriminator loss: 0.588097, acc.: 74.22%] [Generator loss: 0.910806]\n",
      "9043 [Discriminator loss: 0.674954, acc.: 63.28%] [Generator loss: 0.965031]\n",
      "9044 [Discriminator loss: 0.628823, acc.: 64.06%] [Generator loss: 1.070183]\n",
      "9045 [Discriminator loss: 0.655459, acc.: 60.94%] [Generator loss: 1.005083]\n",
      "9046 [Discriminator loss: 0.595521, acc.: 67.97%] [Generator loss: 0.869940]\n",
      "9047 [Discriminator loss: 0.602266, acc.: 64.84%] [Generator loss: 0.898170]\n",
      "9048 [Discriminator loss: 0.786384, acc.: 43.75%] [Generator loss: 0.835787]\n",
      "9049 [Discriminator loss: 0.741860, acc.: 52.34%] [Generator loss: 0.877850]\n",
      "9050 [Discriminator loss: 0.526090, acc.: 78.12%] [Generator loss: 0.799565]\n",
      "9051 [Discriminator loss: 0.526131, acc.: 75.00%] [Generator loss: 0.791524]\n",
      "9052 [Discriminator loss: 0.741504, acc.: 57.81%] [Generator loss: 0.826561]\n",
      "9053 [Discriminator loss: 0.610282, acc.: 74.22%] [Generator loss: 0.786520]\n",
      "9054 [Discriminator loss: 0.740042, acc.: 52.34%] [Generator loss: 0.820153]\n",
      "9055 [Discriminator loss: 0.568716, acc.: 73.44%] [Generator loss: 1.021875]\n",
      "9056 [Discriminator loss: 0.742018, acc.: 57.03%] [Generator loss: 0.915871]\n",
      "9057 [Discriminator loss: 0.718906, acc.: 50.78%] [Generator loss: 0.883094]\n",
      "9058 [Discriminator loss: 0.867599, acc.: 39.84%] [Generator loss: 1.004140]\n",
      "9059 [Discriminator loss: 0.649338, acc.: 62.50%] [Generator loss: 1.181929]\n",
      "9060 [Discriminator loss: 0.680851, acc.: 62.50%] [Generator loss: 0.900144]\n",
      "9061 [Discriminator loss: 0.668683, acc.: 63.28%] [Generator loss: 0.863365]\n",
      "9062 [Discriminator loss: 0.780960, acc.: 46.09%] [Generator loss: 0.873651]\n",
      "9063 [Discriminator loss: 0.636099, acc.: 74.22%] [Generator loss: 0.894194]\n",
      "9064 [Discriminator loss: 0.648455, acc.: 57.81%] [Generator loss: 0.826000]\n",
      "9065 [Discriminator loss: 0.637519, acc.: 61.72%] [Generator loss: 0.751707]\n",
      "9066 [Discriminator loss: 0.740476, acc.: 53.12%] [Generator loss: 0.873972]\n",
      "9067 [Discriminator loss: 0.717801, acc.: 49.22%] [Generator loss: 0.850708]\n",
      "9068 [Discriminator loss: 0.724884, acc.: 48.44%] [Generator loss: 0.855042]\n",
      "9069 [Discriminator loss: 0.634859, acc.: 63.28%] [Generator loss: 1.069402]\n",
      "9070 [Discriminator loss: 0.595565, acc.: 66.41%] [Generator loss: 1.040837]\n",
      "9071 [Discriminator loss: 0.698428, acc.: 61.72%] [Generator loss: 0.786798]\n",
      "9072 [Discriminator loss: 0.677604, acc.: 62.50%] [Generator loss: 0.766893]\n",
      "9073 [Discriminator loss: 0.700423, acc.: 56.25%] [Generator loss: 0.825043]\n",
      "9074 [Discriminator loss: 0.745594, acc.: 55.47%] [Generator loss: 0.817147]\n",
      "9075 [Discriminator loss: 0.611450, acc.: 63.28%] [Generator loss: 1.116152]\n",
      "9076 [Discriminator loss: 0.686410, acc.: 57.03%] [Generator loss: 0.984541]\n",
      "9077 [Discriminator loss: 0.618558, acc.: 67.19%] [Generator loss: 0.930546]\n",
      "9078 [Discriminator loss: 0.691562, acc.: 60.94%] [Generator loss: 0.788096]\n",
      "9079 [Discriminator loss: 0.789933, acc.: 53.12%] [Generator loss: 0.892487]\n",
      "9080 [Discriminator loss: 0.517194, acc.: 76.56%] [Generator loss: 0.981861]\n",
      "9081 [Discriminator loss: 0.634633, acc.: 63.28%] [Generator loss: 0.870980]\n",
      "9082 [Discriminator loss: 0.588355, acc.: 67.19%] [Generator loss: 0.944678]\n",
      "9083 [Discriminator loss: 0.674546, acc.: 59.38%] [Generator loss: 0.833595]\n",
      "9084 [Discriminator loss: 0.645287, acc.: 64.06%] [Generator loss: 0.856210]\n",
      "9085 [Discriminator loss: 0.648975, acc.: 62.50%] [Generator loss: 0.816838]\n",
      "9086 [Discriminator loss: 0.651723, acc.: 57.81%] [Generator loss: 0.775635]\n",
      "9087 [Discriminator loss: 0.628266, acc.: 69.53%] [Generator loss: 0.970120]\n",
      "9088 [Discriminator loss: 0.682271, acc.: 60.94%] [Generator loss: 1.007769]\n",
      "9089 [Discriminator loss: 0.646715, acc.: 56.25%] [Generator loss: 1.010366]\n",
      "9090 [Discriminator loss: 0.705302, acc.: 60.94%] [Generator loss: 0.986316]\n",
      "9091 [Discriminator loss: 0.737862, acc.: 52.34%] [Generator loss: 0.984030]\n",
      "9092 [Discriminator loss: 0.698143, acc.: 60.16%] [Generator loss: 0.783617]\n",
      "9093 [Discriminator loss: 0.735400, acc.: 57.81%] [Generator loss: 0.815604]\n",
      "9094 [Discriminator loss: 0.757182, acc.: 48.44%] [Generator loss: 0.847221]\n",
      "9095 [Discriminator loss: 0.697155, acc.: 57.03%] [Generator loss: 0.819479]\n",
      "9096 [Discriminator loss: 0.643871, acc.: 63.28%] [Generator loss: 0.837356]\n",
      "9097 [Discriminator loss: 0.677604, acc.: 64.06%] [Generator loss: 0.992723]\n",
      "9098 [Discriminator loss: 0.753354, acc.: 49.22%] [Generator loss: 0.940814]\n",
      "9099 [Discriminator loss: 0.700819, acc.: 62.50%] [Generator loss: 0.807386]\n",
      "9100 [Discriminator loss: 0.716877, acc.: 60.16%] [Generator loss: 0.925385]\n",
      "9101 [Discriminator loss: 0.643365, acc.: 67.97%] [Generator loss: 1.032076]\n",
      "9102 [Discriminator loss: 0.805336, acc.: 50.78%] [Generator loss: 0.889138]\n",
      "9103 [Discriminator loss: 0.545185, acc.: 75.78%] [Generator loss: 0.839843]\n",
      "9104 [Discriminator loss: 0.757868, acc.: 56.25%] [Generator loss: 0.772145]\n",
      "9105 [Discriminator loss: 0.698619, acc.: 59.38%] [Generator loss: 0.781761]\n",
      "9106 [Discriminator loss: 0.615715, acc.: 66.41%] [Generator loss: 0.865249]\n",
      "9107 [Discriminator loss: 0.719154, acc.: 57.81%] [Generator loss: 1.002199]\n",
      "9108 [Discriminator loss: 0.828003, acc.: 43.75%] [Generator loss: 0.873580]\n",
      "9109 [Discriminator loss: 0.643275, acc.: 62.50%] [Generator loss: 0.950516]\n",
      "9110 [Discriminator loss: 0.663861, acc.: 58.59%] [Generator loss: 1.082820]\n",
      "9111 [Discriminator loss: 0.713246, acc.: 52.34%] [Generator loss: 0.825863]\n",
      "9112 [Discriminator loss: 0.707286, acc.: 51.56%] [Generator loss: 0.956485]\n",
      "9113 [Discriminator loss: 0.722208, acc.: 50.78%] [Generator loss: 0.838840]\n",
      "9114 [Discriminator loss: 0.680638, acc.: 58.59%] [Generator loss: 0.810212]\n",
      "9115 [Discriminator loss: 0.628093, acc.: 65.62%] [Generator loss: 0.769704]\n",
      "9116 [Discriminator loss: 0.593615, acc.: 67.19%] [Generator loss: 0.890627]\n",
      "9117 [Discriminator loss: 0.653108, acc.: 65.62%] [Generator loss: 0.727741]\n",
      "9118 [Discriminator loss: 0.761350, acc.: 53.12%] [Generator loss: 0.746283]\n",
      "9119 [Discriminator loss: 0.680764, acc.: 60.94%] [Generator loss: 0.995662]\n",
      "9120 [Discriminator loss: 0.744745, acc.: 51.56%] [Generator loss: 1.030103]\n",
      "9121 [Discriminator loss: 0.598934, acc.: 64.84%] [Generator loss: 1.101273]\n",
      "9122 [Discriminator loss: 0.631726, acc.: 64.84%] [Generator loss: 0.900857]\n",
      "9123 [Discriminator loss: 0.573948, acc.: 67.97%] [Generator loss: 1.117133]\n",
      "9124 [Discriminator loss: 0.587310, acc.: 63.28%] [Generator loss: 0.699250]\n",
      "9125 [Discriminator loss: 0.649790, acc.: 56.25%] [Generator loss: 0.609284]\n",
      "9126 [Discriminator loss: 0.689273, acc.: 58.59%] [Generator loss: 0.711040]\n",
      "9127 [Discriminator loss: 0.730510, acc.: 52.34%] [Generator loss: 0.797365]\n",
      "9128 [Discriminator loss: 0.768538, acc.: 48.44%] [Generator loss: 0.618167]\n",
      "9129 [Discriminator loss: 0.788314, acc.: 51.56%] [Generator loss: 0.955071]\n",
      "9130 [Discriminator loss: 0.548923, acc.: 78.12%] [Generator loss: 1.028440]\n",
      "9131 [Discriminator loss: 0.543380, acc.: 72.66%] [Generator loss: 1.154119]\n",
      "9132 [Discriminator loss: 0.553303, acc.: 70.31%] [Generator loss: 1.100760]\n",
      "9133 [Discriminator loss: 0.612548, acc.: 67.97%] [Generator loss: 0.967466]\n",
      "9134 [Discriminator loss: 0.676494, acc.: 62.50%] [Generator loss: 0.907741]\n",
      "9135 [Discriminator loss: 0.707312, acc.: 57.03%] [Generator loss: 0.858196]\n",
      "9136 [Discriminator loss: 0.758430, acc.: 46.88%] [Generator loss: 0.761040]\n",
      "9137 [Discriminator loss: 0.619841, acc.: 67.97%] [Generator loss: 0.786476]\n",
      "9138 [Discriminator loss: 0.674508, acc.: 53.12%] [Generator loss: 0.791471]\n",
      "9139 [Discriminator loss: 0.660305, acc.: 58.59%] [Generator loss: 0.756210]\n",
      "9140 [Discriminator loss: 0.646840, acc.: 63.28%] [Generator loss: 0.849377]\n",
      "9141 [Discriminator loss: 0.599997, acc.: 65.62%] [Generator loss: 0.855253]\n",
      "9142 [Discriminator loss: 0.670262, acc.: 66.41%] [Generator loss: 0.784415]\n",
      "9143 [Discriminator loss: 0.698442, acc.: 59.38%] [Generator loss: 0.839732]\n",
      "9144 [Discriminator loss: 0.840545, acc.: 43.75%] [Generator loss: 0.821888]\n",
      "9145 [Discriminator loss: 0.681633, acc.: 57.03%] [Generator loss: 0.858978]\n",
      "9146 [Discriminator loss: 0.609932, acc.: 69.53%] [Generator loss: 0.919408]\n",
      "9147 [Discriminator loss: 0.569612, acc.: 73.44%] [Generator loss: 1.126344]\n",
      "9148 [Discriminator loss: 0.684528, acc.: 60.16%] [Generator loss: 1.079188]\n",
      "9149 [Discriminator loss: 0.761287, acc.: 53.12%] [Generator loss: 0.848185]\n",
      "9150 [Discriminator loss: 0.627857, acc.: 66.41%] [Generator loss: 0.868385]\n",
      "9151 [Discriminator loss: 0.755805, acc.: 46.09%] [Generator loss: 0.726208]\n",
      "9152 [Discriminator loss: 0.707390, acc.: 54.69%] [Generator loss: 0.652872]\n",
      "9153 [Discriminator loss: 0.609204, acc.: 66.41%] [Generator loss: 0.680750]\n",
      "9154 [Discriminator loss: 0.674650, acc.: 59.38%] [Generator loss: 0.737761]\n",
      "9155 [Discriminator loss: 0.648776, acc.: 60.16%] [Generator loss: 0.637333]\n",
      "9156 [Discriminator loss: 0.743449, acc.: 55.47%] [Generator loss: 0.827952]\n",
      "9157 [Discriminator loss: 0.705606, acc.: 53.91%] [Generator loss: 0.909931]\n",
      "9158 [Discriminator loss: 0.775874, acc.: 53.12%] [Generator loss: 0.937299]\n",
      "9159 [Discriminator loss: 0.632578, acc.: 64.06%] [Generator loss: 0.834030]\n",
      "9160 [Discriminator loss: 0.833117, acc.: 44.53%] [Generator loss: 0.780112]\n",
      "9161 [Discriminator loss: 0.779598, acc.: 47.66%] [Generator loss: 1.076328]\n",
      "9162 [Discriminator loss: 0.669438, acc.: 58.59%] [Generator loss: 1.053558]\n",
      "9163 [Discriminator loss: 0.646931, acc.: 67.19%] [Generator loss: 0.945036]\n",
      "9164 [Discriminator loss: 0.570086, acc.: 69.53%] [Generator loss: 0.877141]\n",
      "9165 [Discriminator loss: 0.684633, acc.: 60.16%] [Generator loss: 0.803289]\n",
      "9166 [Discriminator loss: 0.679417, acc.: 59.38%] [Generator loss: 0.824999]\n",
      "9167 [Discriminator loss: 0.593234, acc.: 74.22%] [Generator loss: 0.930075]\n",
      "9168 [Discriminator loss: 0.636371, acc.: 63.28%] [Generator loss: 0.836282]\n",
      "9169 [Discriminator loss: 0.620592, acc.: 67.19%] [Generator loss: 0.768461]\n",
      "9170 [Discriminator loss: 0.701043, acc.: 56.25%] [Generator loss: 0.885578]\n",
      "9171 [Discriminator loss: 0.661065, acc.: 61.72%] [Generator loss: 0.976219]\n",
      "9172 [Discriminator loss: 0.716700, acc.: 59.38%] [Generator loss: 0.889229]\n",
      "9173 [Discriminator loss: 0.654951, acc.: 57.03%] [Generator loss: 0.887087]\n",
      "9174 [Discriminator loss: 0.635237, acc.: 70.31%] [Generator loss: 0.806551]\n",
      "9175 [Discriminator loss: 0.635866, acc.: 60.16%] [Generator loss: 0.873473]\n",
      "9176 [Discriminator loss: 0.582560, acc.: 65.62%] [Generator loss: 1.104644]\n",
      "9177 [Discriminator loss: 0.566536, acc.: 70.31%] [Generator loss: 1.170077]\n",
      "9178 [Discriminator loss: 0.587280, acc.: 70.31%] [Generator loss: 0.925079]\n",
      "9179 [Discriminator loss: 0.533864, acc.: 75.78%] [Generator loss: 0.853761]\n",
      "9180 [Discriminator loss: 0.649394, acc.: 60.94%] [Generator loss: 0.680024]\n",
      "9181 [Discriminator loss: 0.520640, acc.: 80.47%] [Generator loss: 0.991182]\n",
      "9182 [Discriminator loss: 0.575517, acc.: 71.88%] [Generator loss: 0.747350]\n",
      "9183 [Discriminator loss: 0.655376, acc.: 66.41%] [Generator loss: 0.678317]\n",
      "9184 [Discriminator loss: 0.469063, acc.: 78.91%] [Generator loss: 1.009127]\n",
      "9185 [Discriminator loss: 0.606374, acc.: 70.31%] [Generator loss: 0.875161]\n",
      "9186 [Discriminator loss: 0.571987, acc.: 68.75%] [Generator loss: 0.822579]\n",
      "9187 [Discriminator loss: 0.510672, acc.: 75.78%] [Generator loss: 0.843601]\n",
      "9188 [Discriminator loss: 0.438635, acc.: 86.72%] [Generator loss: 0.859112]\n",
      "9189 [Discriminator loss: 0.651874, acc.: 62.50%] [Generator loss: 0.875711]\n",
      "9190 [Discriminator loss: 0.566801, acc.: 71.88%] [Generator loss: 1.044367]\n",
      "9191 [Discriminator loss: 0.619768, acc.: 61.72%] [Generator loss: 1.002954]\n",
      "9192 [Discriminator loss: 0.555216, acc.: 75.00%] [Generator loss: 1.137465]\n",
      "9193 [Discriminator loss: 0.763196, acc.: 51.56%] [Generator loss: 0.948933]\n",
      "9194 [Discriminator loss: 0.663536, acc.: 60.16%] [Generator loss: 0.665181]\n",
      "9195 [Discriminator loss: 0.499942, acc.: 73.44%] [Generator loss: 0.871072]\n",
      "9196 [Discriminator loss: 0.619531, acc.: 63.28%] [Generator loss: 0.751182]\n",
      "9197 [Discriminator loss: 0.672546, acc.: 65.62%] [Generator loss: 0.897235]\n",
      "9198 [Discriminator loss: 0.989598, acc.: 28.91%] [Generator loss: 0.661442]\n",
      "9199 [Discriminator loss: 0.495978, acc.: 79.69%] [Generator loss: 0.769680]\n",
      "9200 [Discriminator loss: 0.661356, acc.: 58.59%] [Generator loss: 0.905930]\n",
      "9201 [Discriminator loss: 0.601695, acc.: 71.88%] [Generator loss: 1.057309]\n",
      "9202 [Discriminator loss: 0.826539, acc.: 46.09%] [Generator loss: 0.894345]\n",
      "9203 [Discriminator loss: 0.638291, acc.: 64.06%] [Generator loss: 0.880157]\n",
      "9204 [Discriminator loss: 0.677056, acc.: 63.28%] [Generator loss: 1.092202]\n",
      "9205 [Discriminator loss: 0.813890, acc.: 46.88%] [Generator loss: 1.010438]\n",
      "9206 [Discriminator loss: 0.579845, acc.: 75.00%] [Generator loss: 1.062568]\n",
      "9207 [Discriminator loss: 0.660060, acc.: 59.38%] [Generator loss: 1.161695]\n",
      "9208 [Discriminator loss: 0.748929, acc.: 56.25%] [Generator loss: 0.906903]\n",
      "9209 [Discriminator loss: 0.643880, acc.: 63.28%] [Generator loss: 1.041793]\n",
      "9210 [Discriminator loss: 0.644259, acc.: 64.84%] [Generator loss: 1.028191]\n",
      "9211 [Discriminator loss: 0.786047, acc.: 50.78%] [Generator loss: 1.174321]\n",
      "9212 [Discriminator loss: 0.685865, acc.: 60.16%] [Generator loss: 0.879256]\n",
      "9213 [Discriminator loss: 0.817456, acc.: 50.00%] [Generator loss: 0.801200]\n",
      "9214 [Discriminator loss: 0.625670, acc.: 64.84%] [Generator loss: 0.772495]\n",
      "9215 [Discriminator loss: 0.835267, acc.: 39.06%] [Generator loss: 0.813446]\n",
      "9216 [Discriminator loss: 0.774103, acc.: 48.44%] [Generator loss: 0.953054]\n",
      "9217 [Discriminator loss: 0.607921, acc.: 71.88%] [Generator loss: 0.976321]\n",
      "9218 [Discriminator loss: 0.604019, acc.: 64.06%] [Generator loss: 1.000675]\n",
      "9219 [Discriminator loss: 0.496277, acc.: 83.59%] [Generator loss: 1.049601]\n",
      "9220 [Discriminator loss: 0.588786, acc.: 70.31%] [Generator loss: 1.080698]\n",
      "9221 [Discriminator loss: 0.608616, acc.: 71.09%] [Generator loss: 1.041121]\n",
      "9222 [Discriminator loss: 0.893328, acc.: 42.19%] [Generator loss: 0.789296]\n",
      "9223 [Discriminator loss: 0.923925, acc.: 42.97%] [Generator loss: 0.777498]\n",
      "9224 [Discriminator loss: 0.652597, acc.: 60.94%] [Generator loss: 0.932901]\n",
      "9225 [Discriminator loss: 0.583148, acc.: 67.19%] [Generator loss: 1.054697]\n",
      "9226 [Discriminator loss: 0.608003, acc.: 66.41%] [Generator loss: 1.156941]\n",
      "9227 [Discriminator loss: 0.715454, acc.: 56.25%] [Generator loss: 0.859329]\n",
      "9228 [Discriminator loss: 0.722869, acc.: 54.69%] [Generator loss: 0.889907]\n",
      "9229 [Discriminator loss: 0.680203, acc.: 56.25%] [Generator loss: 0.895313]\n",
      "9230 [Discriminator loss: 0.597784, acc.: 70.31%] [Generator loss: 0.668859]\n",
      "9231 [Discriminator loss: 0.612814, acc.: 61.72%] [Generator loss: 0.758870]\n",
      "9232 [Discriminator loss: 0.505187, acc.: 82.03%] [Generator loss: 0.809307]\n",
      "9233 [Discriminator loss: 0.702101, acc.: 57.03%] [Generator loss: 0.778062]\n",
      "9234 [Discriminator loss: 0.628741, acc.: 65.62%] [Generator loss: 0.924319]\n",
      "9235 [Discriminator loss: 0.563192, acc.: 75.78%] [Generator loss: 0.823736]\n",
      "9236 [Discriminator loss: 0.579159, acc.: 75.00%] [Generator loss: 0.972536]\n",
      "9237 [Discriminator loss: 0.634398, acc.: 64.06%] [Generator loss: 1.224760]\n",
      "9238 [Discriminator loss: 0.573496, acc.: 71.88%] [Generator loss: 1.026499]\n",
      "9239 [Discriminator loss: 0.798072, acc.: 51.56%] [Generator loss: 0.884947]\n",
      "9240 [Discriminator loss: 0.813827, acc.: 46.09%] [Generator loss: 0.806665]\n",
      "9241 [Discriminator loss: 0.607601, acc.: 75.00%] [Generator loss: 0.751864]\n",
      "9242 [Discriminator loss: 0.745738, acc.: 52.34%] [Generator loss: 0.670654]\n",
      "9243 [Discriminator loss: 0.713400, acc.: 56.25%] [Generator loss: 0.895385]\n",
      "9244 [Discriminator loss: 0.776327, acc.: 46.88%] [Generator loss: 0.803976]\n",
      "9245 [Discriminator loss: 0.808051, acc.: 50.78%] [Generator loss: 0.844168]\n",
      "9246 [Discriminator loss: 0.592436, acc.: 71.09%] [Generator loss: 1.278112]\n",
      "9247 [Discriminator loss: 0.695779, acc.: 60.94%] [Generator loss: 1.230707]\n",
      "9248 [Discriminator loss: 0.739844, acc.: 57.03%] [Generator loss: 1.260356]\n",
      "9249 [Discriminator loss: 0.642801, acc.: 64.06%] [Generator loss: 1.119212]\n",
      "9250 [Discriminator loss: 0.647359, acc.: 63.28%] [Generator loss: 1.184359]\n",
      "9251 [Discriminator loss: 0.577309, acc.: 71.88%] [Generator loss: 0.830486]\n",
      "9252 [Discriminator loss: 0.751504, acc.: 51.56%] [Generator loss: 0.920054]\n",
      "9253 [Discriminator loss: 0.718904, acc.: 60.16%] [Generator loss: 0.786238]\n",
      "9254 [Discriminator loss: 0.710609, acc.: 60.16%] [Generator loss: 0.707109]\n",
      "9255 [Discriminator loss: 0.714749, acc.: 54.69%] [Generator loss: 0.969503]\n",
      "9256 [Discriminator loss: 0.627057, acc.: 67.19%] [Generator loss: 1.133914]\n",
      "9257 [Discriminator loss: 0.665128, acc.: 64.84%] [Generator loss: 1.026814]\n",
      "9258 [Discriminator loss: 0.630308, acc.: 68.75%] [Generator loss: 0.965797]\n",
      "9259 [Discriminator loss: 0.558639, acc.: 73.44%] [Generator loss: 0.938185]\n",
      "9260 [Discriminator loss: 0.600562, acc.: 68.75%] [Generator loss: 0.786371]\n",
      "9261 [Discriminator loss: 0.816309, acc.: 46.88%] [Generator loss: 0.828891]\n",
      "9262 [Discriminator loss: 0.746801, acc.: 53.91%] [Generator loss: 0.922456]\n",
      "9263 [Discriminator loss: 0.609501, acc.: 67.97%] [Generator loss: 1.021455]\n",
      "9264 [Discriminator loss: 0.647174, acc.: 63.28%] [Generator loss: 0.909301]\n",
      "9265 [Discriminator loss: 0.679423, acc.: 58.59%] [Generator loss: 1.052782]\n",
      "9266 [Discriminator loss: 0.612953, acc.: 67.97%] [Generator loss: 1.167144]\n",
      "9267 [Discriminator loss: 0.560245, acc.: 70.31%] [Generator loss: 1.000341]\n",
      "9268 [Discriminator loss: 0.689363, acc.: 63.28%] [Generator loss: 0.824837]\n",
      "9269 [Discriminator loss: 0.657625, acc.: 59.38%] [Generator loss: 0.821677]\n",
      "9270 [Discriminator loss: 0.656022, acc.: 63.28%] [Generator loss: 0.882264]\n",
      "9271 [Discriminator loss: 0.568304, acc.: 70.31%] [Generator loss: 0.900801]\n",
      "9272 [Discriminator loss: 0.652945, acc.: 63.28%] [Generator loss: 0.858297]\n",
      "9273 [Discriminator loss: 0.524454, acc.: 74.22%] [Generator loss: 0.900945]\n",
      "9274 [Discriminator loss: 0.503224, acc.: 79.69%] [Generator loss: 0.888396]\n",
      "9275 [Discriminator loss: 0.536762, acc.: 71.09%] [Generator loss: 0.903035]\n",
      "9276 [Discriminator loss: 0.540513, acc.: 75.78%] [Generator loss: 0.572535]\n",
      "9277 [Discriminator loss: 0.593867, acc.: 69.53%] [Generator loss: 0.824191]\n",
      "9278 [Discriminator loss: 0.633784, acc.: 62.50%] [Generator loss: 0.849912]\n",
      "9279 [Discriminator loss: 0.932680, acc.: 37.50%] [Generator loss: 0.916422]\n",
      "9280 [Discriminator loss: 0.498962, acc.: 80.47%] [Generator loss: 0.975946]\n",
      "9281 [Discriminator loss: 0.540502, acc.: 69.53%] [Generator loss: 1.331331]\n",
      "9282 [Discriminator loss: 0.848610, acc.: 47.66%] [Generator loss: 0.878204]\n",
      "9283 [Discriminator loss: 0.731506, acc.: 51.56%] [Generator loss: 0.963238]\n",
      "9284 [Discriminator loss: 0.662792, acc.: 66.41%] [Generator loss: 1.064834]\n",
      "9285 [Discriminator loss: 0.942689, acc.: 38.28%] [Generator loss: 0.900549]\n",
      "9286 [Discriminator loss: 0.586103, acc.: 70.31%] [Generator loss: 0.807244]\n",
      "9287 [Discriminator loss: 0.697151, acc.: 57.03%] [Generator loss: 0.854579]\n",
      "9288 [Discriminator loss: 0.749769, acc.: 53.91%] [Generator loss: 0.952289]\n",
      "9289 [Discriminator loss: 0.597742, acc.: 75.78%] [Generator loss: 0.882861]\n",
      "9290 [Discriminator loss: 0.597385, acc.: 67.19%] [Generator loss: 0.962241]\n",
      "9291 [Discriminator loss: 0.478471, acc.: 82.81%] [Generator loss: 0.814410]\n",
      "9292 [Discriminator loss: 0.816296, acc.: 42.19%] [Generator loss: 0.767478]\n",
      "9293 [Discriminator loss: 0.477361, acc.: 84.38%] [Generator loss: 0.907290]\n",
      "9294 [Discriminator loss: 0.521728, acc.: 80.47%] [Generator loss: 0.829203]\n",
      "9295 [Discriminator loss: 0.609348, acc.: 73.44%] [Generator loss: 0.850993]\n",
      "9296 [Discriminator loss: 0.623892, acc.: 65.62%] [Generator loss: 0.815184]\n",
      "9297 [Discriminator loss: 0.629118, acc.: 67.97%] [Generator loss: 0.795988]\n",
      "9298 [Discriminator loss: 0.563298, acc.: 73.44%] [Generator loss: 0.860460]\n",
      "9299 [Discriminator loss: 0.596581, acc.: 67.19%] [Generator loss: 0.836894]\n",
      "9300 [Discriminator loss: 0.687220, acc.: 61.72%] [Generator loss: 0.650482]\n",
      "9301 [Discriminator loss: 0.685586, acc.: 53.91%] [Generator loss: 0.702950]\n",
      "9302 [Discriminator loss: 0.608591, acc.: 65.62%] [Generator loss: 0.795252]\n",
      "9303 [Discriminator loss: 0.800012, acc.: 53.91%] [Generator loss: 0.970196]\n",
      "9304 [Discriminator loss: 0.657027, acc.: 64.84%] [Generator loss: 1.033572]\n",
      "9305 [Discriminator loss: 0.671113, acc.: 64.06%] [Generator loss: 0.714884]\n",
      "9306 [Discriminator loss: 0.657631, acc.: 56.25%] [Generator loss: 0.800373]\n",
      "9307 [Discriminator loss: 0.624012, acc.: 71.88%] [Generator loss: 0.592765]\n",
      "9308 [Discriminator loss: 0.743123, acc.: 53.91%] [Generator loss: 0.716098]\n",
      "9309 [Discriminator loss: 0.470430, acc.: 85.94%] [Generator loss: 0.758649]\n",
      "9310 [Discriminator loss: 0.566238, acc.: 75.78%] [Generator loss: 0.705712]\n",
      "9311 [Discriminator loss: 0.538924, acc.: 76.56%] [Generator loss: 0.894614]\n",
      "9312 [Discriminator loss: 0.653307, acc.: 62.50%] [Generator loss: 0.914703]\n",
      "9313 [Discriminator loss: 0.632209, acc.: 65.62%] [Generator loss: 0.705653]\n",
      "9314 [Discriminator loss: 0.810051, acc.: 45.31%] [Generator loss: 0.908834]\n",
      "9315 [Discriminator loss: 0.690065, acc.: 64.06%] [Generator loss: 0.928457]\n",
      "9316 [Discriminator loss: 0.518228, acc.: 71.88%] [Generator loss: 0.925251]\n",
      "9317 [Discriminator loss: 0.598665, acc.: 69.53%] [Generator loss: 0.827186]\n",
      "9318 [Discriminator loss: 0.618616, acc.: 67.19%] [Generator loss: 0.976177]\n",
      "9319 [Discriminator loss: 0.906761, acc.: 39.06%] [Generator loss: 0.991328]\n",
      "9320 [Discriminator loss: 0.666894, acc.: 59.38%] [Generator loss: 1.188941]\n",
      "9321 [Discriminator loss: 0.736512, acc.: 53.91%] [Generator loss: 1.020800]\n",
      "9322 [Discriminator loss: 0.691982, acc.: 60.16%] [Generator loss: 0.936330]\n",
      "9323 [Discriminator loss: 0.569726, acc.: 76.56%] [Generator loss: 0.890388]\n",
      "9324 [Discriminator loss: 0.833624, acc.: 47.66%] [Generator loss: 0.876854]\n",
      "9325 [Discriminator loss: 0.569236, acc.: 72.66%] [Generator loss: 0.850484]\n",
      "9326 [Discriminator loss: 0.603137, acc.: 69.53%] [Generator loss: 0.733095]\n",
      "9327 [Discriminator loss: 0.689312, acc.: 59.38%] [Generator loss: 0.810043]\n",
      "9328 [Discriminator loss: 0.715196, acc.: 59.38%] [Generator loss: 0.781332]\n",
      "9329 [Discriminator loss: 0.439411, acc.: 85.16%] [Generator loss: 0.645455]\n",
      "9330 [Discriminator loss: 0.805922, acc.: 50.00%] [Generator loss: 0.858573]\n",
      "9331 [Discriminator loss: 0.623671, acc.: 67.19%] [Generator loss: 0.988938]\n",
      "9332 [Discriminator loss: 0.844925, acc.: 46.09%] [Generator loss: 0.903274]\n",
      "9333 [Discriminator loss: 0.556196, acc.: 78.12%] [Generator loss: 0.912154]\n",
      "9334 [Discriminator loss: 0.549874, acc.: 77.34%] [Generator loss: 0.854576]\n",
      "9335 [Discriminator loss: 0.677150, acc.: 55.47%] [Generator loss: 0.650098]\n",
      "9336 [Discriminator loss: 0.749334, acc.: 53.12%] [Generator loss: 0.929747]\n",
      "9337 [Discriminator loss: 0.643817, acc.: 57.81%] [Generator loss: 0.912658]\n",
      "9338 [Discriminator loss: 0.745642, acc.: 53.91%] [Generator loss: 0.911346]\n",
      "9339 [Discriminator loss: 0.707968, acc.: 56.25%] [Generator loss: 1.016238]\n",
      "9340 [Discriminator loss: 0.833802, acc.: 53.91%] [Generator loss: 1.073543]\n",
      "9341 [Discriminator loss: 0.701116, acc.: 55.47%] [Generator loss: 0.853231]\n",
      "9342 [Discriminator loss: 0.791045, acc.: 49.22%] [Generator loss: 0.977592]\n",
      "9343 [Discriminator loss: 0.627863, acc.: 67.19%] [Generator loss: 1.576968]\n",
      "9344 [Discriminator loss: 0.626627, acc.: 67.19%] [Generator loss: 1.155105]\n",
      "9345 [Discriminator loss: 0.790972, acc.: 43.75%] [Generator loss: 1.194104]\n",
      "9346 [Discriminator loss: 0.693893, acc.: 60.94%] [Generator loss: 0.978894]\n",
      "9347 [Discriminator loss: 0.782964, acc.: 46.88%] [Generator loss: 1.029181]\n",
      "9348 [Discriminator loss: 0.784128, acc.: 53.91%] [Generator loss: 0.758529]\n",
      "9349 [Discriminator loss: 0.753369, acc.: 53.12%] [Generator loss: 0.960834]\n",
      "9350 [Discriminator loss: 0.743115, acc.: 50.78%] [Generator loss: 0.933887]\n",
      "9351 [Discriminator loss: 0.737387, acc.: 56.25%] [Generator loss: 1.082718]\n",
      "9352 [Discriminator loss: 0.688681, acc.: 56.25%] [Generator loss: 1.025860]\n",
      "9353 [Discriminator loss: 0.685212, acc.: 60.16%] [Generator loss: 0.714660]\n",
      "9354 [Discriminator loss: 0.588054, acc.: 67.97%] [Generator loss: 0.788907]\n",
      "9355 [Discriminator loss: 0.720025, acc.: 58.59%] [Generator loss: 0.940040]\n",
      "9356 [Discriminator loss: 0.608013, acc.: 67.97%] [Generator loss: 1.084211]\n",
      "9357 [Discriminator loss: 0.560729, acc.: 75.78%] [Generator loss: 1.157506]\n",
      "9358 [Discriminator loss: 0.758761, acc.: 50.78%] [Generator loss: 0.975616]\n",
      "9359 [Discriminator loss: 0.722184, acc.: 57.81%] [Generator loss: 0.993354]\n",
      "9360 [Discriminator loss: 0.572728, acc.: 74.22%] [Generator loss: 0.888564]\n",
      "9361 [Discriminator loss: 0.733968, acc.: 45.31%] [Generator loss: 0.861114]\n",
      "9362 [Discriminator loss: 0.730914, acc.: 58.59%] [Generator loss: 0.855690]\n",
      "9363 [Discriminator loss: 0.556334, acc.: 74.22%] [Generator loss: 1.116170]\n",
      "9364 [Discriminator loss: 0.631940, acc.: 64.06%] [Generator loss: 1.086505]\n",
      "9365 [Discriminator loss: 0.510477, acc.: 76.56%] [Generator loss: 1.091564]\n",
      "9366 [Discriminator loss: 0.628261, acc.: 68.75%] [Generator loss: 0.801232]\n",
      "9367 [Discriminator loss: 0.618385, acc.: 69.53%] [Generator loss: 0.876000]\n",
      "9368 [Discriminator loss: 0.624780, acc.: 64.84%] [Generator loss: 0.910924]\n",
      "9369 [Discriminator loss: 0.520582, acc.: 77.34%] [Generator loss: 0.759540]\n",
      "9370 [Discriminator loss: 0.652095, acc.: 61.72%] [Generator loss: 0.860076]\n",
      "9371 [Discriminator loss: 0.618321, acc.: 72.66%] [Generator loss: 0.826643]\n",
      "9372 [Discriminator loss: 0.610830, acc.: 70.31%] [Generator loss: 0.843055]\n",
      "9373 [Discriminator loss: 0.893304, acc.: 46.09%] [Generator loss: 0.997058]\n",
      "9374 [Discriminator loss: 0.717724, acc.: 56.25%] [Generator loss: 1.018983]\n",
      "9375 [Discriminator loss: 0.757565, acc.: 51.56%] [Generator loss: 0.900449]\n",
      "9376 [Discriminator loss: 0.648018, acc.: 60.94%] [Generator loss: 0.935107]\n",
      "9377 [Discriminator loss: 0.791636, acc.: 44.53%] [Generator loss: 0.898399]\n",
      "9378 [Discriminator loss: 0.656639, acc.: 64.84%] [Generator loss: 0.842189]\n",
      "9379 [Discriminator loss: 0.717155, acc.: 50.78%] [Generator loss: 0.909000]\n",
      "9380 [Discriminator loss: 0.448301, acc.: 85.94%] [Generator loss: 0.828036]\n",
      "9381 [Discriminator loss: 0.668438, acc.: 75.78%] [Generator loss: 0.910178]\n",
      "9382 [Discriminator loss: 0.615333, acc.: 63.28%] [Generator loss: 0.720855]\n",
      "9383 [Discriminator loss: 0.582042, acc.: 67.97%] [Generator loss: 0.671268]\n",
      "9384 [Discriminator loss: 0.554720, acc.: 68.75%] [Generator loss: 0.761789]\n",
      "9385 [Discriminator loss: 0.602130, acc.: 70.31%] [Generator loss: 0.633440]\n",
      "9386 [Discriminator loss: 0.672916, acc.: 61.72%] [Generator loss: 0.855162]\n",
      "9387 [Discriminator loss: 0.772252, acc.: 44.53%] [Generator loss: 1.150799]\n",
      "9388 [Discriminator loss: 0.707435, acc.: 61.72%] [Generator loss: 0.995706]\n",
      "9389 [Discriminator loss: 0.593396, acc.: 71.09%] [Generator loss: 0.836789]\n",
      "9390 [Discriminator loss: 0.698861, acc.: 55.47%] [Generator loss: 0.909656]\n",
      "9391 [Discriminator loss: 0.785164, acc.: 42.97%] [Generator loss: 0.783162]\n",
      "9392 [Discriminator loss: 0.692654, acc.: 53.91%] [Generator loss: 0.789329]\n",
      "9393 [Discriminator loss: 0.812728, acc.: 45.31%] [Generator loss: 0.900907]\n",
      "9394 [Discriminator loss: 0.649934, acc.: 63.28%] [Generator loss: 0.812522]\n",
      "9395 [Discriminator loss: 0.680959, acc.: 52.34%] [Generator loss: 0.973596]\n",
      "9396 [Discriminator loss: 0.671110, acc.: 60.94%] [Generator loss: 1.041919]\n",
      "9397 [Discriminator loss: 0.640520, acc.: 66.41%] [Generator loss: 0.932576]\n",
      "9398 [Discriminator loss: 0.686456, acc.: 53.91%] [Generator loss: 0.920545]\n",
      "9399 [Discriminator loss: 0.611376, acc.: 68.75%] [Generator loss: 0.915343]\n",
      "9400 [Discriminator loss: 0.656723, acc.: 55.47%] [Generator loss: 0.859420]\n",
      "9401 [Discriminator loss: 0.574618, acc.: 70.31%] [Generator loss: 0.833462]\n",
      "9402 [Discriminator loss: 0.685060, acc.: 62.50%] [Generator loss: 0.789794]\n",
      "9403 [Discriminator loss: 0.570481, acc.: 67.97%] [Generator loss: 0.776254]\n",
      "9404 [Discriminator loss: 0.715752, acc.: 56.25%] [Generator loss: 0.880885]\n",
      "9405 [Discriminator loss: 0.750386, acc.: 52.34%] [Generator loss: 0.888413]\n",
      "9406 [Discriminator loss: 0.556252, acc.: 78.91%] [Generator loss: 0.884669]\n",
      "9407 [Discriminator loss: 0.765444, acc.: 50.00%] [Generator loss: 0.961507]\n",
      "9408 [Discriminator loss: 0.680555, acc.: 60.94%] [Generator loss: 0.912302]\n",
      "9409 [Discriminator loss: 0.816885, acc.: 38.28%] [Generator loss: 0.695366]\n",
      "9410 [Discriminator loss: 0.857313, acc.: 39.84%] [Generator loss: 0.844034]\n",
      "9411 [Discriminator loss: 0.711768, acc.: 56.25%] [Generator loss: 1.050935]\n",
      "9412 [Discriminator loss: 0.649073, acc.: 59.38%] [Generator loss: 1.171867]\n",
      "9413 [Discriminator loss: 0.780069, acc.: 53.91%] [Generator loss: 1.048188]\n",
      "9414 [Discriminator loss: 0.726695, acc.: 50.00%] [Generator loss: 1.257246]\n",
      "9415 [Discriminator loss: 0.731576, acc.: 51.56%] [Generator loss: 0.933548]\n",
      "9416 [Discriminator loss: 0.713641, acc.: 57.81%] [Generator loss: 0.732407]\n",
      "9417 [Discriminator loss: 0.775661, acc.: 39.84%] [Generator loss: 0.938009]\n",
      "9418 [Discriminator loss: 0.743088, acc.: 47.66%] [Generator loss: 1.004065]\n",
      "9419 [Discriminator loss: 0.654976, acc.: 56.25%] [Generator loss: 0.960894]\n",
      "9420 [Discriminator loss: 0.664976, acc.: 60.16%] [Generator loss: 1.122556]\n",
      "9421 [Discriminator loss: 0.670734, acc.: 62.50%] [Generator loss: 0.970361]\n",
      "9422 [Discriminator loss: 0.554765, acc.: 78.12%] [Generator loss: 1.015520]\n",
      "9423 [Discriminator loss: 0.556470, acc.: 76.56%] [Generator loss: 0.824551]\n",
      "9424 [Discriminator loss: 0.745058, acc.: 57.03%] [Generator loss: 0.797723]\n",
      "9425 [Discriminator loss: 0.596636, acc.: 71.09%] [Generator loss: 0.725634]\n",
      "9426 [Discriminator loss: 0.647448, acc.: 62.50%] [Generator loss: 0.859113]\n",
      "9427 [Discriminator loss: 0.541407, acc.: 74.22%] [Generator loss: 0.926513]\n",
      "9428 [Discriminator loss: 0.553986, acc.: 78.91%] [Generator loss: 0.953062]\n",
      "9429 [Discriminator loss: 0.662195, acc.: 60.94%] [Generator loss: 0.810671]\n",
      "9430 [Discriminator loss: 0.630654, acc.: 69.53%] [Generator loss: 0.845744]\n",
      "9431 [Discriminator loss: 0.623658, acc.: 65.62%] [Generator loss: 0.801571]\n",
      "9432 [Discriminator loss: 0.593549, acc.: 72.66%] [Generator loss: 0.877402]\n",
      "9433 [Discriminator loss: 0.587347, acc.: 68.75%] [Generator loss: 0.838203]\n",
      "9434 [Discriminator loss: 0.691142, acc.: 62.50%] [Generator loss: 0.686270]\n",
      "9435 [Discriminator loss: 0.455863, acc.: 79.69%] [Generator loss: 0.748513]\n",
      "9436 [Discriminator loss: 0.668183, acc.: 56.25%] [Generator loss: 0.829069]\n",
      "9437 [Discriminator loss: 0.752553, acc.: 51.56%] [Generator loss: 0.836126]\n",
      "9438 [Discriminator loss: 0.614799, acc.: 67.19%] [Generator loss: 0.961549]\n",
      "9439 [Discriminator loss: 0.671045, acc.: 57.03%] [Generator loss: 0.789078]\n",
      "9440 [Discriminator loss: 0.593013, acc.: 73.44%] [Generator loss: 1.067418]\n",
      "9441 [Discriminator loss: 0.724612, acc.: 55.47%] [Generator loss: 0.902453]\n",
      "9442 [Discriminator loss: 0.721991, acc.: 59.38%] [Generator loss: 0.942375]\n",
      "9443 [Discriminator loss: 0.643983, acc.: 63.28%] [Generator loss: 0.978439]\n",
      "9444 [Discriminator loss: 0.829466, acc.: 40.62%] [Generator loss: 0.922847]\n",
      "9445 [Discriminator loss: 0.756644, acc.: 50.78%] [Generator loss: 1.023541]\n",
      "9446 [Discriminator loss: 0.685661, acc.: 60.94%] [Generator loss: 1.017227]\n",
      "9447 [Discriminator loss: 0.552835, acc.: 77.34%] [Generator loss: 0.860860]\n",
      "9448 [Discriminator loss: 0.619324, acc.: 71.09%] [Generator loss: 0.606580]\n",
      "9449 [Discriminator loss: 0.963969, acc.: 46.09%] [Generator loss: 1.153383]\n",
      "9450 [Discriminator loss: 0.634362, acc.: 59.38%] [Generator loss: 1.102434]\n",
      "9451 [Discriminator loss: 0.692969, acc.: 60.94%] [Generator loss: 0.816707]\n",
      "9452 [Discriminator loss: 0.733880, acc.: 49.22%] [Generator loss: 0.807444]\n",
      "9453 [Discriminator loss: 0.830092, acc.: 44.53%] [Generator loss: 0.603943]\n",
      "9454 [Discriminator loss: 0.628741, acc.: 64.06%] [Generator loss: 0.682757]\n",
      "9455 [Discriminator loss: 0.662288, acc.: 63.28%] [Generator loss: 0.674661]\n",
      "9456 [Discriminator loss: 0.794484, acc.: 53.91%] [Generator loss: 0.903113]\n",
      "9457 [Discriminator loss: 0.553920, acc.: 71.09%] [Generator loss: 0.965601]\n",
      "9458 [Discriminator loss: 0.873507, acc.: 45.31%] [Generator loss: 0.789645]\n",
      "9459 [Discriminator loss: 0.606820, acc.: 68.75%] [Generator loss: 0.830427]\n",
      "9460 [Discriminator loss: 0.526760, acc.: 80.47%] [Generator loss: 0.894818]\n",
      "9461 [Discriminator loss: 0.647384, acc.: 62.50%] [Generator loss: 0.807958]\n",
      "9462 [Discriminator loss: 0.589974, acc.: 66.41%] [Generator loss: 0.753251]\n",
      "9463 [Discriminator loss: 0.671858, acc.: 59.38%] [Generator loss: 0.937741]\n",
      "9464 [Discriminator loss: 0.682737, acc.: 57.81%] [Generator loss: 0.891298]\n",
      "9465 [Discriminator loss: 0.687836, acc.: 64.06%] [Generator loss: 0.978481]\n",
      "9466 [Discriminator loss: 0.631381, acc.: 61.72%] [Generator loss: 0.998399]\n",
      "9467 [Discriminator loss: 0.601434, acc.: 71.88%] [Generator loss: 0.918320]\n",
      "9468 [Discriminator loss: 0.755515, acc.: 52.34%] [Generator loss: 0.741675]\n",
      "9469 [Discriminator loss: 0.828259, acc.: 42.19%] [Generator loss: 1.209475]\n",
      "9470 [Discriminator loss: 0.708849, acc.: 57.03%] [Generator loss: 1.044810]\n",
      "9471 [Discriminator loss: 0.812099, acc.: 45.31%] [Generator loss: 0.848555]\n",
      "9472 [Discriminator loss: 0.726218, acc.: 54.69%] [Generator loss: 0.919184]\n",
      "9473 [Discriminator loss: 0.633414, acc.: 62.50%] [Generator loss: 0.918757]\n",
      "9474 [Discriminator loss: 0.658568, acc.: 61.72%] [Generator loss: 0.965704]\n",
      "9475 [Discriminator loss: 0.737969, acc.: 50.78%] [Generator loss: 0.985063]\n",
      "9476 [Discriminator loss: 0.635478, acc.: 67.97%] [Generator loss: 1.219201]\n",
      "9477 [Discriminator loss: 0.588478, acc.: 69.53%] [Generator loss: 1.011488]\n",
      "9478 [Discriminator loss: 0.543812, acc.: 71.09%] [Generator loss: 1.210950]\n",
      "9479 [Discriminator loss: 0.609680, acc.: 70.31%] [Generator loss: 1.237697]\n",
      "9480 [Discriminator loss: 0.910070, acc.: 34.38%] [Generator loss: 0.916833]\n",
      "9481 [Discriminator loss: 0.674668, acc.: 57.81%] [Generator loss: 0.952047]\n",
      "9482 [Discriminator loss: 0.664391, acc.: 67.97%] [Generator loss: 0.955793]\n",
      "9483 [Discriminator loss: 0.629445, acc.: 67.19%] [Generator loss: 0.871100]\n",
      "9484 [Discriminator loss: 0.552242, acc.: 78.91%] [Generator loss: 0.823731]\n",
      "9485 [Discriminator loss: 0.719672, acc.: 53.12%] [Generator loss: 0.953946]\n",
      "9486 [Discriminator loss: 0.561896, acc.: 74.22%] [Generator loss: 0.897523]\n",
      "9487 [Discriminator loss: 0.668409, acc.: 70.31%] [Generator loss: 0.722159]\n",
      "9488 [Discriminator loss: 0.716415, acc.: 53.12%] [Generator loss: 0.702765]\n",
      "9489 [Discriminator loss: 0.590508, acc.: 71.09%] [Generator loss: 0.739975]\n",
      "9490 [Discriminator loss: 0.720990, acc.: 57.81%] [Generator loss: 0.895885]\n",
      "9491 [Discriminator loss: 0.760739, acc.: 50.78%] [Generator loss: 0.798832]\n",
      "9492 [Discriminator loss: 0.675239, acc.: 62.50%] [Generator loss: 0.999007]\n",
      "9493 [Discriminator loss: 0.762567, acc.: 51.56%] [Generator loss: 0.956064]\n",
      "9494 [Discriminator loss: 0.571666, acc.: 68.75%] [Generator loss: 0.980323]\n",
      "9495 [Discriminator loss: 0.672799, acc.: 62.50%] [Generator loss: 0.990341]\n",
      "9496 [Discriminator loss: 0.729253, acc.: 60.94%] [Generator loss: 0.923393]\n",
      "9497 [Discriminator loss: 0.582043, acc.: 70.31%] [Generator loss: 0.840714]\n",
      "9498 [Discriminator loss: 0.728676, acc.: 52.34%] [Generator loss: 0.780620]\n",
      "9499 [Discriminator loss: 0.722500, acc.: 59.38%] [Generator loss: 0.771594]\n",
      "9500 [Discriminator loss: 0.539532, acc.: 77.34%] [Generator loss: 0.843958]\n",
      "9501 [Discriminator loss: 0.628537, acc.: 61.72%] [Generator loss: 0.895234]\n",
      "9502 [Discriminator loss: 0.585927, acc.: 68.75%] [Generator loss: 1.000734]\n",
      "9503 [Discriminator loss: 0.680356, acc.: 60.16%] [Generator loss: 0.859972]\n",
      "9504 [Discriminator loss: 0.546623, acc.: 78.12%] [Generator loss: 0.764252]\n",
      "9505 [Discriminator loss: 0.529677, acc.: 77.34%] [Generator loss: 0.778614]\n",
      "9506 [Discriminator loss: 0.845371, acc.: 42.19%] [Generator loss: 0.764124]\n",
      "9507 [Discriminator loss: 0.615218, acc.: 66.41%] [Generator loss: 0.805556]\n",
      "9508 [Discriminator loss: 0.552037, acc.: 70.31%] [Generator loss: 0.878954]\n",
      "9509 [Discriminator loss: 0.705062, acc.: 53.12%] [Generator loss: 0.666456]\n",
      "9510 [Discriminator loss: 0.675626, acc.: 57.81%] [Generator loss: 0.813007]\n",
      "9511 [Discriminator loss: 0.484768, acc.: 78.91%] [Generator loss: 0.965550]\n",
      "9512 [Discriminator loss: 0.569632, acc.: 68.75%] [Generator loss: 0.813145]\n",
      "9513 [Discriminator loss: 0.706412, acc.: 57.81%] [Generator loss: 0.936800]\n",
      "9514 [Discriminator loss: 0.613490, acc.: 63.28%] [Generator loss: 0.917046]\n",
      "9515 [Discriminator loss: 0.569244, acc.: 67.19%] [Generator loss: 0.794714]\n",
      "9516 [Discriminator loss: 0.507769, acc.: 80.47%] [Generator loss: 0.741091]\n",
      "9517 [Discriminator loss: 0.615117, acc.: 63.28%] [Generator loss: 0.774788]\n",
      "9518 [Discriminator loss: 0.637406, acc.: 62.50%] [Generator loss: 0.892666]\n",
      "9519 [Discriminator loss: 0.776028, acc.: 49.22%] [Generator loss: 0.773937]\n",
      "9520 [Discriminator loss: 0.482368, acc.: 78.91%] [Generator loss: 0.650857]\n",
      "9521 [Discriminator loss: 0.750323, acc.: 53.91%] [Generator loss: 0.918979]\n",
      "9522 [Discriminator loss: 0.599103, acc.: 62.50%] [Generator loss: 1.057410]\n",
      "9523 [Discriminator loss: 0.612583, acc.: 70.31%] [Generator loss: 0.918651]\n",
      "9524 [Discriminator loss: 0.597565, acc.: 71.09%] [Generator loss: 0.870456]\n",
      "9525 [Discriminator loss: 0.620979, acc.: 70.31%] [Generator loss: 0.723061]\n",
      "9526 [Discriminator loss: 0.534148, acc.: 79.69%] [Generator loss: 0.641584]\n",
      "9527 [Discriminator loss: 0.676387, acc.: 62.50%] [Generator loss: 0.794288]\n",
      "9528 [Discriminator loss: 0.995863, acc.: 39.06%] [Generator loss: 0.896922]\n",
      "9529 [Discriminator loss: 0.645903, acc.: 65.62%] [Generator loss: 1.228639]\n",
      "9530 [Discriminator loss: 0.628659, acc.: 70.31%] [Generator loss: 1.206669]\n",
      "9531 [Discriminator loss: 0.642110, acc.: 64.06%] [Generator loss: 1.104902]\n",
      "9532 [Discriminator loss: 0.840824, acc.: 45.31%] [Generator loss: 0.814041]\n",
      "9533 [Discriminator loss: 0.712337, acc.: 53.12%] [Generator loss: 0.853179]\n",
      "9534 [Discriminator loss: 0.604038, acc.: 66.41%] [Generator loss: 0.853828]\n",
      "9535 [Discriminator loss: 0.477585, acc.: 82.03%] [Generator loss: 0.809424]\n",
      "9536 [Discriminator loss: 0.635031, acc.: 65.62%] [Generator loss: 0.806604]\n",
      "9537 [Discriminator loss: 0.614274, acc.: 60.94%] [Generator loss: 0.819978]\n",
      "9538 [Discriminator loss: 0.674597, acc.: 59.38%] [Generator loss: 0.760391]\n",
      "9539 [Discriminator loss: 0.730799, acc.: 58.59%] [Generator loss: 0.867692]\n",
      "9540 [Discriminator loss: 0.661580, acc.: 55.47%] [Generator loss: 0.844334]\n",
      "9541 [Discriminator loss: 0.814026, acc.: 42.97%] [Generator loss: 0.859029]\n",
      "9542 [Discriminator loss: 0.719273, acc.: 54.69%] [Generator loss: 0.872764]\n",
      "9543 [Discriminator loss: 0.737812, acc.: 57.81%] [Generator loss: 0.808850]\n",
      "9544 [Discriminator loss: 0.684643, acc.: 54.69%] [Generator loss: 1.154220]\n",
      "9545 [Discriminator loss: 0.600841, acc.: 74.22%] [Generator loss: 1.029082]\n",
      "9546 [Discriminator loss: 0.611710, acc.: 62.50%] [Generator loss: 1.079041]\n",
      "9547 [Discriminator loss: 0.610344, acc.: 64.06%] [Generator loss: 1.049000]\n",
      "9548 [Discriminator loss: 0.521928, acc.: 76.56%] [Generator loss: 0.961004]\n",
      "9549 [Discriminator loss: 0.622502, acc.: 69.53%] [Generator loss: 0.805579]\n",
      "9550 [Discriminator loss: 0.666673, acc.: 61.72%] [Generator loss: 0.989517]\n",
      "9551 [Discriminator loss: 0.515202, acc.: 77.34%] [Generator loss: 0.799266]\n",
      "9552 [Discriminator loss: 0.512883, acc.: 75.78%] [Generator loss: 0.861300]\n",
      "9553 [Discriminator loss: 0.642874, acc.: 57.81%] [Generator loss: 0.764884]\n",
      "9554 [Discriminator loss: 0.669291, acc.: 60.16%] [Generator loss: 0.725935]\n",
      "9555 [Discriminator loss: 0.687360, acc.: 56.25%] [Generator loss: 0.844551]\n",
      "9556 [Discriminator loss: 0.693111, acc.: 58.59%] [Generator loss: 0.831471]\n",
      "9557 [Discriminator loss: 0.859338, acc.: 48.44%] [Generator loss: 0.603642]\n",
      "9558 [Discriminator loss: 1.253773, acc.: 14.84%] [Generator loss: 0.539162]\n",
      "9559 [Discriminator loss: 0.610876, acc.: 60.94%] [Generator loss: 0.734860]\n",
      "9560 [Discriminator loss: 0.614683, acc.: 65.62%] [Generator loss: 0.873506]\n",
      "9561 [Discriminator loss: 0.604320, acc.: 71.09%] [Generator loss: 0.817014]\n",
      "9562 [Discriminator loss: 0.646631, acc.: 64.06%] [Generator loss: 1.042789]\n",
      "9563 [Discriminator loss: 0.690253, acc.: 54.69%] [Generator loss: 0.918944]\n",
      "9564 [Discriminator loss: 0.618109, acc.: 64.06%] [Generator loss: 1.132412]\n",
      "9565 [Discriminator loss: 0.554854, acc.: 72.66%] [Generator loss: 1.173958]\n",
      "9566 [Discriminator loss: 0.591197, acc.: 67.97%] [Generator loss: 1.023519]\n",
      "9567 [Discriminator loss: 0.667033, acc.: 61.72%] [Generator loss: 1.029281]\n",
      "9568 [Discriminator loss: 0.584960, acc.: 68.75%] [Generator loss: 1.140334]\n",
      "9569 [Discriminator loss: 0.790829, acc.: 49.22%] [Generator loss: 0.953646]\n",
      "9570 [Discriminator loss: 0.554608, acc.: 72.66%] [Generator loss: 0.733703]\n",
      "9571 [Discriminator loss: 0.534700, acc.: 71.88%] [Generator loss: 0.774293]\n",
      "9572 [Discriminator loss: 0.599780, acc.: 64.84%] [Generator loss: 0.772396]\n",
      "9573 [Discriminator loss: 0.599976, acc.: 67.19%] [Generator loss: 0.848538]\n",
      "9574 [Discriminator loss: 0.699032, acc.: 57.81%] [Generator loss: 0.813819]\n",
      "9575 [Discriminator loss: 0.640639, acc.: 59.38%] [Generator loss: 0.911256]\n",
      "9576 [Discriminator loss: 0.662886, acc.: 60.94%] [Generator loss: 0.756742]\n",
      "9577 [Discriminator loss: 0.776924, acc.: 48.44%] [Generator loss: 0.821624]\n",
      "9578 [Discriminator loss: 0.592923, acc.: 68.75%] [Generator loss: 0.830466]\n",
      "9579 [Discriminator loss: 0.705274, acc.: 53.91%] [Generator loss: 0.889204]\n",
      "9580 [Discriminator loss: 0.577088, acc.: 66.41%] [Generator loss: 0.791007]\n",
      "9581 [Discriminator loss: 0.657237, acc.: 63.28%] [Generator loss: 0.843946]\n",
      "9582 [Discriminator loss: 0.453952, acc.: 80.47%] [Generator loss: 1.061538]\n",
      "9583 [Discriminator loss: 0.687080, acc.: 53.91%] [Generator loss: 0.986891]\n",
      "9584 [Discriminator loss: 0.606013, acc.: 59.38%] [Generator loss: 0.976693]\n",
      "9585 [Discriminator loss: 0.712090, acc.: 63.28%] [Generator loss: 0.841443]\n",
      "9586 [Discriminator loss: 0.537498, acc.: 71.09%] [Generator loss: 0.848140]\n",
      "9587 [Discriminator loss: 0.563561, acc.: 73.44%] [Generator loss: 0.912381]\n",
      "9588 [Discriminator loss: 0.684560, acc.: 59.38%] [Generator loss: 0.960264]\n",
      "9589 [Discriminator loss: 0.645378, acc.: 59.38%] [Generator loss: 0.931363]\n",
      "9590 [Discriminator loss: 0.574600, acc.: 67.97%] [Generator loss: 0.783621]\n",
      "9591 [Discriminator loss: 0.673254, acc.: 65.62%] [Generator loss: 0.709920]\n",
      "9592 [Discriminator loss: 0.825047, acc.: 48.44%] [Generator loss: 0.828256]\n",
      "9593 [Discriminator loss: 0.682682, acc.: 56.25%] [Generator loss: 1.017112]\n",
      "9594 [Discriminator loss: 0.722214, acc.: 60.16%] [Generator loss: 0.838753]\n",
      "9595 [Discriminator loss: 0.601169, acc.: 67.97%] [Generator loss: 0.783409]\n",
      "9596 [Discriminator loss: 0.811729, acc.: 50.00%] [Generator loss: 0.777025]\n",
      "9597 [Discriminator loss: 0.635081, acc.: 61.72%] [Generator loss: 0.795425]\n",
      "9598 [Discriminator loss: 0.565126, acc.: 72.66%] [Generator loss: 0.858232]\n",
      "9599 [Discriminator loss: 0.834603, acc.: 45.31%] [Generator loss: 0.835756]\n",
      "9600 [Discriminator loss: 0.500241, acc.: 79.69%] [Generator loss: 1.045251]\n",
      "9601 [Discriminator loss: 0.761111, acc.: 55.47%] [Generator loss: 1.037457]\n",
      "9602 [Discriminator loss: 0.648757, acc.: 61.72%] [Generator loss: 1.189837]\n",
      "9603 [Discriminator loss: 0.503044, acc.: 72.66%] [Generator loss: 1.012832]\n",
      "9604 [Discriminator loss: 0.431051, acc.: 82.81%] [Generator loss: 0.802450]\n",
      "9605 [Discriminator loss: 0.524687, acc.: 72.66%] [Generator loss: 0.857228]\n",
      "9606 [Discriminator loss: 0.640723, acc.: 67.19%] [Generator loss: 0.701536]\n",
      "9607 [Discriminator loss: 0.544078, acc.: 75.00%] [Generator loss: 0.608682]\n",
      "9608 [Discriminator loss: 0.573106, acc.: 67.97%] [Generator loss: 0.518688]\n",
      "9609 [Discriminator loss: 0.764081, acc.: 51.56%] [Generator loss: 0.732832]\n",
      "9610 [Discriminator loss: 0.584384, acc.: 67.97%] [Generator loss: 0.800754]\n",
      "9611 [Discriminator loss: 0.586112, acc.: 71.88%] [Generator loss: 0.796350]\n",
      "9612 [Discriminator loss: 0.463942, acc.: 83.59%] [Generator loss: 0.773759]\n",
      "9613 [Discriminator loss: 0.675020, acc.: 64.84%] [Generator loss: 0.889630]\n",
      "9614 [Discriminator loss: 0.590028, acc.: 69.53%] [Generator loss: 0.866387]\n",
      "9615 [Discriminator loss: 0.953183, acc.: 36.72%] [Generator loss: 0.827017]\n",
      "9616 [Discriminator loss: 0.501349, acc.: 75.00%] [Generator loss: 1.074291]\n",
      "9617 [Discriminator loss: 0.557874, acc.: 72.66%] [Generator loss: 0.753966]\n",
      "9618 [Discriminator loss: 0.398423, acc.: 87.50%] [Generator loss: 0.901416]\n",
      "9619 [Discriminator loss: 0.379431, acc.: 90.62%] [Generator loss: 0.769715]\n",
      "9620 [Discriminator loss: 0.644417, acc.: 63.28%] [Generator loss: 0.780701]\n",
      "9621 [Discriminator loss: 0.601743, acc.: 67.97%] [Generator loss: 0.842887]\n",
      "9622 [Discriminator loss: 0.543621, acc.: 73.44%] [Generator loss: 0.870328]\n",
      "9623 [Discriminator loss: 0.649306, acc.: 62.50%] [Generator loss: 0.709444]\n",
      "9624 [Discriminator loss: 0.530627, acc.: 73.44%] [Generator loss: 0.560568]\n",
      "9625 [Discriminator loss: 1.068110, acc.: 32.03%] [Generator loss: 0.628190]\n",
      "9626 [Discriminator loss: 0.766683, acc.: 50.78%] [Generator loss: 0.814986]\n",
      "9627 [Discriminator loss: 0.857234, acc.: 39.84%] [Generator loss: 0.716621]\n",
      "9628 [Discriminator loss: 0.774934, acc.: 46.09%] [Generator loss: 0.721689]\n",
      "9629 [Discriminator loss: 1.194212, acc.: 29.69%] [Generator loss: 0.602376]\n",
      "9630 [Discriminator loss: 0.688532, acc.: 60.94%] [Generator loss: 1.195170]\n",
      "9631 [Discriminator loss: 0.594138, acc.: 70.31%] [Generator loss: 1.153472]\n",
      "9632 [Discriminator loss: 0.511194, acc.: 73.44%] [Generator loss: 1.201077]\n",
      "9633 [Discriminator loss: 0.739702, acc.: 57.81%] [Generator loss: 1.016205]\n",
      "9634 [Discriminator loss: 0.629448, acc.: 61.72%] [Generator loss: 1.045881]\n",
      "9635 [Discriminator loss: 0.738054, acc.: 53.12%] [Generator loss: 0.991023]\n",
      "9636 [Discriminator loss: 0.752342, acc.: 57.03%] [Generator loss: 1.198017]\n",
      "9637 [Discriminator loss: 0.658128, acc.: 56.25%] [Generator loss: 0.950143]\n",
      "9638 [Discriminator loss: 0.768103, acc.: 54.69%] [Generator loss: 0.876607]\n",
      "9639 [Discriminator loss: 0.786699, acc.: 52.34%] [Generator loss: 0.799495]\n",
      "9640 [Discriminator loss: 0.719447, acc.: 54.69%] [Generator loss: 0.853229]\n",
      "9641 [Discriminator loss: 0.607140, acc.: 72.66%] [Generator loss: 0.826936]\n",
      "9642 [Discriminator loss: 0.732149, acc.: 61.72%] [Generator loss: 0.870458]\n",
      "9643 [Discriminator loss: 0.722781, acc.: 54.69%] [Generator loss: 0.908717]\n",
      "9644 [Discriminator loss: 0.884061, acc.: 43.75%] [Generator loss: 0.844779]\n",
      "9645 [Discriminator loss: 0.708595, acc.: 52.34%] [Generator loss: 0.949865]\n",
      "9646 [Discriminator loss: 0.570851, acc.: 67.19%] [Generator loss: 1.000479]\n",
      "9647 [Discriminator loss: 0.718827, acc.: 56.25%] [Generator loss: 0.919096]\n",
      "9648 [Discriminator loss: 0.669674, acc.: 54.69%] [Generator loss: 0.930689]\n",
      "9649 [Discriminator loss: 0.662186, acc.: 60.94%] [Generator loss: 0.931982]\n",
      "9650 [Discriminator loss: 0.584705, acc.: 67.97%] [Generator loss: 0.979952]\n",
      "9651 [Discriminator loss: 0.692394, acc.: 62.50%] [Generator loss: 0.852408]\n",
      "9652 [Discriminator loss: 0.618455, acc.: 70.31%] [Generator loss: 0.719160]\n",
      "9653 [Discriminator loss: 0.807443, acc.: 50.00%] [Generator loss: 0.722759]\n",
      "9654 [Discriminator loss: 0.628194, acc.: 68.75%] [Generator loss: 0.781919]\n",
      "9655 [Discriminator loss: 0.580925, acc.: 71.09%] [Generator loss: 0.838676]\n",
      "9656 [Discriminator loss: 0.712330, acc.: 61.72%] [Generator loss: 0.857764]\n",
      "9657 [Discriminator loss: 0.665599, acc.: 58.59%] [Generator loss: 0.862097]\n",
      "9658 [Discriminator loss: 0.581781, acc.: 71.09%] [Generator loss: 0.885533]\n",
      "9659 [Discriminator loss: 0.699920, acc.: 66.41%] [Generator loss: 0.923367]\n",
      "9660 [Discriminator loss: 0.663453, acc.: 60.94%] [Generator loss: 1.047165]\n",
      "9661 [Discriminator loss: 0.560343, acc.: 71.88%] [Generator loss: 1.006079]\n",
      "9662 [Discriminator loss: 0.815568, acc.: 42.19%] [Generator loss: 0.875396]\n",
      "9663 [Discriminator loss: 0.655731, acc.: 58.59%] [Generator loss: 1.011148]\n",
      "9664 [Discriminator loss: 0.588858, acc.: 64.84%] [Generator loss: 0.914750]\n",
      "9665 [Discriminator loss: 0.562737, acc.: 75.00%] [Generator loss: 0.970381]\n",
      "9666 [Discriminator loss: 0.657249, acc.: 62.50%] [Generator loss: 0.881797]\n",
      "9667 [Discriminator loss: 0.720403, acc.: 52.34%] [Generator loss: 0.825530]\n",
      "9668 [Discriminator loss: 0.766190, acc.: 50.78%] [Generator loss: 0.802106]\n",
      "9669 [Discriminator loss: 0.723151, acc.: 53.12%] [Generator loss: 0.876063]\n",
      "9670 [Discriminator loss: 0.629165, acc.: 63.28%] [Generator loss: 0.860909]\n",
      "9671 [Discriminator loss: 0.815125, acc.: 42.19%] [Generator loss: 0.704370]\n",
      "9672 [Discriminator loss: 0.575919, acc.: 69.53%] [Generator loss: 0.791687]\n",
      "9673 [Discriminator loss: 0.572600, acc.: 75.78%] [Generator loss: 0.828819]\n",
      "9674 [Discriminator loss: 0.596138, acc.: 71.88%] [Generator loss: 0.875667]\n",
      "9675 [Discriminator loss: 0.722673, acc.: 53.12%] [Generator loss: 0.768325]\n",
      "9676 [Discriminator loss: 0.702971, acc.: 57.03%] [Generator loss: 1.065546]\n",
      "9677 [Discriminator loss: 0.639778, acc.: 66.41%] [Generator loss: 1.251208]\n",
      "9678 [Discriminator loss: 0.768941, acc.: 54.69%] [Generator loss: 0.983911]\n",
      "9679 [Discriminator loss: 0.660299, acc.: 62.50%] [Generator loss: 0.813781]\n",
      "9680 [Discriminator loss: 0.557570, acc.: 75.00%] [Generator loss: 0.705085]\n",
      "9681 [Discriminator loss: 0.567671, acc.: 75.78%] [Generator loss: 0.735333]\n",
      "9682 [Discriminator loss: 0.522926, acc.: 79.69%] [Generator loss: 0.787676]\n",
      "9683 [Discriminator loss: 0.425590, acc.: 86.72%] [Generator loss: 0.906321]\n",
      "9684 [Discriminator loss: 0.736893, acc.: 47.66%] [Generator loss: 0.754587]\n",
      "9685 [Discriminator loss: 0.672759, acc.: 60.16%] [Generator loss: 0.700270]\n",
      "9686 [Discriminator loss: 0.438286, acc.: 83.59%] [Generator loss: 0.620750]\n",
      "9687 [Discriminator loss: 0.621629, acc.: 62.50%] [Generator loss: 0.629066]\n",
      "9688 [Discriminator loss: 0.816245, acc.: 52.34%] [Generator loss: 0.817969]\n",
      "9689 [Discriminator loss: 0.955746, acc.: 39.06%] [Generator loss: 0.748348]\n",
      "9690 [Discriminator loss: 0.689293, acc.: 59.38%] [Generator loss: 0.825151]\n",
      "9691 [Discriminator loss: 0.716758, acc.: 54.69%] [Generator loss: 0.800691]\n",
      "9692 [Discriminator loss: 0.643761, acc.: 63.28%] [Generator loss: 0.853137]\n",
      "9693 [Discriminator loss: 0.689899, acc.: 50.78%] [Generator loss: 0.952914]\n",
      "9694 [Discriminator loss: 0.553771, acc.: 75.00%] [Generator loss: 0.998726]\n",
      "9695 [Discriminator loss: 0.813910, acc.: 50.00%] [Generator loss: 1.014307]\n",
      "9696 [Discriminator loss: 0.808523, acc.: 49.22%] [Generator loss: 0.745006]\n",
      "9697 [Discriminator loss: 0.693651, acc.: 57.03%] [Generator loss: 0.772815]\n",
      "9698 [Discriminator loss: 0.591890, acc.: 71.09%] [Generator loss: 0.689657]\n",
      "9699 [Discriminator loss: 0.856187, acc.: 46.88%] [Generator loss: 0.686018]\n",
      "9700 [Discriminator loss: 0.695624, acc.: 55.47%] [Generator loss: 0.669969]\n",
      "9701 [Discriminator loss: 0.653316, acc.: 64.06%] [Generator loss: 0.818485]\n",
      "9702 [Discriminator loss: 0.513314, acc.: 76.56%] [Generator loss: 0.980188]\n",
      "9703 [Discriminator loss: 0.838455, acc.: 48.44%] [Generator loss: 0.783606]\n",
      "9704 [Discriminator loss: 0.667669, acc.: 59.38%] [Generator loss: 0.760624]\n",
      "9705 [Discriminator loss: 0.665825, acc.: 56.25%] [Generator loss: 0.764335]\n",
      "9706 [Discriminator loss: 0.696277, acc.: 53.91%] [Generator loss: 0.962688]\n",
      "9707 [Discriminator loss: 0.504547, acc.: 79.69%] [Generator loss: 1.128236]\n",
      "9708 [Discriminator loss: 0.706819, acc.: 57.81%] [Generator loss: 1.008469]\n",
      "9709 [Discriminator loss: 0.695653, acc.: 58.59%] [Generator loss: 0.740498]\n",
      "9710 [Discriminator loss: 0.671952, acc.: 61.72%] [Generator loss: 0.761009]\n",
      "9711 [Discriminator loss: 0.777509, acc.: 45.31%] [Generator loss: 0.658165]\n",
      "9712 [Discriminator loss: 0.651051, acc.: 65.62%] [Generator loss: 0.882716]\n",
      "9713 [Discriminator loss: 0.609185, acc.: 69.53%] [Generator loss: 0.839646]\n",
      "9714 [Discriminator loss: 0.586145, acc.: 72.66%] [Generator loss: 0.786209]\n",
      "9715 [Discriminator loss: 0.562561, acc.: 75.78%] [Generator loss: 0.893174]\n",
      "9716 [Discriminator loss: 0.514432, acc.: 81.25%] [Generator loss: 0.852019]\n",
      "9717 [Discriminator loss: 0.424498, acc.: 85.94%] [Generator loss: 0.733174]\n",
      "9718 [Discriminator loss: 0.515553, acc.: 75.78%] [Generator loss: 0.687988]\n",
      "9719 [Discriminator loss: 0.646131, acc.: 62.50%] [Generator loss: 0.804740]\n",
      "9720 [Discriminator loss: 0.713997, acc.: 59.38%] [Generator loss: 0.922788]\n",
      "9721 [Discriminator loss: 0.697231, acc.: 57.03%] [Generator loss: 0.991143]\n",
      "9722 [Discriminator loss: 0.434341, acc.: 80.47%] [Generator loss: 1.125040]\n",
      "9723 [Discriminator loss: 1.070302, acc.: 25.78%] [Generator loss: 0.816286]\n",
      "9724 [Discriminator loss: 0.576801, acc.: 67.97%] [Generator loss: 0.849630]\n",
      "9725 [Discriminator loss: 0.643914, acc.: 64.06%] [Generator loss: 0.663381]\n",
      "9726 [Discriminator loss: 0.700647, acc.: 55.47%] [Generator loss: 0.587047]\n",
      "9727 [Discriminator loss: 0.579050, acc.: 74.22%] [Generator loss: 0.751744]\n",
      "9728 [Discriminator loss: 0.691724, acc.: 60.94%] [Generator loss: 0.684526]\n",
      "9729 [Discriminator loss: 0.709872, acc.: 61.72%] [Generator loss: 0.614251]\n",
      "9730 [Discriminator loss: 0.584093, acc.: 71.88%] [Generator loss: 0.748566]\n",
      "9731 [Discriminator loss: 0.693214, acc.: 61.72%] [Generator loss: 0.684445]\n",
      "9732 [Discriminator loss: 0.457414, acc.: 85.94%] [Generator loss: 0.854781]\n",
      "9733 [Discriminator loss: 0.533465, acc.: 77.34%] [Generator loss: 1.031699]\n",
      "9734 [Discriminator loss: 0.686649, acc.: 59.38%] [Generator loss: 0.783190]\n",
      "9735 [Discriminator loss: 0.620494, acc.: 66.41%] [Generator loss: 0.767441]\n",
      "9736 [Discriminator loss: 0.499794, acc.: 78.91%] [Generator loss: 0.946547]\n",
      "9737 [Discriminator loss: 0.645530, acc.: 64.84%] [Generator loss: 0.682449]\n",
      "9738 [Discriminator loss: 0.471310, acc.: 82.03%] [Generator loss: 0.577498]\n",
      "9739 [Discriminator loss: 0.694421, acc.: 60.16%] [Generator loss: 0.862900]\n",
      "9740 [Discriminator loss: 0.649733, acc.: 60.94%] [Generator loss: 0.934307]\n",
      "9741 [Discriminator loss: 0.520025, acc.: 72.66%] [Generator loss: 1.031785]\n",
      "9742 [Discriminator loss: 0.636190, acc.: 63.28%] [Generator loss: 0.737332]\n",
      "9743 [Discriminator loss: 1.102896, acc.: 29.69%] [Generator loss: 0.740277]\n",
      "9744 [Discriminator loss: 0.661135, acc.: 64.06%] [Generator loss: 0.742717]\n",
      "9745 [Discriminator loss: 0.646213, acc.: 61.72%] [Generator loss: 0.984244]\n",
      "9746 [Discriminator loss: 0.504138, acc.: 80.47%] [Generator loss: 1.039630]\n",
      "9747 [Discriminator loss: 0.821731, acc.: 50.78%] [Generator loss: 0.828936]\n",
      "9748 [Discriminator loss: 0.500628, acc.: 80.47%] [Generator loss: 0.885683]\n",
      "9749 [Discriminator loss: 0.462078, acc.: 83.59%] [Generator loss: 0.729038]\n",
      "9750 [Discriminator loss: 0.584170, acc.: 71.09%] [Generator loss: 0.632267]\n",
      "9751 [Discriminator loss: 0.473386, acc.: 85.16%] [Generator loss: 0.603558]\n",
      "9752 [Discriminator loss: 0.503118, acc.: 78.12%] [Generator loss: 0.649421]\n",
      "9753 [Discriminator loss: 0.562553, acc.: 74.22%] [Generator loss: 0.773128]\n",
      "9754 [Discriminator loss: 0.417127, acc.: 85.94%] [Generator loss: 0.716260]\n",
      "9755 [Discriminator loss: 0.782722, acc.: 53.12%] [Generator loss: 0.701114]\n",
      "9756 [Discriminator loss: 0.524398, acc.: 77.34%] [Generator loss: 0.618830]\n",
      "9757 [Discriminator loss: 0.539539, acc.: 71.88%] [Generator loss: 0.931816]\n",
      "9758 [Discriminator loss: 0.879445, acc.: 45.31%] [Generator loss: 0.807337]\n",
      "9759 [Discriminator loss: 0.892258, acc.: 45.31%] [Generator loss: 0.858435]\n",
      "9760 [Discriminator loss: 0.488634, acc.: 80.47%] [Generator loss: 0.725083]\n",
      "9761 [Discriminator loss: 0.633409, acc.: 64.84%] [Generator loss: 0.768943]\n",
      "9762 [Discriminator loss: 0.562206, acc.: 71.09%] [Generator loss: 0.949264]\n",
      "9763 [Discriminator loss: 0.763893, acc.: 54.69%] [Generator loss: 0.854007]\n",
      "9764 [Discriminator loss: 0.740389, acc.: 51.56%] [Generator loss: 0.852709]\n",
      "9765 [Discriminator loss: 0.797810, acc.: 50.00%] [Generator loss: 0.902141]\n",
      "9766 [Discriminator loss: 0.650696, acc.: 65.62%] [Generator loss: 0.861981]\n",
      "9767 [Discriminator loss: 0.607047, acc.: 67.19%] [Generator loss: 0.871917]\n",
      "9768 [Discriminator loss: 0.773693, acc.: 57.81%] [Generator loss: 0.714737]\n",
      "9769 [Discriminator loss: 0.907967, acc.: 40.62%] [Generator loss: 0.780520]\n",
      "9770 [Discriminator loss: 0.773215, acc.: 57.81%] [Generator loss: 0.642603]\n",
      "9771 [Discriminator loss: 0.657258, acc.: 66.41%] [Generator loss: 0.688787]\n",
      "9772 [Discriminator loss: 0.772387, acc.: 42.19%] [Generator loss: 0.884543]\n",
      "9773 [Discriminator loss: 0.601286, acc.: 74.22%] [Generator loss: 1.080900]\n",
      "9774 [Discriminator loss: 0.718999, acc.: 56.25%] [Generator loss: 1.271808]\n",
      "9775 [Discriminator loss: 0.660998, acc.: 64.06%] [Generator loss: 1.080157]\n",
      "9776 [Discriminator loss: 0.693025, acc.: 61.72%] [Generator loss: 1.181986]\n",
      "9777 [Discriminator loss: 0.676188, acc.: 61.72%] [Generator loss: 1.092700]\n",
      "9778 [Discriminator loss: 0.713602, acc.: 52.34%] [Generator loss: 0.862259]\n",
      "9779 [Discriminator loss: 0.849528, acc.: 36.72%] [Generator loss: 0.915281]\n",
      "9780 [Discriminator loss: 0.637488, acc.: 71.09%] [Generator loss: 0.918235]\n",
      "9781 [Discriminator loss: 0.664980, acc.: 60.94%] [Generator loss: 1.138231]\n",
      "9782 [Discriminator loss: 0.757399, acc.: 46.09%] [Generator loss: 0.910807]\n",
      "9783 [Discriminator loss: 0.812541, acc.: 43.75%] [Generator loss: 0.834154]\n",
      "9784 [Discriminator loss: 0.726550, acc.: 50.78%] [Generator loss: 0.907842]\n",
      "9785 [Discriminator loss: 0.587362, acc.: 69.53%] [Generator loss: 0.995467]\n",
      "9786 [Discriminator loss: 0.683592, acc.: 57.03%] [Generator loss: 0.851153]\n",
      "9787 [Discriminator loss: 0.615514, acc.: 64.06%] [Generator loss: 0.936941]\n",
      "9788 [Discriminator loss: 0.526880, acc.: 76.56%] [Generator loss: 0.818488]\n",
      "9789 [Discriminator loss: 0.587322, acc.: 67.19%] [Generator loss: 0.841536]\n",
      "9790 [Discriminator loss: 0.587925, acc.: 67.97%] [Generator loss: 0.992339]\n",
      "9791 [Discriminator loss: 0.528639, acc.: 77.34%] [Generator loss: 0.826800]\n",
      "9792 [Discriminator loss: 0.557145, acc.: 76.56%] [Generator loss: 0.893030]\n",
      "9793 [Discriminator loss: 0.685956, acc.: 67.19%] [Generator loss: 0.832510]\n",
      "9794 [Discriminator loss: 0.702086, acc.: 54.69%] [Generator loss: 0.816235]\n",
      "9795 [Discriminator loss: 0.613313, acc.: 65.62%] [Generator loss: 0.736662]\n",
      "9796 [Discriminator loss: 1.048524, acc.: 35.16%] [Generator loss: 0.693390]\n",
      "9797 [Discriminator loss: 0.656536, acc.: 70.31%] [Generator loss: 0.681039]\n",
      "9798 [Discriminator loss: 0.702934, acc.: 59.38%] [Generator loss: 0.791861]\n",
      "9799 [Discriminator loss: 0.705264, acc.: 56.25%] [Generator loss: 0.808488]\n",
      "9800 [Discriminator loss: 0.585802, acc.: 71.09%] [Generator loss: 0.994861]\n",
      "9801 [Discriminator loss: 0.533315, acc.: 79.69%] [Generator loss: 0.946246]\n",
      "9802 [Discriminator loss: 0.574318, acc.: 71.09%] [Generator loss: 0.890384]\n",
      "9803 [Discriminator loss: 0.549832, acc.: 74.22%] [Generator loss: 0.939603]\n",
      "9804 [Discriminator loss: 0.595143, acc.: 72.66%] [Generator loss: 0.947213]\n",
      "9805 [Discriminator loss: 0.533611, acc.: 76.56%] [Generator loss: 0.820253]\n",
      "9806 [Discriminator loss: 0.803373, acc.: 46.88%] [Generator loss: 0.722416]\n",
      "9807 [Discriminator loss: 0.618189, acc.: 64.84%] [Generator loss: 1.072382]\n",
      "9808 [Discriminator loss: 0.680792, acc.: 54.69%] [Generator loss: 0.966556]\n",
      "9809 [Discriminator loss: 0.511153, acc.: 80.47%] [Generator loss: 0.843735]\n",
      "9810 [Discriminator loss: 0.830512, acc.: 41.41%] [Generator loss: 0.835614]\n",
      "9811 [Discriminator loss: 0.613713, acc.: 67.19%] [Generator loss: 0.901919]\n",
      "9812 [Discriminator loss: 0.754181, acc.: 52.34%] [Generator loss: 0.716390]\n",
      "9813 [Discriminator loss: 0.661241, acc.: 61.72%] [Generator loss: 0.856132]\n",
      "9814 [Discriminator loss: 0.595320, acc.: 70.31%] [Generator loss: 0.854897]\n",
      "9815 [Discriminator loss: 0.634699, acc.: 63.28%] [Generator loss: 0.913987]\n",
      "9816 [Discriminator loss: 0.809042, acc.: 42.97%] [Generator loss: 0.854547]\n",
      "9817 [Discriminator loss: 0.589745, acc.: 71.88%] [Generator loss: 0.979390]\n",
      "9818 [Discriminator loss: 0.670102, acc.: 61.72%] [Generator loss: 0.893910]\n",
      "9819 [Discriminator loss: 0.617101, acc.: 67.97%] [Generator loss: 0.704449]\n",
      "9820 [Discriminator loss: 0.639175, acc.: 77.34%] [Generator loss: 0.886000]\n",
      "9821 [Discriminator loss: 0.477721, acc.: 84.38%] [Generator loss: 0.845619]\n",
      "9822 [Discriminator loss: 0.481692, acc.: 82.03%] [Generator loss: 0.771908]\n",
      "9823 [Discriminator loss: 0.646615, acc.: 60.94%] [Generator loss: 0.637155]\n",
      "9824 [Discriminator loss: 0.536400, acc.: 81.25%] [Generator loss: 0.825882]\n",
      "9825 [Discriminator loss: 0.492372, acc.: 82.03%] [Generator loss: 0.786503]\n",
      "9826 [Discriminator loss: 0.597947, acc.: 64.84%] [Generator loss: 0.693737]\n",
      "9827 [Discriminator loss: 0.603370, acc.: 60.94%] [Generator loss: 0.677451]\n",
      "9828 [Discriminator loss: 0.477790, acc.: 85.94%] [Generator loss: 0.653438]\n",
      "9829 [Discriminator loss: 0.657563, acc.: 63.28%] [Generator loss: 0.605341]\n",
      "9830 [Discriminator loss: 0.585943, acc.: 67.97%] [Generator loss: 0.669349]\n",
      "9831 [Discriminator loss: 0.737103, acc.: 58.59%] [Generator loss: 0.831187]\n",
      "9832 [Discriminator loss: 0.463803, acc.: 85.16%] [Generator loss: 0.993047]\n",
      "9833 [Discriminator loss: 0.740610, acc.: 50.00%] [Generator loss: 0.948704]\n",
      "9834 [Discriminator loss: 0.815264, acc.: 56.25%] [Generator loss: 1.286282]\n",
      "9835 [Discriminator loss: 0.597781, acc.: 64.84%] [Generator loss: 1.418154]\n",
      "9836 [Discriminator loss: 0.657020, acc.: 61.72%] [Generator loss: 0.880846]\n",
      "9837 [Discriminator loss: 0.681449, acc.: 58.59%] [Generator loss: 0.981299]\n",
      "9838 [Discriminator loss: 0.803696, acc.: 43.75%] [Generator loss: 0.670892]\n",
      "9839 [Discriminator loss: 0.739926, acc.: 46.09%] [Generator loss: 0.663693]\n",
      "9840 [Discriminator loss: 0.594867, acc.: 80.47%] [Generator loss: 0.669386]\n",
      "9841 [Discriminator loss: 0.532478, acc.: 80.47%] [Generator loss: 0.513283]\n",
      "9842 [Discriminator loss: 0.763078, acc.: 47.66%] [Generator loss: 0.562931]\n",
      "9843 [Discriminator loss: 0.563773, acc.: 75.78%] [Generator loss: 0.630310]\n",
      "9844 [Discriminator loss: 0.523496, acc.: 78.91%] [Generator loss: 0.608192]\n",
      "9845 [Discriminator loss: 0.879347, acc.: 43.75%] [Generator loss: 0.877271]\n",
      "9846 [Discriminator loss: 0.817654, acc.: 49.22%] [Generator loss: 1.087965]\n",
      "9847 [Discriminator loss: 0.750511, acc.: 51.56%] [Generator loss: 0.917417]\n",
      "9848 [Discriminator loss: 0.683790, acc.: 60.94%] [Generator loss: 0.874432]\n",
      "9849 [Discriminator loss: 0.922279, acc.: 37.50%] [Generator loss: 0.934002]\n",
      "9850 [Discriminator loss: 0.689755, acc.: 57.81%] [Generator loss: 0.970971]\n",
      "9851 [Discriminator loss: 0.722099, acc.: 61.72%] [Generator loss: 0.888853]\n",
      "9852 [Discriminator loss: 0.775936, acc.: 46.88%] [Generator loss: 1.039765]\n",
      "9853 [Discriminator loss: 0.733970, acc.: 53.91%] [Generator loss: 0.983997]\n",
      "9854 [Discriminator loss: 0.608360, acc.: 65.62%] [Generator loss: 0.878262]\n",
      "9855 [Discriminator loss: 0.694571, acc.: 57.81%] [Generator loss: 0.854561]\n",
      "9856 [Discriminator loss: 0.576157, acc.: 71.09%] [Generator loss: 0.717704]\n",
      "9857 [Discriminator loss: 0.632419, acc.: 64.84%] [Generator loss: 0.784109]\n",
      "9858 [Discriminator loss: 0.605593, acc.: 63.28%] [Generator loss: 1.029209]\n",
      "9859 [Discriminator loss: 0.516026, acc.: 71.09%] [Generator loss: 0.982246]\n",
      "9860 [Discriminator loss: 0.612366, acc.: 64.06%] [Generator loss: 0.828321]\n",
      "9861 [Discriminator loss: 0.747792, acc.: 52.34%] [Generator loss: 0.701529]\n",
      "9862 [Discriminator loss: 0.480655, acc.: 82.03%] [Generator loss: 0.866644]\n",
      "9863 [Discriminator loss: 0.816838, acc.: 53.91%] [Generator loss: 0.683109]\n",
      "9864 [Discriminator loss: 0.799761, acc.: 43.75%] [Generator loss: 0.695067]\n",
      "9865 [Discriminator loss: 0.711661, acc.: 53.12%] [Generator loss: 0.789894]\n",
      "9866 [Discriminator loss: 0.678072, acc.: 61.72%] [Generator loss: 0.902913]\n",
      "9867 [Discriminator loss: 0.523042, acc.: 78.12%] [Generator loss: 0.932096]\n",
      "9868 [Discriminator loss: 0.590724, acc.: 68.75%] [Generator loss: 0.983842]\n",
      "9869 [Discriminator loss: 0.764023, acc.: 52.34%] [Generator loss: 0.790651]\n",
      "9870 [Discriminator loss: 0.654353, acc.: 60.16%] [Generator loss: 1.028474]\n",
      "9871 [Discriminator loss: 0.811607, acc.: 46.88%] [Generator loss: 0.856129]\n",
      "9872 [Discriminator loss: 0.569631, acc.: 72.66%] [Generator loss: 0.731526]\n",
      "9873 [Discriminator loss: 1.039576, acc.: 33.59%] [Generator loss: 0.637756]\n",
      "9874 [Discriminator loss: 0.747511, acc.: 53.91%] [Generator loss: 0.733769]\n",
      "9875 [Discriminator loss: 0.789755, acc.: 45.31%] [Generator loss: 0.837581]\n",
      "9876 [Discriminator loss: 0.632378, acc.: 67.97%] [Generator loss: 0.951890]\n",
      "9877 [Discriminator loss: 0.586451, acc.: 72.66%] [Generator loss: 0.757787]\n",
      "9878 [Discriminator loss: 0.637791, acc.: 61.72%] [Generator loss: 0.727281]\n",
      "9879 [Discriminator loss: 0.600578, acc.: 68.75%] [Generator loss: 0.767270]\n",
      "9880 [Discriminator loss: 0.821005, acc.: 46.09%] [Generator loss: 0.959422]\n",
      "9881 [Discriminator loss: 0.724754, acc.: 60.94%] [Generator loss: 0.998219]\n",
      "9882 [Discriminator loss: 0.613584, acc.: 60.94%] [Generator loss: 1.046927]\n",
      "9883 [Discriminator loss: 0.671345, acc.: 53.12%] [Generator loss: 1.192241]\n",
      "9884 [Discriminator loss: 0.512376, acc.: 72.66%] [Generator loss: 1.111362]\n",
      "9885 [Discriminator loss: 0.719570, acc.: 57.81%] [Generator loss: 0.936922]\n",
      "9886 [Discriminator loss: 0.705689, acc.: 56.25%] [Generator loss: 0.825520]\n",
      "9887 [Discriminator loss: 0.666210, acc.: 60.16%] [Generator loss: 0.834529]\n",
      "9888 [Discriminator loss: 0.754707, acc.: 54.69%] [Generator loss: 0.890423]\n",
      "9889 [Discriminator loss: 0.611296, acc.: 69.53%] [Generator loss: 1.076972]\n",
      "9890 [Discriminator loss: 0.568062, acc.: 72.66%] [Generator loss: 1.140494]\n",
      "9891 [Discriminator loss: 0.705541, acc.: 57.81%] [Generator loss: 0.965238]\n",
      "9892 [Discriminator loss: 0.721739, acc.: 61.72%] [Generator loss: 1.095395]\n",
      "9893 [Discriminator loss: 0.687314, acc.: 57.03%] [Generator loss: 0.891856]\n",
      "9894 [Discriminator loss: 0.645125, acc.: 64.84%] [Generator loss: 0.823726]\n",
      "9895 [Discriminator loss: 0.638361, acc.: 63.28%] [Generator loss: 0.723871]\n",
      "9896 [Discriminator loss: 0.504796, acc.: 79.69%] [Generator loss: 0.896076]\n",
      "9897 [Discriminator loss: 0.754483, acc.: 50.78%] [Generator loss: 0.836718]\n",
      "9898 [Discriminator loss: 0.669039, acc.: 67.19%] [Generator loss: 0.821914]\n",
      "9899 [Discriminator loss: 0.506357, acc.: 79.69%] [Generator loss: 0.936879]\n",
      "9900 [Discriminator loss: 0.626928, acc.: 72.66%] [Generator loss: 0.917545]\n",
      "9901 [Discriminator loss: 0.611259, acc.: 67.19%] [Generator loss: 0.978122]\n",
      "9902 [Discriminator loss: 0.703576, acc.: 49.22%] [Generator loss: 1.019016]\n",
      "9903 [Discriminator loss: 0.597963, acc.: 69.53%] [Generator loss: 1.024897]\n",
      "9904 [Discriminator loss: 0.790587, acc.: 49.22%] [Generator loss: 0.945453]\n",
      "9905 [Discriminator loss: 0.781348, acc.: 56.25%] [Generator loss: 0.897419]\n",
      "9906 [Discriminator loss: 0.654539, acc.: 60.94%] [Generator loss: 0.906207]\n",
      "9907 [Discriminator loss: 0.582939, acc.: 72.66%] [Generator loss: 0.984035]\n",
      "9908 [Discriminator loss: 0.700050, acc.: 57.81%] [Generator loss: 0.863805]\n",
      "9909 [Discriminator loss: 0.615012, acc.: 65.62%] [Generator loss: 1.041438]\n",
      "9910 [Discriminator loss: 0.569263, acc.: 72.66%] [Generator loss: 1.031243]\n",
      "9911 [Discriminator loss: 0.630167, acc.: 66.41%] [Generator loss: 1.104001]\n",
      "9912 [Discriminator loss: 0.640346, acc.: 68.75%] [Generator loss: 1.019494]\n",
      "9913 [Discriminator loss: 0.646621, acc.: 67.97%] [Generator loss: 1.064043]\n",
      "9914 [Discriminator loss: 0.644012, acc.: 61.72%] [Generator loss: 0.932465]\n",
      "9915 [Discriminator loss: 0.515873, acc.: 82.03%] [Generator loss: 0.863787]\n",
      "9916 [Discriminator loss: 0.619765, acc.: 63.28%] [Generator loss: 0.837213]\n",
      "9917 [Discriminator loss: 0.810208, acc.: 44.53%] [Generator loss: 0.889087]\n",
      "9918 [Discriminator loss: 0.717017, acc.: 55.47%] [Generator loss: 0.792078]\n",
      "9919 [Discriminator loss: 0.804083, acc.: 45.31%] [Generator loss: 0.902166]\n",
      "9920 [Discriminator loss: 0.642969, acc.: 64.84%] [Generator loss: 0.861602]\n",
      "9921 [Discriminator loss: 0.656143, acc.: 65.62%] [Generator loss: 0.776955]\n",
      "9922 [Discriminator loss: 0.517375, acc.: 76.56%] [Generator loss: 0.776226]\n",
      "9923 [Discriminator loss: 0.718214, acc.: 55.47%] [Generator loss: 0.773603]\n",
      "9924 [Discriminator loss: 0.635647, acc.: 65.62%] [Generator loss: 0.887168]\n",
      "9925 [Discriminator loss: 0.708827, acc.: 50.00%] [Generator loss: 1.010340]\n",
      "9926 [Discriminator loss: 0.679305, acc.: 50.78%] [Generator loss: 0.926409]\n",
      "9927 [Discriminator loss: 0.508129, acc.: 82.81%] [Generator loss: 0.811273]\n",
      "9928 [Discriminator loss: 0.654852, acc.: 64.84%] [Generator loss: 1.102128]\n",
      "9929 [Discriminator loss: 0.691701, acc.: 64.84%] [Generator loss: 0.925495]\n",
      "9930 [Discriminator loss: 0.713741, acc.: 59.38%] [Generator loss: 0.965070]\n",
      "9931 [Discriminator loss: 0.708640, acc.: 54.69%] [Generator loss: 0.972496]\n",
      "9932 [Discriminator loss: 0.735911, acc.: 52.34%] [Generator loss: 0.982830]\n",
      "9933 [Discriminator loss: 0.667629, acc.: 57.03%] [Generator loss: 0.883310]\n",
      "9934 [Discriminator loss: 0.624526, acc.: 67.19%] [Generator loss: 0.762221]\n",
      "9935 [Discriminator loss: 0.625996, acc.: 60.16%] [Generator loss: 0.709153]\n",
      "9936 [Discriminator loss: 0.627217, acc.: 64.06%] [Generator loss: 0.770993]\n",
      "9937 [Discriminator loss: 0.559966, acc.: 74.22%] [Generator loss: 0.664798]\n",
      "9938 [Discriminator loss: 0.544196, acc.: 78.12%] [Generator loss: 0.769518]\n",
      "9939 [Discriminator loss: 0.643023, acc.: 60.94%] [Generator loss: 0.747004]\n",
      "9940 [Discriminator loss: 0.724400, acc.: 56.25%] [Generator loss: 0.802899]\n",
      "9941 [Discriminator loss: 0.661132, acc.: 60.16%] [Generator loss: 0.824533]\n",
      "9942 [Discriminator loss: 0.579480, acc.: 68.75%] [Generator loss: 0.842864]\n",
      "9943 [Discriminator loss: 0.659527, acc.: 55.47%] [Generator loss: 0.939308]\n",
      "9944 [Discriminator loss: 0.670892, acc.: 64.06%] [Generator loss: 0.839155]\n",
      "9945 [Discriminator loss: 0.809407, acc.: 45.31%] [Generator loss: 0.832318]\n",
      "9946 [Discriminator loss: 0.637465, acc.: 64.84%] [Generator loss: 1.175470]\n",
      "9947 [Discriminator loss: 0.698855, acc.: 54.69%] [Generator loss: 1.002449]\n",
      "9948 [Discriminator loss: 0.617426, acc.: 65.62%] [Generator loss: 1.017393]\n",
      "9949 [Discriminator loss: 0.665887, acc.: 59.38%] [Generator loss: 0.941364]\n",
      "9950 [Discriminator loss: 0.617885, acc.: 70.31%] [Generator loss: 0.811221]\n",
      "9951 [Discriminator loss: 0.592432, acc.: 67.97%] [Generator loss: 0.988077]\n",
      "9952 [Discriminator loss: 0.521321, acc.: 75.78%] [Generator loss: 0.871161]\n",
      "9953 [Discriminator loss: 0.601155, acc.: 60.16%] [Generator loss: 0.811084]\n",
      "9954 [Discriminator loss: 0.648343, acc.: 64.84%] [Generator loss: 1.015719]\n",
      "9955 [Discriminator loss: 0.439125, acc.: 86.72%] [Generator loss: 0.850469]\n",
      "9956 [Discriminator loss: 0.540395, acc.: 71.09%] [Generator loss: 0.886934]\n",
      "9957 [Discriminator loss: 0.699808, acc.: 65.62%] [Generator loss: 0.876298]\n",
      "9958 [Discriminator loss: 0.529855, acc.: 74.22%] [Generator loss: 0.811521]\n",
      "9959 [Discriminator loss: 0.860804, acc.: 45.31%] [Generator loss: 0.872270]\n",
      "9960 [Discriminator loss: 0.811573, acc.: 54.69%] [Generator loss: 0.898527]\n",
      "9961 [Discriminator loss: 0.570675, acc.: 72.66%] [Generator loss: 1.014095]\n",
      "9962 [Discriminator loss: 0.721063, acc.: 50.78%] [Generator loss: 0.778489]\n",
      "9963 [Discriminator loss: 0.812830, acc.: 50.00%] [Generator loss: 1.030256]\n",
      "9964 [Discriminator loss: 0.693371, acc.: 62.50%] [Generator loss: 0.965080]\n",
      "9965 [Discriminator loss: 0.754668, acc.: 53.12%] [Generator loss: 1.082790]\n",
      "9966 [Discriminator loss: 0.889816, acc.: 38.28%] [Generator loss: 1.034737]\n",
      "9967 [Discriminator loss: 0.708918, acc.: 56.25%] [Generator loss: 0.937226]\n",
      "9968 [Discriminator loss: 0.745330, acc.: 46.88%] [Generator loss: 0.887933]\n",
      "9969 [Discriminator loss: 0.665692, acc.: 67.19%] [Generator loss: 0.881142]\n",
      "9970 [Discriminator loss: 0.375053, acc.: 85.94%] [Generator loss: 0.786710]\n",
      "9971 [Discriminator loss: 0.616117, acc.: 71.09%] [Generator loss: 0.955840]\n",
      "9972 [Discriminator loss: 0.584061, acc.: 65.62%] [Generator loss: 0.814395]\n",
      "9973 [Discriminator loss: 0.436667, acc.: 81.25%] [Generator loss: 0.523006]\n",
      "9974 [Discriminator loss: 0.579211, acc.: 64.84%] [Generator loss: 0.656310]\n",
      "9975 [Discriminator loss: 0.674902, acc.: 62.50%] [Generator loss: 0.936628]\n",
      "9976 [Discriminator loss: 0.510702, acc.: 77.34%] [Generator loss: 1.058703]\n",
      "9977 [Discriminator loss: 0.886722, acc.: 42.19%] [Generator loss: 0.692968]\n",
      "9978 [Discriminator loss: 0.549472, acc.: 75.00%] [Generator loss: 0.629418]\n",
      "9979 [Discriminator loss: 0.712257, acc.: 58.59%] [Generator loss: 0.778023]\n",
      "9980 [Discriminator loss: 0.739616, acc.: 50.78%] [Generator loss: 0.856434]\n",
      "9981 [Discriminator loss: 0.746007, acc.: 46.88%] [Generator loss: 1.102160]\n",
      "9982 [Discriminator loss: 0.732167, acc.: 57.81%] [Generator loss: 0.885913]\n",
      "9983 [Discriminator loss: 0.644539, acc.: 65.62%] [Generator loss: 0.795513]\n",
      "9984 [Discriminator loss: 0.756135, acc.: 52.34%] [Generator loss: 0.834319]\n",
      "9985 [Discriminator loss: 0.702664, acc.: 58.59%] [Generator loss: 0.930448]\n",
      "9986 [Discriminator loss: 0.677641, acc.: 57.03%] [Generator loss: 0.883534]\n",
      "9987 [Discriminator loss: 0.578017, acc.: 75.00%] [Generator loss: 1.011532]\n",
      "9988 [Discriminator loss: 0.588349, acc.: 70.31%] [Generator loss: 0.879044]\n",
      "9989 [Discriminator loss: 0.694310, acc.: 55.47%] [Generator loss: 0.863797]\n",
      "9990 [Discriminator loss: 1.006045, acc.: 47.66%] [Generator loss: 0.703471]\n",
      "9991 [Discriminator loss: 0.726523, acc.: 57.03%] [Generator loss: 0.675753]\n",
      "9992 [Discriminator loss: 0.630524, acc.: 57.03%] [Generator loss: 0.682726]\n",
      "9993 [Discriminator loss: 0.542162, acc.: 78.12%] [Generator loss: 0.763985]\n",
      "9994 [Discriminator loss: 0.482359, acc.: 78.12%] [Generator loss: 0.841854]\n",
      "9995 [Discriminator loss: 0.718049, acc.: 57.81%] [Generator loss: 0.960271]\n",
      "9996 [Discriminator loss: 0.616002, acc.: 64.84%] [Generator loss: 0.927848]\n",
      "9997 [Discriminator loss: 0.515097, acc.: 82.03%] [Generator loss: 0.851515]\n",
      "9998 [Discriminator loss: 0.710212, acc.: 58.59%] [Generator loss: 0.976989]\n",
      "9999 [Discriminator loss: 0.682633, acc.: 57.81%] [Generator loss: 1.164778]\n",
      "10000 [Discriminator loss: 0.776764, acc.: 45.31%] [Generator loss: 0.970824]\n",
      "10001 [Discriminator loss: 0.734011, acc.: 54.69%] [Generator loss: 0.939770]\n",
      "10002 [Discriminator loss: 0.763620, acc.: 46.88%] [Generator loss: 0.936335]\n",
      "10003 [Discriminator loss: 0.733571, acc.: 52.34%] [Generator loss: 1.014582]\n",
      "10004 [Discriminator loss: 0.788312, acc.: 57.03%] [Generator loss: 0.911017]\n",
      "10005 [Discriminator loss: 0.637997, acc.: 63.28%] [Generator loss: 0.920708]\n",
      "10006 [Discriminator loss: 0.630853, acc.: 67.19%] [Generator loss: 0.910545]\n",
      "10007 [Discriminator loss: 0.611347, acc.: 65.62%] [Generator loss: 0.910843]\n",
      "10008 [Discriminator loss: 0.656121, acc.: 58.59%] [Generator loss: 1.144317]\n",
      "10009 [Discriminator loss: 0.660148, acc.: 61.72%] [Generator loss: 0.804509]\n",
      "10010 [Discriminator loss: 0.727686, acc.: 55.47%] [Generator loss: 0.856240]\n",
      "10011 [Discriminator loss: 0.691630, acc.: 56.25%] [Generator loss: 0.874697]\n",
      "10012 [Discriminator loss: 0.693477, acc.: 59.38%] [Generator loss: 0.875593]\n",
      "10013 [Discriminator loss: 0.734608, acc.: 52.34%] [Generator loss: 0.909217]\n",
      "10014 [Discriminator loss: 0.639322, acc.: 64.84%] [Generator loss: 0.893029]\n",
      "10015 [Discriminator loss: 0.713109, acc.: 57.81%] [Generator loss: 0.844645]\n",
      "10016 [Discriminator loss: 0.750743, acc.: 54.69%] [Generator loss: 1.142290]\n",
      "10017 [Discriminator loss: 0.626058, acc.: 64.84%] [Generator loss: 1.185084]\n",
      "10018 [Discriminator loss: 0.532441, acc.: 76.56%] [Generator loss: 1.258169]\n",
      "10019 [Discriminator loss: 0.620866, acc.: 65.62%] [Generator loss: 1.032682]\n",
      "10020 [Discriminator loss: 0.624779, acc.: 60.16%] [Generator loss: 1.139831]\n",
      "10021 [Discriminator loss: 0.718856, acc.: 56.25%] [Generator loss: 1.231105]\n",
      "10022 [Discriminator loss: 0.673857, acc.: 59.38%] [Generator loss: 0.956678]\n",
      "10023 [Discriminator loss: 0.793907, acc.: 46.88%] [Generator loss: 0.903161]\n",
      "10024 [Discriminator loss: 0.588266, acc.: 74.22%] [Generator loss: 0.963626]\n",
      "10025 [Discriminator loss: 0.762351, acc.: 57.81%] [Generator loss: 0.912752]\n",
      "10026 [Discriminator loss: 0.757414, acc.: 52.34%] [Generator loss: 0.715673]\n",
      "10027 [Discriminator loss: 0.490895, acc.: 81.25%] [Generator loss: 0.795646]\n",
      "10028 [Discriminator loss: 0.728150, acc.: 53.91%] [Generator loss: 0.832304]\n",
      "10029 [Discriminator loss: 0.618541, acc.: 66.41%] [Generator loss: 0.827972]\n",
      "10030 [Discriminator loss: 0.673157, acc.: 58.59%] [Generator loss: 0.745307]\n",
      "10031 [Discriminator loss: 0.688542, acc.: 56.25%] [Generator loss: 0.856393]\n",
      "10032 [Discriminator loss: 0.599590, acc.: 64.84%] [Generator loss: 0.869246]\n",
      "10033 [Discriminator loss: 0.603954, acc.: 67.19%] [Generator loss: 0.905624]\n",
      "10034 [Discriminator loss: 0.731388, acc.: 54.69%] [Generator loss: 0.754013]\n",
      "10035 [Discriminator loss: 0.638840, acc.: 61.72%] [Generator loss: 0.851349]\n",
      "10036 [Discriminator loss: 0.714536, acc.: 46.88%] [Generator loss: 0.750445]\n",
      "10037 [Discriminator loss: 0.571613, acc.: 75.00%] [Generator loss: 0.800077]\n",
      "10038 [Discriminator loss: 0.494119, acc.: 79.69%] [Generator loss: 0.625225]\n",
      "10039 [Discriminator loss: 0.652101, acc.: 70.31%] [Generator loss: 0.630098]\n",
      "10040 [Discriminator loss: 0.638633, acc.: 59.38%] [Generator loss: 0.701987]\n",
      "10041 [Discriminator loss: 0.672788, acc.: 54.69%] [Generator loss: 0.744490]\n",
      "10042 [Discriminator loss: 0.461237, acc.: 80.47%] [Generator loss: 0.742123]\n",
      "10043 [Discriminator loss: 0.674046, acc.: 57.81%] [Generator loss: 0.740844]\n",
      "10044 [Discriminator loss: 0.700073, acc.: 60.16%] [Generator loss: 1.078794]\n",
      "10045 [Discriminator loss: 0.588305, acc.: 66.41%] [Generator loss: 0.891946]\n",
      "10046 [Discriminator loss: 0.540143, acc.: 80.47%] [Generator loss: 0.962046]\n",
      "10047 [Discriminator loss: 0.599032, acc.: 65.62%] [Generator loss: 0.808197]\n",
      "10048 [Discriminator loss: 0.503813, acc.: 75.00%] [Generator loss: 0.716370]\n",
      "10049 [Discriminator loss: 0.627432, acc.: 64.84%] [Generator loss: 0.571426]\n",
      "10050 [Discriminator loss: 0.759540, acc.: 56.25%] [Generator loss: 0.822645]\n",
      "10051 [Discriminator loss: 0.544886, acc.: 71.88%] [Generator loss: 0.880433]\n",
      "10052 [Discriminator loss: 0.623401, acc.: 64.06%] [Generator loss: 0.714239]\n",
      "10053 [Discriminator loss: 0.592501, acc.: 70.31%] [Generator loss: 0.639762]\n",
      "10054 [Discriminator loss: 0.804695, acc.: 37.50%] [Generator loss: 0.853994]\n",
      "10055 [Discriminator loss: 0.548573, acc.: 69.53%] [Generator loss: 0.742061]\n",
      "10056 [Discriminator loss: 0.477861, acc.: 82.03%] [Generator loss: 0.896051]\n",
      "10057 [Discriminator loss: 0.884074, acc.: 37.50%] [Generator loss: 0.794835]\n",
      "10058 [Discriminator loss: 0.459229, acc.: 80.47%] [Generator loss: 0.740148]\n",
      "10059 [Discriminator loss: 0.466427, acc.: 85.16%] [Generator loss: 0.706521]\n",
      "10060 [Discriminator loss: 0.436520, acc.: 85.16%] [Generator loss: 0.707063]\n",
      "10061 [Discriminator loss: 0.578736, acc.: 73.44%] [Generator loss: 0.836235]\n",
      "10062 [Discriminator loss: 0.581811, acc.: 72.66%] [Generator loss: 0.756464]\n",
      "10063 [Discriminator loss: 0.453445, acc.: 85.94%] [Generator loss: 0.661212]\n",
      "10064 [Discriminator loss: 0.325704, acc.: 92.97%] [Generator loss: 0.544230]\n",
      "10065 [Discriminator loss: 0.640539, acc.: 67.19%] [Generator loss: 0.581819]\n",
      "10066 [Discriminator loss: 0.471205, acc.: 82.81%] [Generator loss: 0.789282]\n",
      "10067 [Discriminator loss: 0.468222, acc.: 85.16%] [Generator loss: 0.807612]\n",
      "10068 [Discriminator loss: 0.896550, acc.: 33.59%] [Generator loss: 0.716572]\n",
      "10069 [Discriminator loss: 0.470979, acc.: 78.91%] [Generator loss: 0.707354]\n",
      "10070 [Discriminator loss: 0.434200, acc.: 81.25%] [Generator loss: 1.079362]\n",
      "10071 [Discriminator loss: 0.946304, acc.: 35.94%] [Generator loss: 0.985433]\n",
      "10072 [Discriminator loss: 0.608152, acc.: 69.53%] [Generator loss: 1.208638]\n",
      "10073 [Discriminator loss: 0.670429, acc.: 57.03%] [Generator loss: 0.938261]\n",
      "10074 [Discriminator loss: 0.575923, acc.: 71.09%] [Generator loss: 0.815784]\n",
      "10075 [Discriminator loss: 0.658827, acc.: 63.28%] [Generator loss: 0.836767]\n",
      "10076 [Discriminator loss: 0.516464, acc.: 79.69%] [Generator loss: 0.825481]\n",
      "10077 [Discriminator loss: 0.577313, acc.: 73.44%] [Generator loss: 0.856500]\n",
      "10078 [Discriminator loss: 0.873772, acc.: 38.28%] [Generator loss: 0.873400]\n",
      "10079 [Discriminator loss: 0.656020, acc.: 53.91%] [Generator loss: 0.903730]\n",
      "10080 [Discriminator loss: 0.757716, acc.: 52.34%] [Generator loss: 0.902489]\n",
      "10081 [Discriminator loss: 0.658142, acc.: 64.84%] [Generator loss: 1.069758]\n",
      "10082 [Discriminator loss: 0.900682, acc.: 42.97%] [Generator loss: 0.722282]\n",
      "10083 [Discriminator loss: 0.561262, acc.: 82.03%] [Generator loss: 0.728591]\n",
      "10084 [Discriminator loss: 0.704838, acc.: 54.69%] [Generator loss: 0.734791]\n",
      "10085 [Discriminator loss: 0.584412, acc.: 71.09%] [Generator loss: 0.906139]\n",
      "10086 [Discriminator loss: 0.483478, acc.: 77.34%] [Generator loss: 0.880357]\n",
      "10087 [Discriminator loss: 0.613602, acc.: 65.62%] [Generator loss: 0.992477]\n",
      "10088 [Discriminator loss: 0.468267, acc.: 83.59%] [Generator loss: 1.030013]\n",
      "10089 [Discriminator loss: 0.812854, acc.: 39.06%] [Generator loss: 0.925094]\n",
      "10090 [Discriminator loss: 0.692080, acc.: 54.69%] [Generator loss: 1.039271]\n",
      "10091 [Discriminator loss: 0.667277, acc.: 60.16%] [Generator loss: 1.205710]\n",
      "10092 [Discriminator loss: 0.728208, acc.: 60.94%] [Generator loss: 1.077707]\n",
      "10093 [Discriminator loss: 0.657202, acc.: 60.16%] [Generator loss: 0.937168]\n",
      "10094 [Discriminator loss: 0.740175, acc.: 58.59%] [Generator loss: 0.870277]\n",
      "10095 [Discriminator loss: 0.682480, acc.: 58.59%] [Generator loss: 0.850725]\n",
      "10096 [Discriminator loss: 0.607390, acc.: 67.19%] [Generator loss: 0.682512]\n",
      "10097 [Discriminator loss: 0.621359, acc.: 67.19%] [Generator loss: 0.904956]\n",
      "10098 [Discriminator loss: 0.807639, acc.: 42.19%] [Generator loss: 0.805634]\n",
      "10099 [Discriminator loss: 0.542503, acc.: 74.22%] [Generator loss: 0.782695]\n",
      "10100 [Discriminator loss: 0.730360, acc.: 48.44%] [Generator loss: 0.869357]\n",
      "10101 [Discriminator loss: 0.522287, acc.: 78.12%] [Generator loss: 0.840733]\n",
      "10102 [Discriminator loss: 0.870956, acc.: 42.97%] [Generator loss: 1.049475]\n",
      "10103 [Discriminator loss: 0.754823, acc.: 46.88%] [Generator loss: 1.017220]\n",
      "10104 [Discriminator loss: 0.703665, acc.: 56.25%] [Generator loss: 0.942538]\n",
      "10105 [Discriminator loss: 0.604188, acc.: 68.75%] [Generator loss: 0.670661]\n",
      "10106 [Discriminator loss: 0.798922, acc.: 45.31%] [Generator loss: 0.935746]\n",
      "10107 [Discriminator loss: 0.536779, acc.: 74.22%] [Generator loss: 1.078088]\n",
      "10108 [Discriminator loss: 0.658393, acc.: 62.50%] [Generator loss: 0.888570]\n",
      "10109 [Discriminator loss: 0.895676, acc.: 50.00%] [Generator loss: 0.937002]\n",
      "10110 [Discriminator loss: 0.664409, acc.: 64.84%] [Generator loss: 0.976401]\n",
      "10111 [Discriminator loss: 0.627245, acc.: 67.19%] [Generator loss: 0.928733]\n",
      "10112 [Discriminator loss: 0.666268, acc.: 56.25%] [Generator loss: 0.912597]\n",
      "10113 [Discriminator loss: 0.683139, acc.: 58.59%] [Generator loss: 0.899908]\n",
      "10114 [Discriminator loss: 0.711643, acc.: 59.38%] [Generator loss: 0.856664]\n",
      "10115 [Discriminator loss: 0.625231, acc.: 68.75%] [Generator loss: 0.821932]\n",
      "10116 [Discriminator loss: 0.533924, acc.: 75.00%] [Generator loss: 0.940050]\n",
      "10117 [Discriminator loss: 0.578777, acc.: 73.44%] [Generator loss: 0.948530]\n",
      "10118 [Discriminator loss: 0.726130, acc.: 53.12%] [Generator loss: 0.722399]\n",
      "10119 [Discriminator loss: 0.736364, acc.: 53.12%] [Generator loss: 0.908414]\n",
      "10120 [Discriminator loss: 0.752626, acc.: 53.12%] [Generator loss: 0.930260]\n",
      "10121 [Discriminator loss: 0.674797, acc.: 63.28%] [Generator loss: 0.800993]\n",
      "10122 [Discriminator loss: 0.701571, acc.: 54.69%] [Generator loss: 0.723155]\n",
      "10123 [Discriminator loss: 0.822464, acc.: 42.97%] [Generator loss: 0.792853]\n",
      "10124 [Discriminator loss: 0.698180, acc.: 63.28%] [Generator loss: 0.833998]\n",
      "10125 [Discriminator loss: 0.798939, acc.: 49.22%] [Generator loss: 1.141369]\n",
      "10126 [Discriminator loss: 0.672220, acc.: 62.50%] [Generator loss: 1.250313]\n",
      "10127 [Discriminator loss: 0.766330, acc.: 53.91%] [Generator loss: 0.933281]\n",
      "10128 [Discriminator loss: 0.723617, acc.: 57.03%] [Generator loss: 0.904772]\n",
      "10129 [Discriminator loss: 0.648287, acc.: 60.16%] [Generator loss: 0.875815]\n",
      "10130 [Discriminator loss: 0.638583, acc.: 71.88%] [Generator loss: 0.884855]\n",
      "10131 [Discriminator loss: 0.746106, acc.: 53.12%] [Generator loss: 0.971375]\n",
      "10132 [Discriminator loss: 0.613985, acc.: 65.62%] [Generator loss: 1.024921]\n",
      "10133 [Discriminator loss: 0.569788, acc.: 70.31%] [Generator loss: 0.935843]\n",
      "10134 [Discriminator loss: 0.895316, acc.: 35.16%] [Generator loss: 0.788754]\n",
      "10135 [Discriminator loss: 0.535099, acc.: 74.22%] [Generator loss: 0.861487]\n",
      "10136 [Discriminator loss: 0.531268, acc.: 80.47%] [Generator loss: 0.820355]\n",
      "10137 [Discriminator loss: 0.730836, acc.: 54.69%] [Generator loss: 0.845331]\n",
      "10138 [Discriminator loss: 0.650788, acc.: 64.84%] [Generator loss: 0.823304]\n",
      "10139 [Discriminator loss: 0.812710, acc.: 40.62%] [Generator loss: 0.748893]\n",
      "10140 [Discriminator loss: 0.620726, acc.: 63.28%] [Generator loss: 0.893090]\n",
      "10141 [Discriminator loss: 0.671485, acc.: 52.34%] [Generator loss: 0.946394]\n",
      "10142 [Discriminator loss: 0.615334, acc.: 67.19%] [Generator loss: 0.810822]\n",
      "10143 [Discriminator loss: 0.836630, acc.: 42.97%] [Generator loss: 0.880675]\n",
      "10144 [Discriminator loss: 0.633709, acc.: 65.62%] [Generator loss: 0.868019]\n",
      "10145 [Discriminator loss: 0.643702, acc.: 63.28%] [Generator loss: 1.056137]\n",
      "10146 [Discriminator loss: 0.696195, acc.: 52.34%] [Generator loss: 1.255799]\n",
      "10147 [Discriminator loss: 0.698549, acc.: 56.25%] [Generator loss: 1.150818]\n",
      "10148 [Discriminator loss: 0.665000, acc.: 64.06%] [Generator loss: 1.060035]\n",
      "10149 [Discriminator loss: 0.578676, acc.: 68.75%] [Generator loss: 1.075529]\n",
      "10150 [Discriminator loss: 0.574537, acc.: 77.34%] [Generator loss: 0.805418]\n",
      "10151 [Discriminator loss: 0.462480, acc.: 85.94%] [Generator loss: 0.730205]\n",
      "10152 [Discriminator loss: 0.641034, acc.: 66.41%] [Generator loss: 0.669298]\n",
      "10153 [Discriminator loss: 0.589355, acc.: 67.97%] [Generator loss: 0.814698]\n",
      "10154 [Discriminator loss: 0.633308, acc.: 67.19%] [Generator loss: 0.740447]\n",
      "10155 [Discriminator loss: 0.798292, acc.: 49.22%] [Generator loss: 0.858611]\n",
      "10156 [Discriminator loss: 0.618963, acc.: 68.75%] [Generator loss: 0.744959]\n",
      "10157 [Discriminator loss: 0.674063, acc.: 59.38%] [Generator loss: 0.851382]\n",
      "10158 [Discriminator loss: 0.667363, acc.: 59.38%] [Generator loss: 0.881795]\n",
      "10159 [Discriminator loss: 0.989174, acc.: 33.59%] [Generator loss: 0.683064]\n",
      "10160 [Discriminator loss: 0.592313, acc.: 69.53%] [Generator loss: 0.743831]\n",
      "10161 [Discriminator loss: 0.689853, acc.: 56.25%] [Generator loss: 0.834063]\n",
      "10162 [Discriminator loss: 0.633538, acc.: 64.84%] [Generator loss: 0.963324]\n",
      "10163 [Discriminator loss: 0.919927, acc.: 35.16%] [Generator loss: 0.781884]\n",
      "10164 [Discriminator loss: 0.596637, acc.: 67.19%] [Generator loss: 0.722081]\n",
      "10165 [Discriminator loss: 0.759332, acc.: 52.34%] [Generator loss: 0.714102]\n",
      "10166 [Discriminator loss: 0.518333, acc.: 78.91%] [Generator loss: 0.854626]\n",
      "10167 [Discriminator loss: 0.685177, acc.: 57.03%] [Generator loss: 0.882137]\n",
      "10168 [Discriminator loss: 0.800314, acc.: 47.66%] [Generator loss: 0.785289]\n",
      "10169 [Discriminator loss: 0.596657, acc.: 70.31%] [Generator loss: 0.924245]\n",
      "10170 [Discriminator loss: 0.608907, acc.: 72.66%] [Generator loss: 0.799386]\n",
      "10171 [Discriminator loss: 0.406022, acc.: 86.72%] [Generator loss: 0.828595]\n",
      "10172 [Discriminator loss: 0.740219, acc.: 54.69%] [Generator loss: 0.754406]\n",
      "10173 [Discriminator loss: 0.738302, acc.: 53.91%] [Generator loss: 0.800076]\n",
      "10174 [Discriminator loss: 0.644690, acc.: 64.84%] [Generator loss: 0.944502]\n",
      "10175 [Discriminator loss: 0.691079, acc.: 60.16%] [Generator loss: 0.912672]\n",
      "10176 [Discriminator loss: 0.712525, acc.: 54.69%] [Generator loss: 0.678636]\n",
      "10177 [Discriminator loss: 0.709713, acc.: 53.12%] [Generator loss: 0.636679]\n",
      "10178 [Discriminator loss: 0.567796, acc.: 69.53%] [Generator loss: 0.752040]\n",
      "10179 [Discriminator loss: 0.666377, acc.: 62.50%] [Generator loss: 1.046935]\n",
      "10180 [Discriminator loss: 0.640529, acc.: 64.06%] [Generator loss: 0.922444]\n",
      "10181 [Discriminator loss: 0.700079, acc.: 55.47%] [Generator loss: 0.708758]\n",
      "10182 [Discriminator loss: 0.714295, acc.: 55.47%] [Generator loss: 0.746104]\n",
      "10183 [Discriminator loss: 0.714788, acc.: 56.25%] [Generator loss: 1.016052]\n",
      "10184 [Discriminator loss: 0.729225, acc.: 54.69%] [Generator loss: 0.830835]\n",
      "10185 [Discriminator loss: 0.649550, acc.: 64.84%] [Generator loss: 1.099835]\n",
      "10186 [Discriminator loss: 0.790080, acc.: 53.91%] [Generator loss: 0.929007]\n",
      "10187 [Discriminator loss: 0.679650, acc.: 60.16%] [Generator loss: 0.908285]\n",
      "10188 [Discriminator loss: 0.694225, acc.: 54.69%] [Generator loss: 0.883613]\n",
      "10189 [Discriminator loss: 0.691547, acc.: 58.59%] [Generator loss: 0.937848]\n",
      "10190 [Discriminator loss: 0.707835, acc.: 49.22%] [Generator loss: 1.083127]\n",
      "10191 [Discriminator loss: 0.553955, acc.: 74.22%] [Generator loss: 0.869702]\n",
      "10192 [Discriminator loss: 0.577148, acc.: 74.22%] [Generator loss: 0.817810]\n",
      "10193 [Discriminator loss: 0.610596, acc.: 65.62%] [Generator loss: 0.829082]\n",
      "10194 [Discriminator loss: 0.708392, acc.: 59.38%] [Generator loss: 0.956212]\n",
      "10195 [Discriminator loss: 0.547254, acc.: 76.56%] [Generator loss: 1.165733]\n",
      "10196 [Discriminator loss: 0.537987, acc.: 76.56%] [Generator loss: 1.039104]\n",
      "10197 [Discriminator loss: 0.543156, acc.: 74.22%] [Generator loss: 0.857967]\n",
      "10198 [Discriminator loss: 0.494898, acc.: 77.34%] [Generator loss: 0.835288]\n",
      "10199 [Discriminator loss: 0.687044, acc.: 57.81%] [Generator loss: 0.830866]\n",
      "10200 [Discriminator loss: 0.604094, acc.: 73.44%] [Generator loss: 0.753780]\n",
      "10201 [Discriminator loss: 0.471387, acc.: 82.03%] [Generator loss: 0.702533]\n",
      "10202 [Discriminator loss: 0.755185, acc.: 54.69%] [Generator loss: 0.671741]\n",
      "10203 [Discriminator loss: 0.702461, acc.: 57.81%] [Generator loss: 0.607125]\n",
      "10204 [Discriminator loss: 0.569922, acc.: 70.31%] [Generator loss: 0.752376]\n",
      "10205 [Discriminator loss: 0.537420, acc.: 77.34%] [Generator loss: 0.760621]\n",
      "10206 [Discriminator loss: 0.599006, acc.: 63.28%] [Generator loss: 0.779530]\n",
      "10207 [Discriminator loss: 0.742294, acc.: 53.91%] [Generator loss: 0.738200]\n",
      "10208 [Discriminator loss: 0.677543, acc.: 59.38%] [Generator loss: 0.846115]\n",
      "10209 [Discriminator loss: 0.700820, acc.: 55.47%] [Generator loss: 0.835585]\n",
      "10210 [Discriminator loss: 0.749087, acc.: 55.47%] [Generator loss: 0.872751]\n",
      "10211 [Discriminator loss: 0.422753, acc.: 85.94%] [Generator loss: 0.793317]\n",
      "10212 [Discriminator loss: 0.571842, acc.: 70.31%] [Generator loss: 0.768862]\n",
      "10213 [Discriminator loss: 0.447905, acc.: 78.12%] [Generator loss: 0.669179]\n",
      "10214 [Discriminator loss: 0.593700, acc.: 69.53%] [Generator loss: 0.745003]\n",
      "10215 [Discriminator loss: 0.587788, acc.: 65.62%] [Generator loss: 0.699825]\n",
      "10216 [Discriminator loss: 0.493631, acc.: 79.69%] [Generator loss: 0.673698]\n",
      "10217 [Discriminator loss: 0.627112, acc.: 64.06%] [Generator loss: 0.673396]\n",
      "10218 [Discriminator loss: 0.629407, acc.: 69.53%] [Generator loss: 0.918092]\n",
      "10219 [Discriminator loss: 0.684119, acc.: 59.38%] [Generator loss: 0.974240]\n",
      "10220 [Discriminator loss: 0.603091, acc.: 71.09%] [Generator loss: 1.188330]\n",
      "10221 [Discriminator loss: 0.642719, acc.: 65.62%] [Generator loss: 0.936007]\n",
      "10222 [Discriminator loss: 0.998209, acc.: 38.28%] [Generator loss: 0.826018]\n",
      "10223 [Discriminator loss: 0.811949, acc.: 45.31%] [Generator loss: 0.757802]\n",
      "10224 [Discriminator loss: 0.642050, acc.: 64.06%] [Generator loss: 0.818753]\n",
      "10225 [Discriminator loss: 0.888797, acc.: 35.94%] [Generator loss: 0.820891]\n",
      "10226 [Discriminator loss: 0.541862, acc.: 75.00%] [Generator loss: 0.860195]\n",
      "10227 [Discriminator loss: 0.726837, acc.: 57.03%] [Generator loss: 0.949715]\n",
      "10228 [Discriminator loss: 0.525943, acc.: 75.00%] [Generator loss: 0.885640]\n",
      "10229 [Discriminator loss: 0.645452, acc.: 64.06%] [Generator loss: 0.973530]\n",
      "10230 [Discriminator loss: 0.599569, acc.: 65.62%] [Generator loss: 0.887389]\n",
      "10231 [Discriminator loss: 0.581468, acc.: 64.06%] [Generator loss: 1.113386]\n",
      "10232 [Discriminator loss: 0.650542, acc.: 63.28%] [Generator loss: 1.140995]\n",
      "10233 [Discriminator loss: 0.763613, acc.: 53.91%] [Generator loss: 0.886336]\n",
      "10234 [Discriminator loss: 0.589788, acc.: 73.44%] [Generator loss: 1.038279]\n",
      "10235 [Discriminator loss: 0.648394, acc.: 64.84%] [Generator loss: 0.983709]\n",
      "10236 [Discriminator loss: 0.573032, acc.: 73.44%] [Generator loss: 0.744325]\n",
      "10237 [Discriminator loss: 0.622652, acc.: 66.41%] [Generator loss: 0.825378]\n",
      "10238 [Discriminator loss: 1.023463, acc.: 35.16%] [Generator loss: 0.802434]\n",
      "10239 [Discriminator loss: 0.585780, acc.: 69.53%] [Generator loss: 0.945076]\n",
      "10240 [Discriminator loss: 0.662933, acc.: 57.81%] [Generator loss: 0.962083]\n",
      "10241 [Discriminator loss: 0.630134, acc.: 60.94%] [Generator loss: 1.012301]\n",
      "10242 [Discriminator loss: 0.671162, acc.: 62.50%] [Generator loss: 0.811372]\n",
      "10243 [Discriminator loss: 0.841037, acc.: 46.09%] [Generator loss: 0.795308]\n",
      "10244 [Discriminator loss: 0.635636, acc.: 65.62%] [Generator loss: 0.874762]\n",
      "10245 [Discriminator loss: 0.881838, acc.: 43.75%] [Generator loss: 0.701929]\n",
      "10246 [Discriminator loss: 0.583658, acc.: 73.44%] [Generator loss: 1.008158]\n",
      "10247 [Discriminator loss: 0.547752, acc.: 72.66%] [Generator loss: 1.216112]\n",
      "10248 [Discriminator loss: 0.862187, acc.: 35.94%] [Generator loss: 0.753415]\n",
      "10249 [Discriminator loss: 0.738498, acc.: 53.12%] [Generator loss: 1.037382]\n",
      "10250 [Discriminator loss: 0.718836, acc.: 50.00%] [Generator loss: 0.975272]\n",
      "10251 [Discriminator loss: 0.741807, acc.: 50.78%] [Generator loss: 0.931835]\n",
      "10252 [Discriminator loss: 0.565361, acc.: 66.41%] [Generator loss: 1.091194]\n",
      "10253 [Discriminator loss: 0.671756, acc.: 62.50%] [Generator loss: 0.882584]\n",
      "10254 [Discriminator loss: 0.762142, acc.: 52.34%] [Generator loss: 1.167372]\n",
      "10255 [Discriminator loss: 0.616054, acc.: 57.81%] [Generator loss: 1.145366]\n",
      "10256 [Discriminator loss: 0.556412, acc.: 74.22%] [Generator loss: 1.028797]\n",
      "10257 [Discriminator loss: 0.597135, acc.: 67.19%] [Generator loss: 0.702308]\n",
      "10258 [Discriminator loss: 0.566155, acc.: 69.53%] [Generator loss: 0.842410]\n",
      "10259 [Discriminator loss: 0.513603, acc.: 79.69%] [Generator loss: 0.759473]\n",
      "10260 [Discriminator loss: 0.667240, acc.: 60.94%] [Generator loss: 0.736613]\n",
      "10261 [Discriminator loss: 0.559224, acc.: 71.09%] [Generator loss: 0.772655]\n",
      "10262 [Discriminator loss: 0.603994, acc.: 64.84%] [Generator loss: 0.761633]\n",
      "10263 [Discriminator loss: 0.776669, acc.: 50.78%] [Generator loss: 0.635476]\n",
      "10264 [Discriminator loss: 0.570571, acc.: 75.78%] [Generator loss: 0.565997]\n",
      "10265 [Discriminator loss: 0.622812, acc.: 58.59%] [Generator loss: 0.580410]\n",
      "10266 [Discriminator loss: 0.538544, acc.: 75.00%] [Generator loss: 0.679122]\n",
      "10267 [Discriminator loss: 0.650279, acc.: 62.50%] [Generator loss: 0.828209]\n",
      "10268 [Discriminator loss: 0.764889, acc.: 50.78%] [Generator loss: 0.897514]\n",
      "10269 [Discriminator loss: 0.694319, acc.: 56.25%] [Generator loss: 0.783853]\n",
      "10270 [Discriminator loss: 0.830097, acc.: 40.62%] [Generator loss: 0.925504]\n",
      "10271 [Discriminator loss: 0.660365, acc.: 61.72%] [Generator loss: 0.797331]\n",
      "10272 [Discriminator loss: 0.721273, acc.: 57.03%] [Generator loss: 0.955926]\n",
      "10273 [Discriminator loss: 0.804619, acc.: 52.34%] [Generator loss: 0.875467]\n",
      "10274 [Discriminator loss: 0.643622, acc.: 63.28%] [Generator loss: 0.977279]\n",
      "10275 [Discriminator loss: 0.545238, acc.: 76.56%] [Generator loss: 0.869274]\n",
      "10276 [Discriminator loss: 0.685393, acc.: 57.81%] [Generator loss: 0.891906]\n",
      "10277 [Discriminator loss: 0.638297, acc.: 63.28%] [Generator loss: 0.821393]\n",
      "10278 [Discriminator loss: 0.668641, acc.: 58.59%] [Generator loss: 1.049548]\n",
      "10279 [Discriminator loss: 0.720956, acc.: 51.56%] [Generator loss: 0.864303]\n",
      "10280 [Discriminator loss: 0.730050, acc.: 50.00%] [Generator loss: 0.998368]\n",
      "10281 [Discriminator loss: 0.618023, acc.: 68.75%] [Generator loss: 0.985211]\n",
      "10282 [Discriminator loss: 0.617370, acc.: 62.50%] [Generator loss: 0.938837]\n",
      "10283 [Discriminator loss: 0.707926, acc.: 55.47%] [Generator loss: 0.890787]\n",
      "10284 [Discriminator loss: 0.493896, acc.: 84.38%] [Generator loss: 0.807160]\n",
      "10285 [Discriminator loss: 0.590840, acc.: 72.66%] [Generator loss: 0.945745]\n",
      "10286 [Discriminator loss: 0.616939, acc.: 64.06%] [Generator loss: 0.734610]\n",
      "10287 [Discriminator loss: 0.689218, acc.: 54.69%] [Generator loss: 0.910424]\n",
      "10288 [Discriminator loss: 0.642775, acc.: 62.50%] [Generator loss: 0.828569]\n",
      "10289 [Discriminator loss: 0.721537, acc.: 50.78%] [Generator loss: 0.722453]\n",
      "10290 [Discriminator loss: 0.708950, acc.: 60.94%] [Generator loss: 0.857950]\n",
      "10291 [Discriminator loss: 0.613257, acc.: 74.22%] [Generator loss: 0.961362]\n",
      "10292 [Discriminator loss: 0.651091, acc.: 63.28%] [Generator loss: 1.085832]\n",
      "10293 [Discriminator loss: 0.716844, acc.: 55.47%] [Generator loss: 1.166557]\n",
      "10294 [Discriminator loss: 0.733427, acc.: 49.22%] [Generator loss: 1.018090]\n",
      "10295 [Discriminator loss: 1.001349, acc.: 27.34%] [Generator loss: 1.000259]\n",
      "10296 [Discriminator loss: 0.629834, acc.: 60.16%] [Generator loss: 0.983491]\n",
      "10297 [Discriminator loss: 0.612390, acc.: 70.31%] [Generator loss: 0.849290]\n",
      "10298 [Discriminator loss: 0.612319, acc.: 67.97%] [Generator loss: 0.690654]\n",
      "10299 [Discriminator loss: 0.593529, acc.: 65.62%] [Generator loss: 0.766447]\n",
      "10300 [Discriminator loss: 0.541746, acc.: 73.44%] [Generator loss: 0.692278]\n",
      "10301 [Discriminator loss: 0.684459, acc.: 53.12%] [Generator loss: 0.747133]\n",
      "10302 [Discriminator loss: 0.649147, acc.: 60.16%] [Generator loss: 0.750566]\n",
      "10303 [Discriminator loss: 0.627587, acc.: 64.06%] [Generator loss: 0.722872]\n",
      "10304 [Discriminator loss: 0.747131, acc.: 46.09%] [Generator loss: 0.794118]\n",
      "10305 [Discriminator loss: 0.638631, acc.: 66.41%] [Generator loss: 1.054381]\n",
      "10306 [Discriminator loss: 0.754823, acc.: 49.22%] [Generator loss: 0.788037]\n",
      "10307 [Discriminator loss: 0.785330, acc.: 48.44%] [Generator loss: 0.927800]\n",
      "10308 [Discriminator loss: 0.735196, acc.: 53.12%] [Generator loss: 0.873489]\n",
      "10309 [Discriminator loss: 0.693162, acc.: 57.03%] [Generator loss: 1.075733]\n",
      "10310 [Discriminator loss: 0.614415, acc.: 60.94%] [Generator loss: 1.006511]\n",
      "10311 [Discriminator loss: 0.745286, acc.: 51.56%] [Generator loss: 0.848872]\n",
      "10312 [Discriminator loss: 0.592128, acc.: 71.88%] [Generator loss: 0.761431]\n",
      "10313 [Discriminator loss: 0.773178, acc.: 54.69%] [Generator loss: 0.920209]\n",
      "10314 [Discriminator loss: 0.698097, acc.: 50.78%] [Generator loss: 0.890669]\n",
      "10315 [Discriminator loss: 0.550712, acc.: 74.22%] [Generator loss: 0.830994]\n",
      "10316 [Discriminator loss: 0.716792, acc.: 56.25%] [Generator loss: 0.839910]\n",
      "10317 [Discriminator loss: 0.606672, acc.: 66.41%] [Generator loss: 0.871695]\n",
      "10318 [Discriminator loss: 0.681355, acc.: 56.25%] [Generator loss: 0.811352]\n",
      "10319 [Discriminator loss: 0.942439, acc.: 38.28%] [Generator loss: 0.713444]\n",
      "10320 [Discriminator loss: 0.679361, acc.: 65.62%] [Generator loss: 0.837839]\n",
      "10321 [Discriminator loss: 0.762339, acc.: 49.22%] [Generator loss: 0.797061]\n",
      "10322 [Discriminator loss: 0.566847, acc.: 71.88%] [Generator loss: 0.939833]\n",
      "10323 [Discriminator loss: 0.603047, acc.: 68.75%] [Generator loss: 0.749244]\n",
      "10324 [Discriminator loss: 0.656717, acc.: 66.41%] [Generator loss: 0.725231]\n",
      "10325 [Discriminator loss: 0.802936, acc.: 43.75%] [Generator loss: 0.795639]\n",
      "10326 [Discriminator loss: 0.671151, acc.: 61.72%] [Generator loss: 0.911010]\n",
      "10327 [Discriminator loss: 0.627149, acc.: 64.06%] [Generator loss: 0.938347]\n",
      "10328 [Discriminator loss: 0.630402, acc.: 71.88%] [Generator loss: 0.967348]\n",
      "10329 [Discriminator loss: 0.742365, acc.: 60.16%] [Generator loss: 1.007684]\n",
      "10330 [Discriminator loss: 0.582266, acc.: 68.75%] [Generator loss: 1.130921]\n",
      "10331 [Discriminator loss: 0.741072, acc.: 57.03%] [Generator loss: 0.994494]\n",
      "10332 [Discriminator loss: 0.655426, acc.: 65.62%] [Generator loss: 1.071425]\n",
      "10333 [Discriminator loss: 0.581757, acc.: 71.09%] [Generator loss: 1.040761]\n",
      "10334 [Discriminator loss: 0.561820, acc.: 71.09%] [Generator loss: 0.861591]\n",
      "10335 [Discriminator loss: 0.691375, acc.: 59.38%] [Generator loss: 0.898638]\n",
      "10336 [Discriminator loss: 0.615804, acc.: 70.31%] [Generator loss: 0.861326]\n",
      "10337 [Discriminator loss: 0.722186, acc.: 57.03%] [Generator loss: 1.033791]\n",
      "10338 [Discriminator loss: 0.670636, acc.: 60.16%] [Generator loss: 0.939790]\n",
      "10339 [Discriminator loss: 0.613799, acc.: 65.62%] [Generator loss: 0.783723]\n",
      "10340 [Discriminator loss: 0.637506, acc.: 68.75%] [Generator loss: 0.775690]\n",
      "10341 [Discriminator loss: 0.577924, acc.: 78.12%] [Generator loss: 0.867816]\n",
      "10342 [Discriminator loss: 0.542640, acc.: 72.66%] [Generator loss: 0.750897]\n",
      "10343 [Discriminator loss: 0.792839, acc.: 53.12%] [Generator loss: 0.892038]\n",
      "10344 [Discriminator loss: 0.609810, acc.: 70.31%] [Generator loss: 0.997546]\n",
      "10345 [Discriminator loss: 0.648498, acc.: 60.94%] [Generator loss: 0.789846]\n",
      "10346 [Discriminator loss: 0.769762, acc.: 52.34%] [Generator loss: 0.778579]\n",
      "10347 [Discriminator loss: 0.817089, acc.: 57.81%] [Generator loss: 1.139575]\n",
      "10348 [Discriminator loss: 0.725445, acc.: 55.47%] [Generator loss: 1.107385]\n",
      "10349 [Discriminator loss: 0.684687, acc.: 61.72%] [Generator loss: 0.961753]\n",
      "10350 [Discriminator loss: 0.690663, acc.: 56.25%] [Generator loss: 0.988642]\n",
      "10351 [Discriminator loss: 0.710322, acc.: 55.47%] [Generator loss: 1.216750]\n",
      "10352 [Discriminator loss: 0.628236, acc.: 61.72%] [Generator loss: 0.997129]\n",
      "10353 [Discriminator loss: 0.714184, acc.: 55.47%] [Generator loss: 0.767072]\n",
      "10354 [Discriminator loss: 0.524159, acc.: 78.91%] [Generator loss: 0.756612]\n",
      "10355 [Discriminator loss: 0.515152, acc.: 79.69%] [Generator loss: 0.798347]\n",
      "10356 [Discriminator loss: 0.609577, acc.: 69.53%] [Generator loss: 0.917519]\n",
      "10357 [Discriminator loss: 0.563014, acc.: 71.09%] [Generator loss: 0.844840]\n",
      "10358 [Discriminator loss: 0.653053, acc.: 70.31%] [Generator loss: 1.007543]\n",
      "10359 [Discriminator loss: 0.517753, acc.: 76.56%] [Generator loss: 1.151166]\n",
      "10360 [Discriminator loss: 0.692624, acc.: 56.25%] [Generator loss: 0.985551]\n",
      "10361 [Discriminator loss: 0.805434, acc.: 44.53%] [Generator loss: 1.016241]\n",
      "10362 [Discriminator loss: 0.568879, acc.: 69.53%] [Generator loss: 1.039722]\n",
      "10363 [Discriminator loss: 0.559763, acc.: 75.00%] [Generator loss: 0.878845]\n",
      "10364 [Discriminator loss: 0.604436, acc.: 73.44%] [Generator loss: 0.945635]\n",
      "10365 [Discriminator loss: 0.633615, acc.: 66.41%] [Generator loss: 0.806985]\n",
      "10366 [Discriminator loss: 0.688160, acc.: 60.16%] [Generator loss: 0.882943]\n",
      "10367 [Discriminator loss: 0.650352, acc.: 57.03%] [Generator loss: 0.928687]\n",
      "10368 [Discriminator loss: 0.583979, acc.: 67.97%] [Generator loss: 0.837062]\n",
      "10369 [Discriminator loss: 0.739204, acc.: 53.12%] [Generator loss: 0.847866]\n",
      "10370 [Discriminator loss: 0.689905, acc.: 63.28%] [Generator loss: 0.736123]\n",
      "10371 [Discriminator loss: 0.692042, acc.: 57.03%] [Generator loss: 0.734295]\n",
      "10372 [Discriminator loss: 0.618224, acc.: 63.28%] [Generator loss: 0.716639]\n",
      "10373 [Discriminator loss: 0.510316, acc.: 81.25%] [Generator loss: 0.813539]\n",
      "10374 [Discriminator loss: 0.738151, acc.: 56.25%] [Generator loss: 0.830564]\n",
      "10375 [Discriminator loss: 0.584852, acc.: 71.09%] [Generator loss: 0.817443]\n",
      "10376 [Discriminator loss: 0.483184, acc.: 82.81%] [Generator loss: 0.731272]\n",
      "10377 [Discriminator loss: 0.546508, acc.: 71.09%] [Generator loss: 0.913171]\n",
      "10378 [Discriminator loss: 0.536926, acc.: 71.88%] [Generator loss: 0.760212]\n",
      "10379 [Discriminator loss: 0.711117, acc.: 56.25%] [Generator loss: 0.838872]\n",
      "10380 [Discriminator loss: 0.499316, acc.: 75.00%] [Generator loss: 0.837579]\n",
      "10381 [Discriminator loss: 0.626783, acc.: 65.62%] [Generator loss: 1.048610]\n",
      "10382 [Discriminator loss: 0.413540, acc.: 85.16%] [Generator loss: 0.914538]\n",
      "10383 [Discriminator loss: 0.636432, acc.: 61.72%] [Generator loss: 0.675863]\n",
      "10384 [Discriminator loss: 0.666596, acc.: 67.19%] [Generator loss: 0.761808]\n",
      "10385 [Discriminator loss: 0.588833, acc.: 68.75%] [Generator loss: 0.687363]\n",
      "10386 [Discriminator loss: 0.605574, acc.: 67.19%] [Generator loss: 0.646335]\n",
      "10387 [Discriminator loss: 0.605857, acc.: 67.19%] [Generator loss: 0.759408]\n",
      "10388 [Discriminator loss: 0.578408, acc.: 66.41%] [Generator loss: 0.882703]\n",
      "10389 [Discriminator loss: 0.539163, acc.: 74.22%] [Generator loss: 0.627531]\n",
      "10390 [Discriminator loss: 0.317861, acc.: 95.31%] [Generator loss: 0.541895]\n",
      "10391 [Discriminator loss: 0.658487, acc.: 58.59%] [Generator loss: 0.479677]\n",
      "10392 [Discriminator loss: 0.694879, acc.: 63.28%] [Generator loss: 0.757741]\n",
      "10393 [Discriminator loss: 0.443696, acc.: 89.84%] [Generator loss: 1.136917]\n",
      "10394 [Discriminator loss: 0.883724, acc.: 42.97%] [Generator loss: 0.829393]\n",
      "10395 [Discriminator loss: 0.754526, acc.: 55.47%] [Generator loss: 0.840452]\n",
      "10396 [Discriminator loss: 0.468848, acc.: 82.03%] [Generator loss: 0.885815]\n",
      "10397 [Discriminator loss: 0.477569, acc.: 83.59%] [Generator loss: 1.171178]\n",
      "10398 [Discriminator loss: 0.486724, acc.: 82.81%] [Generator loss: 0.994593]\n",
      "10399 [Discriminator loss: 0.581020, acc.: 70.31%] [Generator loss: 0.931483]\n",
      "10400 [Discriminator loss: 0.793102, acc.: 54.69%] [Generator loss: 0.787508]\n",
      "10401 [Discriminator loss: 0.639543, acc.: 64.06%] [Generator loss: 1.142948]\n",
      "10402 [Discriminator loss: 1.125278, acc.: 35.94%] [Generator loss: 0.676812]\n",
      "10403 [Discriminator loss: 0.595991, acc.: 74.22%] [Generator loss: 0.783001]\n",
      "10404 [Discriminator loss: 0.800471, acc.: 53.91%] [Generator loss: 0.800021]\n",
      "10405 [Discriminator loss: 0.730994, acc.: 51.56%] [Generator loss: 0.874255]\n",
      "10406 [Discriminator loss: 0.650333, acc.: 64.06%] [Generator loss: 0.957998]\n",
      "10407 [Discriminator loss: 1.213959, acc.: 26.56%] [Generator loss: 0.809911]\n",
      "10408 [Discriminator loss: 0.581449, acc.: 68.75%] [Generator loss: 0.917609]\n",
      "10409 [Discriminator loss: 0.401965, acc.: 85.94%] [Generator loss: 0.793727]\n",
      "10410 [Discriminator loss: 0.554496, acc.: 72.66%] [Generator loss: 0.770102]\n",
      "10411 [Discriminator loss: 0.869376, acc.: 44.53%] [Generator loss: 0.838755]\n",
      "10412 [Discriminator loss: 0.607819, acc.: 65.62%] [Generator loss: 0.939098]\n",
      "10413 [Discriminator loss: 0.556413, acc.: 67.97%] [Generator loss: 1.145743]\n",
      "10414 [Discriminator loss: 0.691993, acc.: 54.69%] [Generator loss: 0.815222]\n",
      "10415 [Discriminator loss: 0.761337, acc.: 50.00%] [Generator loss: 0.818880]\n",
      "10416 [Discriminator loss: 0.700752, acc.: 57.03%] [Generator loss: 0.783678]\n",
      "10417 [Discriminator loss: 0.651291, acc.: 63.28%] [Generator loss: 1.100655]\n",
      "10418 [Discriminator loss: 0.733366, acc.: 55.47%] [Generator loss: 1.390900]\n",
      "10419 [Discriminator loss: 0.708599, acc.: 60.16%] [Generator loss: 1.069662]\n",
      "10420 [Discriminator loss: 0.656465, acc.: 60.94%] [Generator loss: 1.262751]\n",
      "10421 [Discriminator loss: 1.050709, acc.: 42.97%] [Generator loss: 0.828331]\n",
      "10422 [Discriminator loss: 0.797250, acc.: 50.00%] [Generator loss: 0.833593]\n",
      "10423 [Discriminator loss: 0.556827, acc.: 69.53%] [Generator loss: 1.217424]\n",
      "10424 [Discriminator loss: 0.678335, acc.: 56.25%] [Generator loss: 1.052890]\n",
      "10425 [Discriminator loss: 0.535055, acc.: 76.56%] [Generator loss: 1.045929]\n",
      "10426 [Discriminator loss: 0.589763, acc.: 71.09%] [Generator loss: 0.903901]\n",
      "10427 [Discriminator loss: 0.692624, acc.: 57.81%] [Generator loss: 0.676997]\n",
      "10428 [Discriminator loss: 0.781435, acc.: 49.22%] [Generator loss: 0.651825]\n",
      "10429 [Discriminator loss: 0.672325, acc.: 67.19%] [Generator loss: 0.830850]\n",
      "10430 [Discriminator loss: 0.783758, acc.: 44.53%] [Generator loss: 0.872259]\n",
      "10431 [Discriminator loss: 0.602989, acc.: 69.53%] [Generator loss: 0.938251]\n",
      "10432 [Discriminator loss: 0.653651, acc.: 62.50%] [Generator loss: 0.825689]\n",
      "10433 [Discriminator loss: 0.794699, acc.: 51.56%] [Generator loss: 0.829582]\n",
      "10434 [Discriminator loss: 0.672434, acc.: 60.16%] [Generator loss: 0.866246]\n",
      "10435 [Discriminator loss: 0.599573, acc.: 69.53%] [Generator loss: 0.834484]\n",
      "10436 [Discriminator loss: 0.712229, acc.: 64.06%] [Generator loss: 1.019264]\n",
      "10437 [Discriminator loss: 0.545350, acc.: 77.34%] [Generator loss: 1.147334]\n",
      "10438 [Discriminator loss: 0.564212, acc.: 72.66%] [Generator loss: 1.168798]\n",
      "10439 [Discriminator loss: 0.721818, acc.: 54.69%] [Generator loss: 0.993870]\n",
      "10440 [Discriminator loss: 0.662208, acc.: 63.28%] [Generator loss: 1.010045]\n",
      "10441 [Discriminator loss: 0.663728, acc.: 56.25%] [Generator loss: 0.832236]\n",
      "10442 [Discriminator loss: 0.681892, acc.: 61.72%] [Generator loss: 0.796100]\n",
      "10443 [Discriminator loss: 0.595050, acc.: 67.19%] [Generator loss: 0.814251]\n",
      "10444 [Discriminator loss: 0.682206, acc.: 61.72%] [Generator loss: 0.996356]\n",
      "10445 [Discriminator loss: 0.581143, acc.: 71.88%] [Generator loss: 0.892700]\n",
      "10446 [Discriminator loss: 0.557371, acc.: 71.88%] [Generator loss: 0.860055]\n",
      "10447 [Discriminator loss: 0.639786, acc.: 66.41%] [Generator loss: 0.848749]\n",
      "10448 [Discriminator loss: 0.709209, acc.: 55.47%] [Generator loss: 0.796925]\n",
      "10449 [Discriminator loss: 0.501964, acc.: 75.78%] [Generator loss: 0.918712]\n",
      "10450 [Discriminator loss: 0.860787, acc.: 41.41%] [Generator loss: 0.903077]\n",
      "10451 [Discriminator loss: 0.553818, acc.: 74.22%] [Generator loss: 0.994280]\n",
      "10452 [Discriminator loss: 0.732951, acc.: 54.69%] [Generator loss: 0.790199]\n",
      "10453 [Discriminator loss: 1.012071, acc.: 25.00%] [Generator loss: 0.914594]\n",
      "10454 [Discriminator loss: 0.740585, acc.: 56.25%] [Generator loss: 0.867116]\n",
      "10455 [Discriminator loss: 0.745639, acc.: 45.31%] [Generator loss: 0.818660]\n",
      "10456 [Discriminator loss: 0.607417, acc.: 65.62%] [Generator loss: 0.905939]\n",
      "10457 [Discriminator loss: 0.595111, acc.: 67.97%] [Generator loss: 0.876005]\n",
      "10458 [Discriminator loss: 0.718004, acc.: 54.69%] [Generator loss: 0.926574]\n",
      "10459 [Discriminator loss: 0.657803, acc.: 63.28%] [Generator loss: 0.901950]\n",
      "10460 [Discriminator loss: 0.735017, acc.: 50.78%] [Generator loss: 0.974521]\n",
      "10461 [Discriminator loss: 0.671362, acc.: 60.94%] [Generator loss: 0.962736]\n",
      "10462 [Discriminator loss: 0.647165, acc.: 66.41%] [Generator loss: 0.840191]\n",
      "10463 [Discriminator loss: 0.745721, acc.: 42.97%] [Generator loss: 0.812304]\n",
      "10464 [Discriminator loss: 0.759459, acc.: 52.34%] [Generator loss: 1.204492]\n",
      "10465 [Discriminator loss: 0.672257, acc.: 60.16%] [Generator loss: 1.191322]\n",
      "10466 [Discriminator loss: 0.760583, acc.: 50.78%] [Generator loss: 0.996469]\n",
      "10467 [Discriminator loss: 0.730955, acc.: 53.91%] [Generator loss: 0.986919]\n",
      "10468 [Discriminator loss: 0.687335, acc.: 57.03%] [Generator loss: 1.018780]\n",
      "10469 [Discriminator loss: 0.683767, acc.: 60.94%] [Generator loss: 0.850676]\n",
      "10470 [Discriminator loss: 0.708701, acc.: 53.91%] [Generator loss: 0.702465]\n",
      "10471 [Discriminator loss: 0.775872, acc.: 50.78%] [Generator loss: 0.721323]\n",
      "10472 [Discriminator loss: 0.770051, acc.: 46.88%] [Generator loss: 0.842559]\n",
      "10473 [Discriminator loss: 0.705070, acc.: 60.94%] [Generator loss: 0.931482]\n",
      "10474 [Discriminator loss: 0.746742, acc.: 53.12%] [Generator loss: 0.771945]\n",
      "10475 [Discriminator loss: 0.730470, acc.: 48.44%] [Generator loss: 0.783155]\n",
      "10476 [Discriminator loss: 0.630304, acc.: 71.09%] [Generator loss: 0.712823]\n",
      "10477 [Discriminator loss: 0.637299, acc.: 67.19%] [Generator loss: 0.721040]\n",
      "10478 [Discriminator loss: 0.703502, acc.: 53.91%] [Generator loss: 0.806298]\n",
      "10479 [Discriminator loss: 0.694944, acc.: 53.12%] [Generator loss: 1.068145]\n",
      "10480 [Discriminator loss: 0.661746, acc.: 65.62%] [Generator loss: 0.938731]\n",
      "10481 [Discriminator loss: 0.591564, acc.: 70.31%] [Generator loss: 0.981332]\n",
      "10482 [Discriminator loss: 0.549556, acc.: 76.56%] [Generator loss: 1.025626]\n",
      "10483 [Discriminator loss: 0.798712, acc.: 49.22%] [Generator loss: 1.068933]\n",
      "10484 [Discriminator loss: 0.820477, acc.: 48.44%] [Generator loss: 0.910843]\n",
      "10485 [Discriminator loss: 0.573308, acc.: 70.31%] [Generator loss: 0.970717]\n",
      "10486 [Discriminator loss: 0.582573, acc.: 76.56%] [Generator loss: 0.781700]\n",
      "10487 [Discriminator loss: 0.542452, acc.: 75.00%] [Generator loss: 0.744291]\n",
      "10488 [Discriminator loss: 0.818252, acc.: 43.75%] [Generator loss: 0.766999]\n",
      "10489 [Discriminator loss: 1.108230, acc.: 30.47%] [Generator loss: 0.880621]\n",
      "10490 [Discriminator loss: 0.622285, acc.: 70.31%] [Generator loss: 1.147175]\n",
      "10491 [Discriminator loss: 0.704688, acc.: 55.47%] [Generator loss: 1.302211]\n",
      "10492 [Discriminator loss: 0.715116, acc.: 55.47%] [Generator loss: 1.037160]\n",
      "10493 [Discriminator loss: 0.787516, acc.: 49.22%] [Generator loss: 1.085917]\n",
      "10494 [Discriminator loss: 0.613703, acc.: 71.09%] [Generator loss: 1.123240]\n",
      "10495 [Discriminator loss: 0.822898, acc.: 45.31%] [Generator loss: 0.840649]\n",
      "10496 [Discriminator loss: 0.701343, acc.: 57.81%] [Generator loss: 0.890538]\n",
      "10497 [Discriminator loss: 0.722024, acc.: 53.91%] [Generator loss: 0.957338]\n",
      "10498 [Discriminator loss: 0.679322, acc.: 60.16%] [Generator loss: 0.936574]\n",
      "10499 [Discriminator loss: 0.577948, acc.: 79.69%] [Generator loss: 0.929532]\n",
      "10500 [Discriminator loss: 0.656189, acc.: 63.28%] [Generator loss: 0.841352]\n",
      "10501 [Discriminator loss: 0.609658, acc.: 72.66%] [Generator loss: 0.880704]\n",
      "10502 [Discriminator loss: 0.622630, acc.: 71.09%] [Generator loss: 0.801255]\n",
      "10503 [Discriminator loss: 0.568858, acc.: 70.31%] [Generator loss: 0.790869]\n",
      "10504 [Discriminator loss: 0.572621, acc.: 74.22%] [Generator loss: 0.727102]\n",
      "10505 [Discriminator loss: 0.546045, acc.: 76.56%] [Generator loss: 0.684401]\n",
      "10506 [Discriminator loss: 0.585700, acc.: 68.75%] [Generator loss: 0.752856]\n",
      "10507 [Discriminator loss: 0.470098, acc.: 85.16%] [Generator loss: 0.780115]\n",
      "10508 [Discriminator loss: 0.588878, acc.: 66.41%] [Generator loss: 0.648405]\n",
      "10509 [Discriminator loss: 0.616466, acc.: 72.66%] [Generator loss: 0.831536]\n",
      "10510 [Discriminator loss: 0.635051, acc.: 61.72%] [Generator loss: 0.896595]\n",
      "10511 [Discriminator loss: 0.769608, acc.: 46.09%] [Generator loss: 0.892878]\n",
      "10512 [Discriminator loss: 0.808640, acc.: 40.62%] [Generator loss: 1.040186]\n",
      "10513 [Discriminator loss: 0.720717, acc.: 65.62%] [Generator loss: 1.156817]\n",
      "10514 [Discriminator loss: 0.609000, acc.: 67.97%] [Generator loss: 1.161452]\n",
      "10515 [Discriminator loss: 0.782344, acc.: 51.56%] [Generator loss: 0.845041]\n",
      "10516 [Discriminator loss: 0.631433, acc.: 64.06%] [Generator loss: 0.881674]\n",
      "10517 [Discriminator loss: 0.587847, acc.: 71.09%] [Generator loss: 0.774077]\n",
      "10518 [Discriminator loss: 0.749094, acc.: 50.78%] [Generator loss: 1.103971]\n",
      "10519 [Discriminator loss: 0.507312, acc.: 79.69%] [Generator loss: 1.013192]\n",
      "10520 [Discriminator loss: 0.644698, acc.: 68.75%] [Generator loss: 0.861170]\n",
      "10521 [Discriminator loss: 0.628267, acc.: 69.53%] [Generator loss: 0.835931]\n",
      "10522 [Discriminator loss: 0.752501, acc.: 57.81%] [Generator loss: 0.811424]\n",
      "10523 [Discriminator loss: 0.683749, acc.: 60.16%] [Generator loss: 0.764547]\n",
      "10524 [Discriminator loss: 0.671060, acc.: 64.06%] [Generator loss: 0.858768]\n",
      "10525 [Discriminator loss: 0.793463, acc.: 45.31%] [Generator loss: 0.811195]\n",
      "10526 [Discriminator loss: 0.674563, acc.: 54.69%] [Generator loss: 0.813046]\n",
      "10527 [Discriminator loss: 0.650224, acc.: 63.28%] [Generator loss: 1.004695]\n",
      "10528 [Discriminator loss: 0.610631, acc.: 67.97%] [Generator loss: 1.050409]\n",
      "10529 [Discriminator loss: 0.809107, acc.: 55.47%] [Generator loss: 0.747855]\n",
      "10530 [Discriminator loss: 0.727046, acc.: 53.91%] [Generator loss: 0.848993]\n",
      "10531 [Discriminator loss: 0.619193, acc.: 71.88%] [Generator loss: 1.133479]\n",
      "10532 [Discriminator loss: 0.612523, acc.: 69.53%] [Generator loss: 1.166286]\n",
      "10533 [Discriminator loss: 0.736987, acc.: 53.91%] [Generator loss: 0.836705]\n",
      "10534 [Discriminator loss: 0.610552, acc.: 69.53%] [Generator loss: 0.916565]\n",
      "10535 [Discriminator loss: 0.719541, acc.: 54.69%] [Generator loss: 0.769654]\n",
      "10536 [Discriminator loss: 0.777012, acc.: 49.22%] [Generator loss: 0.856737]\n",
      "10537 [Discriminator loss: 0.652177, acc.: 60.16%] [Generator loss: 0.778882]\n",
      "10538 [Discriminator loss: 0.678014, acc.: 60.94%] [Generator loss: 0.742159]\n",
      "10539 [Discriminator loss: 0.763532, acc.: 48.44%] [Generator loss: 0.820386]\n",
      "10540 [Discriminator loss: 0.663662, acc.: 57.81%] [Generator loss: 0.937349]\n",
      "10541 [Discriminator loss: 0.673807, acc.: 57.03%] [Generator loss: 0.863754]\n",
      "10542 [Discriminator loss: 0.683022, acc.: 58.59%] [Generator loss: 0.862110]\n",
      "10543 [Discriminator loss: 0.671891, acc.: 60.16%] [Generator loss: 1.060758]\n",
      "10544 [Discriminator loss: 0.584999, acc.: 66.41%] [Generator loss: 0.983322]\n",
      "10545 [Discriminator loss: 0.767795, acc.: 53.91%] [Generator loss: 0.990604]\n",
      "10546 [Discriminator loss: 0.675837, acc.: 69.53%] [Generator loss: 1.029232]\n",
      "10547 [Discriminator loss: 0.678319, acc.: 60.16%] [Generator loss: 0.918735]\n",
      "10548 [Discriminator loss: 0.589839, acc.: 75.78%] [Generator loss: 0.822773]\n",
      "10549 [Discriminator loss: 0.587896, acc.: 67.97%] [Generator loss: 0.740181]\n",
      "10550 [Discriminator loss: 0.808471, acc.: 46.88%] [Generator loss: 0.724733]\n",
      "10551 [Discriminator loss: 0.609791, acc.: 67.97%] [Generator loss: 0.887412]\n",
      "10552 [Discriminator loss: 0.552397, acc.: 75.00%] [Generator loss: 0.972179]\n",
      "10553 [Discriminator loss: 0.691345, acc.: 60.94%] [Generator loss: 0.687602]\n",
      "10554 [Discriminator loss: 0.583639, acc.: 67.97%] [Generator loss: 0.890596]\n",
      "10555 [Discriminator loss: 0.572464, acc.: 71.09%] [Generator loss: 0.801335]\n",
      "10556 [Discriminator loss: 0.575771, acc.: 69.53%] [Generator loss: 0.771884]\n",
      "10557 [Discriminator loss: 0.703747, acc.: 55.47%] [Generator loss: 0.887645]\n",
      "10558 [Discriminator loss: 0.497858, acc.: 82.81%] [Generator loss: 0.886959]\n",
      "10559 [Discriminator loss: 0.502495, acc.: 75.00%] [Generator loss: 0.783711]\n",
      "10560 [Discriminator loss: 0.844326, acc.: 46.09%] [Generator loss: 0.883986]\n",
      "10561 [Discriminator loss: 0.582901, acc.: 62.50%] [Generator loss: 1.346483]\n",
      "10562 [Discriminator loss: 0.661140, acc.: 59.38%] [Generator loss: 0.931868]\n",
      "10563 [Discriminator loss: 0.738820, acc.: 53.12%] [Generator loss: 0.807370]\n",
      "10564 [Discriminator loss: 0.605453, acc.: 68.75%] [Generator loss: 0.806696]\n",
      "10565 [Discriminator loss: 0.695564, acc.: 57.03%] [Generator loss: 0.844921]\n",
      "10566 [Discriminator loss: 0.717085, acc.: 55.47%] [Generator loss: 0.826898]\n",
      "10567 [Discriminator loss: 0.749800, acc.: 46.88%] [Generator loss: 0.951120]\n",
      "10568 [Discriminator loss: 0.725706, acc.: 55.47%] [Generator loss: 0.895881]\n",
      "10569 [Discriminator loss: 0.733340, acc.: 54.69%] [Generator loss: 0.940638]\n",
      "10570 [Discriminator loss: 0.728931, acc.: 59.38%] [Generator loss: 0.928349]\n",
      "10571 [Discriminator loss: 0.696249, acc.: 56.25%] [Generator loss: 0.888003]\n",
      "10572 [Discriminator loss: 0.713016, acc.: 50.78%] [Generator loss: 0.882768]\n",
      "10573 [Discriminator loss: 0.620465, acc.: 64.06%] [Generator loss: 1.011654]\n",
      "10574 [Discriminator loss: 0.676514, acc.: 56.25%] [Generator loss: 1.070456]\n",
      "10575 [Discriminator loss: 0.684369, acc.: 56.25%] [Generator loss: 0.953736]\n",
      "10576 [Discriminator loss: 0.557438, acc.: 71.88%] [Generator loss: 0.910256]\n",
      "10577 [Discriminator loss: 0.668817, acc.: 60.94%] [Generator loss: 0.785574]\n",
      "10578 [Discriminator loss: 0.678629, acc.: 63.28%] [Generator loss: 0.689643]\n",
      "10579 [Discriminator loss: 0.644631, acc.: 62.50%] [Generator loss: 0.788624]\n",
      "10580 [Discriminator loss: 0.716289, acc.: 56.25%] [Generator loss: 0.837560]\n",
      "10581 [Discriminator loss: 0.479931, acc.: 79.69%] [Generator loss: 0.906428]\n",
      "10582 [Discriminator loss: 0.595720, acc.: 70.31%] [Generator loss: 0.881336]\n",
      "10583 [Discriminator loss: 0.737183, acc.: 53.12%] [Generator loss: 1.153987]\n",
      "10584 [Discriminator loss: 0.693545, acc.: 63.28%] [Generator loss: 1.072721]\n",
      "10585 [Discriminator loss: 0.535437, acc.: 79.69%] [Generator loss: 1.028036]\n",
      "10586 [Discriminator loss: 0.777773, acc.: 44.53%] [Generator loss: 0.998442]\n",
      "10587 [Discriminator loss: 0.637579, acc.: 61.72%] [Generator loss: 1.000791]\n",
      "10588 [Discriminator loss: 0.642319, acc.: 61.72%] [Generator loss: 0.889700]\n",
      "10589 [Discriminator loss: 0.668424, acc.: 59.38%] [Generator loss: 0.915052]\n",
      "10590 [Discriminator loss: 0.567303, acc.: 76.56%] [Generator loss: 0.765710]\n",
      "10591 [Discriminator loss: 0.578703, acc.: 68.75%] [Generator loss: 0.865897]\n",
      "10592 [Discriminator loss: 0.728153, acc.: 53.12%] [Generator loss: 0.838751]\n",
      "10593 [Discriminator loss: 0.568752, acc.: 75.00%] [Generator loss: 0.813239]\n",
      "10594 [Discriminator loss: 0.579493, acc.: 72.66%] [Generator loss: 0.853971]\n",
      "10595 [Discriminator loss: 0.528797, acc.: 71.88%] [Generator loss: 0.735489]\n",
      "10596 [Discriminator loss: 0.678068, acc.: 67.19%] [Generator loss: 0.806012]\n",
      "10597 [Discriminator loss: 0.476768, acc.: 81.25%] [Generator loss: 0.809978]\n",
      "10598 [Discriminator loss: 0.529566, acc.: 75.00%] [Generator loss: 0.923370]\n",
      "10599 [Discriminator loss: 0.471951, acc.: 83.59%] [Generator loss: 0.906190]\n",
      "10600 [Discriminator loss: 0.524863, acc.: 78.12%] [Generator loss: 0.875148]\n",
      "10601 [Discriminator loss: 0.584198, acc.: 71.88%] [Generator loss: 0.953677]\n",
      "10602 [Discriminator loss: 0.986334, acc.: 40.62%] [Generator loss: 0.847447]\n",
      "10603 [Discriminator loss: 0.696674, acc.: 53.91%] [Generator loss: 0.736823]\n",
      "10604 [Discriminator loss: 0.724677, acc.: 59.38%] [Generator loss: 1.192017]\n",
      "10605 [Discriminator loss: 0.552570, acc.: 75.78%] [Generator loss: 1.115625]\n",
      "10606 [Discriminator loss: 0.652858, acc.: 61.72%] [Generator loss: 0.956645]\n",
      "10607 [Discriminator loss: 0.592436, acc.: 66.41%] [Generator loss: 0.880125]\n",
      "10608 [Discriminator loss: 0.593957, acc.: 68.75%] [Generator loss: 0.844594]\n",
      "10609 [Discriminator loss: 0.763336, acc.: 49.22%] [Generator loss: 0.775895]\n",
      "10610 [Discriminator loss: 0.657101, acc.: 62.50%] [Generator loss: 0.933067]\n",
      "10611 [Discriminator loss: 0.626846, acc.: 66.41%] [Generator loss: 1.108485]\n",
      "10612 [Discriminator loss: 0.732206, acc.: 58.59%] [Generator loss: 0.991645]\n",
      "10613 [Discriminator loss: 0.751847, acc.: 54.69%] [Generator loss: 0.854907]\n",
      "10614 [Discriminator loss: 0.679598, acc.: 59.38%] [Generator loss: 0.858804]\n",
      "10615 [Discriminator loss: 0.656844, acc.: 63.28%] [Generator loss: 0.903028]\n",
      "10616 [Discriminator loss: 0.501027, acc.: 77.34%] [Generator loss: 1.004485]\n",
      "10617 [Discriminator loss: 0.548461, acc.: 74.22%] [Generator loss: 0.740638]\n",
      "10618 [Discriminator loss: 1.001088, acc.: 45.31%] [Generator loss: 0.956711]\n",
      "10619 [Discriminator loss: 0.529953, acc.: 82.81%] [Generator loss: 1.011098]\n",
      "10620 [Discriminator loss: 0.497491, acc.: 75.78%] [Generator loss: 0.834957]\n",
      "10621 [Discriminator loss: 0.728435, acc.: 54.69%] [Generator loss: 1.018239]\n",
      "10622 [Discriminator loss: 0.578094, acc.: 71.09%] [Generator loss: 0.891851]\n",
      "10623 [Discriminator loss: 0.827274, acc.: 45.31%] [Generator loss: 0.740487]\n",
      "10624 [Discriminator loss: 0.456668, acc.: 78.91%] [Generator loss: 0.776663]\n",
      "10625 [Discriminator loss: 0.660029, acc.: 63.28%] [Generator loss: 0.584170]\n",
      "10626 [Discriminator loss: 0.685556, acc.: 62.50%] [Generator loss: 0.685095]\n",
      "10627 [Discriminator loss: 0.556144, acc.: 69.53%] [Generator loss: 1.051060]\n",
      "10628 [Discriminator loss: 0.670552, acc.: 55.47%] [Generator loss: 1.195370]\n",
      "10629 [Discriminator loss: 0.704213, acc.: 59.38%] [Generator loss: 0.967733]\n",
      "10630 [Discriminator loss: 0.601631, acc.: 68.75%] [Generator loss: 0.942283]\n",
      "10631 [Discriminator loss: 0.512251, acc.: 76.56%] [Generator loss: 1.015771]\n",
      "10632 [Discriminator loss: 0.606511, acc.: 70.31%] [Generator loss: 0.833811]\n",
      "10633 [Discriminator loss: 0.595466, acc.: 68.75%] [Generator loss: 0.853289]\n",
      "10634 [Discriminator loss: 0.625189, acc.: 59.38%] [Generator loss: 0.916052]\n",
      "10635 [Discriminator loss: 0.539153, acc.: 76.56%] [Generator loss: 0.888792]\n",
      "10636 [Discriminator loss: 0.702060, acc.: 60.16%] [Generator loss: 0.666026]\n",
      "10637 [Discriminator loss: 0.830146, acc.: 47.66%] [Generator loss: 0.824913]\n",
      "10638 [Discriminator loss: 0.990227, acc.: 38.28%] [Generator loss: 0.934847]\n",
      "10639 [Discriminator loss: 0.591242, acc.: 71.88%] [Generator loss: 0.860257]\n",
      "10640 [Discriminator loss: 0.846040, acc.: 41.41%] [Generator loss: 0.903589]\n",
      "10641 [Discriminator loss: 0.625373, acc.: 68.75%] [Generator loss: 1.022319]\n",
      "10642 [Discriminator loss: 0.565564, acc.: 77.34%] [Generator loss: 1.027537]\n",
      "10643 [Discriminator loss: 0.614428, acc.: 75.00%] [Generator loss: 0.964376]\n",
      "10644 [Discriminator loss: 0.573788, acc.: 67.19%] [Generator loss: 0.997364]\n",
      "10645 [Discriminator loss: 0.707982, acc.: 66.41%] [Generator loss: 0.823113]\n",
      "10646 [Discriminator loss: 0.710204, acc.: 53.12%] [Generator loss: 0.659787]\n",
      "10647 [Discriminator loss: 0.675606, acc.: 59.38%] [Generator loss: 0.960892]\n",
      "10648 [Discriminator loss: 0.659969, acc.: 64.06%] [Generator loss: 0.816027]\n",
      "10649 [Discriminator loss: 0.717294, acc.: 54.69%] [Generator loss: 0.787402]\n",
      "10650 [Discriminator loss: 0.484023, acc.: 81.25%] [Generator loss: 0.940625]\n",
      "10651 [Discriminator loss: 0.661930, acc.: 63.28%] [Generator loss: 1.103407]\n",
      "10652 [Discriminator loss: 0.727462, acc.: 49.22%] [Generator loss: 0.814289]\n",
      "10653 [Discriminator loss: 0.552662, acc.: 71.88%] [Generator loss: 1.124543]\n",
      "10654 [Discriminator loss: 0.595393, acc.: 68.75%] [Generator loss: 1.015499]\n",
      "10655 [Discriminator loss: 0.540100, acc.: 80.47%] [Generator loss: 1.053125]\n",
      "10656 [Discriminator loss: 0.565280, acc.: 69.53%] [Generator loss: 0.969741]\n",
      "10657 [Discriminator loss: 0.669259, acc.: 61.72%] [Generator loss: 0.772180]\n",
      "10658 [Discriminator loss: 0.484054, acc.: 78.12%] [Generator loss: 0.645174]\n",
      "10659 [Discriminator loss: 0.759278, acc.: 48.44%] [Generator loss: 0.673002]\n",
      "10660 [Discriminator loss: 0.559480, acc.: 75.78%] [Generator loss: 0.915198]\n",
      "10661 [Discriminator loss: 0.472585, acc.: 81.25%] [Generator loss: 0.701717]\n",
      "10662 [Discriminator loss: 0.644314, acc.: 66.41%] [Generator loss: 0.799375]\n",
      "10663 [Discriminator loss: 0.489593, acc.: 79.69%] [Generator loss: 0.772626]\n",
      "10664 [Discriminator loss: 0.645362, acc.: 61.72%] [Generator loss: 0.642864]\n",
      "10665 [Discriminator loss: 0.481885, acc.: 79.69%] [Generator loss: 0.720401]\n",
      "10666 [Discriminator loss: 0.638703, acc.: 67.19%] [Generator loss: 0.665286]\n",
      "10667 [Discriminator loss: 0.973048, acc.: 32.81%] [Generator loss: 0.837502]\n",
      "10668 [Discriminator loss: 0.741728, acc.: 52.34%] [Generator loss: 1.118516]\n",
      "10669 [Discriminator loss: 0.543205, acc.: 71.09%] [Generator loss: 1.157812]\n",
      "10670 [Discriminator loss: 0.741088, acc.: 62.50%] [Generator loss: 1.449119]\n",
      "10671 [Discriminator loss: 0.509090, acc.: 75.78%] [Generator loss: 1.101417]\n",
      "10672 [Discriminator loss: 0.670680, acc.: 61.72%] [Generator loss: 1.113374]\n",
      "10673 [Discriminator loss: 0.680377, acc.: 64.06%] [Generator loss: 1.214290]\n",
      "10674 [Discriminator loss: 0.722172, acc.: 54.69%] [Generator loss: 0.910918]\n",
      "10675 [Discriminator loss: 0.634854, acc.: 66.41%] [Generator loss: 0.855556]\n",
      "10676 [Discriminator loss: 0.515596, acc.: 77.34%] [Generator loss: 0.633336]\n",
      "10677 [Discriminator loss: 0.558983, acc.: 68.75%] [Generator loss: 0.786016]\n",
      "10678 [Discriminator loss: 0.640477, acc.: 67.97%] [Generator loss: 0.781292]\n",
      "10679 [Discriminator loss: 0.641056, acc.: 58.59%] [Generator loss: 0.912643]\n",
      "10680 [Discriminator loss: 0.580329, acc.: 80.47%] [Generator loss: 0.773181]\n",
      "10681 [Discriminator loss: 0.679751, acc.: 60.16%] [Generator loss: 0.705431]\n",
      "10682 [Discriminator loss: 0.450687, acc.: 84.38%] [Generator loss: 0.662076]\n",
      "10683 [Discriminator loss: 0.433335, acc.: 84.38%] [Generator loss: 0.649942]\n",
      "10684 [Discriminator loss: 0.585821, acc.: 67.19%] [Generator loss: 0.923196]\n",
      "10685 [Discriminator loss: 0.318848, acc.: 92.19%] [Generator loss: 0.728499]\n",
      "10686 [Discriminator loss: 0.891182, acc.: 41.41%] [Generator loss: 0.626169]\n",
      "10687 [Discriminator loss: 0.497806, acc.: 77.34%] [Generator loss: 0.672989]\n",
      "10688 [Discriminator loss: 0.682488, acc.: 62.50%] [Generator loss: 1.156953]\n",
      "10689 [Discriminator loss: 0.411357, acc.: 84.38%] [Generator loss: 1.193015]\n",
      "10690 [Discriminator loss: 0.820432, acc.: 43.75%] [Generator loss: 0.944469]\n",
      "10691 [Discriminator loss: 0.653018, acc.: 62.50%] [Generator loss: 0.767734]\n",
      "10692 [Discriminator loss: 0.568690, acc.: 76.56%] [Generator loss: 0.982948]\n",
      "10693 [Discriminator loss: 0.737400, acc.: 53.91%] [Generator loss: 0.842511]\n",
      "10694 [Discriminator loss: 0.838182, acc.: 46.09%] [Generator loss: 0.974306]\n",
      "10695 [Discriminator loss: 0.678339, acc.: 56.25%] [Generator loss: 1.043411]\n",
      "10696 [Discriminator loss: 0.765973, acc.: 48.44%] [Generator loss: 0.883350]\n",
      "10697 [Discriminator loss: 0.738213, acc.: 46.09%] [Generator loss: 1.087353]\n",
      "10698 [Discriminator loss: 0.734359, acc.: 56.25%] [Generator loss: 1.176112]\n",
      "10699 [Discriminator loss: 1.031651, acc.: 32.81%] [Generator loss: 0.786969]\n",
      "10700 [Discriminator loss: 0.649509, acc.: 67.97%] [Generator loss: 0.793522]\n",
      "10701 [Discriminator loss: 0.601650, acc.: 67.19%] [Generator loss: 1.027098]\n",
      "10702 [Discriminator loss: 0.717658, acc.: 57.81%] [Generator loss: 0.933045]\n",
      "10703 [Discriminator loss: 0.758020, acc.: 50.78%] [Generator loss: 0.896994]\n",
      "10704 [Discriminator loss: 0.752384, acc.: 50.00%] [Generator loss: 1.076913]\n",
      "10705 [Discriminator loss: 0.775636, acc.: 60.16%] [Generator loss: 0.899908]\n",
      "10706 [Discriminator loss: 0.936269, acc.: 34.38%] [Generator loss: 0.875525]\n",
      "10707 [Discriminator loss: 0.825972, acc.: 47.66%] [Generator loss: 0.879120]\n",
      "10708 [Discriminator loss: 0.634693, acc.: 66.41%] [Generator loss: 1.104837]\n",
      "10709 [Discriminator loss: 0.714596, acc.: 58.59%] [Generator loss: 0.937853]\n",
      "10710 [Discriminator loss: 0.897185, acc.: 35.94%] [Generator loss: 1.245807]\n",
      "10711 [Discriminator loss: 0.600294, acc.: 66.41%] [Generator loss: 1.037702]\n",
      "10712 [Discriminator loss: 0.722533, acc.: 53.91%] [Generator loss: 0.943071]\n",
      "10713 [Discriminator loss: 0.911023, acc.: 32.81%] [Generator loss: 0.854058]\n",
      "10714 [Discriminator loss: 0.657808, acc.: 66.41%] [Generator loss: 0.734035]\n",
      "10715 [Discriminator loss: 0.690286, acc.: 71.09%] [Generator loss: 0.898249]\n",
      "10716 [Discriminator loss: 0.680669, acc.: 62.50%] [Generator loss: 0.981777]\n",
      "10717 [Discriminator loss: 0.654856, acc.: 59.38%] [Generator loss: 0.978303]\n",
      "10718 [Discriminator loss: 0.529464, acc.: 76.56%] [Generator loss: 1.001856]\n",
      "10719 [Discriminator loss: 0.716309, acc.: 57.81%] [Generator loss: 0.856370]\n",
      "10720 [Discriminator loss: 0.607404, acc.: 67.97%] [Generator loss: 0.803746]\n",
      "10721 [Discriminator loss: 0.673351, acc.: 62.50%] [Generator loss: 0.881310]\n",
      "10722 [Discriminator loss: 0.700970, acc.: 64.84%] [Generator loss: 0.834941]\n",
      "10723 [Discriminator loss: 0.714082, acc.: 50.00%] [Generator loss: 0.995933]\n",
      "10724 [Discriminator loss: 0.591380, acc.: 69.53%] [Generator loss: 0.984971]\n",
      "10725 [Discriminator loss: 0.686171, acc.: 62.50%] [Generator loss: 1.006182]\n",
      "10726 [Discriminator loss: 0.710610, acc.: 60.16%] [Generator loss: 1.055440]\n",
      "10727 [Discriminator loss: 0.604028, acc.: 69.53%] [Generator loss: 1.156007]\n",
      "10728 [Discriminator loss: 0.737577, acc.: 47.66%] [Generator loss: 0.926760]\n",
      "10729 [Discriminator loss: 0.670486, acc.: 60.16%] [Generator loss: 0.960578]\n",
      "10730 [Discriminator loss: 0.644235, acc.: 60.94%] [Generator loss: 0.812048]\n",
      "10731 [Discriminator loss: 0.761160, acc.: 51.56%] [Generator loss: 0.881350]\n",
      "10732 [Discriminator loss: 0.664053, acc.: 62.50%] [Generator loss: 0.837637]\n",
      "10733 [Discriminator loss: 0.808916, acc.: 53.12%] [Generator loss: 0.805421]\n",
      "10734 [Discriminator loss: 0.692074, acc.: 54.69%] [Generator loss: 0.828460]\n",
      "10735 [Discriminator loss: 0.617842, acc.: 66.41%] [Generator loss: 1.040462]\n",
      "10736 [Discriminator loss: 0.731319, acc.: 60.94%] [Generator loss: 0.863420]\n",
      "10737 [Discriminator loss: 0.612413, acc.: 66.41%] [Generator loss: 0.929572]\n",
      "10738 [Discriminator loss: 0.700926, acc.: 59.38%] [Generator loss: 0.903653]\n",
      "10739 [Discriminator loss: 0.791229, acc.: 50.00%] [Generator loss: 0.945970]\n",
      "10740 [Discriminator loss: 0.597658, acc.: 70.31%] [Generator loss: 1.000200]\n",
      "10741 [Discriminator loss: 0.649119, acc.: 60.94%] [Generator loss: 1.005456]\n",
      "10742 [Discriminator loss: 0.810999, acc.: 52.34%] [Generator loss: 0.698975]\n",
      "10743 [Discriminator loss: 0.520724, acc.: 78.91%] [Generator loss: 0.754169]\n",
      "10744 [Discriminator loss: 0.658304, acc.: 59.38%] [Generator loss: 0.617697]\n",
      "10745 [Discriminator loss: 0.821835, acc.: 51.56%] [Generator loss: 0.728535]\n",
      "10746 [Discriminator loss: 0.475735, acc.: 85.16%] [Generator loss: 0.803078]\n",
      "10747 [Discriminator loss: 0.704866, acc.: 53.91%] [Generator loss: 0.926325]\n",
      "10748 [Discriminator loss: 0.664184, acc.: 54.69%] [Generator loss: 0.910591]\n",
      "10749 [Discriminator loss: 0.869228, acc.: 38.28%] [Generator loss: 0.912245]\n",
      "10750 [Discriminator loss: 0.842777, acc.: 40.62%] [Generator loss: 0.871914]\n",
      "10751 [Discriminator loss: 0.684984, acc.: 53.91%] [Generator loss: 0.951811]\n",
      "10752 [Discriminator loss: 0.633982, acc.: 67.19%] [Generator loss: 1.087615]\n",
      "10753 [Discriminator loss: 0.574100, acc.: 72.66%] [Generator loss: 1.208201]\n",
      "10754 [Discriminator loss: 0.540390, acc.: 78.12%] [Generator loss: 0.966283]\n",
      "10755 [Discriminator loss: 0.441916, acc.: 85.94%] [Generator loss: 0.856769]\n",
      "10756 [Discriminator loss: 0.688156, acc.: 60.16%] [Generator loss: 0.962758]\n",
      "10757 [Discriminator loss: 0.479470, acc.: 76.56%] [Generator loss: 0.846546]\n",
      "10758 [Discriminator loss: 0.459338, acc.: 83.59%] [Generator loss: 0.761997]\n",
      "10759 [Discriminator loss: 0.452581, acc.: 82.81%] [Generator loss: 0.683592]\n",
      "10760 [Discriminator loss: 0.577381, acc.: 71.88%] [Generator loss: 0.712894]\n",
      "10761 [Discriminator loss: 0.767655, acc.: 51.56%] [Generator loss: 0.707191]\n",
      "10762 [Discriminator loss: 1.062735, acc.: 32.03%] [Generator loss: 0.804137]\n",
      "10763 [Discriminator loss: 0.601187, acc.: 68.75%] [Generator loss: 0.788473]\n",
      "10764 [Discriminator loss: 0.780303, acc.: 51.56%] [Generator loss: 0.796249]\n",
      "10765 [Discriminator loss: 0.628827, acc.: 67.19%] [Generator loss: 0.825818]\n",
      "10766 [Discriminator loss: 0.689037, acc.: 53.91%] [Generator loss: 0.808163]\n",
      "10767 [Discriminator loss: 0.795491, acc.: 42.97%] [Generator loss: 0.936511]\n",
      "10768 [Discriminator loss: 0.724137, acc.: 57.03%] [Generator loss: 1.051427]\n",
      "10769 [Discriminator loss: 0.732294, acc.: 57.03%] [Generator loss: 1.157739]\n",
      "10770 [Discriminator loss: 0.493321, acc.: 77.34%] [Generator loss: 1.201484]\n",
      "10771 [Discriminator loss: 0.587078, acc.: 68.75%] [Generator loss: 0.965679]\n",
      "10772 [Discriminator loss: 0.634980, acc.: 66.41%] [Generator loss: 0.720106]\n",
      "10773 [Discriminator loss: 0.619452, acc.: 67.19%] [Generator loss: 0.627340]\n",
      "10774 [Discriminator loss: 0.517354, acc.: 75.78%] [Generator loss: 0.756748]\n",
      "10775 [Discriminator loss: 0.678137, acc.: 60.94%] [Generator loss: 0.921232]\n",
      "10776 [Discriminator loss: 0.539265, acc.: 70.31%] [Generator loss: 0.805505]\n",
      "10777 [Discriminator loss: 0.590589, acc.: 68.75%] [Generator loss: 0.710680]\n",
      "10778 [Discriminator loss: 0.968887, acc.: 39.84%] [Generator loss: 0.643209]\n",
      "10779 [Discriminator loss: 0.599437, acc.: 68.75%] [Generator loss: 0.568493]\n",
      "10780 [Discriminator loss: 0.716321, acc.: 61.72%] [Generator loss: 0.843031]\n",
      "10781 [Discriminator loss: 0.809514, acc.: 44.53%] [Generator loss: 0.893912]\n",
      "10782 [Discriminator loss: 0.646106, acc.: 64.06%] [Generator loss: 0.933330]\n",
      "10783 [Discriminator loss: 0.725068, acc.: 59.38%] [Generator loss: 1.014280]\n",
      "10784 [Discriminator loss: 0.720764, acc.: 49.22%] [Generator loss: 0.964780]\n",
      "10785 [Discriminator loss: 0.711449, acc.: 47.66%] [Generator loss: 0.907213]\n",
      "10786 [Discriminator loss: 0.778506, acc.: 49.22%] [Generator loss: 0.902584]\n",
      "10787 [Discriminator loss: 0.674770, acc.: 64.06%] [Generator loss: 0.966907]\n",
      "10788 [Discriminator loss: 0.712957, acc.: 56.25%] [Generator loss: 0.922163]\n",
      "10789 [Discriminator loss: 0.522248, acc.: 79.69%] [Generator loss: 0.833715]\n",
      "10790 [Discriminator loss: 0.695646, acc.: 64.06%] [Generator loss: 0.915988]\n",
      "10791 [Discriminator loss: 0.608629, acc.: 64.84%] [Generator loss: 0.864538]\n",
      "10792 [Discriminator loss: 0.651314, acc.: 65.62%] [Generator loss: 0.836593]\n",
      "10793 [Discriminator loss: 0.954794, acc.: 40.62%] [Generator loss: 0.908945]\n",
      "10794 [Discriminator loss: 0.617691, acc.: 71.09%] [Generator loss: 0.753073]\n",
      "10795 [Discriminator loss: 0.432909, acc.: 83.59%] [Generator loss: 0.932978]\n",
      "10796 [Discriminator loss: 0.643124, acc.: 60.94%] [Generator loss: 0.790864]\n",
      "10797 [Discriminator loss: 0.680869, acc.: 59.38%] [Generator loss: 0.823944]\n",
      "10798 [Discriminator loss: 0.625077, acc.: 67.19%] [Generator loss: 0.791769]\n",
      "10799 [Discriminator loss: 0.916119, acc.: 35.16%] [Generator loss: 0.753202]\n",
      "10800 [Discriminator loss: 0.717435, acc.: 55.47%] [Generator loss: 0.879418]\n",
      "10801 [Discriminator loss: 0.776107, acc.: 49.22%] [Generator loss: 0.795608]\n",
      "10802 [Discriminator loss: 0.613024, acc.: 68.75%] [Generator loss: 0.892102]\n",
      "10803 [Discriminator loss: 0.534622, acc.: 78.12%] [Generator loss: 0.963139]\n",
      "10804 [Discriminator loss: 0.637009, acc.: 65.62%] [Generator loss: 0.940085]\n",
      "10805 [Discriminator loss: 0.565680, acc.: 77.34%] [Generator loss: 0.892035]\n",
      "10806 [Discriminator loss: 0.685012, acc.: 65.62%] [Generator loss: 0.839043]\n",
      "10807 [Discriminator loss: 0.651516, acc.: 67.97%] [Generator loss: 0.928251]\n",
      "10808 [Discriminator loss: 0.611789, acc.: 65.62%] [Generator loss: 0.803500]\n",
      "10809 [Discriminator loss: 0.588416, acc.: 71.88%] [Generator loss: 0.777669]\n",
      "10810 [Discriminator loss: 0.657330, acc.: 65.62%] [Generator loss: 0.854187]\n",
      "10811 [Discriminator loss: 0.650033, acc.: 58.59%] [Generator loss: 1.025389]\n",
      "10812 [Discriminator loss: 0.695308, acc.: 57.81%] [Generator loss: 0.762453]\n",
      "10813 [Discriminator loss: 0.745301, acc.: 49.22%] [Generator loss: 0.866691]\n",
      "10814 [Discriminator loss: 0.896435, acc.: 41.41%] [Generator loss: 0.738625]\n",
      "10815 [Discriminator loss: 0.649349, acc.: 61.72%] [Generator loss: 0.738753]\n",
      "10816 [Discriminator loss: 0.805851, acc.: 40.62%] [Generator loss: 0.796681]\n",
      "10817 [Discriminator loss: 0.587327, acc.: 69.53%] [Generator loss: 0.876256]\n",
      "10818 [Discriminator loss: 0.726979, acc.: 57.03%] [Generator loss: 0.831041]\n",
      "10819 [Discriminator loss: 0.634857, acc.: 59.38%] [Generator loss: 0.746692]\n",
      "10820 [Discriminator loss: 0.760779, acc.: 49.22%] [Generator loss: 0.902698]\n",
      "10821 [Discriminator loss: 0.574418, acc.: 64.06%] [Generator loss: 1.028202]\n",
      "10822 [Discriminator loss: 0.704102, acc.: 61.72%] [Generator loss: 0.966130]\n",
      "10823 [Discriminator loss: 0.631357, acc.: 64.06%] [Generator loss: 0.826551]\n",
      "10824 [Discriminator loss: 0.686771, acc.: 64.06%] [Generator loss: 0.809348]\n",
      "10825 [Discriminator loss: 0.669639, acc.: 57.03%] [Generator loss: 0.789525]\n",
      "10826 [Discriminator loss: 0.553671, acc.: 72.66%] [Generator loss: 0.912355]\n",
      "10827 [Discriminator loss: 0.773481, acc.: 51.56%] [Generator loss: 0.901472]\n",
      "10828 [Discriminator loss: 0.649553, acc.: 61.72%] [Generator loss: 0.771557]\n",
      "10829 [Discriminator loss: 0.623109, acc.: 62.50%] [Generator loss: 0.896954]\n",
      "10830 [Discriminator loss: 0.630031, acc.: 61.72%] [Generator loss: 0.990108]\n",
      "10831 [Discriminator loss: 0.658450, acc.: 59.38%] [Generator loss: 1.074123]\n",
      "10832 [Discriminator loss: 0.787535, acc.: 46.09%] [Generator loss: 0.864132]\n",
      "10833 [Discriminator loss: 0.601093, acc.: 72.66%] [Generator loss: 0.937610]\n",
      "10834 [Discriminator loss: 0.642368, acc.: 57.81%] [Generator loss: 0.843615]\n",
      "10835 [Discriminator loss: 0.667888, acc.: 64.06%] [Generator loss: 0.694499]\n",
      "10836 [Discriminator loss: 0.579296, acc.: 71.09%] [Generator loss: 0.950019]\n",
      "10837 [Discriminator loss: 0.573502, acc.: 72.66%] [Generator loss: 0.984551]\n",
      "10838 [Discriminator loss: 0.674323, acc.: 57.81%] [Generator loss: 0.994909]\n",
      "10839 [Discriminator loss: 0.638909, acc.: 65.62%] [Generator loss: 0.838080]\n",
      "10840 [Discriminator loss: 0.599386, acc.: 66.41%] [Generator loss: 0.985797]\n",
      "10841 [Discriminator loss: 0.540007, acc.: 77.34%] [Generator loss: 0.788010]\n",
      "10842 [Discriminator loss: 0.611421, acc.: 69.53%] [Generator loss: 0.842575]\n",
      "10843 [Discriminator loss: 0.701170, acc.: 61.72%] [Generator loss: 0.886257]\n",
      "10844 [Discriminator loss: 0.576562, acc.: 71.09%] [Generator loss: 0.768106]\n",
      "10845 [Discriminator loss: 0.765897, acc.: 46.09%] [Generator loss: 0.833510]\n",
      "10846 [Discriminator loss: 0.662437, acc.: 67.97%] [Generator loss: 1.083254]\n",
      "10847 [Discriminator loss: 0.610427, acc.: 64.06%] [Generator loss: 1.130302]\n",
      "10848 [Discriminator loss: 0.785498, acc.: 48.44%] [Generator loss: 1.152638]\n",
      "10849 [Discriminator loss: 0.565538, acc.: 72.66%] [Generator loss: 1.036085]\n",
      "10850 [Discriminator loss: 0.619885, acc.: 67.97%] [Generator loss: 0.915965]\n",
      "10851 [Discriminator loss: 0.550554, acc.: 76.56%] [Generator loss: 0.789531]\n",
      "10852 [Discriminator loss: 0.592189, acc.: 75.00%] [Generator loss: 0.763510]\n",
      "10853 [Discriminator loss: 0.626725, acc.: 65.62%] [Generator loss: 0.839842]\n",
      "10854 [Discriminator loss: 0.585565, acc.: 70.31%] [Generator loss: 0.729050]\n",
      "10855 [Discriminator loss: 0.653241, acc.: 58.59%] [Generator loss: 0.921575]\n",
      "10856 [Discriminator loss: 0.625503, acc.: 69.53%] [Generator loss: 0.875708]\n",
      "10857 [Discriminator loss: 0.616627, acc.: 62.50%] [Generator loss: 0.894797]\n",
      "10858 [Discriminator loss: 0.631777, acc.: 60.94%] [Generator loss: 0.870910]\n",
      "10859 [Discriminator loss: 0.524001, acc.: 79.69%] [Generator loss: 1.055547]\n",
      "10860 [Discriminator loss: 0.629689, acc.: 61.72%] [Generator loss: 0.978965]\n",
      "10861 [Discriminator loss: 0.617433, acc.: 67.97%] [Generator loss: 0.868975]\n",
      "10862 [Discriminator loss: 0.564792, acc.: 71.88%] [Generator loss: 0.968240]\n",
      "10863 [Discriminator loss: 0.629094, acc.: 65.62%] [Generator loss: 0.942584]\n",
      "10864 [Discriminator loss: 0.476123, acc.: 83.59%] [Generator loss: 0.898139]\n",
      "10865 [Discriminator loss: 0.512575, acc.: 79.69%] [Generator loss: 0.924382]\n",
      "10866 [Discriminator loss: 0.746711, acc.: 53.12%] [Generator loss: 0.710427]\n",
      "10867 [Discriminator loss: 0.576143, acc.: 71.09%] [Generator loss: 0.949134]\n",
      "10868 [Discriminator loss: 0.581362, acc.: 71.09%] [Generator loss: 0.642636]\n",
      "10869 [Discriminator loss: 0.593075, acc.: 71.09%] [Generator loss: 0.592863]\n",
      "10870 [Discriminator loss: 0.731230, acc.: 57.03%] [Generator loss: 0.762920]\n",
      "10871 [Discriminator loss: 0.529744, acc.: 75.78%] [Generator loss: 1.137891]\n",
      "10872 [Discriminator loss: 0.547834, acc.: 71.88%] [Generator loss: 1.025793]\n",
      "10873 [Discriminator loss: 0.856098, acc.: 40.62%] [Generator loss: 0.782461]\n",
      "10874 [Discriminator loss: 0.744058, acc.: 54.69%] [Generator loss: 0.811544]\n",
      "10875 [Discriminator loss: 0.594589, acc.: 67.97%] [Generator loss: 0.878101]\n",
      "10876 [Discriminator loss: 0.931784, acc.: 39.06%] [Generator loss: 0.870335]\n",
      "10877 [Discriminator loss: 0.622029, acc.: 75.00%] [Generator loss: 0.992842]\n",
      "10878 [Discriminator loss: 0.539649, acc.: 76.56%] [Generator loss: 1.167749]\n",
      "10879 [Discriminator loss: 0.635337, acc.: 64.84%] [Generator loss: 1.140812]\n",
      "10880 [Discriminator loss: 0.570489, acc.: 68.75%] [Generator loss: 1.014657]\n",
      "10881 [Discriminator loss: 0.599612, acc.: 71.09%] [Generator loss: 0.926591]\n",
      "10882 [Discriminator loss: 0.672280, acc.: 60.94%] [Generator loss: 0.894535]\n",
      "10883 [Discriminator loss: 0.445851, acc.: 76.56%] [Generator loss: 1.051748]\n",
      "10884 [Discriminator loss: 0.935048, acc.: 33.59%] [Generator loss: 1.048386]\n",
      "10885 [Discriminator loss: 0.638245, acc.: 65.62%] [Generator loss: 0.935597]\n",
      "10886 [Discriminator loss: 0.449632, acc.: 83.59%] [Generator loss: 1.174883]\n",
      "10887 [Discriminator loss: 0.584921, acc.: 75.00%] [Generator loss: 0.628351]\n",
      "10888 [Discriminator loss: 0.693695, acc.: 54.69%] [Generator loss: 0.765248]\n",
      "10889 [Discriminator loss: 0.573878, acc.: 68.75%] [Generator loss: 0.742383]\n",
      "10890 [Discriminator loss: 0.556865, acc.: 74.22%] [Generator loss: 0.679724]\n",
      "10891 [Discriminator loss: 0.476809, acc.: 82.03%] [Generator loss: 0.835965]\n",
      "10892 [Discriminator loss: 0.835208, acc.: 42.19%] [Generator loss: 0.850788]\n",
      "10893 [Discriminator loss: 0.796680, acc.: 40.62%] [Generator loss: 1.059471]\n",
      "10894 [Discriminator loss: 0.715227, acc.: 53.91%] [Generator loss: 1.076208]\n",
      "10895 [Discriminator loss: 0.633453, acc.: 65.62%] [Generator loss: 1.102830]\n",
      "10896 [Discriminator loss: 0.704439, acc.: 59.38%] [Generator loss: 0.894560]\n",
      "10897 [Discriminator loss: 0.871062, acc.: 45.31%] [Generator loss: 0.774047]\n",
      "10898 [Discriminator loss: 0.520113, acc.: 77.34%] [Generator loss: 0.797510]\n",
      "10899 [Discriminator loss: 0.619274, acc.: 71.09%] [Generator loss: 0.853024]\n",
      "10900 [Discriminator loss: 0.616520, acc.: 69.53%] [Generator loss: 0.837502]\n",
      "10901 [Discriminator loss: 0.579231, acc.: 72.66%] [Generator loss: 0.781632]\n",
      "10902 [Discriminator loss: 0.481401, acc.: 82.03%] [Generator loss: 0.666128]\n",
      "10903 [Discriminator loss: 0.622554, acc.: 67.97%] [Generator loss: 0.697641]\n",
      "10904 [Discriminator loss: 0.514643, acc.: 75.78%] [Generator loss: 0.889278]\n",
      "10905 [Discriminator loss: 0.522063, acc.: 77.34%] [Generator loss: 0.905005]\n",
      "10906 [Discriminator loss: 0.736477, acc.: 48.44%] [Generator loss: 0.734056]\n",
      "10907 [Discriminator loss: 0.524507, acc.: 75.00%] [Generator loss: 0.799507]\n",
      "10908 [Discriminator loss: 0.659413, acc.: 62.50%] [Generator loss: 0.819731]\n",
      "10909 [Discriminator loss: 0.625032, acc.: 65.62%] [Generator loss: 0.849813]\n",
      "10910 [Discriminator loss: 0.789523, acc.: 52.34%] [Generator loss: 0.610482]\n",
      "10911 [Discriminator loss: 0.714103, acc.: 64.06%] [Generator loss: 0.719274]\n",
      "10912 [Discriminator loss: 0.703219, acc.: 54.69%] [Generator loss: 0.725791]\n",
      "10913 [Discriminator loss: 0.687758, acc.: 59.38%] [Generator loss: 0.851598]\n",
      "10914 [Discriminator loss: 0.746258, acc.: 50.78%] [Generator loss: 0.864367]\n",
      "10915 [Discriminator loss: 0.817508, acc.: 46.09%] [Generator loss: 0.945351]\n",
      "10916 [Discriminator loss: 0.559537, acc.: 75.00%] [Generator loss: 0.973486]\n",
      "10917 [Discriminator loss: 0.629319, acc.: 61.72%] [Generator loss: 1.002115]\n",
      "10918 [Discriminator loss: 0.622097, acc.: 70.31%] [Generator loss: 1.033477]\n",
      "10919 [Discriminator loss: 0.632472, acc.: 61.72%] [Generator loss: 0.951818]\n",
      "10920 [Discriminator loss: 0.696727, acc.: 56.25%] [Generator loss: 0.753981]\n",
      "10921 [Discriminator loss: 0.842518, acc.: 43.75%] [Generator loss: 0.995240]\n",
      "10922 [Discriminator loss: 0.710994, acc.: 50.00%] [Generator loss: 0.970994]\n",
      "10923 [Discriminator loss: 0.804453, acc.: 50.78%] [Generator loss: 0.869273]\n",
      "10924 [Discriminator loss: 0.783722, acc.: 54.69%] [Generator loss: 0.598907]\n",
      "10925 [Discriminator loss: 0.732041, acc.: 57.81%] [Generator loss: 0.825341]\n",
      "10926 [Discriminator loss: 0.614428, acc.: 68.75%] [Generator loss: 0.975583]\n",
      "10927 [Discriminator loss: 0.670311, acc.: 63.28%] [Generator loss: 1.002000]\n",
      "10928 [Discriminator loss: 0.774374, acc.: 50.00%] [Generator loss: 0.900014]\n",
      "10929 [Discriminator loss: 0.776547, acc.: 49.22%] [Generator loss: 0.898845]\n",
      "10930 [Discriminator loss: 0.573029, acc.: 71.88%] [Generator loss: 1.083361]\n",
      "10931 [Discriminator loss: 0.658695, acc.: 64.84%] [Generator loss: 1.131630]\n",
      "10932 [Discriminator loss: 0.771682, acc.: 51.56%] [Generator loss: 0.993839]\n",
      "10933 [Discriminator loss: 0.597414, acc.: 67.19%] [Generator loss: 0.971253]\n",
      "10934 [Discriminator loss: 0.853773, acc.: 42.97%] [Generator loss: 1.147867]\n",
      "10935 [Discriminator loss: 0.638764, acc.: 69.53%] [Generator loss: 1.001706]\n",
      "10936 [Discriminator loss: 0.529325, acc.: 75.00%] [Generator loss: 1.058478]\n",
      "10937 [Discriminator loss: 0.587781, acc.: 67.19%] [Generator loss: 0.814178]\n",
      "10938 [Discriminator loss: 0.777780, acc.: 46.09%] [Generator loss: 0.799025]\n",
      "10939 [Discriminator loss: 0.665961, acc.: 60.94%] [Generator loss: 0.917866]\n",
      "10940 [Discriminator loss: 0.770463, acc.: 53.12%] [Generator loss: 0.843885]\n",
      "10941 [Discriminator loss: 0.646013, acc.: 65.62%] [Generator loss: 0.795777]\n",
      "10942 [Discriminator loss: 0.555851, acc.: 71.88%] [Generator loss: 0.797299]\n",
      "10943 [Discriminator loss: 0.592939, acc.: 71.88%] [Generator loss: 0.770975]\n",
      "10944 [Discriminator loss: 0.496920, acc.: 78.91%] [Generator loss: 0.860267]\n",
      "10945 [Discriminator loss: 0.547384, acc.: 78.12%] [Generator loss: 0.662935]\n",
      "10946 [Discriminator loss: 0.467442, acc.: 88.28%] [Generator loss: 0.638592]\n",
      "10947 [Discriminator loss: 0.648680, acc.: 65.62%] [Generator loss: 0.656536]\n",
      "10948 [Discriminator loss: 0.460172, acc.: 83.59%] [Generator loss: 0.699379]\n",
      "10949 [Discriminator loss: 0.600392, acc.: 67.97%] [Generator loss: 0.616235]\n",
      "10950 [Discriminator loss: 0.822241, acc.: 42.97%] [Generator loss: 0.735629]\n",
      "10951 [Discriminator loss: 0.820102, acc.: 41.41%] [Generator loss: 0.773363]\n",
      "10952 [Discriminator loss: 0.683168, acc.: 64.84%] [Generator loss: 0.835369]\n",
      "10953 [Discriminator loss: 0.696489, acc.: 61.72%] [Generator loss: 0.995766]\n",
      "10954 [Discriminator loss: 0.522055, acc.: 75.00%] [Generator loss: 0.847866]\n",
      "10955 [Discriminator loss: 0.647122, acc.: 67.97%] [Generator loss: 0.816304]\n",
      "10956 [Discriminator loss: 0.775423, acc.: 43.75%] [Generator loss: 0.838964]\n",
      "10957 [Discriminator loss: 0.729500, acc.: 52.34%] [Generator loss: 0.980380]\n",
      "10958 [Discriminator loss: 0.554093, acc.: 75.78%] [Generator loss: 1.123259]\n",
      "10959 [Discriminator loss: 0.905690, acc.: 32.03%] [Generator loss: 0.826877]\n",
      "10960 [Discriminator loss: 0.595126, acc.: 67.97%] [Generator loss: 0.860646]\n",
      "10961 [Discriminator loss: 0.485485, acc.: 84.38%] [Generator loss: 0.755207]\n",
      "10962 [Discriminator loss: 0.671641, acc.: 63.28%] [Generator loss: 0.748947]\n",
      "10963 [Discriminator loss: 0.582167, acc.: 68.75%] [Generator loss: 0.748228]\n",
      "10964 [Discriminator loss: 0.417646, acc.: 88.28%] [Generator loss: 0.872605]\n",
      "10965 [Discriminator loss: 0.772621, acc.: 51.56%] [Generator loss: 0.630179]\n",
      "10966 [Discriminator loss: 0.754648, acc.: 52.34%] [Generator loss: 0.728647]\n",
      "10967 [Discriminator loss: 0.700149, acc.: 53.12%] [Generator loss: 0.750989]\n",
      "10968 [Discriminator loss: 0.699206, acc.: 59.38%] [Generator loss: 1.200517]\n",
      "10969 [Discriminator loss: 0.653130, acc.: 63.28%] [Generator loss: 1.316981]\n",
      "10970 [Discriminator loss: 0.684615, acc.: 55.47%] [Generator loss: 1.104385]\n",
      "10971 [Discriminator loss: 0.687930, acc.: 57.03%] [Generator loss: 0.981344]\n",
      "10972 [Discriminator loss: 0.832521, acc.: 46.88%] [Generator loss: 1.006182]\n",
      "10973 [Discriminator loss: 0.582995, acc.: 72.66%] [Generator loss: 0.834560]\n",
      "10974 [Discriminator loss: 0.709154, acc.: 54.69%] [Generator loss: 0.816919]\n",
      "10975 [Discriminator loss: 0.706519, acc.: 58.59%] [Generator loss: 0.908005]\n",
      "10976 [Discriminator loss: 0.652900, acc.: 60.94%] [Generator loss: 0.921798]\n",
      "10977 [Discriminator loss: 0.675694, acc.: 59.38%] [Generator loss: 0.874320]\n",
      "10978 [Discriminator loss: 0.656136, acc.: 64.84%] [Generator loss: 0.814406]\n",
      "10979 [Discriminator loss: 0.568479, acc.: 75.78%] [Generator loss: 0.912521]\n",
      "10980 [Discriminator loss: 0.735363, acc.: 57.81%] [Generator loss: 0.950393]\n",
      "10981 [Discriminator loss: 0.735919, acc.: 51.56%] [Generator loss: 1.109923]\n",
      "10982 [Discriminator loss: 0.670977, acc.: 66.41%] [Generator loss: 0.865517]\n",
      "10983 [Discriminator loss: 0.597114, acc.: 69.53%] [Generator loss: 0.777772]\n",
      "10984 [Discriminator loss: 0.587147, acc.: 66.41%] [Generator loss: 0.712250]\n",
      "10985 [Discriminator loss: 0.504950, acc.: 73.44%] [Generator loss: 0.712191]\n",
      "10986 [Discriminator loss: 0.569696, acc.: 73.44%] [Generator loss: 0.776378]\n",
      "10987 [Discriminator loss: 0.599468, acc.: 64.06%] [Generator loss: 0.897677]\n",
      "10988 [Discriminator loss: 0.539885, acc.: 78.91%] [Generator loss: 0.721296]\n",
      "10989 [Discriminator loss: 0.795130, acc.: 49.22%] [Generator loss: 0.943565]\n",
      "10990 [Discriminator loss: 0.523285, acc.: 76.56%] [Generator loss: 0.987789]\n",
      "10991 [Discriminator loss: 0.589278, acc.: 70.31%] [Generator loss: 0.966206]\n",
      "10992 [Discriminator loss: 0.516567, acc.: 77.34%] [Generator loss: 1.091177]\n",
      "10993 [Discriminator loss: 0.787003, acc.: 52.34%] [Generator loss: 1.000484]\n",
      "10994 [Discriminator loss: 0.740015, acc.: 54.69%] [Generator loss: 1.131084]\n",
      "10995 [Discriminator loss: 0.789781, acc.: 53.91%] [Generator loss: 0.978500]\n",
      "10996 [Discriminator loss: 0.628118, acc.: 64.06%] [Generator loss: 0.966634]\n",
      "10997 [Discriminator loss: 0.649744, acc.: 63.28%] [Generator loss: 0.882936]\n",
      "10998 [Discriminator loss: 0.541071, acc.: 77.34%] [Generator loss: 0.929936]\n",
      "10999 [Discriminator loss: 0.604933, acc.: 65.62%] [Generator loss: 0.830458]\n",
      "11000 [Discriminator loss: 0.727853, acc.: 54.69%] [Generator loss: 0.714666]\n",
      "11001 [Discriminator loss: 0.689060, acc.: 61.72%] [Generator loss: 0.666210]\n",
      "11002 [Discriminator loss: 0.554866, acc.: 72.66%] [Generator loss: 0.673686]\n",
      "11003 [Discriminator loss: 0.582624, acc.: 76.56%] [Generator loss: 0.696181]\n",
      "11004 [Discriminator loss: 0.724532, acc.: 52.34%] [Generator loss: 0.666536]\n",
      "11005 [Discriminator loss: 0.729796, acc.: 50.78%] [Generator loss: 0.694403]\n",
      "11006 [Discriminator loss: 0.542651, acc.: 75.00%] [Generator loss: 0.827911]\n",
      "11007 [Discriminator loss: 0.774601, acc.: 51.56%] [Generator loss: 0.692451]\n",
      "11008 [Discriminator loss: 0.705633, acc.: 54.69%] [Generator loss: 0.768178]\n",
      "11009 [Discriminator loss: 0.511540, acc.: 81.25%] [Generator loss: 0.932954]\n",
      "11010 [Discriminator loss: 0.878944, acc.: 41.41%] [Generator loss: 0.826387]\n",
      "11011 [Discriminator loss: 0.757449, acc.: 49.22%] [Generator loss: 0.832520]\n",
      "11012 [Discriminator loss: 0.605129, acc.: 66.41%] [Generator loss: 0.853247]\n",
      "11013 [Discriminator loss: 0.663614, acc.: 64.84%] [Generator loss: 0.807083]\n",
      "11014 [Discriminator loss: 0.629051, acc.: 64.06%] [Generator loss: 0.962556]\n",
      "11015 [Discriminator loss: 0.750331, acc.: 51.56%] [Generator loss: 0.896424]\n",
      "11016 [Discriminator loss: 0.516616, acc.: 78.91%] [Generator loss: 1.108061]\n",
      "11017 [Discriminator loss: 0.502703, acc.: 78.91%] [Generator loss: 0.848459]\n",
      "11018 [Discriminator loss: 0.648081, acc.: 61.72%] [Generator loss: 0.841093]\n",
      "11019 [Discriminator loss: 0.648805, acc.: 73.44%] [Generator loss: 0.814166]\n",
      "11020 [Discriminator loss: 0.820328, acc.: 52.34%] [Generator loss: 0.819628]\n",
      "11021 [Discriminator loss: 0.752137, acc.: 61.72%] [Generator loss: 0.750288]\n",
      "11022 [Discriminator loss: 0.663401, acc.: 62.50%] [Generator loss: 0.660899]\n",
      "11023 [Discriminator loss: 0.445598, acc.: 86.72%] [Generator loss: 0.851857]\n",
      "11024 [Discriminator loss: 0.755018, acc.: 52.34%] [Generator loss: 0.665012]\n",
      "11025 [Discriminator loss: 0.508191, acc.: 77.34%] [Generator loss: 0.739539]\n",
      "11026 [Discriminator loss: 0.634830, acc.: 65.62%] [Generator loss: 0.765213]\n",
      "11027 [Discriminator loss: 0.558527, acc.: 69.53%] [Generator loss: 0.770527]\n",
      "11028 [Discriminator loss: 0.533679, acc.: 75.00%] [Generator loss: 0.751976]\n",
      "11029 [Discriminator loss: 0.650509, acc.: 61.72%] [Generator loss: 0.810261]\n",
      "11030 [Discriminator loss: 0.840740, acc.: 41.41%] [Generator loss: 1.083620]\n",
      "11031 [Discriminator loss: 0.605188, acc.: 65.62%] [Generator loss: 0.933134]\n",
      "11032 [Discriminator loss: 0.798543, acc.: 46.09%] [Generator loss: 0.801616]\n",
      "11033 [Discriminator loss: 0.900485, acc.: 35.16%] [Generator loss: 0.853466]\n",
      "11034 [Discriminator loss: 0.522232, acc.: 79.69%] [Generator loss: 0.893605]\n",
      "11035 [Discriminator loss: 0.765902, acc.: 50.00%] [Generator loss: 0.804904]\n",
      "11036 [Discriminator loss: 0.682947, acc.: 54.69%] [Generator loss: 1.191494]\n",
      "11037 [Discriminator loss: 0.573156, acc.: 69.53%] [Generator loss: 0.923588]\n",
      "11038 [Discriminator loss: 0.538656, acc.: 67.97%] [Generator loss: 0.812714]\n",
      "11039 [Discriminator loss: 0.578496, acc.: 68.75%] [Generator loss: 0.693114]\n",
      "11040 [Discriminator loss: 0.490458, acc.: 82.03%] [Generator loss: 0.693957]\n",
      "11041 [Discriminator loss: 0.641067, acc.: 57.81%] [Generator loss: 0.799561]\n",
      "11042 [Discriminator loss: 0.673215, acc.: 60.94%] [Generator loss: 0.968823]\n",
      "11043 [Discriminator loss: 0.486892, acc.: 79.69%] [Generator loss: 0.754108]\n",
      "11044 [Discriminator loss: 0.466358, acc.: 81.25%] [Generator loss: 0.631025]\n",
      "11045 [Discriminator loss: 0.402564, acc.: 85.94%] [Generator loss: 0.612524]\n",
      "11046 [Discriminator loss: 0.568240, acc.: 77.34%] [Generator loss: 0.695374]\n",
      "11047 [Discriminator loss: 0.640677, acc.: 63.28%] [Generator loss: 0.752636]\n",
      "11048 [Discriminator loss: 0.732056, acc.: 51.56%] [Generator loss: 0.983716]\n",
      "11049 [Discriminator loss: 0.514959, acc.: 75.00%] [Generator loss: 1.070797]\n",
      "11050 [Discriminator loss: 0.673498, acc.: 59.38%] [Generator loss: 0.816773]\n",
      "11051 [Discriminator loss: 0.614778, acc.: 66.41%] [Generator loss: 0.812569]\n",
      "11052 [Discriminator loss: 0.684060, acc.: 63.28%] [Generator loss: 0.925656]\n",
      "11053 [Discriminator loss: 0.733718, acc.: 56.25%] [Generator loss: 0.705509]\n",
      "11054 [Discriminator loss: 0.706767, acc.: 54.69%] [Generator loss: 0.822388]\n",
      "11055 [Discriminator loss: 0.438415, acc.: 82.03%] [Generator loss: 0.744407]\n",
      "11056 [Discriminator loss: 0.573048, acc.: 64.84%] [Generator loss: 0.771272]\n",
      "11057 [Discriminator loss: 0.637539, acc.: 69.53%] [Generator loss: 0.725035]\n",
      "11058 [Discriminator loss: 0.592118, acc.: 69.53%] [Generator loss: 0.963038]\n",
      "11059 [Discriminator loss: 0.576393, acc.: 73.44%] [Generator loss: 0.639457]\n",
      "11060 [Discriminator loss: 0.801758, acc.: 45.31%] [Generator loss: 0.807640]\n",
      "11061 [Discriminator loss: 0.986219, acc.: 32.03%] [Generator loss: 0.733382]\n",
      "11062 [Discriminator loss: 0.555114, acc.: 70.31%] [Generator loss: 0.544853]\n",
      "11063 [Discriminator loss: 0.828732, acc.: 51.56%] [Generator loss: 0.812145]\n",
      "11064 [Discriminator loss: 0.701375, acc.: 58.59%] [Generator loss: 1.090292]\n",
      "11065 [Discriminator loss: 0.503103, acc.: 78.91%] [Generator loss: 1.102163]\n",
      "11066 [Discriminator loss: 0.602830, acc.: 70.31%] [Generator loss: 0.853442]\n",
      "11067 [Discriminator loss: 0.847406, acc.: 44.53%] [Generator loss: 0.898325]\n",
      "11068 [Discriminator loss: 0.710865, acc.: 50.00%] [Generator loss: 0.911839]\n",
      "11069 [Discriminator loss: 0.548173, acc.: 75.00%] [Generator loss: 0.933715]\n",
      "11070 [Discriminator loss: 0.789826, acc.: 49.22%] [Generator loss: 0.988483]\n",
      "11071 [Discriminator loss: 0.673645, acc.: 56.25%] [Generator loss: 0.951662]\n",
      "11072 [Discriminator loss: 0.707641, acc.: 53.91%] [Generator loss: 0.902975]\n",
      "11073 [Discriminator loss: 0.662896, acc.: 59.38%] [Generator loss: 0.713514]\n",
      "11074 [Discriminator loss: 0.725332, acc.: 57.03%] [Generator loss: 0.697010]\n",
      "11075 [Discriminator loss: 0.801173, acc.: 52.34%] [Generator loss: 0.663890]\n",
      "11076 [Discriminator loss: 0.634600, acc.: 64.84%] [Generator loss: 0.699706]\n",
      "11077 [Discriminator loss: 0.743181, acc.: 52.34%] [Generator loss: 0.895217]\n",
      "11078 [Discriminator loss: 0.677026, acc.: 64.84%] [Generator loss: 0.877892]\n",
      "11079 [Discriminator loss: 0.649681, acc.: 67.97%] [Generator loss: 0.794476]\n",
      "11080 [Discriminator loss: 0.790929, acc.: 47.66%] [Generator loss: 0.889844]\n",
      "11081 [Discriminator loss: 0.660721, acc.: 59.38%] [Generator loss: 1.020182]\n",
      "11082 [Discriminator loss: 0.663487, acc.: 64.84%] [Generator loss: 0.899248]\n",
      "11083 [Discriminator loss: 0.692794, acc.: 55.47%] [Generator loss: 0.938185]\n",
      "11084 [Discriminator loss: 0.542992, acc.: 74.22%] [Generator loss: 0.886891]\n",
      "11085 [Discriminator loss: 0.616355, acc.: 64.06%] [Generator loss: 0.915754]\n",
      "11086 [Discriminator loss: 0.594890, acc.: 66.41%] [Generator loss: 0.870327]\n",
      "11087 [Discriminator loss: 0.706144, acc.: 51.56%] [Generator loss: 0.875196]\n",
      "11088 [Discriminator loss: 0.524472, acc.: 75.00%] [Generator loss: 0.987827]\n",
      "11089 [Discriminator loss: 0.610527, acc.: 64.84%] [Generator loss: 0.968639]\n",
      "11090 [Discriminator loss: 0.720523, acc.: 54.69%] [Generator loss: 0.770285]\n",
      "11091 [Discriminator loss: 0.668547, acc.: 59.38%] [Generator loss: 0.895387]\n",
      "11092 [Discriminator loss: 0.608143, acc.: 64.06%] [Generator loss: 1.004920]\n",
      "11093 [Discriminator loss: 0.549302, acc.: 67.97%] [Generator loss: 0.999901]\n",
      "11094 [Discriminator loss: 0.473172, acc.: 79.69%] [Generator loss: 0.921439]\n",
      "11095 [Discriminator loss: 0.544752, acc.: 76.56%] [Generator loss: 0.894782]\n",
      "11096 [Discriminator loss: 0.648394, acc.: 56.25%] [Generator loss: 0.956359]\n",
      "11097 [Discriminator loss: 0.724765, acc.: 52.34%] [Generator loss: 0.865850]\n",
      "11098 [Discriminator loss: 0.802475, acc.: 44.53%] [Generator loss: 0.949447]\n",
      "11099 [Discriminator loss: 0.714119, acc.: 56.25%] [Generator loss: 0.956130]\n",
      "11100 [Discriminator loss: 0.640054, acc.: 63.28%] [Generator loss: 0.990479]\n",
      "11101 [Discriminator loss: 0.593492, acc.: 66.41%] [Generator loss: 0.976928]\n",
      "11102 [Discriminator loss: 0.689015, acc.: 59.38%] [Generator loss: 0.877673]\n",
      "11103 [Discriminator loss: 0.630870, acc.: 68.75%] [Generator loss: 0.952873]\n",
      "11104 [Discriminator loss: 0.847069, acc.: 49.22%] [Generator loss: 0.767219]\n",
      "11105 [Discriminator loss: 0.735547, acc.: 58.59%] [Generator loss: 0.801891]\n",
      "11106 [Discriminator loss: 0.716292, acc.: 50.78%] [Generator loss: 0.948991]\n",
      "11107 [Discriminator loss: 0.795073, acc.: 46.09%] [Generator loss: 0.939504]\n",
      "11108 [Discriminator loss: 0.621214, acc.: 69.53%] [Generator loss: 0.861486]\n",
      "11109 [Discriminator loss: 0.549571, acc.: 71.09%] [Generator loss: 0.903510]\n",
      "11110 [Discriminator loss: 0.657634, acc.: 60.16%] [Generator loss: 0.917071]\n",
      "11111 [Discriminator loss: 0.715466, acc.: 57.81%] [Generator loss: 0.897856]\n",
      "11112 [Discriminator loss: 0.654230, acc.: 59.38%] [Generator loss: 0.906681]\n",
      "11113 [Discriminator loss: 0.667806, acc.: 64.06%] [Generator loss: 1.136659]\n",
      "11114 [Discriminator loss: 0.613669, acc.: 67.19%] [Generator loss: 0.912251]\n",
      "11115 [Discriminator loss: 0.646163, acc.: 71.88%] [Generator loss: 1.132603]\n",
      "11116 [Discriminator loss: 0.765398, acc.: 43.75%] [Generator loss: 1.004990]\n",
      "11117 [Discriminator loss: 0.993056, acc.: 39.06%] [Generator loss: 0.872683]\n",
      "11118 [Discriminator loss: 0.809208, acc.: 47.66%] [Generator loss: 0.644450]\n",
      "11119 [Discriminator loss: 0.648671, acc.: 59.38%] [Generator loss: 0.728010]\n",
      "11120 [Discriminator loss: 0.745989, acc.: 57.81%] [Generator loss: 0.772599]\n",
      "11121 [Discriminator loss: 0.675303, acc.: 64.06%] [Generator loss: 0.783688]\n",
      "11122 [Discriminator loss: 0.566204, acc.: 73.44%] [Generator loss: 0.938780]\n",
      "11123 [Discriminator loss: 0.593718, acc.: 67.97%] [Generator loss: 0.936227]\n",
      "11124 [Discriminator loss: 0.624181, acc.: 64.84%] [Generator loss: 0.822063]\n",
      "11125 [Discriminator loss: 0.443849, acc.: 78.12%] [Generator loss: 0.796595]\n",
      "11126 [Discriminator loss: 0.813182, acc.: 43.75%] [Generator loss: 0.663487]\n",
      "11127 [Discriminator loss: 0.515627, acc.: 74.22%] [Generator loss: 0.750042]\n",
      "11128 [Discriminator loss: 0.643713, acc.: 68.75%] [Generator loss: 0.769206]\n",
      "11129 [Discriminator loss: 0.644949, acc.: 63.28%] [Generator loss: 0.875398]\n",
      "11130 [Discriminator loss: 0.770249, acc.: 49.22%] [Generator loss: 0.665125]\n",
      "11131 [Discriminator loss: 0.684640, acc.: 59.38%] [Generator loss: 0.761935]\n",
      "11132 [Discriminator loss: 0.611283, acc.: 66.41%] [Generator loss: 1.075384]\n",
      "11133 [Discriminator loss: 0.712836, acc.: 58.59%] [Generator loss: 0.860543]\n",
      "11134 [Discriminator loss: 0.757264, acc.: 53.91%] [Generator loss: 0.807682]\n",
      "11135 [Discriminator loss: 0.714342, acc.: 54.69%] [Generator loss: 0.815711]\n",
      "11136 [Discriminator loss: 0.736766, acc.: 54.69%] [Generator loss: 0.818250]\n",
      "11137 [Discriminator loss: 0.650678, acc.: 65.62%] [Generator loss: 0.857618]\n",
      "11138 [Discriminator loss: 0.684560, acc.: 67.19%] [Generator loss: 1.124154]\n",
      "11139 [Discriminator loss: 0.761427, acc.: 57.03%] [Generator loss: 0.868637]\n",
      "11140 [Discriminator loss: 0.645986, acc.: 64.06%] [Generator loss: 0.849285]\n",
      "11141 [Discriminator loss: 0.673197, acc.: 61.72%] [Generator loss: 0.941447]\n",
      "11142 [Discriminator loss: 0.545996, acc.: 73.44%] [Generator loss: 0.927936]\n",
      "11143 [Discriminator loss: 0.601570, acc.: 67.97%] [Generator loss: 0.897220]\n",
      "11144 [Discriminator loss: 0.685734, acc.: 59.38%] [Generator loss: 0.865629]\n",
      "11145 [Discriminator loss: 0.797183, acc.: 49.22%] [Generator loss: 0.853325]\n",
      "11146 [Discriminator loss: 0.703714, acc.: 59.38%] [Generator loss: 0.751707]\n",
      "11147 [Discriminator loss: 0.656846, acc.: 58.59%] [Generator loss: 0.805330]\n",
      "11148 [Discriminator loss: 0.724351, acc.: 62.50%] [Generator loss: 0.837537]\n",
      "11149 [Discriminator loss: 0.503213, acc.: 82.81%] [Generator loss: 0.833022]\n",
      "11150 [Discriminator loss: 0.600335, acc.: 64.06%] [Generator loss: 0.946685]\n",
      "11151 [Discriminator loss: 0.742139, acc.: 57.03%] [Generator loss: 0.847364]\n",
      "11152 [Discriminator loss: 0.576893, acc.: 71.09%] [Generator loss: 0.904643]\n",
      "11153 [Discriminator loss: 0.777072, acc.: 50.78%] [Generator loss: 0.639840]\n",
      "11154 [Discriminator loss: 0.686749, acc.: 59.38%] [Generator loss: 0.670631]\n",
      "11155 [Discriminator loss: 0.578577, acc.: 75.78%] [Generator loss: 0.781532]\n",
      "11156 [Discriminator loss: 0.659163, acc.: 57.81%] [Generator loss: 0.966246]\n",
      "11157 [Discriminator loss: 0.692502, acc.: 55.47%] [Generator loss: 0.950895]\n",
      "11158 [Discriminator loss: 0.566576, acc.: 66.41%] [Generator loss: 0.964442]\n",
      "11159 [Discriminator loss: 0.687599, acc.: 57.81%] [Generator loss: 0.998508]\n",
      "11160 [Discriminator loss: 0.890685, acc.: 41.41%] [Generator loss: 0.864338]\n",
      "11161 [Discriminator loss: 0.781555, acc.: 51.56%] [Generator loss: 0.996575]\n",
      "11162 [Discriminator loss: 0.538045, acc.: 77.34%] [Generator loss: 0.974808]\n",
      "11163 [Discriminator loss: 0.554893, acc.: 71.88%] [Generator loss: 0.888111]\n",
      "11164 [Discriminator loss: 0.883475, acc.: 42.97%] [Generator loss: 0.881353]\n",
      "11165 [Discriminator loss: 0.674091, acc.: 58.59%] [Generator loss: 0.792096]\n",
      "11166 [Discriminator loss: 0.566168, acc.: 67.97%] [Generator loss: 0.924612]\n",
      "11167 [Discriminator loss: 0.711644, acc.: 60.16%] [Generator loss: 0.839992]\n",
      "11168 [Discriminator loss: 0.621356, acc.: 68.75%] [Generator loss: 0.767947]\n",
      "11169 [Discriminator loss: 0.618735, acc.: 67.97%] [Generator loss: 0.784615]\n",
      "11170 [Discriminator loss: 0.497686, acc.: 78.12%] [Generator loss: 0.974589]\n",
      "11171 [Discriminator loss: 0.529213, acc.: 80.47%] [Generator loss: 0.764263]\n",
      "11172 [Discriminator loss: 0.632900, acc.: 66.41%] [Generator loss: 0.993887]\n",
      "11173 [Discriminator loss: 0.564637, acc.: 62.50%] [Generator loss: 1.013044]\n",
      "11174 [Discriminator loss: 0.744919, acc.: 53.91%] [Generator loss: 0.807674]\n",
      "11175 [Discriminator loss: 0.703439, acc.: 56.25%] [Generator loss: 0.835136]\n",
      "11176 [Discriminator loss: 0.591077, acc.: 68.75%] [Generator loss: 0.942808]\n",
      "11177 [Discriminator loss: 0.804943, acc.: 53.91%] [Generator loss: 0.930019]\n",
      "11178 [Discriminator loss: 0.634268, acc.: 67.19%] [Generator loss: 0.924938]\n",
      "11179 [Discriminator loss: 0.738962, acc.: 55.47%] [Generator loss: 0.838844]\n",
      "11180 [Discriminator loss: 0.635921, acc.: 63.28%] [Generator loss: 0.759363]\n",
      "11181 [Discriminator loss: 0.800236, acc.: 58.59%] [Generator loss: 1.174181]\n",
      "11182 [Discriminator loss: 0.460405, acc.: 78.91%] [Generator loss: 1.181344]\n",
      "11183 [Discriminator loss: 0.774943, acc.: 51.56%] [Generator loss: 0.760772]\n",
      "11184 [Discriminator loss: 0.584504, acc.: 63.28%] [Generator loss: 0.798284]\n",
      "11185 [Discriminator loss: 0.569786, acc.: 71.09%] [Generator loss: 1.005730]\n",
      "11186 [Discriminator loss: 0.629727, acc.: 66.41%] [Generator loss: 0.925072]\n",
      "11187 [Discriminator loss: 0.747482, acc.: 57.81%] [Generator loss: 0.871227]\n",
      "11188 [Discriminator loss: 0.652424, acc.: 64.06%] [Generator loss: 0.742174]\n",
      "11189 [Discriminator loss: 0.802835, acc.: 45.31%] [Generator loss: 0.802049]\n",
      "11190 [Discriminator loss: 0.601316, acc.: 64.84%] [Generator loss: 1.104491]\n",
      "11191 [Discriminator loss: 0.549609, acc.: 73.44%] [Generator loss: 0.934025]\n",
      "11192 [Discriminator loss: 0.701577, acc.: 62.50%] [Generator loss: 1.004852]\n",
      "11193 [Discriminator loss: 0.654261, acc.: 65.62%] [Generator loss: 0.933048]\n",
      "11194 [Discriminator loss: 0.515578, acc.: 76.56%] [Generator loss: 0.992382]\n",
      "11195 [Discriminator loss: 0.584103, acc.: 68.75%] [Generator loss: 0.808294]\n",
      "11196 [Discriminator loss: 0.675140, acc.: 63.28%] [Generator loss: 0.689544]\n",
      "11197 [Discriminator loss: 0.609351, acc.: 67.97%] [Generator loss: 0.610003]\n",
      "11198 [Discriminator loss: 0.588507, acc.: 70.31%] [Generator loss: 0.813166]\n",
      "11199 [Discriminator loss: 0.770590, acc.: 49.22%] [Generator loss: 0.756621]\n",
      "11200 [Discriminator loss: 0.720774, acc.: 57.81%] [Generator loss: 0.709465]\n",
      "11201 [Discriminator loss: 0.787676, acc.: 50.00%] [Generator loss: 0.681070]\n",
      "11202 [Discriminator loss: 0.671462, acc.: 58.59%] [Generator loss: 0.917785]\n",
      "11203 [Discriminator loss: 0.627648, acc.: 67.19%] [Generator loss: 0.869749]\n",
      "11204 [Discriminator loss: 0.795210, acc.: 46.09%] [Generator loss: 1.054097]\n",
      "11205 [Discriminator loss: 0.729263, acc.: 62.50%] [Generator loss: 0.965692]\n",
      "11206 [Discriminator loss: 0.658533, acc.: 64.06%] [Generator loss: 1.116514]\n",
      "11207 [Discriminator loss: 0.647520, acc.: 69.53%] [Generator loss: 1.052098]\n",
      "11208 [Discriminator loss: 0.527923, acc.: 82.03%] [Generator loss: 0.828769]\n",
      "11209 [Discriminator loss: 0.477040, acc.: 83.59%] [Generator loss: 0.808208]\n",
      "11210 [Discriminator loss: 0.499727, acc.: 81.25%] [Generator loss: 0.702142]\n",
      "11211 [Discriminator loss: 0.540300, acc.: 75.78%] [Generator loss: 0.869432]\n",
      "11212 [Discriminator loss: 0.529497, acc.: 73.44%] [Generator loss: 0.802786]\n",
      "11213 [Discriminator loss: 0.577943, acc.: 69.53%] [Generator loss: 0.727084]\n",
      "11214 [Discriminator loss: 0.644147, acc.: 64.06%] [Generator loss: 0.762160]\n",
      "11215 [Discriminator loss: 0.644481, acc.: 64.84%] [Generator loss: 0.787945]\n",
      "11216 [Discriminator loss: 0.664989, acc.: 57.03%] [Generator loss: 0.851058]\n",
      "11217 [Discriminator loss: 0.768610, acc.: 48.44%] [Generator loss: 0.624728]\n",
      "11218 [Discriminator loss: 0.614881, acc.: 69.53%] [Generator loss: 0.846900]\n",
      "11219 [Discriminator loss: 0.677843, acc.: 66.41%] [Generator loss: 0.762645]\n",
      "11220 [Discriminator loss: 0.796377, acc.: 39.84%] [Generator loss: 0.675057]\n",
      "11221 [Discriminator loss: 0.661951, acc.: 59.38%] [Generator loss: 0.836193]\n",
      "11222 [Discriminator loss: 0.747020, acc.: 50.78%] [Generator loss: 0.820313]\n",
      "11223 [Discriminator loss: 0.554321, acc.: 73.44%] [Generator loss: 0.807671]\n",
      "11224 [Discriminator loss: 0.881905, acc.: 35.16%] [Generator loss: 0.695420]\n",
      "11225 [Discriminator loss: 0.654358, acc.: 60.94%] [Generator loss: 0.806684]\n",
      "11226 [Discriminator loss: 0.620546, acc.: 69.53%] [Generator loss: 0.934887]\n",
      "11227 [Discriminator loss: 0.660422, acc.: 60.16%] [Generator loss: 1.035209]\n",
      "11228 [Discriminator loss: 0.654547, acc.: 66.41%] [Generator loss: 0.986726]\n",
      "11229 [Discriminator loss: 0.739226, acc.: 53.91%] [Generator loss: 0.892536]\n",
      "11230 [Discriminator loss: 0.691430, acc.: 61.72%] [Generator loss: 0.752412]\n",
      "11231 [Discriminator loss: 0.723687, acc.: 61.72%] [Generator loss: 0.871055]\n",
      "11232 [Discriminator loss: 0.628642, acc.: 64.84%] [Generator loss: 0.830878]\n",
      "11233 [Discriminator loss: 0.653113, acc.: 60.94%] [Generator loss: 0.965233]\n",
      "11234 [Discriminator loss: 0.513223, acc.: 76.56%] [Generator loss: 0.916428]\n",
      "11235 [Discriminator loss: 0.669602, acc.: 61.72%] [Generator loss: 0.995286]\n",
      "11236 [Discriminator loss: 0.690331, acc.: 60.94%] [Generator loss: 0.997052]\n",
      "11237 [Discriminator loss: 0.665541, acc.: 60.16%] [Generator loss: 1.181003]\n",
      "11238 [Discriminator loss: 0.739899, acc.: 47.66%] [Generator loss: 1.062057]\n",
      "11239 [Discriminator loss: 0.674289, acc.: 63.28%] [Generator loss: 1.029512]\n",
      "11240 [Discriminator loss: 0.608804, acc.: 66.41%] [Generator loss: 0.719655]\n",
      "11241 [Discriminator loss: 0.619773, acc.: 67.97%] [Generator loss: 0.638429]\n",
      "11242 [Discriminator loss: 0.865227, acc.: 41.41%] [Generator loss: 0.821315]\n",
      "11243 [Discriminator loss: 0.695103, acc.: 59.38%] [Generator loss: 0.944588]\n",
      "11244 [Discriminator loss: 0.613644, acc.: 64.06%] [Generator loss: 0.910996]\n",
      "11245 [Discriminator loss: 0.697254, acc.: 59.38%] [Generator loss: 0.791084]\n",
      "11246 [Discriminator loss: 0.661624, acc.: 60.16%] [Generator loss: 0.812116]\n",
      "11247 [Discriminator loss: 0.804880, acc.: 49.22%] [Generator loss: 0.934098]\n",
      "11248 [Discriminator loss: 0.628751, acc.: 63.28%] [Generator loss: 0.974370]\n",
      "11249 [Discriminator loss: 0.546457, acc.: 74.22%] [Generator loss: 0.814448]\n",
      "11250 [Discriminator loss: 0.662302, acc.: 62.50%] [Generator loss: 0.923593]\n",
      "11251 [Discriminator loss: 0.643841, acc.: 70.31%] [Generator loss: 0.900085]\n",
      "11252 [Discriminator loss: 0.515230, acc.: 82.03%] [Generator loss: 0.856729]\n",
      "11253 [Discriminator loss: 0.691679, acc.: 60.16%] [Generator loss: 0.922925]\n",
      "11254 [Discriminator loss: 0.845786, acc.: 43.75%] [Generator loss: 0.865984]\n",
      "11255 [Discriminator loss: 0.566426, acc.: 70.31%] [Generator loss: 0.853913]\n",
      "11256 [Discriminator loss: 0.651272, acc.: 60.16%] [Generator loss: 0.890956]\n",
      "11257 [Discriminator loss: 0.821462, acc.: 39.06%] [Generator loss: 0.818595]\n",
      "11258 [Discriminator loss: 0.776369, acc.: 48.44%] [Generator loss: 0.806033]\n",
      "11259 [Discriminator loss: 0.547534, acc.: 68.75%] [Generator loss: 0.741633]\n",
      "11260 [Discriminator loss: 0.826843, acc.: 48.44%] [Generator loss: 0.666165]\n",
      "11261 [Discriminator loss: 0.594764, acc.: 66.41%] [Generator loss: 0.862636]\n",
      "11262 [Discriminator loss: 0.757840, acc.: 47.66%] [Generator loss: 0.687313]\n",
      "11263 [Discriminator loss: 0.646069, acc.: 63.28%] [Generator loss: 0.817769]\n",
      "11264 [Discriminator loss: 0.660103, acc.: 64.84%] [Generator loss: 0.822219]\n",
      "11265 [Discriminator loss: 0.724918, acc.: 47.66%] [Generator loss: 0.829387]\n",
      "11266 [Discriminator loss: 0.596795, acc.: 74.22%] [Generator loss: 0.849462]\n",
      "11267 [Discriminator loss: 0.755156, acc.: 51.56%] [Generator loss: 0.869157]\n",
      "11268 [Discriminator loss: 0.589821, acc.: 71.88%] [Generator loss: 1.028369]\n",
      "11269 [Discriminator loss: 0.622736, acc.: 60.94%] [Generator loss: 0.878737]\n",
      "11270 [Discriminator loss: 0.601656, acc.: 70.31%] [Generator loss: 0.791223]\n",
      "11271 [Discriminator loss: 0.603926, acc.: 67.19%] [Generator loss: 0.868471]\n",
      "11272 [Discriminator loss: 0.499061, acc.: 76.56%] [Generator loss: 0.775989]\n",
      "11273 [Discriminator loss: 0.737337, acc.: 52.34%] [Generator loss: 0.924247]\n",
      "11274 [Discriminator loss: 0.535273, acc.: 74.22%] [Generator loss: 0.738382]\n",
      "11275 [Discriminator loss: 0.668557, acc.: 59.38%] [Generator loss: 0.573605]\n",
      "11276 [Discriminator loss: 0.478964, acc.: 82.81%] [Generator loss: 0.711573]\n",
      "11277 [Discriminator loss: 0.545097, acc.: 71.09%] [Generator loss: 0.700977]\n",
      "11278 [Discriminator loss: 0.707083, acc.: 52.34%] [Generator loss: 0.787969]\n",
      "11279 [Discriminator loss: 0.460678, acc.: 80.47%] [Generator loss: 0.768564]\n",
      "11280 [Discriminator loss: 0.694265, acc.: 55.47%] [Generator loss: 0.674743]\n",
      "11281 [Discriminator loss: 0.724696, acc.: 60.16%] [Generator loss: 0.784617]\n",
      "11282 [Discriminator loss: 0.751874, acc.: 56.25%] [Generator loss: 1.091730]\n",
      "11283 [Discriminator loss: 0.726759, acc.: 53.91%] [Generator loss: 0.999218]\n",
      "11284 [Discriminator loss: 0.672740, acc.: 54.69%] [Generator loss: 0.831391]\n",
      "11285 [Discriminator loss: 0.746019, acc.: 51.56%] [Generator loss: 0.804293]\n",
      "11286 [Discriminator loss: 0.632078, acc.: 67.19%] [Generator loss: 0.809395]\n",
      "11287 [Discriminator loss: 0.809358, acc.: 44.53%] [Generator loss: 0.811142]\n",
      "11288 [Discriminator loss: 0.805734, acc.: 42.97%] [Generator loss: 0.861789]\n",
      "11289 [Discriminator loss: 0.708540, acc.: 62.50%] [Generator loss: 0.918863]\n",
      "11290 [Discriminator loss: 0.597103, acc.: 68.75%] [Generator loss: 0.899668]\n",
      "11291 [Discriminator loss: 0.695753, acc.: 64.06%] [Generator loss: 0.759484]\n",
      "11292 [Discriminator loss: 0.563309, acc.: 74.22%] [Generator loss: 0.776293]\n",
      "11293 [Discriminator loss: 0.769259, acc.: 49.22%] [Generator loss: 0.738086]\n",
      "11294 [Discriminator loss: 0.466768, acc.: 81.25%] [Generator loss: 0.802794]\n",
      "11295 [Discriminator loss: 0.551873, acc.: 78.12%] [Generator loss: 0.806232]\n",
      "11296 [Discriminator loss: 0.561128, acc.: 71.09%] [Generator loss: 0.794224]\n",
      "11297 [Discriminator loss: 0.524821, acc.: 76.56%] [Generator loss: 0.704018]\n",
      "11298 [Discriminator loss: 0.598514, acc.: 73.44%] [Generator loss: 0.556333]\n",
      "11299 [Discriminator loss: 0.537423, acc.: 69.53%] [Generator loss: 0.531539]\n",
      "11300 [Discriminator loss: 0.696866, acc.: 55.47%] [Generator loss: 0.782658]\n",
      "11301 [Discriminator loss: 0.707336, acc.: 62.50%] [Generator loss: 1.170659]\n",
      "11302 [Discriminator loss: 0.585096, acc.: 62.50%] [Generator loss: 0.851191]\n",
      "11303 [Discriminator loss: 0.678901, acc.: 59.38%] [Generator loss: 0.806232]\n",
      "11304 [Discriminator loss: 0.606270, acc.: 67.19%] [Generator loss: 0.716153]\n",
      "11305 [Discriminator loss: 0.648198, acc.: 60.94%] [Generator loss: 0.652959]\n",
      "11306 [Discriminator loss: 1.121305, acc.: 18.75%] [Generator loss: 0.706156]\n",
      "11307 [Discriminator loss: 0.854236, acc.: 42.19%] [Generator loss: 0.709039]\n",
      "11308 [Discriminator loss: 0.782885, acc.: 55.47%] [Generator loss: 0.828687]\n",
      "11309 [Discriminator loss: 0.609860, acc.: 64.84%] [Generator loss: 1.010001]\n",
      "11310 [Discriminator loss: 0.714676, acc.: 54.69%] [Generator loss: 0.908499]\n",
      "11311 [Discriminator loss: 0.622820, acc.: 68.75%] [Generator loss: 0.951394]\n",
      "11312 [Discriminator loss: 0.603833, acc.: 68.75%] [Generator loss: 0.967064]\n",
      "11313 [Discriminator loss: 0.652183, acc.: 58.59%] [Generator loss: 1.074701]\n",
      "11314 [Discriminator loss: 0.621210, acc.: 65.62%] [Generator loss: 1.002474]\n",
      "11315 [Discriminator loss: 0.564501, acc.: 71.88%] [Generator loss: 1.016528]\n",
      "11316 [Discriminator loss: 0.714265, acc.: 58.59%] [Generator loss: 0.966113]\n",
      "11317 [Discriminator loss: 0.711251, acc.: 57.03%] [Generator loss: 0.864451]\n",
      "11318 [Discriminator loss: 0.673037, acc.: 59.38%] [Generator loss: 0.845835]\n",
      "11319 [Discriminator loss: 0.712723, acc.: 51.56%] [Generator loss: 0.812423]\n",
      "11320 [Discriminator loss: 0.636817, acc.: 64.06%] [Generator loss: 0.826569]\n",
      "11321 [Discriminator loss: 0.573954, acc.: 71.09%] [Generator loss: 0.869808]\n",
      "11322 [Discriminator loss: 0.558672, acc.: 73.44%] [Generator loss: 0.821433]\n",
      "11323 [Discriminator loss: 0.701163, acc.: 58.59%] [Generator loss: 0.825137]\n",
      "11324 [Discriminator loss: 0.623097, acc.: 71.09%] [Generator loss: 0.699208]\n",
      "11325 [Discriminator loss: 0.595354, acc.: 72.66%] [Generator loss: 0.830525]\n",
      "11326 [Discriminator loss: 0.742825, acc.: 53.91%] [Generator loss: 0.844714]\n",
      "11327 [Discriminator loss: 0.839258, acc.: 35.94%] [Generator loss: 0.672362]\n",
      "11328 [Discriminator loss: 0.736710, acc.: 60.16%] [Generator loss: 0.672493]\n",
      "11329 [Discriminator loss: 0.567560, acc.: 73.44%] [Generator loss: 0.797743]\n",
      "11330 [Discriminator loss: 0.595955, acc.: 73.44%] [Generator loss: 0.823864]\n",
      "11331 [Discriminator loss: 0.549739, acc.: 69.53%] [Generator loss: 0.795695]\n",
      "11332 [Discriminator loss: 0.683936, acc.: 55.47%] [Generator loss: 0.884123]\n",
      "11333 [Discriminator loss: 0.708854, acc.: 54.69%] [Generator loss: 0.797350]\n",
      "11334 [Discriminator loss: 0.591977, acc.: 68.75%] [Generator loss: 0.767240]\n",
      "11335 [Discriminator loss: 0.787087, acc.: 44.53%] [Generator loss: 0.871836]\n",
      "11336 [Discriminator loss: 0.564460, acc.: 78.12%] [Generator loss: 0.806777]\n",
      "11337 [Discriminator loss: 0.660190, acc.: 60.94%] [Generator loss: 0.899888]\n",
      "11338 [Discriminator loss: 0.574489, acc.: 72.66%] [Generator loss: 1.008495]\n",
      "11339 [Discriminator loss: 0.787051, acc.: 47.66%] [Generator loss: 0.937297]\n",
      "11340 [Discriminator loss: 0.685412, acc.: 58.59%] [Generator loss: 1.063854]\n",
      "11341 [Discriminator loss: 0.700103, acc.: 58.59%] [Generator loss: 0.980420]\n",
      "11342 [Discriminator loss: 0.635369, acc.: 63.28%] [Generator loss: 1.098549]\n",
      "11343 [Discriminator loss: 0.667301, acc.: 63.28%] [Generator loss: 0.963196]\n",
      "11344 [Discriminator loss: 0.609058, acc.: 65.62%] [Generator loss: 0.933566]\n",
      "11345 [Discriminator loss: 0.701761, acc.: 60.16%] [Generator loss: 0.818787]\n",
      "11346 [Discriminator loss: 0.623854, acc.: 72.66%] [Generator loss: 0.803409]\n",
      "11347 [Discriminator loss: 0.574015, acc.: 72.66%] [Generator loss: 0.866732]\n",
      "11348 [Discriminator loss: 0.746761, acc.: 50.78%] [Generator loss: 0.916464]\n",
      "11349 [Discriminator loss: 0.705908, acc.: 58.59%] [Generator loss: 1.013979]\n",
      "11350 [Discriminator loss: 0.629985, acc.: 67.19%] [Generator loss: 0.974120]\n",
      "11351 [Discriminator loss: 0.674378, acc.: 62.50%] [Generator loss: 1.067313]\n",
      "11352 [Discriminator loss: 0.838164, acc.: 45.31%] [Generator loss: 0.925503]\n",
      "11353 [Discriminator loss: 0.587804, acc.: 71.09%] [Generator loss: 0.846605]\n",
      "11354 [Discriminator loss: 0.638852, acc.: 65.62%] [Generator loss: 0.819195]\n",
      "11355 [Discriminator loss: 0.758762, acc.: 51.56%] [Generator loss: 0.815324]\n",
      "11356 [Discriminator loss: 0.755332, acc.: 51.56%] [Generator loss: 0.951512]\n",
      "11357 [Discriminator loss: 0.738209, acc.: 53.91%] [Generator loss: 0.953405]\n",
      "11358 [Discriminator loss: 0.597040, acc.: 67.19%] [Generator loss: 0.894824]\n",
      "11359 [Discriminator loss: 0.712300, acc.: 59.38%] [Generator loss: 1.023126]\n",
      "11360 [Discriminator loss: 0.754577, acc.: 53.91%] [Generator loss: 0.916636]\n",
      "11361 [Discriminator loss: 0.595292, acc.: 73.44%] [Generator loss: 0.913956]\n",
      "11362 [Discriminator loss: 0.768302, acc.: 46.09%] [Generator loss: 0.880310]\n",
      "11363 [Discriminator loss: 0.837316, acc.: 45.31%] [Generator loss: 0.792801]\n",
      "11364 [Discriminator loss: 0.792360, acc.: 48.44%] [Generator loss: 0.934394]\n",
      "11365 [Discriminator loss: 0.677454, acc.: 58.59%] [Generator loss: 0.830500]\n",
      "11366 [Discriminator loss: 0.807485, acc.: 47.66%] [Generator loss: 0.973795]\n",
      "11367 [Discriminator loss: 0.551902, acc.: 75.78%] [Generator loss: 0.935253]\n",
      "11368 [Discriminator loss: 0.695064, acc.: 61.72%] [Generator loss: 1.080966]\n",
      "11369 [Discriminator loss: 0.605359, acc.: 71.88%] [Generator loss: 1.065777]\n",
      "11370 [Discriminator loss: 0.744567, acc.: 53.91%] [Generator loss: 1.047209]\n",
      "11371 [Discriminator loss: 0.611691, acc.: 67.19%] [Generator loss: 0.896252]\n",
      "11372 [Discriminator loss: 0.559407, acc.: 71.88%] [Generator loss: 0.971676]\n",
      "11373 [Discriminator loss: 0.616021, acc.: 65.62%] [Generator loss: 0.907722]\n",
      "11374 [Discriminator loss: 0.609698, acc.: 62.50%] [Generator loss: 0.803599]\n",
      "11375 [Discriminator loss: 0.765897, acc.: 52.34%] [Generator loss: 0.853283]\n",
      "11376 [Discriminator loss: 0.573897, acc.: 71.09%] [Generator loss: 0.881214]\n",
      "11377 [Discriminator loss: 0.848428, acc.: 35.94%] [Generator loss: 0.767241]\n",
      "11378 [Discriminator loss: 0.738874, acc.: 51.56%] [Generator loss: 0.686551]\n",
      "11379 [Discriminator loss: 0.705603, acc.: 59.38%] [Generator loss: 0.815452]\n",
      "11380 [Discriminator loss: 0.643816, acc.: 62.50%] [Generator loss: 1.006535]\n",
      "11381 [Discriminator loss: 0.654445, acc.: 65.62%] [Generator loss: 0.840879]\n",
      "11382 [Discriminator loss: 0.671928, acc.: 62.50%] [Generator loss: 0.808594]\n",
      "11383 [Discriminator loss: 0.630907, acc.: 60.16%] [Generator loss: 0.818928]\n",
      "11384 [Discriminator loss: 0.617989, acc.: 67.19%] [Generator loss: 0.761092]\n",
      "11385 [Discriminator loss: 0.787512, acc.: 42.97%] [Generator loss: 0.818143]\n",
      "11386 [Discriminator loss: 0.544130, acc.: 73.44%] [Generator loss: 0.789905]\n",
      "11387 [Discriminator loss: 0.758094, acc.: 54.69%] [Generator loss: 0.755772]\n",
      "11388 [Discriminator loss: 0.616534, acc.: 72.66%] [Generator loss: 0.811244]\n",
      "11389 [Discriminator loss: 0.591995, acc.: 73.44%] [Generator loss: 0.725366]\n",
      "11390 [Discriminator loss: 0.565143, acc.: 72.66%] [Generator loss: 0.728324]\n",
      "11391 [Discriminator loss: 0.583266, acc.: 70.31%] [Generator loss: 0.797421]\n",
      "11392 [Discriminator loss: 0.615177, acc.: 60.94%] [Generator loss: 0.897680]\n",
      "11393 [Discriminator loss: 0.580701, acc.: 71.88%] [Generator loss: 0.918516]\n",
      "11394 [Discriminator loss: 0.781888, acc.: 44.53%] [Generator loss: 1.009493]\n",
      "11395 [Discriminator loss: 0.637607, acc.: 73.44%] [Generator loss: 0.862784]\n",
      "11396 [Discriminator loss: 0.612266, acc.: 66.41%] [Generator loss: 0.829168]\n",
      "11397 [Discriminator loss: 0.512781, acc.: 78.12%] [Generator loss: 1.011608]\n",
      "11398 [Discriminator loss: 0.728283, acc.: 53.12%] [Generator loss: 0.888172]\n",
      "11399 [Discriminator loss: 0.572814, acc.: 74.22%] [Generator loss: 0.846402]\n",
      "11400 [Discriminator loss: 0.532030, acc.: 75.78%] [Generator loss: 0.804730]\n",
      "11401 [Discriminator loss: 0.744937, acc.: 56.25%] [Generator loss: 0.909707]\n",
      "11402 [Discriminator loss: 0.609821, acc.: 67.19%] [Generator loss: 0.890656]\n",
      "11403 [Discriminator loss: 0.585142, acc.: 71.88%] [Generator loss: 0.920125]\n",
      "11404 [Discriminator loss: 0.750844, acc.: 49.22%] [Generator loss: 0.901364]\n",
      "11405 [Discriminator loss: 0.733131, acc.: 54.69%] [Generator loss: 0.791499]\n",
      "11406 [Discriminator loss: 0.582792, acc.: 72.66%] [Generator loss: 0.884329]\n",
      "11407 [Discriminator loss: 0.480967, acc.: 85.16%] [Generator loss: 0.818599]\n",
      "11408 [Discriminator loss: 0.700022, acc.: 56.25%] [Generator loss: 0.633520]\n",
      "11409 [Discriminator loss: 0.595334, acc.: 69.53%] [Generator loss: 0.721435]\n",
      "11410 [Discriminator loss: 0.647790, acc.: 65.62%] [Generator loss: 0.706467]\n",
      "11411 [Discriminator loss: 0.669816, acc.: 64.06%] [Generator loss: 0.988222]\n",
      "11412 [Discriminator loss: 0.671126, acc.: 53.12%] [Generator loss: 0.922895]\n",
      "11413 [Discriminator loss: 0.628367, acc.: 62.50%] [Generator loss: 0.780203]\n",
      "11414 [Discriminator loss: 0.690237, acc.: 60.94%] [Generator loss: 0.716401]\n",
      "11415 [Discriminator loss: 0.699854, acc.: 67.19%] [Generator loss: 0.951548]\n",
      "11416 [Discriminator loss: 0.820805, acc.: 44.53%] [Generator loss: 0.807917]\n",
      "11417 [Discriminator loss: 0.774733, acc.: 46.88%] [Generator loss: 0.864318]\n",
      "11418 [Discriminator loss: 0.686207, acc.: 61.72%] [Generator loss: 0.722032]\n",
      "11419 [Discriminator loss: 0.755031, acc.: 59.38%] [Generator loss: 0.766670]\n",
      "11420 [Discriminator loss: 0.588433, acc.: 71.88%] [Generator loss: 0.875940]\n",
      "11421 [Discriminator loss: 0.634472, acc.: 67.19%] [Generator loss: 0.822957]\n",
      "11422 [Discriminator loss: 0.748378, acc.: 57.81%] [Generator loss: 0.764948]\n",
      "11423 [Discriminator loss: 0.681204, acc.: 53.91%] [Generator loss: 0.723635]\n",
      "11424 [Discriminator loss: 0.686486, acc.: 60.16%] [Generator loss: 0.902380]\n",
      "11425 [Discriminator loss: 0.602232, acc.: 67.19%] [Generator loss: 0.978983]\n",
      "11426 [Discriminator loss: 0.721863, acc.: 52.34%] [Generator loss: 0.980744]\n",
      "11427 [Discriminator loss: 0.813630, acc.: 50.78%] [Generator loss: 0.751731]\n",
      "11428 [Discriminator loss: 0.603512, acc.: 70.31%] [Generator loss: 0.732310]\n",
      "11429 [Discriminator loss: 0.818461, acc.: 47.66%] [Generator loss: 0.764055]\n",
      "11430 [Discriminator loss: 0.662927, acc.: 61.72%] [Generator loss: 0.889521]\n",
      "11431 [Discriminator loss: 0.661484, acc.: 62.50%] [Generator loss: 0.937104]\n",
      "11432 [Discriminator loss: 0.661420, acc.: 62.50%] [Generator loss: 1.154630]\n",
      "11433 [Discriminator loss: 0.591380, acc.: 67.19%] [Generator loss: 1.156761]\n",
      "11434 [Discriminator loss: 0.796396, acc.: 51.56%] [Generator loss: 1.010677]\n",
      "11435 [Discriminator loss: 0.715317, acc.: 55.47%] [Generator loss: 0.892533]\n",
      "11436 [Discriminator loss: 0.656867, acc.: 57.81%] [Generator loss: 1.015756]\n",
      "11437 [Discriminator loss: 0.665994, acc.: 68.75%] [Generator loss: 0.930623]\n",
      "11438 [Discriminator loss: 0.533441, acc.: 75.78%] [Generator loss: 0.804153]\n",
      "11439 [Discriminator loss: 0.678875, acc.: 50.78%] [Generator loss: 0.710661]\n",
      "11440 [Discriminator loss: 0.716083, acc.: 54.69%] [Generator loss: 0.688530]\n",
      "11441 [Discriminator loss: 0.640497, acc.: 63.28%] [Generator loss: 0.963349]\n",
      "11442 [Discriminator loss: 0.583128, acc.: 70.31%] [Generator loss: 0.926853]\n",
      "11443 [Discriminator loss: 0.674520, acc.: 60.94%] [Generator loss: 0.998363]\n",
      "11444 [Discriminator loss: 0.845793, acc.: 42.97%] [Generator loss: 0.906483]\n",
      "11445 [Discriminator loss: 0.761945, acc.: 52.34%] [Generator loss: 0.941888]\n",
      "11446 [Discriminator loss: 0.632338, acc.: 65.62%] [Generator loss: 0.717038]\n",
      "11447 [Discriminator loss: 0.592174, acc.: 64.84%] [Generator loss: 0.883716]\n",
      "11448 [Discriminator loss: 0.723105, acc.: 53.91%] [Generator loss: 1.029894]\n",
      "11449 [Discriminator loss: 0.920687, acc.: 37.50%] [Generator loss: 0.992876]\n",
      "11450 [Discriminator loss: 0.649361, acc.: 68.75%] [Generator loss: 0.900851]\n",
      "11451 [Discriminator loss: 0.652560, acc.: 59.38%] [Generator loss: 1.068947]\n",
      "11452 [Discriminator loss: 0.603053, acc.: 64.84%] [Generator loss: 0.885523]\n",
      "11453 [Discriminator loss: 0.599427, acc.: 71.09%] [Generator loss: 0.885575]\n",
      "11454 [Discriminator loss: 0.686618, acc.: 58.59%] [Generator loss: 0.884219]\n",
      "11455 [Discriminator loss: 0.607576, acc.: 67.19%] [Generator loss: 0.897571]\n",
      "11456 [Discriminator loss: 0.633083, acc.: 65.62%] [Generator loss: 0.775090]\n",
      "11457 [Discriminator loss: 0.606326, acc.: 66.41%] [Generator loss: 1.139369]\n",
      "11458 [Discriminator loss: 0.612128, acc.: 64.06%] [Generator loss: 1.113753]\n",
      "11459 [Discriminator loss: 0.693820, acc.: 61.72%] [Generator loss: 0.923282]\n",
      "11460 [Discriminator loss: 0.734973, acc.: 50.00%] [Generator loss: 0.815225]\n",
      "11461 [Discriminator loss: 0.570004, acc.: 77.34%] [Generator loss: 0.833690]\n",
      "11462 [Discriminator loss: 0.779121, acc.: 56.25%] [Generator loss: 0.823548]\n",
      "11463 [Discriminator loss: 0.519928, acc.: 74.22%] [Generator loss: 0.748891]\n",
      "11464 [Discriminator loss: 0.634324, acc.: 71.88%] [Generator loss: 1.029270]\n",
      "11465 [Discriminator loss: 0.502894, acc.: 78.12%] [Generator loss: 1.061112]\n",
      "11466 [Discriminator loss: 0.603997, acc.: 65.62%] [Generator loss: 1.000208]\n",
      "11467 [Discriminator loss: 0.684861, acc.: 64.06%] [Generator loss: 0.877167]\n",
      "11468 [Discriminator loss: 0.749409, acc.: 50.00%] [Generator loss: 0.852761]\n",
      "11469 [Discriminator loss: 0.672190, acc.: 60.16%] [Generator loss: 0.930207]\n",
      "11470 [Discriminator loss: 0.634051, acc.: 62.50%] [Generator loss: 0.812404]\n",
      "11471 [Discriminator loss: 0.564103, acc.: 70.31%] [Generator loss: 0.886111]\n",
      "11472 [Discriminator loss: 0.612013, acc.: 60.94%] [Generator loss: 0.977377]\n",
      "11473 [Discriminator loss: 0.540737, acc.: 71.88%] [Generator loss: 0.877272]\n",
      "11474 [Discriminator loss: 0.646283, acc.: 64.06%] [Generator loss: 0.880459]\n",
      "11475 [Discriminator loss: 0.632525, acc.: 64.84%] [Generator loss: 0.803315]\n",
      "11476 [Discriminator loss: 0.937319, acc.: 33.59%] [Generator loss: 0.719981]\n",
      "11477 [Discriminator loss: 0.689345, acc.: 60.16%] [Generator loss: 0.763094]\n",
      "11478 [Discriminator loss: 0.639418, acc.: 64.06%] [Generator loss: 0.741618]\n",
      "11479 [Discriminator loss: 0.672286, acc.: 61.72%] [Generator loss: 0.798168]\n",
      "11480 [Discriminator loss: 0.588026, acc.: 63.28%] [Generator loss: 0.805627]\n",
      "11481 [Discriminator loss: 0.657512, acc.: 57.03%] [Generator loss: 1.057400]\n",
      "11482 [Discriminator loss: 0.733643, acc.: 52.34%] [Generator loss: 0.980334]\n",
      "11483 [Discriminator loss: 0.676130, acc.: 57.81%] [Generator loss: 1.199501]\n",
      "11484 [Discriminator loss: 0.691761, acc.: 62.50%] [Generator loss: 0.867263]\n",
      "11485 [Discriminator loss: 0.600963, acc.: 63.28%] [Generator loss: 0.905684]\n",
      "11486 [Discriminator loss: 0.680441, acc.: 54.69%] [Generator loss: 0.874872]\n",
      "11487 [Discriminator loss: 0.606341, acc.: 71.88%] [Generator loss: 1.014512]\n",
      "11488 [Discriminator loss: 0.717333, acc.: 52.34%] [Generator loss: 0.830823]\n",
      "11489 [Discriminator loss: 0.589537, acc.: 72.66%] [Generator loss: 0.893584]\n",
      "11490 [Discriminator loss: 0.499002, acc.: 82.03%] [Generator loss: 0.805980]\n",
      "11491 [Discriminator loss: 0.503674, acc.: 77.34%] [Generator loss: 0.710207]\n",
      "11492 [Discriminator loss: 0.755621, acc.: 52.34%] [Generator loss: 0.786467]\n",
      "11493 [Discriminator loss: 0.515826, acc.: 78.91%] [Generator loss: 0.903163]\n",
      "11494 [Discriminator loss: 0.721318, acc.: 52.34%] [Generator loss: 0.836977]\n",
      "11495 [Discriminator loss: 0.674947, acc.: 60.16%] [Generator loss: 0.860065]\n",
      "11496 [Discriminator loss: 0.839897, acc.: 44.53%] [Generator loss: 0.844826]\n",
      "11497 [Discriminator loss: 0.665412, acc.: 60.16%] [Generator loss: 0.827071]\n",
      "11498 [Discriminator loss: 0.634054, acc.: 62.50%] [Generator loss: 0.934017]\n",
      "11499 [Discriminator loss: 0.639203, acc.: 64.84%] [Generator loss: 1.049948]\n",
      "11500 [Discriminator loss: 0.746624, acc.: 53.12%] [Generator loss: 0.926271]\n",
      "11501 [Discriminator loss: 0.867654, acc.: 41.41%] [Generator loss: 0.854394]\n",
      "11502 [Discriminator loss: 0.667706, acc.: 62.50%] [Generator loss: 0.843210]\n",
      "11503 [Discriminator loss: 0.699717, acc.: 57.81%] [Generator loss: 0.872425]\n",
      "11504 [Discriminator loss: 0.633316, acc.: 62.50%] [Generator loss: 0.777018]\n",
      "11505 [Discriminator loss: 0.762804, acc.: 46.09%] [Generator loss: 0.846214]\n",
      "11506 [Discriminator loss: 0.625858, acc.: 64.84%] [Generator loss: 0.876392]\n",
      "11507 [Discriminator loss: 0.681755, acc.: 60.16%] [Generator loss: 0.883884]\n",
      "11508 [Discriminator loss: 0.586894, acc.: 71.09%] [Generator loss: 0.810876]\n",
      "11509 [Discriminator loss: 0.776273, acc.: 45.31%] [Generator loss: 0.891970]\n",
      "11510 [Discriminator loss: 0.488628, acc.: 83.59%] [Generator loss: 0.884814]\n",
      "11511 [Discriminator loss: 0.583964, acc.: 75.00%] [Generator loss: 0.841888]\n",
      "11512 [Discriminator loss: 0.692490, acc.: 53.12%] [Generator loss: 0.789947]\n",
      "11513 [Discriminator loss: 0.677803, acc.: 58.59%] [Generator loss: 0.911962]\n",
      "11514 [Discriminator loss: 0.586535, acc.: 75.00%] [Generator loss: 1.105141]\n",
      "11515 [Discriminator loss: 0.709459, acc.: 63.28%] [Generator loss: 1.092329]\n",
      "11516 [Discriminator loss: 0.661759, acc.: 60.16%] [Generator loss: 1.050463]\n",
      "11517 [Discriminator loss: 0.877412, acc.: 41.41%] [Generator loss: 0.747890]\n",
      "11518 [Discriminator loss: 0.694207, acc.: 62.50%] [Generator loss: 0.788234]\n",
      "11519 [Discriminator loss: 0.684450, acc.: 55.47%] [Generator loss: 0.705025]\n",
      "11520 [Discriminator loss: 0.582098, acc.: 67.97%] [Generator loss: 0.904032]\n",
      "11521 [Discriminator loss: 0.448242, acc.: 82.03%] [Generator loss: 0.909819]\n",
      "11522 [Discriminator loss: 0.535839, acc.: 78.91%] [Generator loss: 0.869659]\n",
      "11523 [Discriminator loss: 0.855009, acc.: 32.03%] [Generator loss: 1.067056]\n",
      "11524 [Discriminator loss: 0.764849, acc.: 49.22%] [Generator loss: 1.120469]\n",
      "11525 [Discriminator loss: 0.785406, acc.: 50.00%] [Generator loss: 0.909587]\n",
      "11526 [Discriminator loss: 0.804820, acc.: 48.44%] [Generator loss: 1.005844]\n",
      "11527 [Discriminator loss: 0.721586, acc.: 57.81%] [Generator loss: 0.955164]\n",
      "11528 [Discriminator loss: 0.668047, acc.: 56.25%] [Generator loss: 0.880142]\n",
      "11529 [Discriminator loss: 0.673980, acc.: 63.28%] [Generator loss: 0.747493]\n",
      "11530 [Discriminator loss: 0.639209, acc.: 65.62%] [Generator loss: 0.765226]\n",
      "11531 [Discriminator loss: 0.677251, acc.: 60.16%] [Generator loss: 0.927285]\n",
      "11532 [Discriminator loss: 0.660851, acc.: 58.59%] [Generator loss: 0.860343]\n",
      "11533 [Discriminator loss: 0.674649, acc.: 64.84%] [Generator loss: 0.957387]\n",
      "11534 [Discriminator loss: 0.641913, acc.: 67.19%] [Generator loss: 0.863493]\n",
      "11535 [Discriminator loss: 0.766828, acc.: 53.12%] [Generator loss: 0.934151]\n",
      "11536 [Discriminator loss: 0.634851, acc.: 67.19%] [Generator loss: 0.933628]\n",
      "11537 [Discriminator loss: 0.578066, acc.: 71.88%] [Generator loss: 0.832185]\n",
      "11538 [Discriminator loss: 0.642772, acc.: 65.62%] [Generator loss: 0.862376]\n",
      "11539 [Discriminator loss: 0.600333, acc.: 73.44%] [Generator loss: 0.879802]\n",
      "11540 [Discriminator loss: 0.722304, acc.: 58.59%] [Generator loss: 0.928997]\n",
      "11541 [Discriminator loss: 0.794650, acc.: 45.31%] [Generator loss: 0.990304]\n",
      "11542 [Discriminator loss: 0.590474, acc.: 68.75%] [Generator loss: 1.088814]\n",
      "11543 [Discriminator loss: 0.620638, acc.: 64.06%] [Generator loss: 0.973744]\n",
      "11544 [Discriminator loss: 0.773270, acc.: 46.88%] [Generator loss: 0.981841]\n",
      "11545 [Discriminator loss: 0.661969, acc.: 59.38%] [Generator loss: 0.891849]\n",
      "11546 [Discriminator loss: 0.614739, acc.: 66.41%] [Generator loss: 0.903909]\n",
      "11547 [Discriminator loss: 0.594031, acc.: 71.88%] [Generator loss: 0.869241]\n",
      "11548 [Discriminator loss: 0.669877, acc.: 59.38%] [Generator loss: 0.765016]\n",
      "11549 [Discriminator loss: 0.675178, acc.: 60.94%] [Generator loss: 0.805028]\n",
      "11550 [Discriminator loss: 0.670879, acc.: 64.84%] [Generator loss: 0.835016]\n",
      "11551 [Discriminator loss: 0.618516, acc.: 70.31%] [Generator loss: 0.828458]\n",
      "11552 [Discriminator loss: 0.627918, acc.: 69.53%] [Generator loss: 0.924296]\n",
      "11553 [Discriminator loss: 0.585538, acc.: 75.00%] [Generator loss: 0.996951]\n",
      "11554 [Discriminator loss: 0.575431, acc.: 74.22%] [Generator loss: 0.916645]\n",
      "11555 [Discriminator loss: 0.683790, acc.: 59.38%] [Generator loss: 1.001350]\n",
      "11556 [Discriminator loss: 0.632949, acc.: 58.59%] [Generator loss: 1.084797]\n",
      "11557 [Discriminator loss: 0.574441, acc.: 71.09%] [Generator loss: 0.924689]\n",
      "11558 [Discriminator loss: 0.602864, acc.: 65.62%] [Generator loss: 1.061731]\n",
      "11559 [Discriminator loss: 0.594589, acc.: 70.31%] [Generator loss: 1.081094]\n",
      "11560 [Discriminator loss: 0.615289, acc.: 68.75%] [Generator loss: 0.871610]\n",
      "11561 [Discriminator loss: 0.605701, acc.: 67.97%] [Generator loss: 0.896790]\n",
      "11562 [Discriminator loss: 0.766011, acc.: 48.44%] [Generator loss: 0.834392]\n",
      "11563 [Discriminator loss: 0.770014, acc.: 51.56%] [Generator loss: 0.830916]\n",
      "11564 [Discriminator loss: 0.871563, acc.: 52.34%] [Generator loss: 0.960110]\n",
      "11565 [Discriminator loss: 0.558342, acc.: 75.00%] [Generator loss: 0.919603]\n",
      "11566 [Discriminator loss: 0.546725, acc.: 69.53%] [Generator loss: 0.913083]\n",
      "11567 [Discriminator loss: 0.778793, acc.: 51.56%] [Generator loss: 0.989306]\n",
      "11568 [Discriminator loss: 0.623395, acc.: 65.62%] [Generator loss: 0.779422]\n",
      "11569 [Discriminator loss: 0.565885, acc.: 66.41%] [Generator loss: 0.839095]\n",
      "11570 [Discriminator loss: 0.729655, acc.: 67.19%] [Generator loss: 0.810453]\n",
      "11571 [Discriminator loss: 0.682309, acc.: 60.94%] [Generator loss: 0.872214]\n",
      "11572 [Discriminator loss: 0.629881, acc.: 70.31%] [Generator loss: 1.048537]\n",
      "11573 [Discriminator loss: 0.701516, acc.: 62.50%] [Generator loss: 0.907049]\n",
      "11574 [Discriminator loss: 0.581079, acc.: 76.56%] [Generator loss: 0.636546]\n",
      "11575 [Discriminator loss: 0.611976, acc.: 65.62%] [Generator loss: 0.927191]\n",
      "11576 [Discriminator loss: 0.758997, acc.: 47.66%] [Generator loss: 0.786505]\n",
      "11577 [Discriminator loss: 0.642717, acc.: 57.81%] [Generator loss: 0.860737]\n",
      "11578 [Discriminator loss: 0.671526, acc.: 60.94%] [Generator loss: 0.967728]\n",
      "11579 [Discriminator loss: 0.615332, acc.: 70.31%] [Generator loss: 0.930269]\n",
      "11580 [Discriminator loss: 0.685222, acc.: 60.94%] [Generator loss: 0.822399]\n",
      "11581 [Discriminator loss: 0.632511, acc.: 67.19%] [Generator loss: 0.843424]\n",
      "11582 [Discriminator loss: 0.684179, acc.: 57.03%] [Generator loss: 0.869952]\n",
      "11583 [Discriminator loss: 0.693966, acc.: 59.38%] [Generator loss: 0.898788]\n",
      "11584 [Discriminator loss: 0.560487, acc.: 75.78%] [Generator loss: 0.902981]\n",
      "11585 [Discriminator loss: 0.694800, acc.: 51.56%] [Generator loss: 0.836576]\n",
      "11586 [Discriminator loss: 0.651071, acc.: 66.41%] [Generator loss: 0.930125]\n",
      "11587 [Discriminator loss: 0.837006, acc.: 46.88%] [Generator loss: 0.861439]\n",
      "11588 [Discriminator loss: 0.637296, acc.: 61.72%] [Generator loss: 0.770002]\n",
      "11589 [Discriminator loss: 0.671333, acc.: 62.50%] [Generator loss: 0.885817]\n",
      "11590 [Discriminator loss: 0.654506, acc.: 61.72%] [Generator loss: 1.000991]\n",
      "11591 [Discriminator loss: 0.643605, acc.: 60.94%] [Generator loss: 1.113834]\n",
      "11592 [Discriminator loss: 0.853945, acc.: 39.84%] [Generator loss: 0.949994]\n",
      "11593 [Discriminator loss: 0.753141, acc.: 57.03%] [Generator loss: 0.868603]\n",
      "11594 [Discriminator loss: 0.800074, acc.: 46.88%] [Generator loss: 0.815484]\n",
      "11595 [Discriminator loss: 0.779925, acc.: 51.56%] [Generator loss: 0.756070]\n",
      "11596 [Discriminator loss: 0.679605, acc.: 59.38%] [Generator loss: 0.712138]\n",
      "11597 [Discriminator loss: 0.687365, acc.: 64.84%] [Generator loss: 0.808544]\n",
      "11598 [Discriminator loss: 0.601465, acc.: 72.66%] [Generator loss: 0.926047]\n",
      "11599 [Discriminator loss: 0.637378, acc.: 64.06%] [Generator loss: 0.963357]\n",
      "11600 [Discriminator loss: 0.611930, acc.: 68.75%] [Generator loss: 0.947159]\n",
      "11601 [Discriminator loss: 0.739335, acc.: 60.16%] [Generator loss: 0.951347]\n",
      "11602 [Discriminator loss: 0.678565, acc.: 59.38%] [Generator loss: 0.920626]\n",
      "11603 [Discriminator loss: 0.693202, acc.: 57.81%] [Generator loss: 0.790210]\n",
      "11604 [Discriminator loss: 0.521532, acc.: 79.69%] [Generator loss: 0.902296]\n",
      "11605 [Discriminator loss: 0.682922, acc.: 58.59%] [Generator loss: 0.858814]\n",
      "11606 [Discriminator loss: 0.620592, acc.: 65.62%] [Generator loss: 1.030231]\n",
      "11607 [Discriminator loss: 0.543222, acc.: 75.00%] [Generator loss: 1.024488]\n",
      "11608 [Discriminator loss: 0.573975, acc.: 69.53%] [Generator loss: 1.024012]\n",
      "11609 [Discriminator loss: 0.514375, acc.: 71.88%] [Generator loss: 0.946782]\n",
      "11610 [Discriminator loss: 0.642260, acc.: 62.50%] [Generator loss: 0.924010]\n",
      "11611 [Discriminator loss: 0.651580, acc.: 67.97%] [Generator loss: 0.847342]\n",
      "11612 [Discriminator loss: 0.472208, acc.: 83.59%] [Generator loss: 0.921739]\n",
      "11613 [Discriminator loss: 0.615428, acc.: 64.06%] [Generator loss: 0.732150]\n",
      "11614 [Discriminator loss: 0.740210, acc.: 52.34%] [Generator loss: 0.782115]\n",
      "11615 [Discriminator loss: 0.725670, acc.: 56.25%] [Generator loss: 0.793241]\n",
      "11616 [Discriminator loss: 0.571022, acc.: 75.00%] [Generator loss: 0.821708]\n",
      "11617 [Discriminator loss: 0.663446, acc.: 64.84%] [Generator loss: 0.793858]\n",
      "11618 [Discriminator loss: 0.735359, acc.: 53.12%] [Generator loss: 0.854444]\n",
      "11619 [Discriminator loss: 0.717774, acc.: 59.38%] [Generator loss: 0.914273]\n",
      "11620 [Discriminator loss: 0.713970, acc.: 60.94%] [Generator loss: 1.232315]\n",
      "11621 [Discriminator loss: 0.726976, acc.: 53.12%] [Generator loss: 1.151186]\n",
      "11622 [Discriminator loss: 0.793876, acc.: 42.97%] [Generator loss: 1.020453]\n",
      "11623 [Discriminator loss: 0.612648, acc.: 64.06%] [Generator loss: 0.908303]\n",
      "11624 [Discriminator loss: 0.663569, acc.: 60.94%] [Generator loss: 0.777533]\n",
      "11625 [Discriminator loss: 0.564772, acc.: 72.66%] [Generator loss: 0.709135]\n",
      "11626 [Discriminator loss: 0.816255, acc.: 53.12%] [Generator loss: 0.815473]\n",
      "11627 [Discriminator loss: 0.586658, acc.: 68.75%] [Generator loss: 0.954453]\n",
      "11628 [Discriminator loss: 0.725241, acc.: 51.56%] [Generator loss: 0.980628]\n",
      "11629 [Discriminator loss: 0.596790, acc.: 69.53%] [Generator loss: 0.983529]\n",
      "11630 [Discriminator loss: 0.590379, acc.: 70.31%] [Generator loss: 0.963760]\n",
      "11631 [Discriminator loss: 0.786191, acc.: 42.19%] [Generator loss: 0.891920]\n",
      "11632 [Discriminator loss: 0.614890, acc.: 63.28%] [Generator loss: 0.889633]\n",
      "11633 [Discriminator loss: 0.637286, acc.: 66.41%] [Generator loss: 0.928471]\n",
      "11634 [Discriminator loss: 0.801432, acc.: 43.75%] [Generator loss: 1.050485]\n",
      "11635 [Discriminator loss: 0.611373, acc.: 63.28%] [Generator loss: 1.034977]\n",
      "11636 [Discriminator loss: 0.547222, acc.: 75.00%] [Generator loss: 0.983883]\n",
      "11637 [Discriminator loss: 0.662325, acc.: 63.28%] [Generator loss: 0.748152]\n",
      "11638 [Discriminator loss: 0.706076, acc.: 58.59%] [Generator loss: 0.807806]\n",
      "11639 [Discriminator loss: 0.751595, acc.: 57.03%] [Generator loss: 0.783876]\n",
      "11640 [Discriminator loss: 0.621193, acc.: 67.19%] [Generator loss: 0.922562]\n",
      "11641 [Discriminator loss: 0.641598, acc.: 58.59%] [Generator loss: 1.051548]\n",
      "11642 [Discriminator loss: 0.755136, acc.: 50.00%] [Generator loss: 0.967839]\n",
      "11643 [Discriminator loss: 0.667269, acc.: 64.06%] [Generator loss: 1.134460]\n",
      "11644 [Discriminator loss: 0.740507, acc.: 57.81%] [Generator loss: 1.271905]\n",
      "11645 [Discriminator loss: 0.642051, acc.: 70.31%] [Generator loss: 1.040495]\n",
      "11646 [Discriminator loss: 0.585328, acc.: 73.44%] [Generator loss: 0.945350]\n",
      "11647 [Discriminator loss: 0.614477, acc.: 68.75%] [Generator loss: 0.841846]\n",
      "11648 [Discriminator loss: 0.635169, acc.: 56.25%] [Generator loss: 0.819417]\n",
      "11649 [Discriminator loss: 0.699599, acc.: 62.50%] [Generator loss: 0.785138]\n",
      "11650 [Discriminator loss: 0.474094, acc.: 84.38%] [Generator loss: 0.697672]\n",
      "11651 [Discriminator loss: 0.606210, acc.: 66.41%] [Generator loss: 0.683996]\n",
      "11652 [Discriminator loss: 0.519589, acc.: 69.53%] [Generator loss: 0.632110]\n",
      "11653 [Discriminator loss: 0.624506, acc.: 61.72%] [Generator loss: 0.865515]\n",
      "11654 [Discriminator loss: 0.846249, acc.: 47.66%] [Generator loss: 0.933879]\n",
      "11655 [Discriminator loss: 0.572732, acc.: 70.31%] [Generator loss: 1.373002]\n",
      "11656 [Discriminator loss: 0.623275, acc.: 67.97%] [Generator loss: 1.078582]\n",
      "11657 [Discriminator loss: 0.487896, acc.: 82.03%] [Generator loss: 1.155591]\n",
      "11658 [Discriminator loss: 0.608106, acc.: 66.41%] [Generator loss: 0.916062]\n",
      "11659 [Discriminator loss: 0.569074, acc.: 75.78%] [Generator loss: 0.922125]\n",
      "11660 [Discriminator loss: 0.834605, acc.: 45.31%] [Generator loss: 0.769732]\n",
      "11661 [Discriminator loss: 0.713362, acc.: 59.38%] [Generator loss: 0.807279]\n",
      "11662 [Discriminator loss: 0.658830, acc.: 67.19%] [Generator loss: 0.800686]\n",
      "11663 [Discriminator loss: 0.501928, acc.: 82.03%] [Generator loss: 0.851053]\n",
      "11664 [Discriminator loss: 0.446527, acc.: 82.81%] [Generator loss: 0.770048]\n",
      "11665 [Discriminator loss: 0.631003, acc.: 63.28%] [Generator loss: 0.701539]\n",
      "11666 [Discriminator loss: 0.522841, acc.: 75.00%] [Generator loss: 0.714214]\n",
      "11667 [Discriminator loss: 0.706316, acc.: 64.06%] [Generator loss: 0.852758]\n",
      "11668 [Discriminator loss: 0.760162, acc.: 51.56%] [Generator loss: 0.995420]\n",
      "11669 [Discriminator loss: 0.761147, acc.: 53.91%] [Generator loss: 1.003152]\n",
      "11670 [Discriminator loss: 0.646577, acc.: 64.84%] [Generator loss: 1.163930]\n",
      "11671 [Discriminator loss: 0.705837, acc.: 57.81%] [Generator loss: 1.164965]\n",
      "11672 [Discriminator loss: 0.674589, acc.: 57.03%] [Generator loss: 1.314240]\n",
      "11673 [Discriminator loss: 0.760581, acc.: 53.12%] [Generator loss: 0.980767]\n",
      "11674 [Discriminator loss: 0.828650, acc.: 41.41%] [Generator loss: 0.995492]\n",
      "11675 [Discriminator loss: 0.718910, acc.: 57.81%] [Generator loss: 0.999390]\n",
      "11676 [Discriminator loss: 0.761199, acc.: 49.22%] [Generator loss: 1.119451]\n",
      "11677 [Discriminator loss: 0.731976, acc.: 54.69%] [Generator loss: 1.086830]\n",
      "11678 [Discriminator loss: 0.762612, acc.: 53.12%] [Generator loss: 0.955884]\n",
      "11679 [Discriminator loss: 0.706282, acc.: 53.91%] [Generator loss: 0.842766]\n",
      "11680 [Discriminator loss: 0.789222, acc.: 46.09%] [Generator loss: 0.930515]\n",
      "11681 [Discriminator loss: 0.719339, acc.: 60.16%] [Generator loss: 0.746507]\n",
      "11682 [Discriminator loss: 0.746417, acc.: 57.03%] [Generator loss: 1.074917]\n",
      "11683 [Discriminator loss: 0.653298, acc.: 66.41%] [Generator loss: 1.015075]\n",
      "11684 [Discriminator loss: 0.668094, acc.: 57.81%] [Generator loss: 0.947443]\n",
      "11685 [Discriminator loss: 0.550365, acc.: 75.00%] [Generator loss: 1.100559]\n",
      "11686 [Discriminator loss: 0.675709, acc.: 58.59%] [Generator loss: 0.937961]\n",
      "11687 [Discriminator loss: 0.611658, acc.: 67.97%] [Generator loss: 0.973823]\n",
      "11688 [Discriminator loss: 0.681445, acc.: 57.81%] [Generator loss: 0.923469]\n",
      "11689 [Discriminator loss: 0.573213, acc.: 72.66%] [Generator loss: 0.976996]\n",
      "11690 [Discriminator loss: 0.678112, acc.: 61.72%] [Generator loss: 0.786828]\n",
      "11691 [Discriminator loss: 0.596962, acc.: 75.00%] [Generator loss: 0.844649]\n",
      "11692 [Discriminator loss: 0.718222, acc.: 55.47%] [Generator loss: 0.927948]\n",
      "11693 [Discriminator loss: 0.626240, acc.: 64.84%] [Generator loss: 0.981294]\n",
      "11694 [Discriminator loss: 0.598499, acc.: 72.66%] [Generator loss: 0.879397]\n",
      "11695 [Discriminator loss: 0.610743, acc.: 61.72%] [Generator loss: 0.988481]\n",
      "11696 [Discriminator loss: 0.640314, acc.: 63.28%] [Generator loss: 0.941417]\n",
      "11697 [Discriminator loss: 0.781289, acc.: 52.34%] [Generator loss: 0.925781]\n",
      "11698 [Discriminator loss: 0.701051, acc.: 52.34%] [Generator loss: 1.080341]\n",
      "11699 [Discriminator loss: 0.654420, acc.: 57.81%] [Generator loss: 0.980998]\n",
      "11700 [Discriminator loss: 0.866593, acc.: 37.50%] [Generator loss: 0.798762]\n",
      "11701 [Discriminator loss: 0.799880, acc.: 51.56%] [Generator loss: 1.100550]\n",
      "11702 [Discriminator loss: 0.706585, acc.: 57.81%] [Generator loss: 0.939662]\n",
      "11703 [Discriminator loss: 0.642518, acc.: 66.41%] [Generator loss: 1.025095]\n",
      "11704 [Discriminator loss: 0.748287, acc.: 53.12%] [Generator loss: 0.919579]\n",
      "11705 [Discriminator loss: 0.635359, acc.: 62.50%] [Generator loss: 0.872544]\n",
      "11706 [Discriminator loss: 0.647539, acc.: 60.94%] [Generator loss: 1.111843]\n",
      "11707 [Discriminator loss: 0.640827, acc.: 67.19%] [Generator loss: 0.880859]\n",
      "11708 [Discriminator loss: 0.618927, acc.: 65.62%] [Generator loss: 0.932090]\n",
      "11709 [Discriminator loss: 0.592840, acc.: 70.31%] [Generator loss: 0.986365]\n",
      "11710 [Discriminator loss: 0.644281, acc.: 70.31%] [Generator loss: 1.061121]\n",
      "11711 [Discriminator loss: 0.695718, acc.: 58.59%] [Generator loss: 0.979345]\n",
      "11712 [Discriminator loss: 0.572428, acc.: 78.12%] [Generator loss: 0.817228]\n",
      "11713 [Discriminator loss: 0.722417, acc.: 56.25%] [Generator loss: 0.720379]\n",
      "11714 [Discriminator loss: 0.659345, acc.: 62.50%] [Generator loss: 0.813321]\n",
      "11715 [Discriminator loss: 0.578340, acc.: 72.66%] [Generator loss: 0.892746]\n",
      "11716 [Discriminator loss: 0.631136, acc.: 63.28%] [Generator loss: 0.860917]\n",
      "11717 [Discriminator loss: 0.606987, acc.: 70.31%] [Generator loss: 0.837900]\n",
      "11718 [Discriminator loss: 0.737117, acc.: 51.56%] [Generator loss: 0.853660]\n",
      "11719 [Discriminator loss: 0.795776, acc.: 39.84%] [Generator loss: 1.012541]\n",
      "11720 [Discriminator loss: 0.609564, acc.: 67.19%] [Generator loss: 0.841448]\n",
      "11721 [Discriminator loss: 0.702498, acc.: 56.25%] [Generator loss: 0.820693]\n",
      "11722 [Discriminator loss: 0.629856, acc.: 65.62%] [Generator loss: 0.893151]\n",
      "11723 [Discriminator loss: 0.590884, acc.: 68.75%] [Generator loss: 0.835356]\n",
      "11724 [Discriminator loss: 0.723810, acc.: 50.78%] [Generator loss: 0.684328]\n",
      "11725 [Discriminator loss: 0.715415, acc.: 60.94%] [Generator loss: 0.786292]\n",
      "11726 [Discriminator loss: 0.791858, acc.: 56.25%] [Generator loss: 0.775590]\n",
      "11727 [Discriminator loss: 0.736898, acc.: 62.50%] [Generator loss: 0.879969]\n",
      "11728 [Discriminator loss: 0.758347, acc.: 51.56%] [Generator loss: 0.844391]\n",
      "11729 [Discriminator loss: 0.683725, acc.: 54.69%] [Generator loss: 0.909942]\n",
      "11730 [Discriminator loss: 0.686881, acc.: 61.72%] [Generator loss: 0.833903]\n",
      "11731 [Discriminator loss: 0.662568, acc.: 60.94%] [Generator loss: 0.865775]\n",
      "11732 [Discriminator loss: 0.550256, acc.: 74.22%] [Generator loss: 0.962901]\n",
      "11733 [Discriminator loss: 0.604521, acc.: 70.31%] [Generator loss: 0.949533]\n",
      "11734 [Discriminator loss: 0.615127, acc.: 62.50%] [Generator loss: 1.076421]\n",
      "11735 [Discriminator loss: 0.563451, acc.: 74.22%] [Generator loss: 1.030069]\n",
      "11736 [Discriminator loss: 0.582910, acc.: 75.00%] [Generator loss: 0.937952]\n",
      "11737 [Discriminator loss: 0.655621, acc.: 60.94%] [Generator loss: 0.952170]\n",
      "11738 [Discriminator loss: 0.560751, acc.: 75.00%] [Generator loss: 0.946171]\n",
      "11739 [Discriminator loss: 0.662770, acc.: 56.25%] [Generator loss: 0.948017]\n",
      "11740 [Discriminator loss: 0.641660, acc.: 64.06%] [Generator loss: 0.913465]\n",
      "11741 [Discriminator loss: 0.500182, acc.: 81.25%] [Generator loss: 0.926947]\n",
      "11742 [Discriminator loss: 0.576257, acc.: 75.78%] [Generator loss: 0.858606]\n",
      "11743 [Discriminator loss: 0.654192, acc.: 61.72%] [Generator loss: 0.921191]\n",
      "11744 [Discriminator loss: 0.636211, acc.: 68.75%] [Generator loss: 0.829781]\n",
      "11745 [Discriminator loss: 0.714918, acc.: 55.47%] [Generator loss: 0.859236]\n",
      "11746 [Discriminator loss: 0.669116, acc.: 60.16%] [Generator loss: 0.902454]\n",
      "11747 [Discriminator loss: 0.659928, acc.: 58.59%] [Generator loss: 0.936796]\n",
      "11748 [Discriminator loss: 0.625611, acc.: 69.53%] [Generator loss: 0.835892]\n",
      "11749 [Discriminator loss: 0.608577, acc.: 62.50%] [Generator loss: 0.943831]\n",
      "11750 [Discriminator loss: 0.633538, acc.: 60.94%] [Generator loss: 1.083286]\n",
      "11751 [Discriminator loss: 0.548827, acc.: 77.34%] [Generator loss: 0.920801]\n",
      "11752 [Discriminator loss: 0.553974, acc.: 71.88%] [Generator loss: 0.935585]\n",
      "11753 [Discriminator loss: 0.887391, acc.: 49.22%] [Generator loss: 1.068514]\n",
      "11754 [Discriminator loss: 0.791610, acc.: 60.16%] [Generator loss: 1.006454]\n",
      "11755 [Discriminator loss: 0.544514, acc.: 71.88%] [Generator loss: 0.889297]\n",
      "11756 [Discriminator loss: 0.601407, acc.: 70.31%] [Generator loss: 0.808735]\n",
      "11757 [Discriminator loss: 0.564165, acc.: 76.56%] [Generator loss: 0.766389]\n",
      "11758 [Discriminator loss: 0.801282, acc.: 48.44%] [Generator loss: 0.687042]\n",
      "11759 [Discriminator loss: 0.561374, acc.: 75.78%] [Generator loss: 0.671290]\n",
      "11760 [Discriminator loss: 0.908776, acc.: 33.59%] [Generator loss: 0.783241]\n",
      "11761 [Discriminator loss: 0.603506, acc.: 68.75%] [Generator loss: 0.797742]\n",
      "11762 [Discriminator loss: 0.601017, acc.: 64.84%] [Generator loss: 0.790810]\n",
      "11763 [Discriminator loss: 0.807094, acc.: 45.31%] [Generator loss: 0.829332]\n",
      "11764 [Discriminator loss: 0.587009, acc.: 69.53%] [Generator loss: 1.034313]\n",
      "11765 [Discriminator loss: 0.700547, acc.: 56.25%] [Generator loss: 0.834792]\n",
      "11766 [Discriminator loss: 0.696482, acc.: 60.16%] [Generator loss: 0.940413]\n",
      "11767 [Discriminator loss: 0.577326, acc.: 70.31%] [Generator loss: 1.090673]\n",
      "11768 [Discriminator loss: 0.788331, acc.: 47.66%] [Generator loss: 0.931167]\n",
      "11769 [Discriminator loss: 0.656449, acc.: 64.06%] [Generator loss: 1.053660]\n",
      "11770 [Discriminator loss: 0.629933, acc.: 67.97%] [Generator loss: 0.861882]\n",
      "11771 [Discriminator loss: 0.587064, acc.: 71.09%] [Generator loss: 0.977753]\n",
      "11772 [Discriminator loss: 0.738355, acc.: 65.62%] [Generator loss: 0.812066]\n",
      "11773 [Discriminator loss: 0.733944, acc.: 53.91%] [Generator loss: 0.769085]\n",
      "11774 [Discriminator loss: 0.669227, acc.: 59.38%] [Generator loss: 0.908262]\n",
      "11775 [Discriminator loss: 0.762836, acc.: 53.91%] [Generator loss: 0.753938]\n",
      "11776 [Discriminator loss: 0.796949, acc.: 54.69%] [Generator loss: 0.818486]\n",
      "11777 [Discriminator loss: 0.550554, acc.: 72.66%] [Generator loss: 0.802247]\n",
      "11778 [Discriminator loss: 0.626867, acc.: 64.06%] [Generator loss: 0.996147]\n",
      "11779 [Discriminator loss: 0.741389, acc.: 54.69%] [Generator loss: 0.802720]\n",
      "11780 [Discriminator loss: 0.629149, acc.: 67.19%] [Generator loss: 1.097954]\n",
      "11781 [Discriminator loss: 0.536706, acc.: 71.88%] [Generator loss: 1.154585]\n",
      "11782 [Discriminator loss: 0.673690, acc.: 65.62%] [Generator loss: 0.930733]\n",
      "11783 [Discriminator loss: 0.701382, acc.: 51.56%] [Generator loss: 0.828367]\n",
      "11784 [Discriminator loss: 0.560652, acc.: 68.75%] [Generator loss: 0.806625]\n",
      "11785 [Discriminator loss: 0.693151, acc.: 55.47%] [Generator loss: 0.845956]\n",
      "11786 [Discriminator loss: 0.556922, acc.: 74.22%] [Generator loss: 0.711760]\n",
      "11787 [Discriminator loss: 0.708620, acc.: 58.59%] [Generator loss: 1.047335]\n",
      "11788 [Discriminator loss: 0.558906, acc.: 75.00%] [Generator loss: 0.953816]\n",
      "11789 [Discriminator loss: 0.592866, acc.: 71.09%] [Generator loss: 0.944752]\n",
      "11790 [Discriminator loss: 0.548284, acc.: 71.88%] [Generator loss: 0.989670]\n",
      "11791 [Discriminator loss: 0.792829, acc.: 46.09%] [Generator loss: 0.882049]\n",
      "11792 [Discriminator loss: 0.589136, acc.: 75.78%] [Generator loss: 0.948551]\n",
      "11793 [Discriminator loss: 0.630825, acc.: 67.97%] [Generator loss: 0.838090]\n",
      "11794 [Discriminator loss: 0.712278, acc.: 55.47%] [Generator loss: 0.817725]\n",
      "11795 [Discriminator loss: 0.691493, acc.: 57.03%] [Generator loss: 0.966016]\n",
      "11796 [Discriminator loss: 0.703751, acc.: 55.47%] [Generator loss: 1.082299]\n",
      "11797 [Discriminator loss: 0.851791, acc.: 49.22%] [Generator loss: 1.021217]\n",
      "11798 [Discriminator loss: 0.571663, acc.: 70.31%] [Generator loss: 1.021113]\n",
      "11799 [Discriminator loss: 0.601049, acc.: 67.19%] [Generator loss: 0.894518]\n",
      "11800 [Discriminator loss: 0.701704, acc.: 56.25%] [Generator loss: 0.808328]\n",
      "11801 [Discriminator loss: 0.506289, acc.: 78.12%] [Generator loss: 0.905592]\n",
      "11802 [Discriminator loss: 0.642325, acc.: 64.06%] [Generator loss: 0.986159]\n",
      "11803 [Discriminator loss: 0.612975, acc.: 66.41%] [Generator loss: 0.823540]\n",
      "11804 [Discriminator loss: 0.786598, acc.: 50.00%] [Generator loss: 0.840216]\n",
      "11805 [Discriminator loss: 0.647684, acc.: 61.72%] [Generator loss: 0.869928]\n",
      "11806 [Discriminator loss: 0.595093, acc.: 65.62%] [Generator loss: 0.997180]\n",
      "11807 [Discriminator loss: 0.617901, acc.: 63.28%] [Generator loss: 0.926273]\n",
      "11808 [Discriminator loss: 0.614576, acc.: 65.62%] [Generator loss: 1.048589]\n",
      "11809 [Discriminator loss: 0.671474, acc.: 59.38%] [Generator loss: 1.004003]\n",
      "11810 [Discriminator loss: 0.653245, acc.: 59.38%] [Generator loss: 0.993199]\n",
      "11811 [Discriminator loss: 0.553452, acc.: 71.88%] [Generator loss: 0.963933]\n",
      "11812 [Discriminator loss: 0.642607, acc.: 60.94%] [Generator loss: 0.926589]\n",
      "11813 [Discriminator loss: 0.714176, acc.: 59.38%] [Generator loss: 0.874093]\n",
      "11814 [Discriminator loss: 0.581299, acc.: 67.97%] [Generator loss: 0.970542]\n",
      "11815 [Discriminator loss: 0.485119, acc.: 75.78%] [Generator loss: 0.842503]\n",
      "11816 [Discriminator loss: 0.659418, acc.: 65.62%] [Generator loss: 0.837502]\n",
      "11817 [Discriminator loss: 0.867880, acc.: 42.19%] [Generator loss: 0.611717]\n",
      "11818 [Discriminator loss: 0.671466, acc.: 66.41%] [Generator loss: 0.777332]\n",
      "11819 [Discriminator loss: 0.694289, acc.: 54.69%] [Generator loss: 0.691757]\n",
      "11820 [Discriminator loss: 0.743467, acc.: 45.31%] [Generator loss: 0.804125]\n",
      "11821 [Discriminator loss: 0.654202, acc.: 71.09%] [Generator loss: 0.972194]\n",
      "11822 [Discriminator loss: 0.604694, acc.: 64.84%] [Generator loss: 0.961542]\n",
      "11823 [Discriminator loss: 0.672081, acc.: 58.59%] [Generator loss: 0.878863]\n",
      "11824 [Discriminator loss: 0.758947, acc.: 50.78%] [Generator loss: 1.067554]\n",
      "11825 [Discriminator loss: 0.562555, acc.: 68.75%] [Generator loss: 1.138858]\n",
      "11826 [Discriminator loss: 0.656145, acc.: 62.50%] [Generator loss: 1.170485]\n",
      "11827 [Discriminator loss: 0.636063, acc.: 61.72%] [Generator loss: 1.130653]\n",
      "11828 [Discriminator loss: 0.570366, acc.: 69.53%] [Generator loss: 1.035321]\n",
      "11829 [Discriminator loss: 0.702850, acc.: 52.34%] [Generator loss: 1.049735]\n",
      "11830 [Discriminator loss: 0.597962, acc.: 71.88%] [Generator loss: 0.931983]\n",
      "11831 [Discriminator loss: 0.700642, acc.: 53.91%] [Generator loss: 0.751879]\n",
      "11832 [Discriminator loss: 0.640702, acc.: 63.28%] [Generator loss: 0.769044]\n",
      "11833 [Discriminator loss: 0.721506, acc.: 58.59%] [Generator loss: 0.790145]\n",
      "11834 [Discriminator loss: 0.644652, acc.: 63.28%] [Generator loss: 1.043402]\n",
      "11835 [Discriminator loss: 0.631191, acc.: 64.06%] [Generator loss: 1.013367]\n",
      "11836 [Discriminator loss: 0.655226, acc.: 59.38%] [Generator loss: 0.769678]\n",
      "11837 [Discriminator loss: 0.694452, acc.: 59.38%] [Generator loss: 0.860022]\n",
      "11838 [Discriminator loss: 0.626134, acc.: 71.09%] [Generator loss: 1.009500]\n",
      "11839 [Discriminator loss: 0.524776, acc.: 76.56%] [Generator loss: 0.931052]\n",
      "11840 [Discriminator loss: 0.682315, acc.: 67.97%] [Generator loss: 0.742566]\n",
      "11841 [Discriminator loss: 0.732591, acc.: 50.00%] [Generator loss: 0.779636]\n",
      "11842 [Discriminator loss: 0.675605, acc.: 62.50%] [Generator loss: 0.817700]\n",
      "11843 [Discriminator loss: 0.585219, acc.: 68.75%] [Generator loss: 0.866436]\n",
      "11844 [Discriminator loss: 0.509184, acc.: 80.47%] [Generator loss: 0.845072]\n",
      "11845 [Discriminator loss: 0.585149, acc.: 65.62%] [Generator loss: 0.967308]\n",
      "11846 [Discriminator loss: 0.651194, acc.: 65.62%] [Generator loss: 0.850575]\n",
      "11847 [Discriminator loss: 0.664096, acc.: 61.72%] [Generator loss: 0.927471]\n",
      "11848 [Discriminator loss: 0.731784, acc.: 53.12%] [Generator loss: 0.939351]\n",
      "11849 [Discriminator loss: 0.679590, acc.: 73.44%] [Generator loss: 0.978650]\n",
      "11850 [Discriminator loss: 0.787433, acc.: 57.81%] [Generator loss: 0.815046]\n",
      "11851 [Discriminator loss: 0.653698, acc.: 60.16%] [Generator loss: 0.847656]\n",
      "11852 [Discriminator loss: 0.540302, acc.: 76.56%] [Generator loss: 0.900255]\n",
      "11853 [Discriminator loss: 0.769543, acc.: 52.34%] [Generator loss: 0.850375]\n",
      "11854 [Discriminator loss: 0.593526, acc.: 64.84%] [Generator loss: 1.025460]\n",
      "11855 [Discriminator loss: 0.633060, acc.: 57.81%] [Generator loss: 1.096498]\n",
      "11856 [Discriminator loss: 0.677509, acc.: 56.25%] [Generator loss: 1.036257]\n",
      "11857 [Discriminator loss: 0.784393, acc.: 46.09%] [Generator loss: 1.184637]\n",
      "11858 [Discriminator loss: 0.898357, acc.: 42.19%] [Generator loss: 1.152793]\n",
      "11859 [Discriminator loss: 0.763410, acc.: 46.09%] [Generator loss: 1.183559]\n",
      "11860 [Discriminator loss: 0.638433, acc.: 64.84%] [Generator loss: 1.095420]\n",
      "11861 [Discriminator loss: 0.672874, acc.: 63.28%] [Generator loss: 0.897078]\n",
      "11862 [Discriminator loss: 0.646935, acc.: 62.50%] [Generator loss: 1.023438]\n",
      "11863 [Discriminator loss: 0.910864, acc.: 42.19%] [Generator loss: 1.164198]\n",
      "11864 [Discriminator loss: 0.677621, acc.: 64.84%] [Generator loss: 1.181707]\n",
      "11865 [Discriminator loss: 0.563623, acc.: 71.09%] [Generator loss: 1.123303]\n",
      "11866 [Discriminator loss: 0.772068, acc.: 51.56%] [Generator loss: 0.925645]\n",
      "11867 [Discriminator loss: 0.582991, acc.: 67.97%] [Generator loss: 1.048289]\n",
      "11868 [Discriminator loss: 0.575512, acc.: 74.22%] [Generator loss: 0.996575]\n",
      "11869 [Discriminator loss: 0.620553, acc.: 66.41%] [Generator loss: 1.013981]\n",
      "11870 [Discriminator loss: 0.618793, acc.: 69.53%] [Generator loss: 0.950474]\n",
      "11871 [Discriminator loss: 0.631473, acc.: 62.50%] [Generator loss: 0.874271]\n",
      "11872 [Discriminator loss: 0.793315, acc.: 47.66%] [Generator loss: 0.952306]\n",
      "11873 [Discriminator loss: 0.566330, acc.: 73.44%] [Generator loss: 0.978956]\n",
      "11874 [Discriminator loss: 0.597693, acc.: 73.44%] [Generator loss: 1.036040]\n",
      "11875 [Discriminator loss: 0.747461, acc.: 50.00%] [Generator loss: 1.002389]\n",
      "11876 [Discriminator loss: 0.550395, acc.: 75.78%] [Generator loss: 0.926188]\n",
      "11877 [Discriminator loss: 0.645716, acc.: 65.62%] [Generator loss: 0.967972]\n",
      "11878 [Discriminator loss: 0.665989, acc.: 68.75%] [Generator loss: 0.878611]\n",
      "11879 [Discriminator loss: 0.631562, acc.: 63.28%] [Generator loss: 0.788727]\n",
      "11880 [Discriminator loss: 0.576044, acc.: 67.19%] [Generator loss: 0.911182]\n",
      "11881 [Discriminator loss: 0.693376, acc.: 56.25%] [Generator loss: 0.938349]\n",
      "11882 [Discriminator loss: 0.622819, acc.: 66.41%] [Generator loss: 0.906988]\n",
      "11883 [Discriminator loss: 0.679345, acc.: 63.28%] [Generator loss: 0.759310]\n",
      "11884 [Discriminator loss: 0.625950, acc.: 66.41%] [Generator loss: 0.865940]\n",
      "11885 [Discriminator loss: 0.622020, acc.: 62.50%] [Generator loss: 0.819036]\n",
      "11886 [Discriminator loss: 0.658896, acc.: 60.94%] [Generator loss: 0.940133]\n",
      "11887 [Discriminator loss: 0.694904, acc.: 56.25%] [Generator loss: 0.992405]\n",
      "11888 [Discriminator loss: 0.816733, acc.: 53.91%] [Generator loss: 0.929476]\n",
      "11889 [Discriminator loss: 0.704271, acc.: 56.25%] [Generator loss: 0.844377]\n",
      "11890 [Discriminator loss: 0.515984, acc.: 74.22%] [Generator loss: 0.905030]\n",
      "11891 [Discriminator loss: 0.695457, acc.: 59.38%] [Generator loss: 0.811257]\n",
      "11892 [Discriminator loss: 0.692871, acc.: 53.12%] [Generator loss: 0.873715]\n",
      "11893 [Discriminator loss: 0.646567, acc.: 63.28%] [Generator loss: 0.939859]\n",
      "11894 [Discriminator loss: 0.646310, acc.: 64.06%] [Generator loss: 1.042941]\n",
      "11895 [Discriminator loss: 0.630490, acc.: 59.38%] [Generator loss: 0.890417]\n",
      "11896 [Discriminator loss: 0.646639, acc.: 58.59%] [Generator loss: 0.822254]\n",
      "11897 [Discriminator loss: 0.690241, acc.: 53.12%] [Generator loss: 0.906596]\n",
      "11898 [Discriminator loss: 0.742966, acc.: 51.56%] [Generator loss: 1.024629]\n",
      "11899 [Discriminator loss: 0.622574, acc.: 67.97%] [Generator loss: 1.017861]\n",
      "11900 [Discriminator loss: 0.522117, acc.: 74.22%] [Generator loss: 0.890472]\n",
      "11901 [Discriminator loss: 0.734125, acc.: 50.00%] [Generator loss: 0.913239]\n",
      "11902 [Discriminator loss: 0.654185, acc.: 56.25%] [Generator loss: 1.024496]\n",
      "11903 [Discriminator loss: 0.609008, acc.: 66.41%] [Generator loss: 0.871977]\n",
      "11904 [Discriminator loss: 0.607869, acc.: 69.53%] [Generator loss: 0.836920]\n",
      "11905 [Discriminator loss: 0.708308, acc.: 56.25%] [Generator loss: 0.959983]\n",
      "11906 [Discriminator loss: 0.621349, acc.: 66.41%] [Generator loss: 0.948204]\n",
      "11907 [Discriminator loss: 0.696496, acc.: 61.72%] [Generator loss: 0.792133]\n",
      "11908 [Discriminator loss: 0.903516, acc.: 33.59%] [Generator loss: 0.821318]\n",
      "11909 [Discriminator loss: 0.687031, acc.: 57.03%] [Generator loss: 0.861906]\n",
      "11910 [Discriminator loss: 0.782540, acc.: 53.12%] [Generator loss: 1.176958]\n",
      "11911 [Discriminator loss: 0.654382, acc.: 64.84%] [Generator loss: 1.021048]\n",
      "11912 [Discriminator loss: 0.601631, acc.: 67.97%] [Generator loss: 1.115219]\n",
      "11913 [Discriminator loss: 0.733958, acc.: 56.25%] [Generator loss: 1.064652]\n",
      "11914 [Discriminator loss: 0.659943, acc.: 59.38%] [Generator loss: 0.824058]\n",
      "11915 [Discriminator loss: 0.666663, acc.: 62.50%] [Generator loss: 0.836988]\n",
      "11916 [Discriminator loss: 0.587085, acc.: 69.53%] [Generator loss: 0.819691]\n",
      "11917 [Discriminator loss: 0.653194, acc.: 63.28%] [Generator loss: 1.028926]\n",
      "11918 [Discriminator loss: 0.633785, acc.: 65.62%] [Generator loss: 0.911670]\n",
      "11919 [Discriminator loss: 0.656956, acc.: 65.62%] [Generator loss: 0.877349]\n",
      "11920 [Discriminator loss: 0.714246, acc.: 53.12%] [Generator loss: 0.800965]\n",
      "11921 [Discriminator loss: 0.626837, acc.: 74.22%] [Generator loss: 1.056463]\n",
      "11922 [Discriminator loss: 0.675250, acc.: 62.50%] [Generator loss: 0.861718]\n",
      "11923 [Discriminator loss: 0.581654, acc.: 65.62%] [Generator loss: 0.997175]\n",
      "11924 [Discriminator loss: 0.618954, acc.: 69.53%] [Generator loss: 0.843479]\n",
      "11925 [Discriminator loss: 0.836490, acc.: 45.31%] [Generator loss: 0.921378]\n",
      "11926 [Discriminator loss: 0.674537, acc.: 59.38%] [Generator loss: 0.910147]\n",
      "11927 [Discriminator loss: 0.652731, acc.: 59.38%] [Generator loss: 0.884188]\n",
      "11928 [Discriminator loss: 0.648218, acc.: 69.53%] [Generator loss: 1.003788]\n",
      "11929 [Discriminator loss: 0.722156, acc.: 53.91%] [Generator loss: 1.029073]\n",
      "11930 [Discriminator loss: 0.760128, acc.: 50.78%] [Generator loss: 1.248431]\n",
      "11931 [Discriminator loss: 0.659284, acc.: 62.50%] [Generator loss: 0.892033]\n",
      "11932 [Discriminator loss: 0.543679, acc.: 74.22%] [Generator loss: 0.877074]\n",
      "11933 [Discriminator loss: 0.650040, acc.: 62.50%] [Generator loss: 0.969074]\n",
      "11934 [Discriminator loss: 0.643319, acc.: 64.84%] [Generator loss: 0.970772]\n",
      "11935 [Discriminator loss: 0.681543, acc.: 61.72%] [Generator loss: 0.894700]\n",
      "11936 [Discriminator loss: 0.736655, acc.: 56.25%] [Generator loss: 0.903163]\n",
      "11937 [Discriminator loss: 0.682234, acc.: 54.69%] [Generator loss: 0.885277]\n",
      "11938 [Discriminator loss: 0.723492, acc.: 53.12%] [Generator loss: 0.912906]\n",
      "11939 [Discriminator loss: 0.718286, acc.: 57.81%] [Generator loss: 0.850415]\n",
      "11940 [Discriminator loss: 0.631933, acc.: 61.72%] [Generator loss: 0.853133]\n",
      "11941 [Discriminator loss: 0.751043, acc.: 53.12%] [Generator loss: 0.885033]\n",
      "11942 [Discriminator loss: 0.668992, acc.: 61.72%] [Generator loss: 0.939314]\n",
      "11943 [Discriminator loss: 0.619078, acc.: 71.88%] [Generator loss: 1.103858]\n",
      "11944 [Discriminator loss: 0.555742, acc.: 75.00%] [Generator loss: 1.171612]\n",
      "11945 [Discriminator loss: 0.520634, acc.: 77.34%] [Generator loss: 1.170432]\n",
      "11946 [Discriminator loss: 0.613884, acc.: 71.88%] [Generator loss: 0.970927]\n",
      "11947 [Discriminator loss: 0.544689, acc.: 74.22%] [Generator loss: 1.036008]\n",
      "11948 [Discriminator loss: 0.770928, acc.: 60.16%] [Generator loss: 0.895444]\n",
      "11949 [Discriminator loss: 0.543516, acc.: 75.78%] [Generator loss: 0.976739]\n",
      "11950 [Discriminator loss: 0.705814, acc.: 60.94%] [Generator loss: 0.848392]\n",
      "11951 [Discriminator loss: 0.718447, acc.: 58.59%] [Generator loss: 1.106112]\n",
      "11952 [Discriminator loss: 0.648600, acc.: 61.72%] [Generator loss: 1.053658]\n",
      "11953 [Discriminator loss: 0.611141, acc.: 66.41%] [Generator loss: 0.852554]\n",
      "11954 [Discriminator loss: 0.698892, acc.: 57.03%] [Generator loss: 0.935999]\n",
      "11955 [Discriminator loss: 0.654754, acc.: 61.72%] [Generator loss: 1.044556]\n",
      "11956 [Discriminator loss: 0.622608, acc.: 62.50%] [Generator loss: 1.017402]\n",
      "11957 [Discriminator loss: 0.492532, acc.: 77.34%] [Generator loss: 1.060384]\n",
      "11958 [Discriminator loss: 0.656853, acc.: 60.16%] [Generator loss: 1.061398]\n",
      "11959 [Discriminator loss: 0.550806, acc.: 78.12%] [Generator loss: 1.061233]\n",
      "11960 [Discriminator loss: 0.719935, acc.: 54.69%] [Generator loss: 0.978170]\n",
      "11961 [Discriminator loss: 0.705020, acc.: 60.94%] [Generator loss: 0.993315]\n",
      "11962 [Discriminator loss: 0.606652, acc.: 68.75%] [Generator loss: 0.870764]\n",
      "11963 [Discriminator loss: 0.684696, acc.: 70.31%] [Generator loss: 0.890339]\n",
      "11964 [Discriminator loss: 0.730856, acc.: 52.34%] [Generator loss: 0.850494]\n",
      "11965 [Discriminator loss: 0.667178, acc.: 57.03%] [Generator loss: 0.858896]\n",
      "11966 [Discriminator loss: 0.719570, acc.: 57.81%] [Generator loss: 1.033059]\n",
      "11967 [Discriminator loss: 0.563775, acc.: 71.88%] [Generator loss: 0.926234]\n",
      "11968 [Discriminator loss: 0.579918, acc.: 67.97%] [Generator loss: 0.814331]\n",
      "11969 [Discriminator loss: 0.474175, acc.: 80.47%] [Generator loss: 0.746533]\n",
      "11970 [Discriminator loss: 0.629940, acc.: 64.84%] [Generator loss: 0.844094]\n",
      "11971 [Discriminator loss: 0.627208, acc.: 69.53%] [Generator loss: 0.737535]\n",
      "11972 [Discriminator loss: 0.533682, acc.: 76.56%] [Generator loss: 0.758271]\n",
      "11973 [Discriminator loss: 0.610928, acc.: 63.28%] [Generator loss: 0.804072]\n",
      "11974 [Discriminator loss: 0.678631, acc.: 65.62%] [Generator loss: 0.867878]\n",
      "11975 [Discriminator loss: 0.717866, acc.: 55.47%] [Generator loss: 0.952132]\n",
      "11976 [Discriminator loss: 0.576358, acc.: 67.19%] [Generator loss: 1.031785]\n",
      "11977 [Discriminator loss: 0.600560, acc.: 67.19%] [Generator loss: 1.123379]\n",
      "11978 [Discriminator loss: 0.615168, acc.: 66.41%] [Generator loss: 1.036745]\n",
      "11979 [Discriminator loss: 0.729003, acc.: 56.25%] [Generator loss: 0.958969]\n",
      "11980 [Discriminator loss: 0.684835, acc.: 56.25%] [Generator loss: 0.961653]\n",
      "11981 [Discriminator loss: 0.522116, acc.: 77.34%] [Generator loss: 0.838436]\n",
      "11982 [Discriminator loss: 0.620240, acc.: 64.84%] [Generator loss: 0.750107]\n",
      "11983 [Discriminator loss: 0.640624, acc.: 65.62%] [Generator loss: 0.832030]\n",
      "11984 [Discriminator loss: 0.563378, acc.: 69.53%] [Generator loss: 0.894306]\n",
      "11985 [Discriminator loss: 0.673420, acc.: 53.12%] [Generator loss: 0.858921]\n",
      "11986 [Discriminator loss: 0.676708, acc.: 58.59%] [Generator loss: 1.106537]\n",
      "11987 [Discriminator loss: 0.626146, acc.: 66.41%] [Generator loss: 1.140635]\n",
      "11988 [Discriminator loss: 0.758850, acc.: 46.88%] [Generator loss: 0.957180]\n",
      "11989 [Discriminator loss: 0.643971, acc.: 63.28%] [Generator loss: 0.974730]\n",
      "11990 [Discriminator loss: 0.763596, acc.: 51.56%] [Generator loss: 0.822288]\n",
      "11991 [Discriminator loss: 0.819870, acc.: 50.78%] [Generator loss: 1.306288]\n",
      "11992 [Discriminator loss: 0.712913, acc.: 63.28%] [Generator loss: 1.154886]\n",
      "11993 [Discriminator loss: 0.730888, acc.: 52.34%] [Generator loss: 0.981379]\n",
      "11994 [Discriminator loss: 0.725974, acc.: 52.34%] [Generator loss: 0.758476]\n",
      "11995 [Discriminator loss: 0.679596, acc.: 62.50%] [Generator loss: 0.961128]\n",
      "11996 [Discriminator loss: 0.791663, acc.: 45.31%] [Generator loss: 1.004084]\n",
      "11997 [Discriminator loss: 0.673671, acc.: 58.59%] [Generator loss: 1.018719]\n",
      "11998 [Discriminator loss: 0.718757, acc.: 51.56%] [Generator loss: 1.123748]\n",
      "11999 [Discriminator loss: 0.553319, acc.: 75.00%] [Generator loss: 1.099112]\n",
      "12000 [Discriminator loss: 0.599187, acc.: 70.31%] [Generator loss: 1.185518]\n",
      "12001 [Discriminator loss: 0.599213, acc.: 78.12%] [Generator loss: 0.921699]\n",
      "12002 [Discriminator loss: 0.443107, acc.: 83.59%] [Generator loss: 0.866059]\n",
      "12003 [Discriminator loss: 0.772877, acc.: 46.88%] [Generator loss: 0.749804]\n",
      "12004 [Discriminator loss: 0.775136, acc.: 48.44%] [Generator loss: 0.737364]\n",
      "12005 [Discriminator loss: 0.593098, acc.: 72.66%] [Generator loss: 0.872595]\n",
      "12006 [Discriminator loss: 0.617778, acc.: 64.06%] [Generator loss: 0.857399]\n",
      "12007 [Discriminator loss: 0.591538, acc.: 73.44%] [Generator loss: 0.885852]\n",
      "12008 [Discriminator loss: 0.595389, acc.: 71.09%] [Generator loss: 0.791356]\n",
      "12009 [Discriminator loss: 0.697881, acc.: 55.47%] [Generator loss: 0.770860]\n",
      "12010 [Discriminator loss: 0.571612, acc.: 69.53%] [Generator loss: 0.899035]\n",
      "12011 [Discriminator loss: 0.562519, acc.: 73.44%] [Generator loss: 0.867464]\n",
      "12012 [Discriminator loss: 0.608328, acc.: 64.84%] [Generator loss: 1.028733]\n",
      "12013 [Discriminator loss: 0.543600, acc.: 70.31%] [Generator loss: 0.925607]\n",
      "12014 [Discriminator loss: 0.661857, acc.: 66.41%] [Generator loss: 0.831565]\n",
      "12015 [Discriminator loss: 0.691839, acc.: 60.16%] [Generator loss: 0.680866]\n",
      "12016 [Discriminator loss: 0.659970, acc.: 61.72%] [Generator loss: 0.850162]\n",
      "12017 [Discriminator loss: 0.570720, acc.: 75.78%] [Generator loss: 1.031290]\n",
      "12018 [Discriminator loss: 0.709269, acc.: 56.25%] [Generator loss: 0.962628]\n",
      "12019 [Discriminator loss: 0.573198, acc.: 72.66%] [Generator loss: 1.152884]\n",
      "12020 [Discriminator loss: 0.673820, acc.: 53.12%] [Generator loss: 0.983262]\n",
      "12021 [Discriminator loss: 0.605294, acc.: 63.28%] [Generator loss: 1.094572]\n",
      "12022 [Discriminator loss: 0.615631, acc.: 64.84%] [Generator loss: 0.947969]\n",
      "12023 [Discriminator loss: 0.577542, acc.: 69.53%] [Generator loss: 0.729516]\n",
      "12024 [Discriminator loss: 0.563041, acc.: 71.09%] [Generator loss: 0.710104]\n",
      "12025 [Discriminator loss: 0.529415, acc.: 74.22%] [Generator loss: 0.649581]\n",
      "12026 [Discriminator loss: 0.604253, acc.: 69.53%] [Generator loss: 0.672075]\n",
      "12027 [Discriminator loss: 0.616465, acc.: 66.41%] [Generator loss: 0.875410]\n",
      "12028 [Discriminator loss: 0.490958, acc.: 79.69%] [Generator loss: 0.818263]\n",
      "12029 [Discriminator loss: 0.499932, acc.: 75.00%] [Generator loss: 0.782696]\n",
      "12030 [Discriminator loss: 0.777598, acc.: 51.56%] [Generator loss: 0.838362]\n",
      "12031 [Discriminator loss: 0.653570, acc.: 67.19%] [Generator loss: 1.048306]\n",
      "12032 [Discriminator loss: 0.660338, acc.: 68.75%] [Generator loss: 1.218584]\n",
      "12033 [Discriminator loss: 0.678307, acc.: 62.50%] [Generator loss: 1.004798]\n",
      "12034 [Discriminator loss: 0.514493, acc.: 74.22%] [Generator loss: 0.937366]\n",
      "12035 [Discriminator loss: 0.497856, acc.: 77.34%] [Generator loss: 0.978163]\n",
      "12036 [Discriminator loss: 0.675702, acc.: 60.94%] [Generator loss: 0.991347]\n",
      "12037 [Discriminator loss: 0.710443, acc.: 56.25%] [Generator loss: 0.897520]\n",
      "12038 [Discriminator loss: 0.690548, acc.: 56.25%] [Generator loss: 1.224437]\n",
      "12039 [Discriminator loss: 0.481831, acc.: 80.47%] [Generator loss: 0.900909]\n",
      "12040 [Discriminator loss: 0.740685, acc.: 60.16%] [Generator loss: 0.657721]\n",
      "12041 [Discriminator loss: 0.678928, acc.: 56.25%] [Generator loss: 0.655823]\n",
      "12042 [Discriminator loss: 0.489785, acc.: 82.03%] [Generator loss: 0.602945]\n",
      "12043 [Discriminator loss: 0.865714, acc.: 39.06%] [Generator loss: 0.598094]\n",
      "12044 [Discriminator loss: 0.848437, acc.: 46.09%] [Generator loss: 0.698414]\n",
      "12045 [Discriminator loss: 0.681111, acc.: 57.03%] [Generator loss: 0.907246]\n",
      "12046 [Discriminator loss: 0.638847, acc.: 60.94%] [Generator loss: 1.148937]\n",
      "12047 [Discriminator loss: 0.768918, acc.: 51.56%] [Generator loss: 1.259270]\n",
      "12048 [Discriminator loss: 0.788730, acc.: 48.44%] [Generator loss: 1.065319]\n",
      "12049 [Discriminator loss: 0.584519, acc.: 66.41%] [Generator loss: 0.928231]\n",
      "12050 [Discriminator loss: 0.582734, acc.: 69.53%] [Generator loss: 0.761710]\n",
      "12051 [Discriminator loss: 0.602217, acc.: 72.66%] [Generator loss: 0.775022]\n",
      "12052 [Discriminator loss: 0.443365, acc.: 80.47%] [Generator loss: 0.694829]\n",
      "12053 [Discriminator loss: 0.501107, acc.: 79.69%] [Generator loss: 0.713378]\n",
      "12054 [Discriminator loss: 0.516994, acc.: 80.47%] [Generator loss: 0.652031]\n",
      "12055 [Discriminator loss: 0.635740, acc.: 64.84%] [Generator loss: 0.767912]\n",
      "12056 [Discriminator loss: 0.591042, acc.: 66.41%] [Generator loss: 1.033404]\n",
      "12057 [Discriminator loss: 0.842660, acc.: 39.84%] [Generator loss: 0.936568]\n",
      "12058 [Discriminator loss: 0.589326, acc.: 71.88%] [Generator loss: 1.109844]\n",
      "12059 [Discriminator loss: 0.857308, acc.: 39.06%] [Generator loss: 0.903607]\n",
      "12060 [Discriminator loss: 0.814004, acc.: 46.09%] [Generator loss: 1.019559]\n",
      "12061 [Discriminator loss: 0.613224, acc.: 66.41%] [Generator loss: 1.023407]\n",
      "12062 [Discriminator loss: 0.661879, acc.: 60.94%] [Generator loss: 0.976191]\n",
      "12063 [Discriminator loss: 0.683751, acc.: 64.06%] [Generator loss: 0.954079]\n",
      "12064 [Discriminator loss: 0.654715, acc.: 61.72%] [Generator loss: 0.878280]\n",
      "12065 [Discriminator loss: 0.619496, acc.: 66.41%] [Generator loss: 0.965775]\n",
      "12066 [Discriminator loss: 0.642783, acc.: 62.50%] [Generator loss: 0.963939]\n",
      "12067 [Discriminator loss: 0.655727, acc.: 56.25%] [Generator loss: 0.936656]\n",
      "12068 [Discriminator loss: 0.849871, acc.: 40.62%] [Generator loss: 0.863090]\n",
      "12069 [Discriminator loss: 0.844008, acc.: 39.84%] [Generator loss: 0.670665]\n",
      "12070 [Discriminator loss: 0.622785, acc.: 73.44%] [Generator loss: 0.978741]\n",
      "12071 [Discriminator loss: 0.672080, acc.: 61.72%] [Generator loss: 0.882864]\n",
      "12072 [Discriminator loss: 0.762028, acc.: 51.56%] [Generator loss: 0.788062]\n",
      "12073 [Discriminator loss: 0.707864, acc.: 55.47%] [Generator loss: 0.989604]\n",
      "12074 [Discriminator loss: 0.642896, acc.: 59.38%] [Generator loss: 0.903517]\n",
      "12075 [Discriminator loss: 0.747996, acc.: 57.81%] [Generator loss: 0.810407]\n",
      "12076 [Discriminator loss: 0.552026, acc.: 71.88%] [Generator loss: 0.737936]\n",
      "12077 [Discriminator loss: 0.770720, acc.: 53.12%] [Generator loss: 0.833116]\n",
      "12078 [Discriminator loss: 0.664433, acc.: 61.72%] [Generator loss: 1.011138]\n",
      "12079 [Discriminator loss: 0.498000, acc.: 75.78%] [Generator loss: 1.074016]\n",
      "12080 [Discriminator loss: 0.673671, acc.: 57.03%] [Generator loss: 1.022681]\n",
      "12081 [Discriminator loss: 0.737200, acc.: 53.91%] [Generator loss: 0.984850]\n",
      "12082 [Discriminator loss: 0.570912, acc.: 67.97%] [Generator loss: 1.086335]\n",
      "12083 [Discriminator loss: 0.536455, acc.: 75.78%] [Generator loss: 1.165093]\n",
      "12084 [Discriminator loss: 0.695041, acc.: 59.38%] [Generator loss: 0.856767]\n",
      "12085 [Discriminator loss: 0.607897, acc.: 69.53%] [Generator loss: 0.984174]\n",
      "12086 [Discriminator loss: 0.628069, acc.: 64.84%] [Generator loss: 0.987699]\n",
      "12087 [Discriminator loss: 0.656958, acc.: 64.06%] [Generator loss: 0.952433]\n",
      "12088 [Discriminator loss: 0.658200, acc.: 54.69%] [Generator loss: 0.886404]\n",
      "12089 [Discriminator loss: 0.594760, acc.: 70.31%] [Generator loss: 0.941250]\n",
      "12090 [Discriminator loss: 0.634562, acc.: 67.97%] [Generator loss: 0.865646]\n",
      "12091 [Discriminator loss: 0.694154, acc.: 63.28%] [Generator loss: 1.132086]\n",
      "12092 [Discriminator loss: 0.721683, acc.: 59.38%] [Generator loss: 0.797668]\n",
      "12093 [Discriminator loss: 0.785627, acc.: 53.12%] [Generator loss: 0.717318]\n",
      "12094 [Discriminator loss: 0.589847, acc.: 77.34%] [Generator loss: 0.982411]\n",
      "12095 [Discriminator loss: 0.655093, acc.: 59.38%] [Generator loss: 0.878458]\n",
      "12096 [Discriminator loss: 0.542445, acc.: 81.25%] [Generator loss: 0.855682]\n",
      "12097 [Discriminator loss: 0.683429, acc.: 57.03%] [Generator loss: 0.906646]\n",
      "12098 [Discriminator loss: 0.684173, acc.: 53.12%] [Generator loss: 0.874903]\n",
      "12099 [Discriminator loss: 0.722902, acc.: 56.25%] [Generator loss: 0.807964]\n",
      "12100 [Discriminator loss: 0.968136, acc.: 35.94%] [Generator loss: 0.798434]\n",
      "12101 [Discriminator loss: 0.799594, acc.: 55.47%] [Generator loss: 1.033418]\n",
      "12102 [Discriminator loss: 0.714971, acc.: 59.38%] [Generator loss: 0.946170]\n",
      "12103 [Discriminator loss: 0.652681, acc.: 66.41%] [Generator loss: 0.834578]\n",
      "12104 [Discriminator loss: 0.577474, acc.: 66.41%] [Generator loss: 0.888064]\n",
      "12105 [Discriminator loss: 0.773499, acc.: 50.00%] [Generator loss: 0.885267]\n",
      "12106 [Discriminator loss: 0.646599, acc.: 62.50%] [Generator loss: 0.794966]\n",
      "12107 [Discriminator loss: 0.645886, acc.: 64.84%] [Generator loss: 0.786647]\n",
      "12108 [Discriminator loss: 0.630508, acc.: 63.28%] [Generator loss: 0.785790]\n",
      "12109 [Discriminator loss: 0.648879, acc.: 64.84%] [Generator loss: 0.959418]\n",
      "12110 [Discriminator loss: 0.667225, acc.: 64.84%] [Generator loss: 0.869175]\n",
      "12111 [Discriminator loss: 0.693907, acc.: 57.81%] [Generator loss: 1.120031]\n",
      "12112 [Discriminator loss: 0.574804, acc.: 71.09%] [Generator loss: 1.132684]\n",
      "12113 [Discriminator loss: 0.683737, acc.: 53.12%] [Generator loss: 1.122698]\n",
      "12114 [Discriminator loss: 0.597633, acc.: 68.75%] [Generator loss: 0.981059]\n",
      "12115 [Discriminator loss: 0.640035, acc.: 64.84%] [Generator loss: 0.869626]\n",
      "12116 [Discriminator loss: 0.860707, acc.: 45.31%] [Generator loss: 0.846163]\n",
      "12117 [Discriminator loss: 0.917082, acc.: 36.72%] [Generator loss: 0.902497]\n",
      "12118 [Discriminator loss: 0.649051, acc.: 60.94%] [Generator loss: 1.012910]\n",
      "12119 [Discriminator loss: 0.620924, acc.: 63.28%] [Generator loss: 0.932160]\n",
      "12120 [Discriminator loss: 0.662462, acc.: 63.28%] [Generator loss: 0.858083]\n",
      "12121 [Discriminator loss: 0.671947, acc.: 53.12%] [Generator loss: 0.902962]\n",
      "12122 [Discriminator loss: 0.642254, acc.: 61.72%] [Generator loss: 0.872632]\n",
      "12123 [Discriminator loss: 0.680749, acc.: 59.38%] [Generator loss: 0.904422]\n",
      "12124 [Discriminator loss: 0.776795, acc.: 52.34%] [Generator loss: 0.843314]\n",
      "12125 [Discriminator loss: 0.729231, acc.: 53.12%] [Generator loss: 0.946846]\n",
      "12126 [Discriminator loss: 0.662210, acc.: 60.94%] [Generator loss: 0.851803]\n",
      "12127 [Discriminator loss: 0.715019, acc.: 59.38%] [Generator loss: 0.870429]\n",
      "12128 [Discriminator loss: 0.610122, acc.: 68.75%] [Generator loss: 0.899318]\n",
      "12129 [Discriminator loss: 0.826834, acc.: 40.62%] [Generator loss: 0.915034]\n",
      "12130 [Discriminator loss: 0.630332, acc.: 60.16%] [Generator loss: 0.863286]\n",
      "12131 [Discriminator loss: 0.594740, acc.: 67.97%] [Generator loss: 0.924153]\n",
      "12132 [Discriminator loss: 0.659195, acc.: 55.47%] [Generator loss: 0.876782]\n",
      "12133 [Discriminator loss: 0.718936, acc.: 57.03%] [Generator loss: 1.015349]\n",
      "12134 [Discriminator loss: 0.545157, acc.: 75.78%] [Generator loss: 1.071738]\n",
      "12135 [Discriminator loss: 0.769635, acc.: 50.78%] [Generator loss: 0.949510]\n",
      "12136 [Discriminator loss: 0.602357, acc.: 71.09%] [Generator loss: 0.907701]\n",
      "12137 [Discriminator loss: 0.755904, acc.: 56.25%] [Generator loss: 0.883358]\n",
      "12138 [Discriminator loss: 0.649654, acc.: 65.62%] [Generator loss: 0.920653]\n",
      "12139 [Discriminator loss: 0.719065, acc.: 51.56%] [Generator loss: 0.898108]\n",
      "12140 [Discriminator loss: 0.623464, acc.: 62.50%] [Generator loss: 0.973318]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    facegenerator = FaceGenerator(64,64,3)\n",
    "    facegenerator.train(datafolder='../input/celeba/img_align_celeba/img_align_celeba/',epochs=100001, batch_size=64,\n",
    "                        save_images_interval=500)\n",
    "    #facegenerator.generate_single_image(\"saved_models/facegenerator.h5\",\"test.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
