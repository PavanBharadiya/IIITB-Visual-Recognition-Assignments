{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['onlyiiitbproperdataset', 'iiitbmixed', 'face-combined-dataset', 'indian-movie-face-database-imfdb', 'iiitbcroppeddata', 'iiitbdata']\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Reshape, Dropout, Dense, Flatten, BatchNormalization, Activation, ZeroPadding2D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.optimizers import Adam,SGD,Nadam, RMSprop\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "#tqdm for a progress bar when loading the dataset\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "print(os.listdir(\"../input\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "class FaceGenerator:\n",
    "    def __init__(self,image_width,image_height,channels):\n",
    "        self.image_width = image_width\n",
    "        self.image_height = image_height\n",
    "        self.channels = channels\n",
    "        \n",
    "        \n",
    "        self.image_shape = (self.image_width,self.image_height,self.channels)\n",
    "\n",
    "        #Amount of randomly generated numbers for the first layer of the generator.\n",
    "        self.random_noise_dimension = 100\n",
    "\n",
    "        #Just 10 times higher learning rate would result in generator loss being stuck at 0.\n",
    "        #optimizer = Adam(lr = 0.0005,beta_1 = 0.5)\n",
    "        optimizer = Adam(0.0002,0.5)\n",
    "\n",
    "        self.discriminator = self.build_discriminator()\n",
    "        self.discriminator.compile(loss=\"binary_crossentropy\",optimizer=optimizer,metrics=[\"accuracy\"])\n",
    "        \n",
    "        self.generator = self.build_generator()\n",
    "        self.generator.compile(loss=\"binary_crossentropy\",optimizer=optimizer)\n",
    "        \n",
    "        \n",
    "        #A placeholder for the generator input.\n",
    "        random_input = Input(shape=(self.random_noise_dimension,))\n",
    "\n",
    "        #Generator generates images from random noise.\n",
    "        generated_image = self.generator(random_input)\n",
    "\n",
    "        # For the combined model we will only train the generator\n",
    "        self.discriminator.trainable = False\n",
    "\n",
    "        #Discriminator attempts to determine if image is real or generated\n",
    "        validity = self.discriminator(generated_image)\n",
    "\n",
    "        #Combined model = generator and discriminator combined.\n",
    "        #1. Takes random noise as an input.\n",
    "        #2. Generates an image.\n",
    "        #3. Attempts to determine if image is real or generated.\n",
    "        self.combined = Model(random_input,validity)\n",
    "        self.combined.compile(loss=\"binary_crossentropy\",optimizer=optimizer)\n",
    "        \n",
    "    \n",
    "    \n",
    "    def build_generator(self):\n",
    "        \n",
    "        model = Sequential()\n",
    "        model.add(Dense(256*4*4,activation=\"relu\",input_dim=self.random_noise_dimension))\n",
    "        model.add(Reshape((4,4,256)))\n",
    "\n",
    "        #Four layers of upsampling, convolution, batch normalization and activation.\n",
    "        # 1. Upsampling: Input data is repeated. Default is (2,2). In that case a 4x4x256 array becomes an 8x8x256 array.\n",
    "        # 2. Convolution: If you are not familiar, you should watch this video: https://www.youtube.com/watch?v=FTr3n7uBIuE\n",
    "        # 3. Normalization normalizes outputs from convolution.\n",
    "        # 4. Relu activation:  f(x) = max(0,x). If x < 0, then f(x) = 0.\n",
    "\n",
    "\n",
    "        model.add(UpSampling2D())\n",
    "        model.add(Conv2D(256,kernel_size=3,padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Activation(\"relu\"))\n",
    "\n",
    "        model.add(UpSampling2D())\n",
    "        model.add(Conv2D(256,kernel_size=3,padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Activation(\"relu\"))\n",
    "\n",
    "        model.add(UpSampling2D())\n",
    "        model.add(Conv2D(128,kernel_size=3,padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Activation(\"relu\"))\n",
    "\n",
    "        model.add(UpSampling2D())\n",
    "        model.add(Conv2D(128,kernel_size=3,padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Activation(\"relu\"))\n",
    "\n",
    "\n",
    "        # Last convolutional layer outputs as many featuremaps as channels in the final image.\n",
    "        model.add(Conv2D(self.channels,kernel_size=3,padding=\"same\"))\n",
    "        # tanh maps everything to a range between -1 and 1.\n",
    "        model.add(Activation(\"tanh\"))\n",
    "\n",
    "        # show the summary of the model architecture\n",
    "        model.summary()\n",
    "\n",
    "        # Placeholder for the random noise input\n",
    "        input = Input(shape=(self.random_noise_dimension,))\n",
    "        #Model output\n",
    "        generated_image = model(input)\n",
    "\n",
    "        #Change the model type from Sequential to Model (functional API) More at: https://keras.io/models/model/.\n",
    "        return Model(input,generated_image)\n",
    "    \n",
    "    \n",
    "    def build_discriminator(self):\n",
    "        #Discriminator attempts to classify real and generated images\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Conv2D(32, kernel_size=3, strides=2, input_shape=self.image_shape, padding=\"same\"))\n",
    "        #Leaky relu is similar to usual relu. If x < 0 then f(x) = x * alpha, otherwise f(x) = x.\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "        #Dropout blocks some connections randomly. This help the model to generalize better.\n",
    "        #0.25 means that every connection has a 25% chance of being blocked.\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Conv2D(64, kernel_size=3, strides=2, padding=\"same\"))\n",
    "        #Zero padding adds additional rows and columns to the image. Those rows and columns are made of zeros.\n",
    "        model.add(ZeroPadding2D(padding=((0,1),(0,1))))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Conv2D(128, kernel_size=3, strides=2, padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Conv2D(256, kernel_size=3, strides=1, padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Conv2D(512, kernel_size=3, strides=1, padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "        model.add(Dropout(0.25))\n",
    "        #Flatten layer flattens the output of the previous layer to a single dimension.\n",
    "        model.add(Flatten())\n",
    "        #Outputs a value between 0 and 1 that predicts whether image is real or generated. 0 = generated, 1 = real.\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "        input_image = Input(shape=self.image_shape)\n",
    "\n",
    "        #Model output given an image.\n",
    "        validity = model(input_image)\n",
    "\n",
    "        return Model(input_image, validity)\n",
    "    \n",
    "    \n",
    "    \n",
    "    def get_training_data(self,datafolder):\n",
    "        print(\"Loading training data...\")\n",
    "\n",
    "        training_data = []\n",
    "        #Finds all files in datafolder\n",
    "        filenames = os.listdir(datafolder)\n",
    "        for filename in tqdm(filenames):\n",
    "            #Combines folder name and file name.\n",
    "            path = os.path.join(datafolder,filename)\n",
    "            #Opens an image as an Image object.\n",
    "            image = Image.open(path)\n",
    "            #Resizes to a desired size.\n",
    "            image = image.resize((self.image_width,self.image_height),Image.ANTIALIAS)\n",
    "            #Creates an array of pixel values from the image.\n",
    "            pixel_array = np.asarray(image)\n",
    "\n",
    "            training_data.append(pixel_array)\n",
    "\n",
    "        #training_data is converted to a numpy array\n",
    "        training_data = np.reshape(training_data,(-1,self.image_width,self.image_height,self.channels))\n",
    "        print(\"Shape of training data:\",training_data.shape)\n",
    "        return training_data\n",
    "    \n",
    "    \n",
    "    def train(self, datafolder ,epochs,batch_size,save_images_interval):\n",
    "        #Get the real images\n",
    "        training_data = self.get_training_data(datafolder)\n",
    "        training_data = training_data / 127.5 - 1.\n",
    "\n",
    "        #Two arrays of labels. Labels for real images: [1,1,1 ... 1,1,1], labels for generated images: [0,0,0 ... 0,0,0]\n",
    "        labels_for_real_images = np.ones((batch_size,1))\n",
    "        labels_for_generated_images = np.zeros((batch_size,1))\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            # Select a random half of images\n",
    "            indices = np.random.randint(0,training_data.shape[0],batch_size)\n",
    "            real_images = training_data[indices]\n",
    "\n",
    "            #Generate random noise for a whole batch.\n",
    "            random_noise = np.random.normal(0,1,(batch_size,self.random_noise_dimension))\n",
    "            #Generate a batch of new images.\n",
    "            generated_images = self.generator.predict(random_noise)\n",
    "\n",
    "            #Train the discriminator on real images.\n",
    "            discriminator_loss_real = self.discriminator.train_on_batch(real_images,labels_for_real_images)\n",
    "            #Train the discriminator on generated images.\n",
    "            discriminator_loss_generated = self.discriminator.train_on_batch(generated_images,labels_for_generated_images)\n",
    "            #Calculate the average discriminator loss.\n",
    "            discriminator_loss = 0.5 * np.add(discriminator_loss_real,discriminator_loss_generated)\n",
    "\n",
    "            #Train the generator using the combined model. Generator tries to trick discriminator into mistaking generated images as real.\n",
    "            generator_loss = self.combined.train_on_batch(random_noise,labels_for_real_images)\n",
    "            print (\"%d [Discriminator loss: %f, acc.: %.2f%%] [Generator loss: %f]\" % (epoch, discriminator_loss[0], 100*discriminator_loss[1], generator_loss))\n",
    "\n",
    "            if epoch % save_images_interval == 0:\n",
    "                self.save_images(epoch)\n",
    "\n",
    "        #Save the model for a later use\n",
    "        #self.generator.save(\"saved_models/facegenerator.h5\")\n",
    "\n",
    "\n",
    "    def save_images(self,epoch):\n",
    "        #Save 25 generated images for demonstration purposes using matplotlib.pyplot.\n",
    "        rows, columns = 3, 3\n",
    "        noise = np.random.normal(0, 1, (rows * columns, self.random_noise_dimension))\n",
    "        generated_images = self.generator.predict(noise)\n",
    "\n",
    "        generated_images = 0.5 * generated_images + 0.5\n",
    "\n",
    "        figure, axis = plt.subplots(rows, columns)\n",
    "        image_count = 0\n",
    "        for row in range(rows):\n",
    "            for column in range(columns):\n",
    "                axis[row,column].imshow(generated_images[image_count, :], cmap='spring')\n",
    "                axis[row,column].axis('off')\n",
    "                image_count += 1\n",
    "        figure.savefig(\"new_generated_%d.png\" % epoch)\n",
    "        plt.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 32, 32, 32)        896       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 16, 16, 64)        18496     \n",
      "_________________________________________________________________\n",
      "zero_padding2d_1 (ZeroPaddin (None, 17, 17, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 17, 17, 64)        256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 17, 17, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 17, 17, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 9, 9, 128)         73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 9, 9, 128)         512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 9, 9, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 9, 9, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 9, 9, 256)         295168    \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 9, 9, 256)         1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 9, 9, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 9, 9, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 9, 9, 512)         1180160   \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 9, 9, 512)         2048      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 9, 9, 512)         0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 9, 9, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 41472)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 41473     \n",
      "=================================================================\n",
      "Total params: 1,613,889\n",
      "Trainable params: 1,611,969\n",
      "Non-trainable params: 1,920\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_2 (Dense)              (None, 4096)              413696    \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2 (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 8, 8, 256)         590080    \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 8, 8, 256)         1024      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_2 (UpSampling2 (None, 16, 16, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 16, 16, 256)       590080    \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 16, 16, 256)       1024      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 16, 16, 256)       0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_3 (UpSampling2 (None, 32, 32, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 32, 32, 128)       295040    \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 32, 32, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 32, 32, 128)       0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_4 (UpSampling2 (None, 64, 64, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 64, 64, 128)       147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 64, 64, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 64, 64, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 64, 64, 3)         3459      \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 64, 64, 3)         0         \n",
      "=================================================================\n",
      "Total params: 2,043,011\n",
      "Trainable params: 2,041,475\n",
      "Non-trainable params: 1,536\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█▏        | 66/577 [00:00<00:00, 654.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 577/577 [00:00<00:00, 719.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training data: (577, 64, 64, 3)\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/keras/engine/training.py:490: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [Discriminator loss: 2.505238, acc.: 31.25%] [Generator loss: 0.264103]\n",
      "1 [Discriminator loss: 0.668689, acc.: 67.19%] [Generator loss: 2.758365]\n",
      "2 [Discriminator loss: 0.766628, acc.: 57.81%] [Generator loss: 2.146531]\n",
      "3 [Discriminator loss: 0.699816, acc.: 62.50%] [Generator loss: 1.393736]\n",
      "4 [Discriminator loss: 0.301092, acc.: 92.19%] [Generator loss: 1.686325]\n",
      "5 [Discriminator loss: 0.132576, acc.: 96.88%] [Generator loss: 1.702996]\n",
      "6 [Discriminator loss: 0.266781, acc.: 95.31%] [Generator loss: 1.113998]\n",
      "7 [Discriminator loss: 0.185869, acc.: 92.19%] [Generator loss: 1.444483]\n",
      "8 [Discriminator loss: 0.256186, acc.: 92.19%] [Generator loss: 2.549971]\n",
      "9 [Discriminator loss: 0.400617, acc.: 81.25%] [Generator loss: 5.081616]\n",
      "10 [Discriminator loss: 0.644615, acc.: 78.12%] [Generator loss: 4.043653]\n",
      "11 [Discriminator loss: 0.274627, acc.: 90.62%] [Generator loss: 2.520226]\n",
      "12 [Discriminator loss: 0.258752, acc.: 89.06%] [Generator loss: 2.329696]\n",
      "13 [Discriminator loss: 0.188386, acc.: 92.19%] [Generator loss: 0.919791]\n",
      "14 [Discriminator loss: 0.146375, acc.: 95.31%] [Generator loss: 1.087271]\n",
      "15 [Discriminator loss: 0.113315, acc.: 96.88%] [Generator loss: 1.429208]\n",
      "16 [Discriminator loss: 0.366549, acc.: 85.94%] [Generator loss: 5.305804]\n",
      "17 [Discriminator loss: 0.439331, acc.: 87.50%] [Generator loss: 4.137421]\n",
      "18 [Discriminator loss: 0.157171, acc.: 92.19%] [Generator loss: 1.925580]\n",
      "19 [Discriminator loss: 0.096625, acc.: 98.44%] [Generator loss: 1.385509]\n",
      "20 [Discriminator loss: 0.168412, acc.: 93.75%] [Generator loss: 3.597291]\n",
      "21 [Discriminator loss: 0.992650, acc.: 59.38%] [Generator loss: 9.909847]\n",
      "22 [Discriminator loss: 1.866231, acc.: 50.00%] [Generator loss: 2.659997]\n",
      "23 [Discriminator loss: 0.301236, acc.: 84.38%] [Generator loss: 3.828480]\n",
      "24 [Discriminator loss: 1.066772, acc.: 54.69%] [Generator loss: 12.087513]\n",
      "25 [Discriminator loss: 1.151522, acc.: 67.19%] [Generator loss: 8.420433]\n",
      "26 [Discriminator loss: 0.970735, acc.: 56.25%] [Generator loss: 4.774134]\n",
      "27 [Discriminator loss: 0.222141, acc.: 89.06%] [Generator loss: 1.457969]\n",
      "28 [Discriminator loss: 0.159514, acc.: 93.75%] [Generator loss: 0.245159]\n",
      "29 [Discriminator loss: 0.743400, acc.: 75.00%] [Generator loss: 1.448837]\n",
      "30 [Discriminator loss: 0.240796, acc.: 89.06%] [Generator loss: 1.978770]\n",
      "31 [Discriminator loss: 1.451136, acc.: 45.31%] [Generator loss: 7.133742]\n",
      "32 [Discriminator loss: 0.561319, acc.: 76.56%] [Generator loss: 7.491220]\n",
      "33 [Discriminator loss: 0.465659, acc.: 79.69%] [Generator loss: 4.051038]\n",
      "34 [Discriminator loss: 0.253316, acc.: 85.94%] [Generator loss: 3.852785]\n",
      "35 [Discriminator loss: 0.086227, acc.: 96.88%] [Generator loss: 1.053811]\n",
      "36 [Discriminator loss: 0.464946, acc.: 85.94%] [Generator loss: 1.183399]\n",
      "37 [Discriminator loss: 0.155964, acc.: 93.75%] [Generator loss: 3.365855]\n",
      "38 [Discriminator loss: 0.472215, acc.: 82.81%] [Generator loss: 5.929515]\n",
      "39 [Discriminator loss: 0.695237, acc.: 76.56%] [Generator loss: 2.991676]\n",
      "40 [Discriminator loss: 0.284165, acc.: 87.50%] [Generator loss: 4.048352]\n",
      "41 [Discriminator loss: 0.268555, acc.: 85.94%] [Generator loss: 7.212206]\n",
      "42 [Discriminator loss: 0.414830, acc.: 81.25%] [Generator loss: 5.472226]\n",
      "43 [Discriminator loss: 0.335543, acc.: 84.38%] [Generator loss: 8.216146]\n",
      "44 [Discriminator loss: 0.396277, acc.: 85.94%] [Generator loss: 6.496845]\n",
      "45 [Discriminator loss: 1.016909, acc.: 59.38%] [Generator loss: 9.636045]\n",
      "46 [Discriminator loss: 0.835787, acc.: 75.00%] [Generator loss: 4.923345]\n",
      "47 [Discriminator loss: 0.649035, acc.: 71.88%] [Generator loss: 3.077847]\n",
      "48 [Discriminator loss: 0.157056, acc.: 90.62%] [Generator loss: 1.157634]\n",
      "49 [Discriminator loss: 0.129821, acc.: 93.75%] [Generator loss: 0.319035]\n",
      "50 [Discriminator loss: 0.359326, acc.: 81.25%] [Generator loss: 0.794760]\n",
      "51 [Discriminator loss: 0.157554, acc.: 93.75%] [Generator loss: 2.152560]\n",
      "52 [Discriminator loss: 0.292123, acc.: 85.94%] [Generator loss: 2.928668]\n",
      "53 [Discriminator loss: 0.415528, acc.: 82.81%] [Generator loss: 3.415758]\n",
      "54 [Discriminator loss: 0.029664, acc.: 100.00%] [Generator loss: 4.246489]\n",
      "55 [Discriminator loss: 0.191991, acc.: 92.19%] [Generator loss: 3.635902]\n",
      "56 [Discriminator loss: 0.396416, acc.: 81.25%] [Generator loss: 6.134336]\n",
      "57 [Discriminator loss: 0.483733, acc.: 89.06%] [Generator loss: 4.112316]\n",
      "58 [Discriminator loss: 0.457052, acc.: 76.56%] [Generator loss: 6.270252]\n",
      "59 [Discriminator loss: 0.101237, acc.: 95.31%] [Generator loss: 6.184862]\n",
      "60 [Discriminator loss: 0.191987, acc.: 89.06%] [Generator loss: 3.199958]\n",
      "61 [Discriminator loss: 0.641435, acc.: 71.88%] [Generator loss: 9.439645]\n",
      "62 [Discriminator loss: 0.803900, acc.: 75.00%] [Generator loss: 4.208288]\n",
      "63 [Discriminator loss: 0.156042, acc.: 92.19%] [Generator loss: 1.735078]\n",
      "64 [Discriminator loss: 0.290904, acc.: 92.19%] [Generator loss: 0.565534]\n",
      "65 [Discriminator loss: 0.211046, acc.: 92.19%] [Generator loss: 1.146433]\n",
      "66 [Discriminator loss: 0.567114, acc.: 73.44%] [Generator loss: 4.428539]\n",
      "67 [Discriminator loss: 0.429855, acc.: 82.81%] [Generator loss: 3.969943]\n",
      "68 [Discriminator loss: 0.852036, acc.: 71.88%] [Generator loss: 7.439427]\n",
      "69 [Discriminator loss: 0.471522, acc.: 76.56%] [Generator loss: 2.847518]\n",
      "70 [Discriminator loss: 0.418009, acc.: 76.56%] [Generator loss: 3.794844]\n",
      "71 [Discriminator loss: 0.058291, acc.: 98.44%] [Generator loss: 3.889244]\n",
      "72 [Discriminator loss: 1.951585, acc.: 42.19%] [Generator loss: 12.116896]\n",
      "73 [Discriminator loss: 2.595653, acc.: 56.25%] [Generator loss: 4.390135]\n",
      "74 [Discriminator loss: 0.772107, acc.: 75.00%] [Generator loss: 5.956546]\n",
      "75 [Discriminator loss: 0.283484, acc.: 89.06%] [Generator loss: 5.501397]\n",
      "76 [Discriminator loss: 0.199311, acc.: 92.19%] [Generator loss: 5.283875]\n",
      "77 [Discriminator loss: 0.880861, acc.: 59.38%] [Generator loss: 7.085878]\n",
      "78 [Discriminator loss: 0.314199, acc.: 92.19%] [Generator loss: 6.364423]\n",
      "79 [Discriminator loss: 0.226366, acc.: 85.94%] [Generator loss: 3.998698]\n",
      "80 [Discriminator loss: 0.325315, acc.: 85.94%] [Generator loss: 4.124960]\n",
      "81 [Discriminator loss: 0.148479, acc.: 93.75%] [Generator loss: 3.185812]\n",
      "82 [Discriminator loss: 0.332056, acc.: 87.50%] [Generator loss: 3.380634]\n",
      "83 [Discriminator loss: 0.191868, acc.: 92.19%] [Generator loss: 3.509177]\n",
      "84 [Discriminator loss: 0.200756, acc.: 90.62%] [Generator loss: 6.440396]\n",
      "85 [Discriminator loss: 0.257284, acc.: 90.62%] [Generator loss: 4.347700]\n",
      "86 [Discriminator loss: 0.917092, acc.: 67.19%] [Generator loss: 8.150118]\n",
      "87 [Discriminator loss: 0.706396, acc.: 76.56%] [Generator loss: 4.369130]\n",
      "88 [Discriminator loss: 2.034447, acc.: 40.62%] [Generator loss: 11.404161]\n",
      "89 [Discriminator loss: 1.209391, acc.: 68.75%] [Generator loss: 9.105270]\n",
      "90 [Discriminator loss: 0.233681, acc.: 90.62%] [Generator loss: 6.168608]\n",
      "91 [Discriminator loss: 1.474882, acc.: 54.69%] [Generator loss: 11.442470]\n",
      "92 [Discriminator loss: 0.469804, acc.: 79.69%] [Generator loss: 7.107200]\n",
      "93 [Discriminator loss: 1.511446, acc.: 45.31%] [Generator loss: 9.123212]\n",
      "94 [Discriminator loss: 0.373892, acc.: 85.94%] [Generator loss: 7.324167]\n",
      "95 [Discriminator loss: 0.659600, acc.: 78.12%] [Generator loss: 6.272302]\n",
      "96 [Discriminator loss: 0.560375, acc.: 78.12%] [Generator loss: 3.323458]\n",
      "97 [Discriminator loss: 0.532447, acc.: 75.00%] [Generator loss: 6.990817]\n",
      "98 [Discriminator loss: 0.780304, acc.: 71.88%] [Generator loss: 4.657431]\n",
      "99 [Discriminator loss: 0.527049, acc.: 79.69%] [Generator loss: 7.376415]\n",
      "100 [Discriminator loss: 0.761259, acc.: 73.44%] [Generator loss: 5.992659]\n",
      "101 [Discriminator loss: 0.163681, acc.: 92.19%] [Generator loss: 6.070122]\n",
      "102 [Discriminator loss: 0.350395, acc.: 84.38%] [Generator loss: 6.661494]\n",
      "103 [Discriminator loss: 0.252738, acc.: 92.19%] [Generator loss: 6.230400]\n",
      "104 [Discriminator loss: 0.167974, acc.: 90.62%] [Generator loss: 3.533070]\n",
      "105 [Discriminator loss: 0.320847, acc.: 85.94%] [Generator loss: 5.408805]\n",
      "106 [Discriminator loss: 0.255204, acc.: 90.62%] [Generator loss: 2.628486]\n",
      "107 [Discriminator loss: 0.250502, acc.: 89.06%] [Generator loss: 1.721966]\n",
      "108 [Discriminator loss: 0.222467, acc.: 92.19%] [Generator loss: 2.334636]\n",
      "109 [Discriminator loss: 0.371572, acc.: 87.50%] [Generator loss: 3.710832]\n",
      "110 [Discriminator loss: 0.187319, acc.: 89.06%] [Generator loss: 3.156864]\n",
      "111 [Discriminator loss: 0.660881, acc.: 73.44%] [Generator loss: 6.997942]\n",
      "112 [Discriminator loss: 1.051870, acc.: 65.62%] [Generator loss: 6.117906]\n",
      "113 [Discriminator loss: 0.801602, acc.: 70.31%] [Generator loss: 2.729276]\n",
      "114 [Discriminator loss: 0.407058, acc.: 82.81%] [Generator loss: 6.514967]\n",
      "115 [Discriminator loss: 0.252389, acc.: 89.06%] [Generator loss: 4.687773]\n",
      "116 [Discriminator loss: 1.623213, acc.: 51.56%] [Generator loss: 11.325197]\n",
      "117 [Discriminator loss: 1.196880, acc.: 68.75%] [Generator loss: 5.690286]\n",
      "118 [Discriminator loss: 1.319685, acc.: 56.25%] [Generator loss: 10.159739]\n",
      "119 [Discriminator loss: 0.636155, acc.: 75.00%] [Generator loss: 5.217955]\n",
      "120 [Discriminator loss: 1.100045, acc.: 57.81%] [Generator loss: 6.671246]\n",
      "121 [Discriminator loss: 0.434863, acc.: 84.38%] [Generator loss: 3.657207]\n",
      "122 [Discriminator loss: 0.708536, acc.: 71.88%] [Generator loss: 4.419752]\n",
      "123 [Discriminator loss: 0.785993, acc.: 70.31%] [Generator loss: 3.524808]\n",
      "124 [Discriminator loss: 0.887183, acc.: 60.94%] [Generator loss: 5.526716]\n",
      "125 [Discriminator loss: 0.771631, acc.: 70.31%] [Generator loss: 2.942607]\n",
      "126 [Discriminator loss: 0.150674, acc.: 95.31%] [Generator loss: 3.915895]\n",
      "127 [Discriminator loss: 0.340667, acc.: 84.38%] [Generator loss: 5.324871]\n",
      "128 [Discriminator loss: 0.594434, acc.: 76.56%] [Generator loss: 3.923009]\n",
      "129 [Discriminator loss: 0.238295, acc.: 85.94%] [Generator loss: 6.017617]\n",
      "130 [Discriminator loss: 0.077779, acc.: 98.44%] [Generator loss: 4.696623]\n",
      "131 [Discriminator loss: 0.223352, acc.: 89.06%] [Generator loss: 2.948269]\n",
      "132 [Discriminator loss: 0.187172, acc.: 90.62%] [Generator loss: 2.824214]\n",
      "133 [Discriminator loss: 0.298588, acc.: 89.06%] [Generator loss: 2.873548]\n",
      "134 [Discriminator loss: 1.075385, acc.: 60.94%] [Generator loss: 9.162134]\n",
      "135 [Discriminator loss: 1.994121, acc.: 50.00%] [Generator loss: 3.855018]\n",
      "136 [Discriminator loss: 0.639547, acc.: 68.75%] [Generator loss: 9.529071]\n",
      "137 [Discriminator loss: 0.441312, acc.: 85.94%] [Generator loss: 8.430548]\n",
      "138 [Discriminator loss: 0.284940, acc.: 90.62%] [Generator loss: 3.104734]\n",
      "139 [Discriminator loss: 0.337421, acc.: 84.38%] [Generator loss: 7.986686]\n",
      "140 [Discriminator loss: 0.057582, acc.: 98.44%] [Generator loss: 9.470078]\n",
      "141 [Discriminator loss: 0.362733, acc.: 92.19%] [Generator loss: 4.245862]\n",
      "142 [Discriminator loss: 0.350270, acc.: 81.25%] [Generator loss: 7.200501]\n",
      "143 [Discriminator loss: 0.129764, acc.: 92.19%] [Generator loss: 6.275313]\n",
      "144 [Discriminator loss: 0.574913, acc.: 75.00%] [Generator loss: 4.294222]\n",
      "145 [Discriminator loss: 0.269257, acc.: 85.94%] [Generator loss: 1.940956]\n",
      "146 [Discriminator loss: 1.742874, acc.: 45.31%] [Generator loss: 14.226586]\n",
      "147 [Discriminator loss: 2.936309, acc.: 51.56%] [Generator loss: 5.432883]\n",
      "148 [Discriminator loss: 0.369605, acc.: 84.38%] [Generator loss: 3.074300]\n",
      "149 [Discriminator loss: 0.256225, acc.: 85.94%] [Generator loss: 4.171691]\n",
      "150 [Discriminator loss: 1.680464, acc.: 43.75%] [Generator loss: 4.380171]\n",
      "151 [Discriminator loss: 0.851963, acc.: 73.44%] [Generator loss: 0.545228]\n",
      "152 [Discriminator loss: 0.617668, acc.: 71.88%] [Generator loss: 6.154929]\n",
      "153 [Discriminator loss: 0.373363, acc.: 87.50%] [Generator loss: 4.661955]\n",
      "154 [Discriminator loss: 2.228684, acc.: 34.38%] [Generator loss: 12.099272]\n",
      "155 [Discriminator loss: 1.419844, acc.: 57.81%] [Generator loss: 6.996777]\n",
      "156 [Discriminator loss: 2.602202, acc.: 40.62%] [Generator loss: 7.312822]\n",
      "157 [Discriminator loss: 1.132810, acc.: 57.81%] [Generator loss: 5.476395]\n",
      "158 [Discriminator loss: 2.737556, acc.: 32.81%] [Generator loss: 9.798291]\n",
      "159 [Discriminator loss: 1.791996, acc.: 54.69%] [Generator loss: 6.147375]\n",
      "160 [Discriminator loss: 0.814346, acc.: 62.50%] [Generator loss: 7.738121]\n",
      "161 [Discriminator loss: 0.313500, acc.: 92.19%] [Generator loss: 6.807194]\n",
      "162 [Discriminator loss: 1.287329, acc.: 59.38%] [Generator loss: 5.231674]\n",
      "163 [Discriminator loss: 0.937743, acc.: 75.00%] [Generator loss: 1.958488]\n",
      "164 [Discriminator loss: 0.416474, acc.: 82.81%] [Generator loss: 0.911275]\n",
      "165 [Discriminator loss: 0.114088, acc.: 93.75%] [Generator loss: 0.753608]\n",
      "166 [Discriminator loss: 0.454212, acc.: 81.25%] [Generator loss: 1.585694]\n",
      "167 [Discriminator loss: 0.414232, acc.: 79.69%] [Generator loss: 5.055170]\n",
      "168 [Discriminator loss: 1.147407, acc.: 60.94%] [Generator loss: 3.474123]\n",
      "169 [Discriminator loss: 1.926464, acc.: 34.38%] [Generator loss: 9.010716]\n",
      "170 [Discriminator loss: 1.012401, acc.: 67.19%] [Generator loss: 3.918135]\n",
      "171 [Discriminator loss: 1.123427, acc.: 53.12%] [Generator loss: 7.529115]\n",
      "172 [Discriminator loss: 0.483263, acc.: 81.25%] [Generator loss: 6.739302]\n",
      "173 [Discriminator loss: 3.173806, acc.: 25.00%] [Generator loss: 5.664009]\n",
      "174 [Discriminator loss: 0.941707, acc.: 67.19%] [Generator loss: 5.901205]\n",
      "175 [Discriminator loss: 2.659217, acc.: 18.75%] [Generator loss: 4.291895]\n",
      "176 [Discriminator loss: 0.804377, acc.: 67.19%] [Generator loss: 3.203355]\n",
      "177 [Discriminator loss: 1.072133, acc.: 56.25%] [Generator loss: 3.201670]\n",
      "178 [Discriminator loss: 0.731790, acc.: 75.00%] [Generator loss: 2.878603]\n",
      "179 [Discriminator loss: 1.153219, acc.: 59.38%] [Generator loss: 2.834349]\n",
      "180 [Discriminator loss: 0.652176, acc.: 70.31%] [Generator loss: 2.691268]\n",
      "181 [Discriminator loss: 1.152045, acc.: 56.25%] [Generator loss: 2.450421]\n",
      "182 [Discriminator loss: 0.559808, acc.: 73.44%] [Generator loss: 3.182990]\n",
      "183 [Discriminator loss: 0.864791, acc.: 65.62%] [Generator loss: 2.742346]\n",
      "184 [Discriminator loss: 0.458036, acc.: 87.50%] [Generator loss: 2.652232]\n",
      "185 [Discriminator loss: 0.597538, acc.: 79.69%] [Generator loss: 2.379166]\n",
      "186 [Discriminator loss: 0.561522, acc.: 79.69%] [Generator loss: 1.927044]\n",
      "187 [Discriminator loss: 0.627934, acc.: 75.00%] [Generator loss: 3.639554]\n",
      "188 [Discriminator loss: 0.553315, acc.: 75.00%] [Generator loss: 3.489412]\n",
      "189 [Discriminator loss: 1.031606, acc.: 51.56%] [Generator loss: 4.445908]\n",
      "190 [Discriminator loss: 1.292016, acc.: 48.44%] [Generator loss: 4.498170]\n",
      "191 [Discriminator loss: 1.354522, acc.: 45.31%] [Generator loss: 4.761262]\n",
      "192 [Discriminator loss: 0.788776, acc.: 65.62%] [Generator loss: 3.305420]\n",
      "193 [Discriminator loss: 1.309278, acc.: 45.31%] [Generator loss: 3.046328]\n",
      "194 [Discriminator loss: 0.490230, acc.: 81.25%] [Generator loss: 3.749610]\n",
      "195 [Discriminator loss: 0.605655, acc.: 68.75%] [Generator loss: 3.035572]\n",
      "196 [Discriminator loss: 1.240579, acc.: 64.06%] [Generator loss: 2.892371]\n",
      "197 [Discriminator loss: 0.809385, acc.: 67.19%] [Generator loss: 2.850883]\n",
      "198 [Discriminator loss: 1.520470, acc.: 37.50%] [Generator loss: 3.943173]\n",
      "199 [Discriminator loss: 0.997387, acc.: 56.25%] [Generator loss: 2.859585]\n",
      "200 [Discriminator loss: 1.238987, acc.: 43.75%] [Generator loss: 3.644524]\n",
      "201 [Discriminator loss: 1.002834, acc.: 59.38%] [Generator loss: 4.558161]\n",
      "202 [Discriminator loss: 1.057119, acc.: 53.12%] [Generator loss: 3.650320]\n",
      "203 [Discriminator loss: 0.649637, acc.: 68.75%] [Generator loss: 2.393717]\n",
      "204 [Discriminator loss: 0.968895, acc.: 53.12%] [Generator loss: 2.273571]\n",
      "205 [Discriminator loss: 0.668938, acc.: 68.75%] [Generator loss: 1.577575]\n",
      "206 [Discriminator loss: 1.152655, acc.: 46.88%] [Generator loss: 2.954038]\n",
      "207 [Discriminator loss: 0.827670, acc.: 64.06%] [Generator loss: 2.514583]\n",
      "208 [Discriminator loss: 1.145023, acc.: 39.06%] [Generator loss: 2.468651]\n",
      "209 [Discriminator loss: 0.706072, acc.: 67.19%] [Generator loss: 2.319348]\n",
      "210 [Discriminator loss: 0.346992, acc.: 81.25%] [Generator loss: 1.982489]\n",
      "211 [Discriminator loss: 1.049440, acc.: 48.44%] [Generator loss: 2.210467]\n",
      "212 [Discriminator loss: 0.588262, acc.: 81.25%] [Generator loss: 2.283006]\n",
      "213 [Discriminator loss: 0.640721, acc.: 73.44%] [Generator loss: 3.041362]\n",
      "214 [Discriminator loss: 1.712657, acc.: 28.12%] [Generator loss: 3.294743]\n",
      "215 [Discriminator loss: 0.288949, acc.: 84.38%] [Generator loss: 3.434760]\n",
      "216 [Discriminator loss: 0.789637, acc.: 57.81%] [Generator loss: 2.732618]\n",
      "217 [Discriminator loss: 0.561550, acc.: 79.69%] [Generator loss: 2.377661]\n",
      "218 [Discriminator loss: 0.912583, acc.: 59.38%] [Generator loss: 2.572779]\n",
      "219 [Discriminator loss: 0.706944, acc.: 73.44%] [Generator loss: 2.119416]\n",
      "220 [Discriminator loss: 0.880635, acc.: 59.38%] [Generator loss: 2.729733]\n",
      "221 [Discriminator loss: 0.636740, acc.: 75.00%] [Generator loss: 1.443955]\n",
      "222 [Discriminator loss: 1.195013, acc.: 42.19%] [Generator loss: 1.641703]\n",
      "223 [Discriminator loss: 0.850520, acc.: 54.69%] [Generator loss: 3.721903]\n",
      "224 [Discriminator loss: 1.213562, acc.: 50.00%] [Generator loss: 2.799625]\n",
      "225 [Discriminator loss: 0.791206, acc.: 57.81%] [Generator loss: 3.623855]\n",
      "226 [Discriminator loss: 0.604472, acc.: 71.88%] [Generator loss: 4.703222]\n",
      "227 [Discriminator loss: 1.139201, acc.: 54.69%] [Generator loss: 2.730353]\n",
      "228 [Discriminator loss: 1.067705, acc.: 50.00%] [Generator loss: 2.637478]\n",
      "229 [Discriminator loss: 0.585197, acc.: 68.75%] [Generator loss: 3.498941]\n",
      "230 [Discriminator loss: 1.180393, acc.: 45.31%] [Generator loss: 3.120840]\n",
      "231 [Discriminator loss: 0.780724, acc.: 57.81%] [Generator loss: 2.161796]\n",
      "232 [Discriminator loss: 0.708727, acc.: 67.19%] [Generator loss: 2.394322]\n",
      "233 [Discriminator loss: 0.487903, acc.: 78.12%] [Generator loss: 1.571252]\n",
      "234 [Discriminator loss: 0.779003, acc.: 68.75%] [Generator loss: 2.681085]\n",
      "235 [Discriminator loss: 0.673968, acc.: 73.44%] [Generator loss: 2.637322]\n",
      "236 [Discriminator loss: 0.553496, acc.: 75.00%] [Generator loss: 2.150020]\n",
      "237 [Discriminator loss: 1.157963, acc.: 46.88%] [Generator loss: 3.584653]\n",
      "238 [Discriminator loss: 0.696920, acc.: 60.94%] [Generator loss: 3.138698]\n",
      "239 [Discriminator loss: 0.748729, acc.: 65.62%] [Generator loss: 2.284071]\n",
      "240 [Discriminator loss: 0.537475, acc.: 70.31%] [Generator loss: 2.486556]\n",
      "241 [Discriminator loss: 1.019456, acc.: 53.12%] [Generator loss: 1.866380]\n",
      "242 [Discriminator loss: 0.740178, acc.: 65.62%] [Generator loss: 2.446163]\n",
      "243 [Discriminator loss: 0.986414, acc.: 51.56%] [Generator loss: 2.512782]\n",
      "244 [Discriminator loss: 0.897459, acc.: 51.56%] [Generator loss: 3.091152]\n",
      "245 [Discriminator loss: 0.914850, acc.: 56.25%] [Generator loss: 3.273047]\n",
      "246 [Discriminator loss: 1.239136, acc.: 40.62%] [Generator loss: 2.533827]\n",
      "247 [Discriminator loss: 0.958768, acc.: 57.81%] [Generator loss: 2.426117]\n",
      "248 [Discriminator loss: 0.969699, acc.: 42.19%] [Generator loss: 3.044157]\n",
      "249 [Discriminator loss: 0.767231, acc.: 64.06%] [Generator loss: 2.937190]\n",
      "250 [Discriminator loss: 1.124479, acc.: 48.44%] [Generator loss: 1.863725]\n",
      "251 [Discriminator loss: 0.840738, acc.: 59.38%] [Generator loss: 2.671484]\n",
      "252 [Discriminator loss: 0.533541, acc.: 78.12%] [Generator loss: 2.586937]\n",
      "253 [Discriminator loss: 0.632813, acc.: 65.62%] [Generator loss: 2.607607]\n",
      "254 [Discriminator loss: 0.683110, acc.: 60.94%] [Generator loss: 2.409225]\n",
      "255 [Discriminator loss: 1.211112, acc.: 46.88%] [Generator loss: 2.345186]\n",
      "256 [Discriminator loss: 0.782560, acc.: 59.38%] [Generator loss: 1.534477]\n",
      "257 [Discriminator loss: 0.699371, acc.: 62.50%] [Generator loss: 2.344285]\n",
      "258 [Discriminator loss: 0.853106, acc.: 56.25%] [Generator loss: 1.942974]\n",
      "259 [Discriminator loss: 0.715302, acc.: 64.06%] [Generator loss: 1.964638]\n",
      "260 [Discriminator loss: 0.658093, acc.: 65.62%] [Generator loss: 2.526737]\n",
      "261 [Discriminator loss: 0.821773, acc.: 57.81%] [Generator loss: 1.799691]\n",
      "262 [Discriminator loss: 0.642141, acc.: 70.31%] [Generator loss: 1.546337]\n",
      "263 [Discriminator loss: 0.690991, acc.: 65.62%] [Generator loss: 2.547330]\n",
      "264 [Discriminator loss: 0.891525, acc.: 53.12%] [Generator loss: 2.457175]\n",
      "265 [Discriminator loss: 1.007817, acc.: 53.12%] [Generator loss: 2.914502]\n",
      "266 [Discriminator loss: 0.671538, acc.: 60.94%] [Generator loss: 3.236053]\n",
      "267 [Discriminator loss: 0.738102, acc.: 71.88%] [Generator loss: 2.844671]\n",
      "268 [Discriminator loss: 0.515008, acc.: 73.44%] [Generator loss: 2.417967]\n",
      "269 [Discriminator loss: 0.654213, acc.: 64.06%] [Generator loss: 2.282478]\n",
      "270 [Discriminator loss: 0.872338, acc.: 56.25%] [Generator loss: 2.142823]\n",
      "271 [Discriminator loss: 0.773325, acc.: 59.38%] [Generator loss: 2.211571]\n",
      "272 [Discriminator loss: 0.973188, acc.: 43.75%] [Generator loss: 1.253103]\n",
      "273 [Discriminator loss: 0.704686, acc.: 62.50%] [Generator loss: 2.179444]\n",
      "274 [Discriminator loss: 1.450749, acc.: 40.62%] [Generator loss: 2.303380]\n",
      "275 [Discriminator loss: 0.910294, acc.: 57.81%] [Generator loss: 2.645132]\n",
      "276 [Discriminator loss: 0.775055, acc.: 64.06%] [Generator loss: 2.005023]\n",
      "277 [Discriminator loss: 1.064954, acc.: 59.38%] [Generator loss: 3.329386]\n",
      "278 [Discriminator loss: 0.848144, acc.: 56.25%] [Generator loss: 2.158219]\n",
      "279 [Discriminator loss: 0.447634, acc.: 81.25%] [Generator loss: 2.132544]\n",
      "280 [Discriminator loss: 0.659573, acc.: 64.06%] [Generator loss: 2.316028]\n",
      "281 [Discriminator loss: 0.643565, acc.: 68.75%] [Generator loss: 2.472610]\n",
      "282 [Discriminator loss: 1.325207, acc.: 40.62%] [Generator loss: 2.623109]\n",
      "283 [Discriminator loss: 1.165765, acc.: 43.75%] [Generator loss: 2.473002]\n",
      "284 [Discriminator loss: 0.829933, acc.: 48.44%] [Generator loss: 2.412666]\n",
      "285 [Discriminator loss: 0.763531, acc.: 57.81%] [Generator loss: 2.153256]\n",
      "286 [Discriminator loss: 0.869532, acc.: 54.69%] [Generator loss: 3.024038]\n",
      "287 [Discriminator loss: 0.977111, acc.: 56.25%] [Generator loss: 2.528639]\n",
      "288 [Discriminator loss: 0.827031, acc.: 59.38%] [Generator loss: 1.302490]\n",
      "289 [Discriminator loss: 0.705299, acc.: 65.62%] [Generator loss: 2.337378]\n",
      "290 [Discriminator loss: 0.631478, acc.: 59.38%] [Generator loss: 2.601798]\n",
      "291 [Discriminator loss: 0.854911, acc.: 56.25%] [Generator loss: 1.979345]\n",
      "292 [Discriminator loss: 0.806537, acc.: 54.69%] [Generator loss: 2.407057]\n",
      "293 [Discriminator loss: 1.009383, acc.: 51.56%] [Generator loss: 2.612082]\n",
      "294 [Discriminator loss: 0.791115, acc.: 64.06%] [Generator loss: 2.301956]\n",
      "295 [Discriminator loss: 1.042369, acc.: 42.19%] [Generator loss: 2.836619]\n",
      "296 [Discriminator loss: 0.544508, acc.: 71.88%] [Generator loss: 1.907341]\n",
      "297 [Discriminator loss: 0.769245, acc.: 57.81%] [Generator loss: 1.827194]\n",
      "298 [Discriminator loss: 0.466898, acc.: 79.69%] [Generator loss: 2.237136]\n",
      "299 [Discriminator loss: 1.247047, acc.: 39.06%] [Generator loss: 1.679275]\n",
      "300 [Discriminator loss: 0.642494, acc.: 60.94%] [Generator loss: 1.836455]\n",
      "301 [Discriminator loss: 0.908542, acc.: 42.19%] [Generator loss: 2.541184]\n",
      "302 [Discriminator loss: 0.750364, acc.: 60.94%] [Generator loss: 2.222867]\n",
      "303 [Discriminator loss: 1.115038, acc.: 37.50%] [Generator loss: 3.034708]\n",
      "304 [Discriminator loss: 0.760218, acc.: 62.50%] [Generator loss: 2.430902]\n",
      "305 [Discriminator loss: 0.862615, acc.: 54.69%] [Generator loss: 2.014985]\n",
      "306 [Discriminator loss: 0.624599, acc.: 70.31%] [Generator loss: 1.665287]\n",
      "307 [Discriminator loss: 0.859981, acc.: 65.62%] [Generator loss: 1.748111]\n",
      "308 [Discriminator loss: 0.652104, acc.: 62.50%] [Generator loss: 2.034834]\n",
      "309 [Discriminator loss: 0.743250, acc.: 67.19%] [Generator loss: 1.436725]\n",
      "310 [Discriminator loss: 0.492526, acc.: 76.56%] [Generator loss: 2.333061]\n",
      "311 [Discriminator loss: 0.659924, acc.: 70.31%] [Generator loss: 2.045233]\n",
      "312 [Discriminator loss: 0.581643, acc.: 67.19%] [Generator loss: 2.133524]\n",
      "313 [Discriminator loss: 0.487348, acc.: 76.56%] [Generator loss: 2.655800]\n",
      "314 [Discriminator loss: 0.492993, acc.: 76.56%] [Generator loss: 2.161049]\n",
      "315 [Discriminator loss: 0.829085, acc.: 56.25%] [Generator loss: 2.605739]\n",
      "316 [Discriminator loss: 0.628926, acc.: 65.62%] [Generator loss: 3.176712]\n",
      "317 [Discriminator loss: 0.462873, acc.: 79.69%] [Generator loss: 3.471349]\n",
      "318 [Discriminator loss: 1.248724, acc.: 42.19%] [Generator loss: 1.583794]\n",
      "319 [Discriminator loss: 0.788209, acc.: 59.38%] [Generator loss: 2.879186]\n",
      "320 [Discriminator loss: 0.959802, acc.: 48.44%] [Generator loss: 2.168733]\n",
      "321 [Discriminator loss: 0.690981, acc.: 62.50%] [Generator loss: 2.505436]\n",
      "322 [Discriminator loss: 1.044742, acc.: 50.00%] [Generator loss: 1.877169]\n",
      "323 [Discriminator loss: 0.854834, acc.: 57.81%] [Generator loss: 2.425117]\n",
      "324 [Discriminator loss: 0.708948, acc.: 65.62%] [Generator loss: 3.059057]\n",
      "325 [Discriminator loss: 0.772911, acc.: 65.62%] [Generator loss: 2.647985]\n",
      "326 [Discriminator loss: 0.716329, acc.: 65.62%] [Generator loss: 2.730886]\n",
      "327 [Discriminator loss: 0.939891, acc.: 56.25%] [Generator loss: 2.346556]\n",
      "328 [Discriminator loss: 0.606585, acc.: 70.31%] [Generator loss: 2.572513]\n",
      "329 [Discriminator loss: 0.574163, acc.: 70.31%] [Generator loss: 2.612984]\n",
      "330 [Discriminator loss: 0.510432, acc.: 76.56%] [Generator loss: 2.357436]\n",
      "331 [Discriminator loss: 0.688001, acc.: 64.06%] [Generator loss: 2.207684]\n",
      "332 [Discriminator loss: 0.618307, acc.: 73.44%] [Generator loss: 2.535121]\n",
      "333 [Discriminator loss: 0.783324, acc.: 62.50%] [Generator loss: 2.591426]\n",
      "334 [Discriminator loss: 0.912669, acc.: 53.12%] [Generator loss: 3.527909]\n",
      "335 [Discriminator loss: 0.785969, acc.: 62.50%] [Generator loss: 2.672644]\n",
      "336 [Discriminator loss: 0.603152, acc.: 64.06%] [Generator loss: 2.611693]\n",
      "337 [Discriminator loss: 0.740238, acc.: 62.50%] [Generator loss: 3.248125]\n",
      "338 [Discriminator loss: 0.884549, acc.: 64.06%] [Generator loss: 2.958379]\n",
      "339 [Discriminator loss: 0.801305, acc.: 62.50%] [Generator loss: 3.698817]\n",
      "340 [Discriminator loss: 0.477829, acc.: 76.56%] [Generator loss: 2.757730]\n",
      "341 [Discriminator loss: 0.383469, acc.: 82.81%] [Generator loss: 2.938582]\n",
      "342 [Discriminator loss: 0.345084, acc.: 81.25%] [Generator loss: 3.392086]\n",
      "343 [Discriminator loss: 0.813582, acc.: 56.25%] [Generator loss: 3.945193]\n",
      "344 [Discriminator loss: 0.463552, acc.: 84.38%] [Generator loss: 3.417084]\n",
      "345 [Discriminator loss: 0.735799, acc.: 73.44%] [Generator loss: 3.332042]\n",
      "346 [Discriminator loss: 0.896412, acc.: 60.94%] [Generator loss: 2.368506]\n",
      "347 [Discriminator loss: 0.621464, acc.: 76.56%] [Generator loss: 3.041681]\n",
      "348 [Discriminator loss: 0.764264, acc.: 64.06%] [Generator loss: 3.201337]\n",
      "349 [Discriminator loss: 0.392936, acc.: 81.25%] [Generator loss: 3.288474]\n",
      "350 [Discriminator loss: 0.510250, acc.: 84.38%] [Generator loss: 2.341965]\n",
      "351 [Discriminator loss: 0.529097, acc.: 73.44%] [Generator loss: 2.492236]\n",
      "352 [Discriminator loss: 0.474788, acc.: 79.69%] [Generator loss: 2.073752]\n",
      "353 [Discriminator loss: 0.654591, acc.: 62.50%] [Generator loss: 2.106750]\n",
      "354 [Discriminator loss: 0.857805, acc.: 46.88%] [Generator loss: 2.148163]\n",
      "355 [Discriminator loss: 0.629897, acc.: 71.88%] [Generator loss: 3.573968]\n",
      "356 [Discriminator loss: 0.831306, acc.: 59.38%] [Generator loss: 2.984534]\n",
      "357 [Discriminator loss: 1.051938, acc.: 46.88%] [Generator loss: 2.560337]\n",
      "358 [Discriminator loss: 0.728865, acc.: 67.19%] [Generator loss: 2.636297]\n",
      "359 [Discriminator loss: 0.977531, acc.: 59.38%] [Generator loss: 2.088367]\n",
      "360 [Discriminator loss: 0.778714, acc.: 57.81%] [Generator loss: 2.565198]\n",
      "361 [Discriminator loss: 0.926816, acc.: 53.12%] [Generator loss: 2.239823]\n",
      "362 [Discriminator loss: 0.743133, acc.: 56.25%] [Generator loss: 2.545635]\n",
      "363 [Discriminator loss: 1.042649, acc.: 51.56%] [Generator loss: 1.745065]\n",
      "364 [Discriminator loss: 0.676719, acc.: 59.38%] [Generator loss: 2.340747]\n",
      "365 [Discriminator loss: 0.862061, acc.: 65.62%] [Generator loss: 2.501043]\n",
      "366 [Discriminator loss: 0.862566, acc.: 54.69%] [Generator loss: 2.585919]\n",
      "367 [Discriminator loss: 0.766294, acc.: 64.06%] [Generator loss: 2.449272]\n",
      "368 [Discriminator loss: 0.686944, acc.: 73.44%] [Generator loss: 2.351687]\n",
      "369 [Discriminator loss: 0.775017, acc.: 59.38%] [Generator loss: 3.638131]\n",
      "370 [Discriminator loss: 0.654742, acc.: 71.88%] [Generator loss: 2.904956]\n",
      "371 [Discriminator loss: 0.866058, acc.: 57.81%] [Generator loss: 3.029234]\n",
      "372 [Discriminator loss: 0.852808, acc.: 64.06%] [Generator loss: 2.577990]\n",
      "373 [Discriminator loss: 0.889370, acc.: 54.69%] [Generator loss: 2.146507]\n",
      "374 [Discriminator loss: 0.432450, acc.: 78.12%] [Generator loss: 2.539125]\n",
      "375 [Discriminator loss: 0.293749, acc.: 87.50%] [Generator loss: 2.198746]\n",
      "376 [Discriminator loss: 0.649642, acc.: 68.75%] [Generator loss: 2.167304]\n",
      "377 [Discriminator loss: 0.640474, acc.: 78.12%] [Generator loss: 2.097545]\n",
      "378 [Discriminator loss: 0.601116, acc.: 67.19%] [Generator loss: 2.354731]\n",
      "379 [Discriminator loss: 0.591856, acc.: 71.88%] [Generator loss: 2.115568]\n",
      "380 [Discriminator loss: 0.586231, acc.: 73.44%] [Generator loss: 2.536002]\n",
      "381 [Discriminator loss: 0.808397, acc.: 64.06%] [Generator loss: 2.688205]\n",
      "382 [Discriminator loss: 0.917042, acc.: 59.38%] [Generator loss: 2.273318]\n",
      "383 [Discriminator loss: 1.008380, acc.: 53.12%] [Generator loss: 2.891669]\n",
      "384 [Discriminator loss: 0.739993, acc.: 60.94%] [Generator loss: 2.390063]\n",
      "385 [Discriminator loss: 0.419108, acc.: 82.81%] [Generator loss: 2.813090]\n",
      "386 [Discriminator loss: 0.465424, acc.: 82.81%] [Generator loss: 2.491894]\n",
      "387 [Discriminator loss: 0.774556, acc.: 59.38%] [Generator loss: 1.881927]\n",
      "388 [Discriminator loss: 0.827361, acc.: 54.69%] [Generator loss: 2.856839]\n",
      "389 [Discriminator loss: 0.994208, acc.: 48.44%] [Generator loss: 1.954139]\n",
      "390 [Discriminator loss: 0.665398, acc.: 65.62%] [Generator loss: 2.922951]\n",
      "391 [Discriminator loss: 1.085793, acc.: 40.62%] [Generator loss: 1.685458]\n",
      "392 [Discriminator loss: 1.257623, acc.: 48.44%] [Generator loss: 2.654009]\n",
      "393 [Discriminator loss: 0.897606, acc.: 54.69%] [Generator loss: 2.599005]\n",
      "394 [Discriminator loss: 0.787539, acc.: 57.81%] [Generator loss: 2.652497]\n",
      "395 [Discriminator loss: 0.582298, acc.: 68.75%] [Generator loss: 3.444361]\n",
      "396 [Discriminator loss: 0.815569, acc.: 59.38%] [Generator loss: 2.439875]\n",
      "397 [Discriminator loss: 0.962963, acc.: 54.69%] [Generator loss: 2.515735]\n",
      "398 [Discriminator loss: 0.891373, acc.: 57.81%] [Generator loss: 2.059787]\n",
      "399 [Discriminator loss: 0.695266, acc.: 67.19%] [Generator loss: 2.940451]\n",
      "400 [Discriminator loss: 0.923260, acc.: 56.25%] [Generator loss: 3.211318]\n",
      "401 [Discriminator loss: 0.679722, acc.: 70.31%] [Generator loss: 3.279847]\n",
      "402 [Discriminator loss: 0.593979, acc.: 71.88%] [Generator loss: 2.837189]\n",
      "403 [Discriminator loss: 0.794335, acc.: 54.69%] [Generator loss: 2.055729]\n",
      "404 [Discriminator loss: 0.926780, acc.: 56.25%] [Generator loss: 2.769205]\n",
      "405 [Discriminator loss: 0.796113, acc.: 59.38%] [Generator loss: 2.455655]\n",
      "406 [Discriminator loss: 1.253769, acc.: 40.62%] [Generator loss: 3.114166]\n",
      "407 [Discriminator loss: 0.745009, acc.: 65.62%] [Generator loss: 3.047020]\n",
      "408 [Discriminator loss: 0.810040, acc.: 62.50%] [Generator loss: 2.714177]\n",
      "409 [Discriminator loss: 0.685367, acc.: 65.62%] [Generator loss: 2.546146]\n",
      "410 [Discriminator loss: 0.740912, acc.: 64.06%] [Generator loss: 3.448224]\n",
      "411 [Discriminator loss: 1.035266, acc.: 60.94%] [Generator loss: 2.534968]\n",
      "412 [Discriminator loss: 0.984251, acc.: 51.56%] [Generator loss: 1.436951]\n",
      "413 [Discriminator loss: 0.867768, acc.: 70.31%] [Generator loss: 2.697410]\n",
      "414 [Discriminator loss: 0.502898, acc.: 78.12%] [Generator loss: 3.243681]\n",
      "415 [Discriminator loss: 0.579271, acc.: 75.00%] [Generator loss: 2.703519]\n",
      "416 [Discriminator loss: 0.990556, acc.: 50.00%] [Generator loss: 2.308413]\n",
      "417 [Discriminator loss: 0.287814, acc.: 89.06%] [Generator loss: 2.718921]\n",
      "418 [Discriminator loss: 0.436481, acc.: 82.81%] [Generator loss: 2.421734]\n",
      "419 [Discriminator loss: 0.611485, acc.: 65.62%] [Generator loss: 2.366014]\n",
      "420 [Discriminator loss: 0.369770, acc.: 81.25%] [Generator loss: 2.786881]\n",
      "421 [Discriminator loss: 0.437039, acc.: 79.69%] [Generator loss: 2.753070]\n",
      "422 [Discriminator loss: 0.699535, acc.: 67.19%] [Generator loss: 2.206802]\n",
      "423 [Discriminator loss: 0.680714, acc.: 65.62%] [Generator loss: 2.300941]\n",
      "424 [Discriminator loss: 0.611319, acc.: 71.88%] [Generator loss: 2.119116]\n",
      "425 [Discriminator loss: 0.599152, acc.: 67.19%] [Generator loss: 1.986053]\n",
      "426 [Discriminator loss: 0.595787, acc.: 68.75%] [Generator loss: 2.306178]\n",
      "427 [Discriminator loss: 0.595664, acc.: 70.31%] [Generator loss: 2.337019]\n",
      "428 [Discriminator loss: 0.744347, acc.: 62.50%] [Generator loss: 1.771846]\n",
      "429 [Discriminator loss: 0.824644, acc.: 60.94%] [Generator loss: 2.541600]\n",
      "430 [Discriminator loss: 0.728643, acc.: 57.81%] [Generator loss: 3.048393]\n",
      "431 [Discriminator loss: 0.675008, acc.: 70.31%] [Generator loss: 2.400081]\n",
      "432 [Discriminator loss: 0.577604, acc.: 68.75%] [Generator loss: 3.602360]\n",
      "433 [Discriminator loss: 0.960885, acc.: 53.12%] [Generator loss: 3.943486]\n",
      "434 [Discriminator loss: 0.845349, acc.: 59.38%] [Generator loss: 2.535508]\n",
      "435 [Discriminator loss: 0.566251, acc.: 70.31%] [Generator loss: 2.643982]\n",
      "436 [Discriminator loss: 0.817552, acc.: 56.25%] [Generator loss: 2.675002]\n",
      "437 [Discriminator loss: 1.054130, acc.: 53.12%] [Generator loss: 2.815422]\n",
      "438 [Discriminator loss: 0.525855, acc.: 78.12%] [Generator loss: 2.652645]\n",
      "439 [Discriminator loss: 1.045528, acc.: 51.56%] [Generator loss: 2.124387]\n",
      "440 [Discriminator loss: 0.792142, acc.: 59.38%] [Generator loss: 2.637553]\n",
      "441 [Discriminator loss: 0.808136, acc.: 62.50%] [Generator loss: 2.984566]\n",
      "442 [Discriminator loss: 0.709962, acc.: 64.06%] [Generator loss: 2.993175]\n",
      "443 [Discriminator loss: 1.082992, acc.: 48.44%] [Generator loss: 2.268646]\n",
      "444 [Discriminator loss: 0.657670, acc.: 73.44%] [Generator loss: 3.717951]\n",
      "445 [Discriminator loss: 0.939714, acc.: 50.00%] [Generator loss: 2.654050]\n",
      "446 [Discriminator loss: 0.916339, acc.: 56.25%] [Generator loss: 2.712554]\n",
      "447 [Discriminator loss: 0.752208, acc.: 59.38%] [Generator loss: 2.851945]\n",
      "448 [Discriminator loss: 0.422949, acc.: 75.00%] [Generator loss: 2.259379]\n",
      "449 [Discriminator loss: 0.772254, acc.: 59.38%] [Generator loss: 2.643328]\n",
      "450 [Discriminator loss: 0.566424, acc.: 75.00%] [Generator loss: 2.712646]\n",
      "451 [Discriminator loss: 0.990918, acc.: 45.31%] [Generator loss: 1.707728]\n",
      "452 [Discriminator loss: 0.675151, acc.: 65.62%] [Generator loss: 2.423099]\n",
      "453 [Discriminator loss: 1.018803, acc.: 37.50%] [Generator loss: 1.993673]\n",
      "454 [Discriminator loss: 0.887247, acc.: 54.69%] [Generator loss: 2.069408]\n",
      "455 [Discriminator loss: 0.761248, acc.: 64.06%] [Generator loss: 2.536834]\n",
      "456 [Discriminator loss: 0.573014, acc.: 73.44%] [Generator loss: 2.320472]\n",
      "457 [Discriminator loss: 0.801279, acc.: 53.12%] [Generator loss: 2.094668]\n",
      "458 [Discriminator loss: 0.411096, acc.: 85.94%] [Generator loss: 2.164353]\n",
      "459 [Discriminator loss: 0.577099, acc.: 70.31%] [Generator loss: 2.904625]\n",
      "460 [Discriminator loss: 0.384728, acc.: 84.38%] [Generator loss: 3.149360]\n",
      "461 [Discriminator loss: 0.677255, acc.: 70.31%] [Generator loss: 2.117003]\n",
      "462 [Discriminator loss: 0.633362, acc.: 73.44%] [Generator loss: 2.674327]\n",
      "463 [Discriminator loss: 0.594829, acc.: 75.00%] [Generator loss: 2.195970]\n",
      "464 [Discriminator loss: 0.908556, acc.: 57.81%] [Generator loss: 3.108440]\n",
      "465 [Discriminator loss: 0.911150, acc.: 54.69%] [Generator loss: 2.633501]\n",
      "466 [Discriminator loss: 1.070483, acc.: 46.88%] [Generator loss: 2.674911]\n",
      "467 [Discriminator loss: 0.877198, acc.: 48.44%] [Generator loss: 2.749945]\n",
      "468 [Discriminator loss: 1.254509, acc.: 37.50%] [Generator loss: 2.057593]\n",
      "469 [Discriminator loss: 0.728012, acc.: 59.38%] [Generator loss: 2.918767]\n",
      "470 [Discriminator loss: 0.962719, acc.: 57.81%] [Generator loss: 2.125622]\n",
      "471 [Discriminator loss: 0.876849, acc.: 54.69%] [Generator loss: 1.850816]\n",
      "472 [Discriminator loss: 0.637316, acc.: 70.31%] [Generator loss: 2.115426]\n",
      "473 [Discriminator loss: 1.185397, acc.: 39.06%] [Generator loss: 2.170853]\n",
      "474 [Discriminator loss: 0.658528, acc.: 57.81%] [Generator loss: 2.581454]\n",
      "475 [Discriminator loss: 0.537457, acc.: 71.88%] [Generator loss: 2.402668]\n",
      "476 [Discriminator loss: 0.973025, acc.: 56.25%] [Generator loss: 3.060808]\n",
      "477 [Discriminator loss: 0.752734, acc.: 60.94%] [Generator loss: 2.188455]\n",
      "478 [Discriminator loss: 0.553409, acc.: 71.88%] [Generator loss: 2.484804]\n",
      "479 [Discriminator loss: 0.813995, acc.: 59.38%] [Generator loss: 2.008235]\n",
      "480 [Discriminator loss: 0.485491, acc.: 76.56%] [Generator loss: 2.704789]\n",
      "481 [Discriminator loss: 0.678234, acc.: 68.75%] [Generator loss: 2.719291]\n",
      "482 [Discriminator loss: 0.772382, acc.: 54.69%] [Generator loss: 2.702098]\n",
      "483 [Discriminator loss: 0.554773, acc.: 71.88%] [Generator loss: 2.005681]\n",
      "484 [Discriminator loss: 0.795573, acc.: 60.94%] [Generator loss: 2.423741]\n",
      "485 [Discriminator loss: 0.639401, acc.: 70.31%] [Generator loss: 2.481272]\n",
      "486 [Discriminator loss: 0.406195, acc.: 78.12%] [Generator loss: 2.133240]\n",
      "487 [Discriminator loss: 0.729934, acc.: 59.38%] [Generator loss: 1.961360]\n",
      "488 [Discriminator loss: 0.792057, acc.: 56.25%] [Generator loss: 2.297688]\n",
      "489 [Discriminator loss: 0.824539, acc.: 68.75%] [Generator loss: 2.788254]\n",
      "490 [Discriminator loss: 1.139377, acc.: 48.44%] [Generator loss: 2.332917]\n",
      "491 [Discriminator loss: 1.122804, acc.: 46.88%] [Generator loss: 2.675729]\n",
      "492 [Discriminator loss: 0.720995, acc.: 67.19%] [Generator loss: 2.491264]\n",
      "493 [Discriminator loss: 0.566436, acc.: 71.88%] [Generator loss: 2.715638]\n",
      "494 [Discriminator loss: 0.509065, acc.: 81.25%] [Generator loss: 2.712491]\n",
      "495 [Discriminator loss: 0.503397, acc.: 73.44%] [Generator loss: 2.405328]\n",
      "496 [Discriminator loss: 0.698986, acc.: 70.31%] [Generator loss: 2.136810]\n",
      "497 [Discriminator loss: 1.128103, acc.: 42.19%] [Generator loss: 2.004926]\n",
      "498 [Discriminator loss: 0.598759, acc.: 65.62%] [Generator loss: 3.127535]\n",
      "499 [Discriminator loss: 1.092540, acc.: 51.56%] [Generator loss: 2.153018]\n",
      "500 [Discriminator loss: 0.724647, acc.: 67.19%] [Generator loss: 2.283949]\n",
      "501 [Discriminator loss: 0.820657, acc.: 53.12%] [Generator loss: 1.927252]\n",
      "502 [Discriminator loss: 0.554404, acc.: 70.31%] [Generator loss: 2.538016]\n",
      "503 [Discriminator loss: 0.860240, acc.: 56.25%] [Generator loss: 3.047541]\n",
      "504 [Discriminator loss: 0.849929, acc.: 54.69%] [Generator loss: 3.791524]\n",
      "505 [Discriminator loss: 0.339890, acc.: 85.94%] [Generator loss: 3.328232]\n",
      "506 [Discriminator loss: 0.628546, acc.: 70.31%] [Generator loss: 2.621117]\n",
      "507 [Discriminator loss: 0.694959, acc.: 68.75%] [Generator loss: 3.092025]\n",
      "508 [Discriminator loss: 0.539577, acc.: 73.44%] [Generator loss: 2.480965]\n",
      "509 [Discriminator loss: 0.971137, acc.: 45.31%] [Generator loss: 1.991570]\n",
      "510 [Discriminator loss: 0.947566, acc.: 50.00%] [Generator loss: 1.999101]\n",
      "511 [Discriminator loss: 0.504700, acc.: 75.00%] [Generator loss: 2.026779]\n",
      "512 [Discriminator loss: 0.806044, acc.: 59.38%] [Generator loss: 2.180863]\n",
      "513 [Discriminator loss: 0.512851, acc.: 71.88%] [Generator loss: 2.381912]\n",
      "514 [Discriminator loss: 0.801269, acc.: 59.38%] [Generator loss: 2.032337]\n",
      "515 [Discriminator loss: 0.691264, acc.: 64.06%] [Generator loss: 2.570881]\n",
      "516 [Discriminator loss: 0.486746, acc.: 79.69%] [Generator loss: 3.190004]\n",
      "517 [Discriminator loss: 1.117036, acc.: 53.12%] [Generator loss: 3.438911]\n",
      "518 [Discriminator loss: 0.986365, acc.: 54.69%] [Generator loss: 2.292686]\n",
      "519 [Discriminator loss: 0.632194, acc.: 64.06%] [Generator loss: 2.507387]\n",
      "520 [Discriminator loss: 0.635078, acc.: 59.38%] [Generator loss: 2.738052]\n",
      "521 [Discriminator loss: 0.409753, acc.: 82.81%] [Generator loss: 2.287990]\n",
      "522 [Discriminator loss: 0.587763, acc.: 70.31%] [Generator loss: 3.369655]\n",
      "523 [Discriminator loss: 0.931412, acc.: 50.00%] [Generator loss: 2.935265]\n",
      "524 [Discriminator loss: 0.848769, acc.: 57.81%] [Generator loss: 3.070186]\n",
      "525 [Discriminator loss: 0.644160, acc.: 75.00%] [Generator loss: 3.702461]\n",
      "526 [Discriminator loss: 1.037879, acc.: 53.12%] [Generator loss: 1.787042]\n",
      "527 [Discriminator loss: 1.162118, acc.: 43.75%] [Generator loss: 2.928686]\n",
      "528 [Discriminator loss: 0.583366, acc.: 75.00%] [Generator loss: 3.021815]\n",
      "529 [Discriminator loss: 0.672328, acc.: 64.06%] [Generator loss: 1.816164]\n",
      "530 [Discriminator loss: 0.946404, acc.: 43.75%] [Generator loss: 2.460812]\n",
      "531 [Discriminator loss: 0.621016, acc.: 68.75%] [Generator loss: 2.431970]\n",
      "532 [Discriminator loss: 0.711511, acc.: 62.50%] [Generator loss: 2.172281]\n",
      "533 [Discriminator loss: 0.953684, acc.: 51.56%] [Generator loss: 2.282529]\n",
      "534 [Discriminator loss: 0.590812, acc.: 75.00%] [Generator loss: 1.987433]\n",
      "535 [Discriminator loss: 0.750546, acc.: 68.75%] [Generator loss: 2.110830]\n",
      "536 [Discriminator loss: 0.533502, acc.: 75.00%] [Generator loss: 2.145369]\n",
      "537 [Discriminator loss: 0.704699, acc.: 59.38%] [Generator loss: 2.044766]\n",
      "538 [Discriminator loss: 0.458678, acc.: 76.56%] [Generator loss: 2.469143]\n",
      "539 [Discriminator loss: 0.635201, acc.: 67.19%] [Generator loss: 2.203942]\n",
      "540 [Discriminator loss: 0.770178, acc.: 54.69%] [Generator loss: 2.622648]\n",
      "541 [Discriminator loss: 0.382953, acc.: 84.38%] [Generator loss: 3.292721]\n",
      "542 [Discriminator loss: 0.336381, acc.: 87.50%] [Generator loss: 2.239113]\n",
      "543 [Discriminator loss: 0.237960, acc.: 92.19%] [Generator loss: 2.461279]\n",
      "544 [Discriminator loss: 0.212235, acc.: 93.75%] [Generator loss: 3.438043]\n",
      "545 [Discriminator loss: 0.258902, acc.: 92.19%] [Generator loss: 3.643012]\n",
      "546 [Discriminator loss: 0.339617, acc.: 79.69%] [Generator loss: 3.402773]\n",
      "547 [Discriminator loss: 0.493965, acc.: 75.00%] [Generator loss: 3.227726]\n",
      "548 [Discriminator loss: 0.500861, acc.: 70.31%] [Generator loss: 2.567012]\n",
      "549 [Discriminator loss: 0.673607, acc.: 70.31%] [Generator loss: 1.934258]\n",
      "550 [Discriminator loss: 0.990012, acc.: 43.75%] [Generator loss: 2.374595]\n",
      "551 [Discriminator loss: 0.916026, acc.: 51.56%] [Generator loss: 2.398828]\n",
      "552 [Discriminator loss: 0.776320, acc.: 60.94%] [Generator loss: 2.738052]\n",
      "553 [Discriminator loss: 0.549813, acc.: 81.25%] [Generator loss: 2.875672]\n",
      "554 [Discriminator loss: 0.757959, acc.: 64.06%] [Generator loss: 3.492049]\n",
      "555 [Discriminator loss: 0.627950, acc.: 68.75%] [Generator loss: 3.298659]\n",
      "556 [Discriminator loss: 0.709915, acc.: 62.50%] [Generator loss: 2.841381]\n",
      "557 [Discriminator loss: 0.699919, acc.: 75.00%] [Generator loss: 2.301185]\n",
      "558 [Discriminator loss: 0.629960, acc.: 73.44%] [Generator loss: 2.409506]\n",
      "559 [Discriminator loss: 0.526221, acc.: 68.75%] [Generator loss: 2.043868]\n",
      "560 [Discriminator loss: 0.491799, acc.: 78.12%] [Generator loss: 2.370489]\n",
      "561 [Discriminator loss: 0.490806, acc.: 78.12%] [Generator loss: 2.073336]\n",
      "562 [Discriminator loss: 0.783342, acc.: 60.94%] [Generator loss: 2.230250]\n",
      "563 [Discriminator loss: 0.803095, acc.: 62.50%] [Generator loss: 3.075666]\n",
      "564 [Discriminator loss: 0.498441, acc.: 76.56%] [Generator loss: 2.542828]\n",
      "565 [Discriminator loss: 0.743433, acc.: 62.50%] [Generator loss: 1.909633]\n",
      "566 [Discriminator loss: 0.444510, acc.: 79.69%] [Generator loss: 1.688352]\n",
      "567 [Discriminator loss: 0.691227, acc.: 60.94%] [Generator loss: 2.232309]\n",
      "568 [Discriminator loss: 0.407530, acc.: 78.12%] [Generator loss: 2.700820]\n",
      "569 [Discriminator loss: 0.431521, acc.: 81.25%] [Generator loss: 2.086392]\n",
      "570 [Discriminator loss: 0.412603, acc.: 76.56%] [Generator loss: 2.660117]\n",
      "571 [Discriminator loss: 1.012525, acc.: 48.44%] [Generator loss: 3.278947]\n",
      "572 [Discriminator loss: 0.630161, acc.: 75.00%] [Generator loss: 2.575501]\n",
      "573 [Discriminator loss: 0.913980, acc.: 54.69%] [Generator loss: 2.516741]\n",
      "574 [Discriminator loss: 0.337472, acc.: 85.94%] [Generator loss: 2.502317]\n",
      "575 [Discriminator loss: 0.651520, acc.: 60.94%] [Generator loss: 2.239040]\n",
      "576 [Discriminator loss: 0.543642, acc.: 73.44%] [Generator loss: 3.096185]\n",
      "577 [Discriminator loss: 0.670453, acc.: 71.88%] [Generator loss: 1.988752]\n",
      "578 [Discriminator loss: 0.896276, acc.: 60.94%] [Generator loss: 3.410270]\n",
      "579 [Discriminator loss: 0.653304, acc.: 71.88%] [Generator loss: 2.468663]\n",
      "580 [Discriminator loss: 0.981270, acc.: 50.00%] [Generator loss: 2.420857]\n",
      "581 [Discriminator loss: 0.329211, acc.: 87.50%] [Generator loss: 3.142011]\n",
      "582 [Discriminator loss: 1.031206, acc.: 51.56%] [Generator loss: 2.350281]\n",
      "583 [Discriminator loss: 0.461018, acc.: 73.44%] [Generator loss: 2.395986]\n",
      "584 [Discriminator loss: 0.398879, acc.: 79.69%] [Generator loss: 3.080622]\n",
      "585 [Discriminator loss: 0.638274, acc.: 65.62%] [Generator loss: 2.401918]\n",
      "586 [Discriminator loss: 0.543109, acc.: 78.12%] [Generator loss: 3.101806]\n",
      "587 [Discriminator loss: 0.407868, acc.: 76.56%] [Generator loss: 2.928767]\n",
      "588 [Discriminator loss: 0.939328, acc.: 53.12%] [Generator loss: 2.865746]\n",
      "589 [Discriminator loss: 0.684839, acc.: 70.31%] [Generator loss: 3.469410]\n",
      "590 [Discriminator loss: 0.605575, acc.: 70.31%] [Generator loss: 2.717239]\n",
      "591 [Discriminator loss: 1.027586, acc.: 43.75%] [Generator loss: 2.328155]\n",
      "592 [Discriminator loss: 0.828866, acc.: 59.38%] [Generator loss: 3.479244]\n",
      "593 [Discriminator loss: 0.747218, acc.: 64.06%] [Generator loss: 2.523154]\n",
      "594 [Discriminator loss: 0.787770, acc.: 56.25%] [Generator loss: 3.144417]\n",
      "595 [Discriminator loss: 0.869808, acc.: 62.50%] [Generator loss: 4.073288]\n",
      "596 [Discriminator loss: 0.820841, acc.: 59.38%] [Generator loss: 3.420658]\n",
      "597 [Discriminator loss: 0.761473, acc.: 64.06%] [Generator loss: 2.448554]\n",
      "598 [Discriminator loss: 0.578960, acc.: 73.44%] [Generator loss: 2.403146]\n",
      "599 [Discriminator loss: 0.685702, acc.: 64.06%] [Generator loss: 2.896703]\n",
      "600 [Discriminator loss: 0.458695, acc.: 79.69%] [Generator loss: 2.380004]\n",
      "601 [Discriminator loss: 0.641835, acc.: 64.06%] [Generator loss: 2.564093]\n",
      "602 [Discriminator loss: 0.490003, acc.: 78.12%] [Generator loss: 2.834503]\n",
      "603 [Discriminator loss: 0.599780, acc.: 70.31%] [Generator loss: 2.292554]\n",
      "604 [Discriminator loss: 1.049364, acc.: 43.75%] [Generator loss: 2.148661]\n",
      "605 [Discriminator loss: 0.930274, acc.: 53.12%] [Generator loss: 2.344169]\n",
      "606 [Discriminator loss: 0.768597, acc.: 67.19%] [Generator loss: 2.572887]\n",
      "607 [Discriminator loss: 0.824527, acc.: 60.94%] [Generator loss: 3.422948]\n",
      "608 [Discriminator loss: 1.019047, acc.: 45.31%] [Generator loss: 3.193416]\n",
      "609 [Discriminator loss: 0.698828, acc.: 65.62%] [Generator loss: 2.812559]\n",
      "610 [Discriminator loss: 0.999988, acc.: 51.56%] [Generator loss: 2.739828]\n",
      "611 [Discriminator loss: 0.538270, acc.: 76.56%] [Generator loss: 2.626619]\n",
      "612 [Discriminator loss: 0.585523, acc.: 70.31%] [Generator loss: 2.260203]\n",
      "613 [Discriminator loss: 0.690034, acc.: 59.38%] [Generator loss: 2.297986]\n",
      "614 [Discriminator loss: 0.589828, acc.: 68.75%] [Generator loss: 2.225010]\n",
      "615 [Discriminator loss: 0.973903, acc.: 53.12%] [Generator loss: 2.996413]\n",
      "616 [Discriminator loss: 0.682407, acc.: 68.75%] [Generator loss: 2.267643]\n",
      "617 [Discriminator loss: 0.433623, acc.: 76.56%] [Generator loss: 2.772044]\n",
      "618 [Discriminator loss: 0.656536, acc.: 71.88%] [Generator loss: 2.449660]\n",
      "619 [Discriminator loss: 0.542315, acc.: 73.44%] [Generator loss: 2.585386]\n",
      "620 [Discriminator loss: 0.706656, acc.: 67.19%] [Generator loss: 2.798633]\n",
      "621 [Discriminator loss: 0.341458, acc.: 90.62%] [Generator loss: 2.402857]\n",
      "622 [Discriminator loss: 0.452020, acc.: 85.94%] [Generator loss: 1.765072]\n",
      "623 [Discriminator loss: 0.505358, acc.: 71.88%] [Generator loss: 1.671669]\n",
      "624 [Discriminator loss: 0.754469, acc.: 62.50%] [Generator loss: 1.366085]\n",
      "625 [Discriminator loss: 0.428233, acc.: 79.69%] [Generator loss: 1.743883]\n",
      "626 [Discriminator loss: 0.952826, acc.: 51.56%] [Generator loss: 2.304061]\n",
      "627 [Discriminator loss: 0.530757, acc.: 71.88%] [Generator loss: 3.084296]\n",
      "628 [Discriminator loss: 0.971779, acc.: 53.12%] [Generator loss: 2.920931]\n",
      "629 [Discriminator loss: 0.455061, acc.: 78.12%] [Generator loss: 3.083555]\n",
      "630 [Discriminator loss: 0.827399, acc.: 46.88%] [Generator loss: 1.935634]\n",
      "631 [Discriminator loss: 0.760725, acc.: 59.38%] [Generator loss: 3.251881]\n",
      "632 [Discriminator loss: 0.461200, acc.: 78.12%] [Generator loss: 3.263570]\n",
      "633 [Discriminator loss: 0.504171, acc.: 76.56%] [Generator loss: 2.954907]\n",
      "634 [Discriminator loss: 0.910842, acc.: 50.00%] [Generator loss: 2.405292]\n",
      "635 [Discriminator loss: 0.377005, acc.: 90.62%] [Generator loss: 2.365867]\n",
      "636 [Discriminator loss: 0.552215, acc.: 68.75%] [Generator loss: 2.755420]\n",
      "637 [Discriminator loss: 0.515234, acc.: 76.56%] [Generator loss: 2.731168]\n",
      "638 [Discriminator loss: 0.459490, acc.: 79.69%] [Generator loss: 2.580234]\n",
      "639 [Discriminator loss: 0.353052, acc.: 81.25%] [Generator loss: 2.687017]\n",
      "640 [Discriminator loss: 0.672109, acc.: 67.19%] [Generator loss: 2.013869]\n",
      "641 [Discriminator loss: 0.443114, acc.: 75.00%] [Generator loss: 2.230403]\n",
      "642 [Discriminator loss: 0.266286, acc.: 89.06%] [Generator loss: 2.513699]\n",
      "643 [Discriminator loss: 0.804631, acc.: 51.56%] [Generator loss: 2.113447]\n",
      "644 [Discriminator loss: 0.500169, acc.: 79.69%] [Generator loss: 2.779617]\n",
      "645 [Discriminator loss: 0.768943, acc.: 62.50%] [Generator loss: 2.855016]\n",
      "646 [Discriminator loss: 0.489151, acc.: 78.12%] [Generator loss: 3.768948]\n",
      "647 [Discriminator loss: 0.819301, acc.: 57.81%] [Generator loss: 2.520496]\n",
      "648 [Discriminator loss: 0.544539, acc.: 76.56%] [Generator loss: 2.831843]\n",
      "649 [Discriminator loss: 0.668184, acc.: 67.19%] [Generator loss: 2.597494]\n",
      "650 [Discriminator loss: 0.705744, acc.: 64.06%] [Generator loss: 3.238187]\n",
      "651 [Discriminator loss: 0.798270, acc.: 59.38%] [Generator loss: 2.591843]\n",
      "652 [Discriminator loss: 0.616439, acc.: 64.06%] [Generator loss: 2.563223]\n",
      "653 [Discriminator loss: 0.599458, acc.: 67.19%] [Generator loss: 1.925492]\n",
      "654 [Discriminator loss: 0.657899, acc.: 75.00%] [Generator loss: 2.645784]\n",
      "655 [Discriminator loss: 0.499857, acc.: 78.12%] [Generator loss: 2.524075]\n",
      "656 [Discriminator loss: 0.895481, acc.: 59.38%] [Generator loss: 2.680130]\n",
      "657 [Discriminator loss: 0.522284, acc.: 79.69%] [Generator loss: 3.214767]\n",
      "658 [Discriminator loss: 0.900215, acc.: 59.38%] [Generator loss: 2.199952]\n",
      "659 [Discriminator loss: 0.558042, acc.: 75.00%] [Generator loss: 2.515033]\n",
      "660 [Discriminator loss: 0.642580, acc.: 71.88%] [Generator loss: 2.733044]\n",
      "661 [Discriminator loss: 0.683803, acc.: 67.19%] [Generator loss: 3.100643]\n",
      "662 [Discriminator loss: 0.436814, acc.: 79.69%] [Generator loss: 2.576219]\n",
      "663 [Discriminator loss: 0.528157, acc.: 78.12%] [Generator loss: 1.954960]\n",
      "664 [Discriminator loss: 0.581115, acc.: 73.44%] [Generator loss: 2.893218]\n",
      "665 [Discriminator loss: 0.516450, acc.: 73.44%] [Generator loss: 2.891809]\n",
      "666 [Discriminator loss: 0.611895, acc.: 70.31%] [Generator loss: 3.108541]\n",
      "667 [Discriminator loss: 0.592665, acc.: 64.06%] [Generator loss: 2.453107]\n",
      "668 [Discriminator loss: 0.610817, acc.: 62.50%] [Generator loss: 3.384863]\n",
      "669 [Discriminator loss: 1.062045, acc.: 51.56%] [Generator loss: 3.024541]\n",
      "670 [Discriminator loss: 0.764517, acc.: 59.38%] [Generator loss: 2.269691]\n",
      "671 [Discriminator loss: 1.040926, acc.: 54.69%] [Generator loss: 3.508509]\n",
      "672 [Discriminator loss: 0.924717, acc.: 51.56%] [Generator loss: 3.585152]\n",
      "673 [Discriminator loss: 0.561170, acc.: 68.75%] [Generator loss: 2.944009]\n",
      "674 [Discriminator loss: 0.787970, acc.: 60.94%] [Generator loss: 3.273333]\n",
      "675 [Discriminator loss: 0.782395, acc.: 60.94%] [Generator loss: 2.799594]\n",
      "676 [Discriminator loss: 0.780040, acc.: 62.50%] [Generator loss: 2.681287]\n",
      "677 [Discriminator loss: 0.484885, acc.: 76.56%] [Generator loss: 2.499192]\n",
      "678 [Discriminator loss: 0.852637, acc.: 60.94%] [Generator loss: 2.180845]\n",
      "679 [Discriminator loss: 0.400677, acc.: 81.25%] [Generator loss: 2.088039]\n",
      "680 [Discriminator loss: 1.058902, acc.: 45.31%] [Generator loss: 2.530652]\n",
      "681 [Discriminator loss: 0.517479, acc.: 73.44%] [Generator loss: 3.499246]\n",
      "682 [Discriminator loss: 0.620462, acc.: 68.75%] [Generator loss: 2.844402]\n",
      "683 [Discriminator loss: 0.701095, acc.: 65.62%] [Generator loss: 2.148571]\n",
      "684 [Discriminator loss: 0.751377, acc.: 67.19%] [Generator loss: 2.719514]\n",
      "685 [Discriminator loss: 0.609515, acc.: 62.50%] [Generator loss: 3.203659]\n",
      "686 [Discriminator loss: 1.043323, acc.: 51.56%] [Generator loss: 2.064014]\n",
      "687 [Discriminator loss: 0.799549, acc.: 51.56%] [Generator loss: 4.299507]\n",
      "688 [Discriminator loss: 0.505462, acc.: 70.31%] [Generator loss: 2.738869]\n",
      "689 [Discriminator loss: 1.648977, acc.: 34.38%] [Generator loss: 4.774652]\n",
      "690 [Discriminator loss: 0.910341, acc.: 64.06%] [Generator loss: 4.067886]\n",
      "691 [Discriminator loss: 0.910380, acc.: 56.25%] [Generator loss: 1.987568]\n",
      "692 [Discriminator loss: 0.459280, acc.: 73.44%] [Generator loss: 1.913183]\n",
      "693 [Discriminator loss: 0.574594, acc.: 71.88%] [Generator loss: 2.176358]\n",
      "694 [Discriminator loss: 0.740090, acc.: 57.81%] [Generator loss: 1.807284]\n",
      "695 [Discriminator loss: 0.376955, acc.: 79.69%] [Generator loss: 2.017334]\n",
      "696 [Discriminator loss: 0.563521, acc.: 70.31%] [Generator loss: 2.272118]\n",
      "697 [Discriminator loss: 0.523360, acc.: 75.00%] [Generator loss: 2.842989]\n",
      "698 [Discriminator loss: 0.599585, acc.: 71.88%] [Generator loss: 2.317533]\n",
      "699 [Discriminator loss: 0.685614, acc.: 60.94%] [Generator loss: 3.144700]\n",
      "700 [Discriminator loss: 0.380560, acc.: 81.25%] [Generator loss: 3.728744]\n",
      "701 [Discriminator loss: 0.336700, acc.: 87.50%] [Generator loss: 2.430374]\n",
      "702 [Discriminator loss: 0.630730, acc.: 73.44%] [Generator loss: 3.198054]\n",
      "703 [Discriminator loss: 0.546361, acc.: 70.31%] [Generator loss: 2.605351]\n",
      "704 [Discriminator loss: 0.627392, acc.: 60.94%] [Generator loss: 3.425987]\n",
      "705 [Discriminator loss: 0.393855, acc.: 79.69%] [Generator loss: 1.563155]\n",
      "706 [Discriminator loss: 0.811649, acc.: 64.06%] [Generator loss: 2.609889]\n",
      "707 [Discriminator loss: 0.305719, acc.: 84.38%] [Generator loss: 3.016874]\n",
      "708 [Discriminator loss: 0.956570, acc.: 43.75%] [Generator loss: 2.164126]\n",
      "709 [Discriminator loss: 0.912234, acc.: 62.50%] [Generator loss: 3.047896]\n",
      "710 [Discriminator loss: 0.516277, acc.: 75.00%] [Generator loss: 2.987226]\n",
      "711 [Discriminator loss: 0.818339, acc.: 57.81%] [Generator loss: 2.581840]\n",
      "712 [Discriminator loss: 0.492218, acc.: 71.88%] [Generator loss: 3.022590]\n",
      "713 [Discriminator loss: 0.665833, acc.: 75.00%] [Generator loss: 2.255880]\n",
      "714 [Discriminator loss: 0.716245, acc.: 60.94%] [Generator loss: 2.006078]\n",
      "715 [Discriminator loss: 0.567523, acc.: 68.75%] [Generator loss: 2.471216]\n",
      "716 [Discriminator loss: 0.605932, acc.: 70.31%] [Generator loss: 2.304999]\n",
      "717 [Discriminator loss: 0.785849, acc.: 57.81%] [Generator loss: 2.210726]\n",
      "718 [Discriminator loss: 0.676628, acc.: 62.50%] [Generator loss: 2.831134]\n",
      "719 [Discriminator loss: 0.799838, acc.: 65.62%] [Generator loss: 2.880263]\n",
      "720 [Discriminator loss: 0.949631, acc.: 56.25%] [Generator loss: 2.304192]\n",
      "721 [Discriminator loss: 0.930726, acc.: 46.88%] [Generator loss: 3.069005]\n",
      "722 [Discriminator loss: 0.481741, acc.: 79.69%] [Generator loss: 2.767573]\n",
      "723 [Discriminator loss: 0.958603, acc.: 54.69%] [Generator loss: 2.439264]\n",
      "724 [Discriminator loss: 0.757250, acc.: 65.62%] [Generator loss: 2.263347]\n",
      "725 [Discriminator loss: 0.864279, acc.: 54.69%] [Generator loss: 4.195601]\n",
      "726 [Discriminator loss: 0.720315, acc.: 67.19%] [Generator loss: 2.957824]\n",
      "727 [Discriminator loss: 0.828564, acc.: 57.81%] [Generator loss: 3.134676]\n",
      "728 [Discriminator loss: 0.419833, acc.: 79.69%] [Generator loss: 2.645520]\n",
      "729 [Discriminator loss: 0.577394, acc.: 71.88%] [Generator loss: 2.390622]\n",
      "730 [Discriminator loss: 0.843958, acc.: 59.38%] [Generator loss: 3.388548]\n",
      "731 [Discriminator loss: 0.631866, acc.: 67.19%] [Generator loss: 2.502299]\n",
      "732 [Discriminator loss: 0.678472, acc.: 73.44%] [Generator loss: 2.417634]\n",
      "733 [Discriminator loss: 0.697644, acc.: 68.75%] [Generator loss: 2.675424]\n",
      "734 [Discriminator loss: 0.758350, acc.: 60.94%] [Generator loss: 2.826651]\n",
      "735 [Discriminator loss: 0.517312, acc.: 76.56%] [Generator loss: 2.769804]\n",
      "736 [Discriminator loss: 0.688720, acc.: 59.38%] [Generator loss: 2.570051]\n",
      "737 [Discriminator loss: 0.928314, acc.: 54.69%] [Generator loss: 2.530512]\n",
      "738 [Discriminator loss: 0.882614, acc.: 54.69%] [Generator loss: 3.130737]\n",
      "739 [Discriminator loss: 0.768667, acc.: 65.62%] [Generator loss: 3.241440]\n",
      "740 [Discriminator loss: 1.007814, acc.: 53.12%] [Generator loss: 2.360300]\n",
      "741 [Discriminator loss: 0.624532, acc.: 62.50%] [Generator loss: 3.052240]\n",
      "742 [Discriminator loss: 0.322488, acc.: 85.94%] [Generator loss: 2.762646]\n",
      "743 [Discriminator loss: 0.980058, acc.: 45.31%] [Generator loss: 2.796627]\n",
      "744 [Discriminator loss: 0.374083, acc.: 84.38%] [Generator loss: 2.159862]\n",
      "745 [Discriminator loss: 0.558145, acc.: 71.88%] [Generator loss: 2.703285]\n",
      "746 [Discriminator loss: 0.407682, acc.: 87.50%] [Generator loss: 3.637356]\n",
      "747 [Discriminator loss: 0.686491, acc.: 59.38%] [Generator loss: 3.313439]\n",
      "748 [Discriminator loss: 0.531835, acc.: 67.19%] [Generator loss: 2.949486]\n",
      "749 [Discriminator loss: 0.573051, acc.: 73.44%] [Generator loss: 2.562258]\n",
      "750 [Discriminator loss: 0.901915, acc.: 53.12%] [Generator loss: 2.879656]\n",
      "751 [Discriminator loss: 0.356479, acc.: 87.50%] [Generator loss: 3.037546]\n",
      "752 [Discriminator loss: 0.464715, acc.: 76.56%] [Generator loss: 2.116919]\n",
      "753 [Discriminator loss: 0.410384, acc.: 82.81%] [Generator loss: 3.019467]\n",
      "754 [Discriminator loss: 0.557403, acc.: 78.12%] [Generator loss: 2.076713]\n",
      "755 [Discriminator loss: 0.533615, acc.: 70.31%] [Generator loss: 2.830815]\n",
      "756 [Discriminator loss: 0.658445, acc.: 65.62%] [Generator loss: 1.903340]\n",
      "757 [Discriminator loss: 0.673033, acc.: 59.38%] [Generator loss: 2.198228]\n",
      "758 [Discriminator loss: 0.643415, acc.: 59.38%] [Generator loss: 3.189345]\n",
      "759 [Discriminator loss: 0.897984, acc.: 50.00%] [Generator loss: 3.053648]\n",
      "760 [Discriminator loss: 0.690933, acc.: 64.06%] [Generator loss: 2.544477]\n",
      "761 [Discriminator loss: 0.923489, acc.: 51.56%] [Generator loss: 3.858600]\n",
      "762 [Discriminator loss: 0.427363, acc.: 78.12%] [Generator loss: 4.056660]\n",
      "763 [Discriminator loss: 0.902147, acc.: 57.81%] [Generator loss: 2.942156]\n",
      "764 [Discriminator loss: 0.390826, acc.: 84.38%] [Generator loss: 2.943758]\n",
      "765 [Discriminator loss: 1.192054, acc.: 35.94%] [Generator loss: 3.559077]\n",
      "766 [Discriminator loss: 0.485025, acc.: 78.12%] [Generator loss: 3.195465]\n",
      "767 [Discriminator loss: 0.717626, acc.: 71.88%] [Generator loss: 2.818646]\n",
      "768 [Discriminator loss: 0.857072, acc.: 56.25%] [Generator loss: 3.368052]\n",
      "769 [Discriminator loss: 0.574865, acc.: 73.44%] [Generator loss: 2.871847]\n",
      "770 [Discriminator loss: 0.730244, acc.: 64.06%] [Generator loss: 2.697707]\n",
      "771 [Discriminator loss: 0.615352, acc.: 64.06%] [Generator loss: 2.661089]\n",
      "772 [Discriminator loss: 0.739357, acc.: 59.38%] [Generator loss: 2.416443]\n",
      "773 [Discriminator loss: 0.543160, acc.: 73.44%] [Generator loss: 2.858074]\n",
      "774 [Discriminator loss: 0.525144, acc.: 73.44%] [Generator loss: 2.487664]\n",
      "775 [Discriminator loss: 0.463021, acc.: 81.25%] [Generator loss: 2.969190]\n",
      "776 [Discriminator loss: 0.470420, acc.: 75.00%] [Generator loss: 3.733070]\n",
      "777 [Discriminator loss: 0.516191, acc.: 76.56%] [Generator loss: 3.003333]\n",
      "778 [Discriminator loss: 0.844796, acc.: 54.69%] [Generator loss: 2.404514]\n",
      "779 [Discriminator loss: 0.657660, acc.: 65.62%] [Generator loss: 3.729647]\n",
      "780 [Discriminator loss: 0.580539, acc.: 65.62%] [Generator loss: 3.535924]\n",
      "781 [Discriminator loss: 0.660040, acc.: 67.19%] [Generator loss: 3.002797]\n",
      "782 [Discriminator loss: 0.575907, acc.: 62.50%] [Generator loss: 3.255807]\n",
      "783 [Discriminator loss: 1.185303, acc.: 45.31%] [Generator loss: 4.231816]\n",
      "784 [Discriminator loss: 0.692513, acc.: 65.62%] [Generator loss: 2.641620]\n",
      "785 [Discriminator loss: 0.706778, acc.: 56.25%] [Generator loss: 3.319650]\n",
      "786 [Discriminator loss: 0.584355, acc.: 73.44%] [Generator loss: 2.935524]\n",
      "787 [Discriminator loss: 0.513904, acc.: 70.31%] [Generator loss: 2.895606]\n",
      "788 [Discriminator loss: 0.574181, acc.: 70.31%] [Generator loss: 2.908360]\n",
      "789 [Discriminator loss: 0.423721, acc.: 78.12%] [Generator loss: 2.877269]\n",
      "790 [Discriminator loss: 0.593090, acc.: 71.88%] [Generator loss: 2.543926]\n",
      "791 [Discriminator loss: 0.466441, acc.: 78.12%] [Generator loss: 2.823058]\n",
      "792 [Discriminator loss: 1.173690, acc.: 42.19%] [Generator loss: 3.727188]\n",
      "793 [Discriminator loss: 0.388646, acc.: 81.25%] [Generator loss: 3.429503]\n",
      "794 [Discriminator loss: 0.917467, acc.: 54.69%] [Generator loss: 2.226937]\n",
      "795 [Discriminator loss: 0.706687, acc.: 68.75%] [Generator loss: 3.419415]\n",
      "796 [Discriminator loss: 0.595635, acc.: 75.00%] [Generator loss: 3.894132]\n",
      "797 [Discriminator loss: 0.894058, acc.: 56.25%] [Generator loss: 3.262860]\n",
      "798 [Discriminator loss: 0.713986, acc.: 67.19%] [Generator loss: 2.527737]\n",
      "799 [Discriminator loss: 0.482666, acc.: 85.94%] [Generator loss: 2.537156]\n",
      "800 [Discriminator loss: 0.553148, acc.: 71.88%] [Generator loss: 2.823364]\n",
      "801 [Discriminator loss: 0.683425, acc.: 70.31%] [Generator loss: 2.900481]\n",
      "802 [Discriminator loss: 0.510464, acc.: 78.12%] [Generator loss: 3.325004]\n",
      "803 [Discriminator loss: 1.097744, acc.: 54.69%] [Generator loss: 2.696614]\n",
      "804 [Discriminator loss: 0.661715, acc.: 64.06%] [Generator loss: 2.789340]\n",
      "805 [Discriminator loss: 0.448217, acc.: 79.69%] [Generator loss: 2.392717]\n",
      "806 [Discriminator loss: 0.428219, acc.: 76.56%] [Generator loss: 3.045878]\n",
      "807 [Discriminator loss: 0.519209, acc.: 75.00%] [Generator loss: 2.693871]\n",
      "808 [Discriminator loss: 0.763661, acc.: 60.94%] [Generator loss: 1.779436]\n",
      "809 [Discriminator loss: 0.668350, acc.: 68.75%] [Generator loss: 3.714105]\n",
      "810 [Discriminator loss: 0.641364, acc.: 76.56%] [Generator loss: 3.381911]\n",
      "811 [Discriminator loss: 0.519448, acc.: 75.00%] [Generator loss: 3.051556]\n",
      "812 [Discriminator loss: 0.454180, acc.: 71.88%] [Generator loss: 2.930444]\n",
      "813 [Discriminator loss: 0.551631, acc.: 81.25%] [Generator loss: 3.290904]\n",
      "814 [Discriminator loss: 0.573182, acc.: 68.75%] [Generator loss: 2.715050]\n",
      "815 [Discriminator loss: 0.550334, acc.: 75.00%] [Generator loss: 3.573487]\n",
      "816 [Discriminator loss: 1.308413, acc.: 46.88%] [Generator loss: 3.188358]\n",
      "817 [Discriminator loss: 0.816023, acc.: 53.12%] [Generator loss: 2.814219]\n",
      "818 [Discriminator loss: 0.784228, acc.: 62.50%] [Generator loss: 2.198851]\n",
      "819 [Discriminator loss: 0.950929, acc.: 51.56%] [Generator loss: 3.826358]\n",
      "820 [Discriminator loss: 0.914036, acc.: 54.69%] [Generator loss: 2.677717]\n",
      "821 [Discriminator loss: 0.622923, acc.: 65.62%] [Generator loss: 2.156503]\n",
      "822 [Discriminator loss: 0.475549, acc.: 76.56%] [Generator loss: 2.454623]\n",
      "823 [Discriminator loss: 0.680039, acc.: 65.62%] [Generator loss: 2.739971]\n",
      "824 [Discriminator loss: 0.435983, acc.: 81.25%] [Generator loss: 2.537196]\n",
      "825 [Discriminator loss: 0.582425, acc.: 64.06%] [Generator loss: 3.214425]\n",
      "826 [Discriminator loss: 0.363553, acc.: 82.81%] [Generator loss: 2.396852]\n",
      "827 [Discriminator loss: 0.665235, acc.: 73.44%] [Generator loss: 2.562033]\n",
      "828 [Discriminator loss: 0.470374, acc.: 75.00%] [Generator loss: 2.673059]\n",
      "829 [Discriminator loss: 0.591372, acc.: 76.56%] [Generator loss: 2.731921]\n",
      "830 [Discriminator loss: 0.602102, acc.: 68.75%] [Generator loss: 1.938077]\n",
      "831 [Discriminator loss: 0.861490, acc.: 64.06%] [Generator loss: 2.930863]\n",
      "832 [Discriminator loss: 0.830259, acc.: 54.69%] [Generator loss: 2.673874]\n",
      "833 [Discriminator loss: 0.617875, acc.: 71.88%] [Generator loss: 3.456861]\n",
      "834 [Discriminator loss: 0.947208, acc.: 62.50%] [Generator loss: 2.905692]\n",
      "835 [Discriminator loss: 0.937924, acc.: 51.56%] [Generator loss: 2.935673]\n",
      "836 [Discriminator loss: 0.432789, acc.: 82.81%] [Generator loss: 2.973168]\n",
      "837 [Discriminator loss: 0.628618, acc.: 73.44%] [Generator loss: 3.589871]\n",
      "838 [Discriminator loss: 0.590357, acc.: 68.75%] [Generator loss: 2.611832]\n",
      "839 [Discriminator loss: 0.477459, acc.: 71.88%] [Generator loss: 2.821716]\n",
      "840 [Discriminator loss: 0.430345, acc.: 76.56%] [Generator loss: 3.001118]\n",
      "841 [Discriminator loss: 0.342224, acc.: 89.06%] [Generator loss: 2.766857]\n",
      "842 [Discriminator loss: 0.952413, acc.: 40.62%] [Generator loss: 3.371880]\n",
      "843 [Discriminator loss: 0.309858, acc.: 84.38%] [Generator loss: 3.615008]\n",
      "844 [Discriminator loss: 0.671645, acc.: 62.50%] [Generator loss: 2.509343]\n",
      "845 [Discriminator loss: 0.675806, acc.: 71.88%] [Generator loss: 2.461352]\n",
      "846 [Discriminator loss: 0.670063, acc.: 60.94%] [Generator loss: 2.793079]\n",
      "847 [Discriminator loss: 0.427305, acc.: 76.56%] [Generator loss: 2.306658]\n",
      "848 [Discriminator loss: 0.833625, acc.: 50.00%] [Generator loss: 2.711895]\n",
      "849 [Discriminator loss: 0.758188, acc.: 62.50%] [Generator loss: 2.914671]\n",
      "850 [Discriminator loss: 0.595253, acc.: 70.31%] [Generator loss: 3.011293]\n",
      "851 [Discriminator loss: 0.649758, acc.: 70.31%] [Generator loss: 3.240383]\n",
      "852 [Discriminator loss: 0.991784, acc.: 48.44%] [Generator loss: 4.543136]\n",
      "853 [Discriminator loss: 0.606644, acc.: 71.88%] [Generator loss: 3.052254]\n",
      "854 [Discriminator loss: 0.988062, acc.: 48.44%] [Generator loss: 3.593412]\n",
      "855 [Discriminator loss: 0.964828, acc.: 59.38%] [Generator loss: 3.753546]\n",
      "856 [Discriminator loss: 0.710659, acc.: 57.81%] [Generator loss: 2.691689]\n",
      "857 [Discriminator loss: 0.585637, acc.: 70.31%] [Generator loss: 2.395380]\n",
      "858 [Discriminator loss: 0.527547, acc.: 75.00%] [Generator loss: 3.104012]\n",
      "859 [Discriminator loss: 0.553344, acc.: 75.00%] [Generator loss: 3.149187]\n",
      "860 [Discriminator loss: 0.454719, acc.: 82.81%] [Generator loss: 3.131677]\n",
      "861 [Discriminator loss: 0.733061, acc.: 64.06%] [Generator loss: 2.697286]\n",
      "862 [Discriminator loss: 0.531986, acc.: 75.00%] [Generator loss: 2.935932]\n",
      "863 [Discriminator loss: 0.646248, acc.: 71.88%] [Generator loss: 3.018676]\n",
      "864 [Discriminator loss: 0.658232, acc.: 67.19%] [Generator loss: 3.758251]\n",
      "865 [Discriminator loss: 0.507923, acc.: 79.69%] [Generator loss: 2.441137]\n",
      "866 [Discriminator loss: 0.637794, acc.: 65.62%] [Generator loss: 2.976824]\n",
      "867 [Discriminator loss: 0.430160, acc.: 81.25%] [Generator loss: 2.570045]\n",
      "868 [Discriminator loss: 1.287365, acc.: 35.94%] [Generator loss: 3.303835]\n",
      "869 [Discriminator loss: 0.668538, acc.: 67.19%] [Generator loss: 2.964803]\n",
      "870 [Discriminator loss: 0.743979, acc.: 60.94%] [Generator loss: 4.242621]\n",
      "871 [Discriminator loss: 0.756674, acc.: 68.75%] [Generator loss: 3.277304]\n",
      "872 [Discriminator loss: 0.467649, acc.: 76.56%] [Generator loss: 3.305275]\n",
      "873 [Discriminator loss: 0.383538, acc.: 84.38%] [Generator loss: 2.880316]\n",
      "874 [Discriminator loss: 0.375207, acc.: 85.94%] [Generator loss: 3.551124]\n",
      "875 [Discriminator loss: 0.779305, acc.: 62.50%] [Generator loss: 2.288109]\n",
      "876 [Discriminator loss: 0.701070, acc.: 65.62%] [Generator loss: 2.720640]\n",
      "877 [Discriminator loss: 0.738494, acc.: 62.50%] [Generator loss: 2.996283]\n",
      "878 [Discriminator loss: 0.587957, acc.: 70.31%] [Generator loss: 2.441032]\n",
      "879 [Discriminator loss: 0.577598, acc.: 68.75%] [Generator loss: 3.158429]\n",
      "880 [Discriminator loss: 0.484029, acc.: 73.44%] [Generator loss: 3.419954]\n",
      "881 [Discriminator loss: 0.529133, acc.: 71.88%] [Generator loss: 3.218395]\n",
      "882 [Discriminator loss: 0.465609, acc.: 73.44%] [Generator loss: 3.450899]\n",
      "883 [Discriminator loss: 0.922287, acc.: 56.25%] [Generator loss: 2.923769]\n",
      "884 [Discriminator loss: 0.457880, acc.: 78.12%] [Generator loss: 2.888213]\n",
      "885 [Discriminator loss: 0.682586, acc.: 67.19%] [Generator loss: 2.002687]\n",
      "886 [Discriminator loss: 0.589711, acc.: 64.06%] [Generator loss: 3.245580]\n",
      "887 [Discriminator loss: 1.046664, acc.: 54.69%] [Generator loss: 2.590576]\n",
      "888 [Discriminator loss: 0.737981, acc.: 65.62%] [Generator loss: 3.444830]\n",
      "889 [Discriminator loss: 0.702786, acc.: 67.19%] [Generator loss: 3.041909]\n",
      "890 [Discriminator loss: 0.820653, acc.: 65.62%] [Generator loss: 3.396111]\n",
      "891 [Discriminator loss: 0.650013, acc.: 68.75%] [Generator loss: 3.484141]\n",
      "892 [Discriminator loss: 0.524089, acc.: 71.88%] [Generator loss: 3.751265]\n",
      "893 [Discriminator loss: 0.602093, acc.: 64.06%] [Generator loss: 2.647465]\n",
      "894 [Discriminator loss: 0.530161, acc.: 73.44%] [Generator loss: 3.099374]\n",
      "895 [Discriminator loss: 0.385623, acc.: 79.69%] [Generator loss: 3.971667]\n",
      "896 [Discriminator loss: 0.446275, acc.: 81.25%] [Generator loss: 2.514684]\n",
      "897 [Discriminator loss: 0.915463, acc.: 56.25%] [Generator loss: 3.451039]\n",
      "898 [Discriminator loss: 0.505724, acc.: 75.00%] [Generator loss: 2.664870]\n",
      "899 [Discriminator loss: 0.649298, acc.: 60.94%] [Generator loss: 3.745434]\n",
      "900 [Discriminator loss: 0.537479, acc.: 75.00%] [Generator loss: 2.629597]\n",
      "901 [Discriminator loss: 0.591270, acc.: 70.31%] [Generator loss: 3.024422]\n",
      "902 [Discriminator loss: 0.223903, acc.: 95.31%] [Generator loss: 2.654871]\n",
      "903 [Discriminator loss: 0.607178, acc.: 67.19%] [Generator loss: 2.797049]\n",
      "904 [Discriminator loss: 0.698278, acc.: 59.38%] [Generator loss: 2.795687]\n",
      "905 [Discriminator loss: 0.426843, acc.: 82.81%] [Generator loss: 2.622560]\n",
      "906 [Discriminator loss: 0.595600, acc.: 73.44%] [Generator loss: 3.654528]\n",
      "907 [Discriminator loss: 0.521684, acc.: 73.44%] [Generator loss: 2.994712]\n",
      "908 [Discriminator loss: 1.215999, acc.: 43.75%] [Generator loss: 4.325762]\n",
      "909 [Discriminator loss: 0.609996, acc.: 71.88%] [Generator loss: 3.916715]\n",
      "910 [Discriminator loss: 0.704134, acc.: 60.94%] [Generator loss: 2.676894]\n",
      "911 [Discriminator loss: 0.282455, acc.: 84.38%] [Generator loss: 3.548841]\n",
      "912 [Discriminator loss: 0.755266, acc.: 62.50%] [Generator loss: 3.067114]\n",
      "913 [Discriminator loss: 0.796350, acc.: 60.94%] [Generator loss: 3.208547]\n",
      "914 [Discriminator loss: 0.519575, acc.: 75.00%] [Generator loss: 2.903126]\n",
      "915 [Discriminator loss: 0.326696, acc.: 84.38%] [Generator loss: 2.973598]\n",
      "916 [Discriminator loss: 0.362876, acc.: 81.25%] [Generator loss: 3.419115]\n",
      "917 [Discriminator loss: 0.315271, acc.: 87.50%] [Generator loss: 3.946821]\n",
      "918 [Discriminator loss: 0.277880, acc.: 87.50%] [Generator loss: 3.237923]\n",
      "919 [Discriminator loss: 0.713845, acc.: 59.38%] [Generator loss: 3.118138]\n",
      "920 [Discriminator loss: 0.322779, acc.: 84.38%] [Generator loss: 3.956004]\n",
      "921 [Discriminator loss: 0.647111, acc.: 75.00%] [Generator loss: 3.561318]\n",
      "922 [Discriminator loss: 0.510660, acc.: 73.44%] [Generator loss: 2.953847]\n",
      "923 [Discriminator loss: 0.685443, acc.: 64.06%] [Generator loss: 3.607343]\n",
      "924 [Discriminator loss: 0.685481, acc.: 64.06%] [Generator loss: 3.801471]\n",
      "925 [Discriminator loss: 0.579427, acc.: 81.25%] [Generator loss: 3.377243]\n",
      "926 [Discriminator loss: 0.564095, acc.: 71.88%] [Generator loss: 3.352869]\n",
      "927 [Discriminator loss: 0.647316, acc.: 70.31%] [Generator loss: 3.263418]\n",
      "928 [Discriminator loss: 0.707047, acc.: 64.06%] [Generator loss: 2.881860]\n",
      "929 [Discriminator loss: 0.392237, acc.: 81.25%] [Generator loss: 2.962118]\n",
      "930 [Discriminator loss: 0.611646, acc.: 73.44%] [Generator loss: 3.025818]\n",
      "931 [Discriminator loss: 0.601783, acc.: 70.31%] [Generator loss: 2.526877]\n",
      "932 [Discriminator loss: 0.690128, acc.: 68.75%] [Generator loss: 2.583553]\n",
      "933 [Discriminator loss: 0.427339, acc.: 84.38%] [Generator loss: 3.296769]\n",
      "934 [Discriminator loss: 0.585912, acc.: 70.31%] [Generator loss: 2.766866]\n",
      "935 [Discriminator loss: 0.998168, acc.: 51.56%] [Generator loss: 3.919742]\n",
      "936 [Discriminator loss: 0.634592, acc.: 70.31%] [Generator loss: 1.726138]\n",
      "937 [Discriminator loss: 0.866233, acc.: 57.81%] [Generator loss: 3.180184]\n",
      "938 [Discriminator loss: 0.648205, acc.: 75.00%] [Generator loss: 3.137836]\n",
      "939 [Discriminator loss: 0.476873, acc.: 76.56%] [Generator loss: 2.300037]\n",
      "940 [Discriminator loss: 0.846443, acc.: 59.38%] [Generator loss: 4.416066]\n",
      "941 [Discriminator loss: 0.558766, acc.: 75.00%] [Generator loss: 3.645686]\n",
      "942 [Discriminator loss: 0.373986, acc.: 87.50%] [Generator loss: 3.450394]\n",
      "943 [Discriminator loss: 0.447647, acc.: 79.69%] [Generator loss: 3.428794]\n",
      "944 [Discriminator loss: 0.470209, acc.: 78.12%] [Generator loss: 2.615165]\n",
      "945 [Discriminator loss: 0.265325, acc.: 90.62%] [Generator loss: 3.019208]\n",
      "946 [Discriminator loss: 1.060668, acc.: 50.00%] [Generator loss: 3.324484]\n",
      "947 [Discriminator loss: 0.888009, acc.: 57.81%] [Generator loss: 4.391564]\n",
      "948 [Discriminator loss: 0.755013, acc.: 64.06%] [Generator loss: 2.504484]\n",
      "949 [Discriminator loss: 0.869779, acc.: 59.38%] [Generator loss: 3.994547]\n",
      "950 [Discriminator loss: 0.553661, acc.: 76.56%] [Generator loss: 3.401481]\n",
      "951 [Discriminator loss: 0.623313, acc.: 73.44%] [Generator loss: 3.060721]\n",
      "952 [Discriminator loss: 0.480299, acc.: 81.25%] [Generator loss: 2.739293]\n",
      "953 [Discriminator loss: 0.613491, acc.: 68.75%] [Generator loss: 3.091619]\n",
      "954 [Discriminator loss: 0.335798, acc.: 82.81%] [Generator loss: 3.394897]\n",
      "955 [Discriminator loss: 0.664008, acc.: 73.44%] [Generator loss: 2.486706]\n",
      "956 [Discriminator loss: 0.393535, acc.: 79.69%] [Generator loss: 2.914600]\n",
      "957 [Discriminator loss: 0.486772, acc.: 71.88%] [Generator loss: 3.156178]\n",
      "958 [Discriminator loss: 0.899093, acc.: 51.56%] [Generator loss: 2.842734]\n",
      "959 [Discriminator loss: 0.442221, acc.: 75.00%] [Generator loss: 3.827223]\n",
      "960 [Discriminator loss: 0.391044, acc.: 79.69%] [Generator loss: 2.914361]\n",
      "961 [Discriminator loss: 0.479223, acc.: 70.31%] [Generator loss: 3.240664]\n",
      "962 [Discriminator loss: 0.599944, acc.: 76.56%] [Generator loss: 3.252911]\n",
      "963 [Discriminator loss: 0.332987, acc.: 85.94%] [Generator loss: 3.457871]\n",
      "964 [Discriminator loss: 0.453708, acc.: 79.69%] [Generator loss: 2.828507]\n",
      "965 [Discriminator loss: 0.610613, acc.: 68.75%] [Generator loss: 3.403117]\n",
      "966 [Discriminator loss: 0.418402, acc.: 78.12%] [Generator loss: 3.633820]\n",
      "967 [Discriminator loss: 0.419412, acc.: 82.81%] [Generator loss: 3.220270]\n",
      "968 [Discriminator loss: 0.446270, acc.: 89.06%] [Generator loss: 2.859881]\n",
      "969 [Discriminator loss: 0.605815, acc.: 62.50%] [Generator loss: 2.768757]\n",
      "970 [Discriminator loss: 0.600456, acc.: 68.75%] [Generator loss: 3.735539]\n",
      "971 [Discriminator loss: 0.566916, acc.: 70.31%] [Generator loss: 2.691778]\n",
      "972 [Discriminator loss: 0.554733, acc.: 71.88%] [Generator loss: 3.781481]\n",
      "973 [Discriminator loss: 0.450861, acc.: 84.38%] [Generator loss: 2.999045]\n",
      "974 [Discriminator loss: 0.641832, acc.: 73.44%] [Generator loss: 3.868356]\n",
      "975 [Discriminator loss: 0.671610, acc.: 71.88%] [Generator loss: 3.151174]\n",
      "976 [Discriminator loss: 0.761060, acc.: 62.50%] [Generator loss: 3.588003]\n",
      "977 [Discriminator loss: 0.901185, acc.: 54.69%] [Generator loss: 2.869901]\n",
      "978 [Discriminator loss: 0.496352, acc.: 76.56%] [Generator loss: 3.794753]\n",
      "979 [Discriminator loss: 0.557220, acc.: 76.56%] [Generator loss: 2.626713]\n",
      "980 [Discriminator loss: 0.675345, acc.: 68.75%] [Generator loss: 2.906989]\n",
      "981 [Discriminator loss: 0.324325, acc.: 84.38%] [Generator loss: 3.570373]\n",
      "982 [Discriminator loss: 0.529893, acc.: 76.56%] [Generator loss: 2.614258]\n",
      "983 [Discriminator loss: 0.613386, acc.: 67.19%] [Generator loss: 2.478097]\n",
      "984 [Discriminator loss: 0.403817, acc.: 85.94%] [Generator loss: 2.613247]\n",
      "985 [Discriminator loss: 0.628088, acc.: 68.75%] [Generator loss: 3.229811]\n",
      "986 [Discriminator loss: 0.378163, acc.: 87.50%] [Generator loss: 2.807807]\n",
      "987 [Discriminator loss: 0.469972, acc.: 78.12%] [Generator loss: 2.743005]\n",
      "988 [Discriminator loss: 0.509946, acc.: 70.31%] [Generator loss: 3.659096]\n",
      "989 [Discriminator loss: 0.297302, acc.: 87.50%] [Generator loss: 3.560281]\n",
      "990 [Discriminator loss: 0.606534, acc.: 71.88%] [Generator loss: 3.744713]\n",
      "991 [Discriminator loss: 0.809578, acc.: 60.94%] [Generator loss: 3.386292]\n",
      "992 [Discriminator loss: 0.458260, acc.: 78.12%] [Generator loss: 3.355447]\n",
      "993 [Discriminator loss: 0.548602, acc.: 78.12%] [Generator loss: 2.838436]\n",
      "994 [Discriminator loss: 0.776595, acc.: 59.38%] [Generator loss: 2.818188]\n",
      "995 [Discriminator loss: 0.565245, acc.: 78.12%] [Generator loss: 3.635601]\n",
      "996 [Discriminator loss: 0.257509, acc.: 90.62%] [Generator loss: 3.242231]\n",
      "997 [Discriminator loss: 0.789017, acc.: 62.50%] [Generator loss: 2.841473]\n",
      "998 [Discriminator loss: 0.465685, acc.: 81.25%] [Generator loss: 2.559308]\n",
      "999 [Discriminator loss: 0.454060, acc.: 73.44%] [Generator loss: 4.085773]\n",
      "1000 [Discriminator loss: 0.510629, acc.: 75.00%] [Generator loss: 3.621123]\n",
      "1001 [Discriminator loss: 1.073437, acc.: 51.56%] [Generator loss: 5.582085]\n",
      "1002 [Discriminator loss: 0.600820, acc.: 76.56%] [Generator loss: 4.536295]\n",
      "1003 [Discriminator loss: 0.386043, acc.: 81.25%] [Generator loss: 3.559026]\n",
      "1004 [Discriminator loss: 0.477248, acc.: 82.81%] [Generator loss: 2.861079]\n",
      "1005 [Discriminator loss: 0.469568, acc.: 78.12%] [Generator loss: 3.019435]\n",
      "1006 [Discriminator loss: 0.758449, acc.: 67.19%] [Generator loss: 3.353581]\n",
      "1007 [Discriminator loss: 0.623737, acc.: 70.31%] [Generator loss: 2.701463]\n",
      "1008 [Discriminator loss: 0.254098, acc.: 90.62%] [Generator loss: 3.161045]\n",
      "1009 [Discriminator loss: 0.644925, acc.: 68.75%] [Generator loss: 3.011550]\n",
      "1010 [Discriminator loss: 0.462808, acc.: 71.88%] [Generator loss: 2.700232]\n",
      "1011 [Discriminator loss: 0.355546, acc.: 84.38%] [Generator loss: 2.573517]\n",
      "1012 [Discriminator loss: 0.647154, acc.: 65.62%] [Generator loss: 3.577714]\n",
      "1013 [Discriminator loss: 0.427120, acc.: 78.12%] [Generator loss: 3.365911]\n",
      "1014 [Discriminator loss: 0.466341, acc.: 78.12%] [Generator loss: 3.512906]\n",
      "1015 [Discriminator loss: 0.329057, acc.: 84.38%] [Generator loss: 3.302055]\n",
      "1016 [Discriminator loss: 0.533207, acc.: 70.31%] [Generator loss: 3.438905]\n",
      "1017 [Discriminator loss: 0.364255, acc.: 85.94%] [Generator loss: 4.427078]\n",
      "1018 [Discriminator loss: 0.341408, acc.: 84.38%] [Generator loss: 3.355176]\n",
      "1019 [Discriminator loss: 0.463013, acc.: 81.25%] [Generator loss: 3.923095]\n",
      "1020 [Discriminator loss: 0.886372, acc.: 54.69%] [Generator loss: 3.118758]\n",
      "1021 [Discriminator loss: 0.223901, acc.: 90.62%] [Generator loss: 3.101458]\n",
      "1022 [Discriminator loss: 0.561372, acc.: 71.88%] [Generator loss: 4.064553]\n",
      "1023 [Discriminator loss: 0.554443, acc.: 73.44%] [Generator loss: 2.865983]\n",
      "1024 [Discriminator loss: 0.538771, acc.: 68.75%] [Generator loss: 3.962916]\n",
      "1025 [Discriminator loss: 0.576164, acc.: 70.31%] [Generator loss: 2.950807]\n",
      "1026 [Discriminator loss: 0.678567, acc.: 73.44%] [Generator loss: 4.130065]\n",
      "1027 [Discriminator loss: 0.542641, acc.: 75.00%] [Generator loss: 3.316533]\n",
      "1028 [Discriminator loss: 0.562244, acc.: 76.56%] [Generator loss: 3.178237]\n",
      "1029 [Discriminator loss: 0.237923, acc.: 90.62%] [Generator loss: 3.492585]\n",
      "1030 [Discriminator loss: 0.541290, acc.: 75.00%] [Generator loss: 3.277920]\n",
      "1031 [Discriminator loss: 0.360939, acc.: 85.94%] [Generator loss: 3.474272]\n",
      "1032 [Discriminator loss: 0.562495, acc.: 67.19%] [Generator loss: 2.900577]\n",
      "1033 [Discriminator loss: 0.860996, acc.: 60.94%] [Generator loss: 3.546576]\n",
      "1034 [Discriminator loss: 0.587487, acc.: 76.56%] [Generator loss: 2.509370]\n",
      "1035 [Discriminator loss: 0.636221, acc.: 68.75%] [Generator loss: 2.940734]\n",
      "1036 [Discriminator loss: 0.317819, acc.: 89.06%] [Generator loss: 4.005376]\n",
      "1037 [Discriminator loss: 0.639018, acc.: 71.88%] [Generator loss: 2.859406]\n",
      "1038 [Discriminator loss: 0.449422, acc.: 76.56%] [Generator loss: 3.339237]\n",
      "1039 [Discriminator loss: 0.294699, acc.: 87.50%] [Generator loss: 4.501373]\n",
      "1040 [Discriminator loss: 0.584373, acc.: 68.75%] [Generator loss: 3.398669]\n",
      "1041 [Discriminator loss: 0.508937, acc.: 76.56%] [Generator loss: 3.407847]\n",
      "1042 [Discriminator loss: 0.619214, acc.: 70.31%] [Generator loss: 4.876544]\n",
      "1043 [Discriminator loss: 0.457529, acc.: 79.69%] [Generator loss: 3.985784]\n",
      "1044 [Discriminator loss: 0.415979, acc.: 81.25%] [Generator loss: 4.198086]\n",
      "1045 [Discriminator loss: 0.286744, acc.: 90.62%] [Generator loss: 4.237609]\n",
      "1046 [Discriminator loss: 1.010415, acc.: 59.38%] [Generator loss: 3.924525]\n",
      "1047 [Discriminator loss: 0.726807, acc.: 71.88%] [Generator loss: 3.006825]\n",
      "1048 [Discriminator loss: 0.298818, acc.: 85.94%] [Generator loss: 3.286930]\n",
      "1049 [Discriminator loss: 0.310552, acc.: 84.38%] [Generator loss: 2.590213]\n",
      "1050 [Discriminator loss: 0.768325, acc.: 62.50%] [Generator loss: 3.234048]\n",
      "1051 [Discriminator loss: 0.217398, acc.: 92.19%] [Generator loss: 2.980119]\n",
      "1052 [Discriminator loss: 0.574739, acc.: 73.44%] [Generator loss: 3.191092]\n",
      "1053 [Discriminator loss: 0.562433, acc.: 78.12%] [Generator loss: 1.708868]\n",
      "1054 [Discriminator loss: 0.859532, acc.: 59.38%] [Generator loss: 3.916992]\n",
      "1055 [Discriminator loss: 0.454059, acc.: 81.25%] [Generator loss: 3.092878]\n",
      "1056 [Discriminator loss: 0.317537, acc.: 87.50%] [Generator loss: 2.906642]\n",
      "1057 [Discriminator loss: 0.464200, acc.: 78.12%] [Generator loss: 2.863435]\n",
      "1058 [Discriminator loss: 0.249250, acc.: 89.06%] [Generator loss: 2.484577]\n",
      "1059 [Discriminator loss: 0.905778, acc.: 57.81%] [Generator loss: 5.422884]\n",
      "1060 [Discriminator loss: 0.871533, acc.: 62.50%] [Generator loss: 3.770415]\n",
      "1061 [Discriminator loss: 0.809471, acc.: 53.12%] [Generator loss: 4.580055]\n",
      "1062 [Discriminator loss: 0.596875, acc.: 79.69%] [Generator loss: 2.617257]\n",
      "1063 [Discriminator loss: 0.360058, acc.: 81.25%] [Generator loss: 2.304267]\n",
      "1064 [Discriminator loss: 0.709319, acc.: 70.31%] [Generator loss: 2.493691]\n",
      "1065 [Discriminator loss: 0.244851, acc.: 92.19%] [Generator loss: 2.836923]\n",
      "1066 [Discriminator loss: 0.475861, acc.: 79.69%] [Generator loss: 2.597973]\n",
      "1067 [Discriminator loss: 0.980226, acc.: 56.25%] [Generator loss: 2.208855]\n",
      "1068 [Discriminator loss: 0.581634, acc.: 70.31%] [Generator loss: 2.626429]\n",
      "1069 [Discriminator loss: 0.607269, acc.: 65.62%] [Generator loss: 3.730581]\n",
      "1070 [Discriminator loss: 0.588477, acc.: 75.00%] [Generator loss: 2.543832]\n",
      "1071 [Discriminator loss: 0.269792, acc.: 95.31%] [Generator loss: 2.563054]\n",
      "1072 [Discriminator loss: 0.416516, acc.: 82.81%] [Generator loss: 3.218581]\n",
      "1073 [Discriminator loss: 0.813848, acc.: 56.25%] [Generator loss: 3.157279]\n",
      "1074 [Discriminator loss: 0.466646, acc.: 76.56%] [Generator loss: 3.417753]\n",
      "1075 [Discriminator loss: 0.350348, acc.: 87.50%] [Generator loss: 2.702288]\n",
      "1076 [Discriminator loss: 0.664658, acc.: 65.62%] [Generator loss: 4.139132]\n",
      "1077 [Discriminator loss: 0.550774, acc.: 79.69%] [Generator loss: 3.216919]\n",
      "1078 [Discriminator loss: 0.532787, acc.: 76.56%] [Generator loss: 2.877188]\n",
      "1079 [Discriminator loss: 0.527216, acc.: 71.88%] [Generator loss: 2.991257]\n",
      "1080 [Discriminator loss: 0.472695, acc.: 75.00%] [Generator loss: 2.528391]\n",
      "1081 [Discriminator loss: 0.720299, acc.: 65.62%] [Generator loss: 3.151059]\n",
      "1082 [Discriminator loss: 0.466810, acc.: 76.56%] [Generator loss: 3.510192]\n",
      "1083 [Discriminator loss: 1.097033, acc.: 53.12%] [Generator loss: 3.327452]\n",
      "1084 [Discriminator loss: 0.852269, acc.: 57.81%] [Generator loss: 4.753920]\n",
      "1085 [Discriminator loss: 0.349370, acc.: 82.81%] [Generator loss: 3.550948]\n",
      "1086 [Discriminator loss: 0.787266, acc.: 73.44%] [Generator loss: 2.869456]\n",
      "1087 [Discriminator loss: 0.812242, acc.: 60.94%] [Generator loss: 3.129922]\n",
      "1088 [Discriminator loss: 0.418803, acc.: 78.12%] [Generator loss: 2.640727]\n",
      "1089 [Discriminator loss: 0.594316, acc.: 71.88%] [Generator loss: 3.419515]\n",
      "1090 [Discriminator loss: 0.531957, acc.: 76.56%] [Generator loss: 3.081305]\n",
      "1091 [Discriminator loss: 0.599858, acc.: 67.19%] [Generator loss: 2.896416]\n",
      "1092 [Discriminator loss: 0.334963, acc.: 85.94%] [Generator loss: 3.361371]\n",
      "1093 [Discriminator loss: 0.650848, acc.: 68.75%] [Generator loss: 3.837577]\n",
      "1094 [Discriminator loss: 0.839456, acc.: 64.06%] [Generator loss: 2.839209]\n",
      "1095 [Discriminator loss: 0.554071, acc.: 76.56%] [Generator loss: 3.360783]\n",
      "1096 [Discriminator loss: 0.499851, acc.: 79.69%] [Generator loss: 3.566465]\n",
      "1097 [Discriminator loss: 0.830943, acc.: 65.62%] [Generator loss: 3.416676]\n",
      "1098 [Discriminator loss: 0.874856, acc.: 56.25%] [Generator loss: 2.429357]\n",
      "1099 [Discriminator loss: 0.290831, acc.: 90.62%] [Generator loss: 3.156979]\n",
      "1100 [Discriminator loss: 0.293816, acc.: 84.38%] [Generator loss: 3.267563]\n",
      "1101 [Discriminator loss: 0.290952, acc.: 89.06%] [Generator loss: 3.423433]\n",
      "1102 [Discriminator loss: 0.520165, acc.: 76.56%] [Generator loss: 3.807327]\n",
      "1103 [Discriminator loss: 0.518735, acc.: 79.69%] [Generator loss: 3.858387]\n",
      "1104 [Discriminator loss: 0.559892, acc.: 70.31%] [Generator loss: 3.632639]\n",
      "1105 [Discriminator loss: 0.332970, acc.: 84.38%] [Generator loss: 3.384964]\n",
      "1106 [Discriminator loss: 0.605910, acc.: 67.19%] [Generator loss: 3.752371]\n",
      "1107 [Discriminator loss: 0.607916, acc.: 65.62%] [Generator loss: 3.446235]\n",
      "1108 [Discriminator loss: 0.474833, acc.: 76.56%] [Generator loss: 3.385008]\n",
      "1109 [Discriminator loss: 0.707070, acc.: 65.62%] [Generator loss: 3.178210]\n",
      "1110 [Discriminator loss: 0.744466, acc.: 67.19%] [Generator loss: 3.027095]\n",
      "1111 [Discriminator loss: 0.713239, acc.: 62.50%] [Generator loss: 4.276019]\n",
      "1112 [Discriminator loss: 0.413883, acc.: 79.69%] [Generator loss: 3.369662]\n",
      "1113 [Discriminator loss: 0.598411, acc.: 73.44%] [Generator loss: 4.101036]\n",
      "1114 [Discriminator loss: 0.363969, acc.: 84.38%] [Generator loss: 3.894723]\n",
      "1115 [Discriminator loss: 0.520888, acc.: 73.44%] [Generator loss: 3.293365]\n",
      "1116 [Discriminator loss: 0.550779, acc.: 73.44%] [Generator loss: 3.385321]\n",
      "1117 [Discriminator loss: 0.466095, acc.: 81.25%] [Generator loss: 2.990391]\n",
      "1118 [Discriminator loss: 0.709825, acc.: 75.00%] [Generator loss: 3.972911]\n",
      "1119 [Discriminator loss: 0.435901, acc.: 85.94%] [Generator loss: 3.381082]\n",
      "1120 [Discriminator loss: 0.794170, acc.: 60.94%] [Generator loss: 3.023005]\n",
      "1121 [Discriminator loss: 0.505638, acc.: 78.12%] [Generator loss: 3.032131]\n",
      "1122 [Discriminator loss: 0.556400, acc.: 71.88%] [Generator loss: 2.682266]\n",
      "1123 [Discriminator loss: 0.284009, acc.: 89.06%] [Generator loss: 2.491729]\n",
      "1124 [Discriminator loss: 0.474432, acc.: 71.88%] [Generator loss: 3.152284]\n",
      "1125 [Discriminator loss: 0.551079, acc.: 75.00%] [Generator loss: 3.147992]\n",
      "1126 [Discriminator loss: 0.294458, acc.: 93.75%] [Generator loss: 3.185005]\n",
      "1127 [Discriminator loss: 0.503897, acc.: 73.44%] [Generator loss: 3.097937]\n",
      "1128 [Discriminator loss: 0.248882, acc.: 89.06%] [Generator loss: 2.220241]\n",
      "1129 [Discriminator loss: 0.580755, acc.: 70.31%] [Generator loss: 3.192192]\n",
      "1130 [Discriminator loss: 0.375859, acc.: 85.94%] [Generator loss: 3.104784]\n",
      "1131 [Discriminator loss: 0.673455, acc.: 65.62%] [Generator loss: 2.136535]\n",
      "1132 [Discriminator loss: 0.490542, acc.: 75.00%] [Generator loss: 4.293469]\n",
      "1133 [Discriminator loss: 0.400781, acc.: 87.50%] [Generator loss: 3.238180]\n",
      "1134 [Discriminator loss: 0.825109, acc.: 48.44%] [Generator loss: 4.717848]\n",
      "1135 [Discriminator loss: 0.206422, acc.: 93.75%] [Generator loss: 4.513970]\n",
      "1136 [Discriminator loss: 0.422083, acc.: 82.81%] [Generator loss: 3.321647]\n",
      "1137 [Discriminator loss: 0.226186, acc.: 90.62%] [Generator loss: 3.344394]\n",
      "1138 [Discriminator loss: 0.671062, acc.: 67.19%] [Generator loss: 4.593406]\n",
      "1139 [Discriminator loss: 0.367582, acc.: 78.12%] [Generator loss: 3.053029]\n",
      "1140 [Discriminator loss: 0.649260, acc.: 71.88%] [Generator loss: 3.801394]\n",
      "1141 [Discriminator loss: 0.477748, acc.: 78.12%] [Generator loss: 3.316741]\n",
      "1142 [Discriminator loss: 0.446969, acc.: 79.69%] [Generator loss: 4.020365]\n",
      "1143 [Discriminator loss: 0.758230, acc.: 62.50%] [Generator loss: 2.986158]\n",
      "1144 [Discriminator loss: 0.385681, acc.: 82.81%] [Generator loss: 3.730951]\n",
      "1145 [Discriminator loss: 0.565313, acc.: 70.31%] [Generator loss: 3.829154]\n",
      "1146 [Discriminator loss: 0.757128, acc.: 67.19%] [Generator loss: 4.031481]\n",
      "1147 [Discriminator loss: 0.601014, acc.: 67.19%] [Generator loss: 2.996250]\n",
      "1148 [Discriminator loss: 0.630404, acc.: 68.75%] [Generator loss: 4.252480]\n",
      "1149 [Discriminator loss: 0.592916, acc.: 78.12%] [Generator loss: 2.979761]\n",
      "1150 [Discriminator loss: 0.459221, acc.: 81.25%] [Generator loss: 2.681183]\n",
      "1151 [Discriminator loss: 0.899361, acc.: 53.12%] [Generator loss: 4.380305]\n",
      "1152 [Discriminator loss: 0.692934, acc.: 65.62%] [Generator loss: 5.162599]\n",
      "1153 [Discriminator loss: 0.915455, acc.: 60.94%] [Generator loss: 3.512896]\n",
      "1154 [Discriminator loss: 0.383757, acc.: 79.69%] [Generator loss: 4.098835]\n",
      "1155 [Discriminator loss: 0.286105, acc.: 85.94%] [Generator loss: 3.204583]\n",
      "1156 [Discriminator loss: 0.469867, acc.: 84.38%] [Generator loss: 3.670127]\n",
      "1157 [Discriminator loss: 0.349501, acc.: 82.81%] [Generator loss: 3.774457]\n",
      "1158 [Discriminator loss: 0.238217, acc.: 93.75%] [Generator loss: 3.766807]\n",
      "1159 [Discriminator loss: 0.343575, acc.: 82.81%] [Generator loss: 3.891219]\n",
      "1160 [Discriminator loss: 0.414799, acc.: 78.12%] [Generator loss: 4.919594]\n",
      "1161 [Discriminator loss: 0.165008, acc.: 93.75%] [Generator loss: 3.794804]\n",
      "1162 [Discriminator loss: 0.640627, acc.: 73.44%] [Generator loss: 4.593279]\n",
      "1163 [Discriminator loss: 0.621332, acc.: 67.19%] [Generator loss: 3.933424]\n",
      "1164 [Discriminator loss: 0.929540, acc.: 57.81%] [Generator loss: 4.409445]\n",
      "1165 [Discriminator loss: 0.597487, acc.: 73.44%] [Generator loss: 2.777736]\n",
      "1166 [Discriminator loss: 0.941941, acc.: 59.38%] [Generator loss: 4.840827]\n",
      "1167 [Discriminator loss: 0.497729, acc.: 78.12%] [Generator loss: 3.544276]\n",
      "1168 [Discriminator loss: 0.708466, acc.: 68.75%] [Generator loss: 4.893155]\n",
      "1169 [Discriminator loss: 0.290051, acc.: 85.94%] [Generator loss: 4.091188]\n",
      "1170 [Discriminator loss: 0.595301, acc.: 65.62%] [Generator loss: 4.523170]\n",
      "1171 [Discriminator loss: 0.568982, acc.: 79.69%] [Generator loss: 3.711397]\n",
      "1172 [Discriminator loss: 0.482521, acc.: 78.12%] [Generator loss: 2.827153]\n",
      "1173 [Discriminator loss: 0.429135, acc.: 81.25%] [Generator loss: 4.283593]\n",
      "1174 [Discriminator loss: 0.367866, acc.: 87.50%] [Generator loss: 3.599356]\n",
      "1175 [Discriminator loss: 0.576322, acc.: 73.44%] [Generator loss: 3.297456]\n",
      "1176 [Discriminator loss: 0.479150, acc.: 78.12%] [Generator loss: 4.065402]\n",
      "1177 [Discriminator loss: 0.344078, acc.: 82.81%] [Generator loss: 3.313053]\n",
      "1178 [Discriminator loss: 0.689167, acc.: 70.31%] [Generator loss: 2.829077]\n",
      "1179 [Discriminator loss: 0.621861, acc.: 67.19%] [Generator loss: 4.614079]\n",
      "1180 [Discriminator loss: 0.704875, acc.: 75.00%] [Generator loss: 3.189798]\n",
      "1181 [Discriminator loss: 0.463859, acc.: 82.81%] [Generator loss: 3.649570]\n",
      "1182 [Discriminator loss: 0.488670, acc.: 79.69%] [Generator loss: 3.688704]\n",
      "1183 [Discriminator loss: 0.401820, acc.: 79.69%] [Generator loss: 3.349790]\n",
      "1184 [Discriminator loss: 0.964596, acc.: 54.69%] [Generator loss: 4.037985]\n",
      "1185 [Discriminator loss: 0.663445, acc.: 71.88%] [Generator loss: 3.330521]\n",
      "1186 [Discriminator loss: 0.559571, acc.: 76.56%] [Generator loss: 3.094255]\n",
      "1187 [Discriminator loss: 0.331623, acc.: 87.50%] [Generator loss: 3.051280]\n",
      "1188 [Discriminator loss: 0.760483, acc.: 59.38%] [Generator loss: 3.486349]\n",
      "1189 [Discriminator loss: 0.356699, acc.: 84.38%] [Generator loss: 2.534338]\n",
      "1190 [Discriminator loss: 0.576734, acc.: 68.75%] [Generator loss: 3.953402]\n",
      "1191 [Discriminator loss: 0.519426, acc.: 76.56%] [Generator loss: 3.365936]\n",
      "1192 [Discriminator loss: 0.460193, acc.: 79.69%] [Generator loss: 4.134610]\n",
      "1193 [Discriminator loss: 0.163160, acc.: 93.75%] [Generator loss: 3.632871]\n",
      "1194 [Discriminator loss: 0.546105, acc.: 73.44%] [Generator loss: 2.095270]\n",
      "1195 [Discriminator loss: 0.615129, acc.: 76.56%] [Generator loss: 3.663364]\n",
      "1196 [Discriminator loss: 0.455537, acc.: 76.56%] [Generator loss: 3.742875]\n",
      "1197 [Discriminator loss: 0.796362, acc.: 70.31%] [Generator loss: 3.813362]\n",
      "1198 [Discriminator loss: 0.519785, acc.: 78.12%] [Generator loss: 4.369629]\n",
      "1199 [Discriminator loss: 0.462009, acc.: 79.69%] [Generator loss: 3.185593]\n",
      "1200 [Discriminator loss: 0.248057, acc.: 92.19%] [Generator loss: 3.355753]\n",
      "1201 [Discriminator loss: 0.559810, acc.: 68.75%] [Generator loss: 2.608731]\n",
      "1202 [Discriminator loss: 0.300811, acc.: 92.19%] [Generator loss: 3.300535]\n",
      "1203 [Discriminator loss: 0.630322, acc.: 68.75%] [Generator loss: 3.596746]\n",
      "1204 [Discriminator loss: 0.552754, acc.: 76.56%] [Generator loss: 3.703773]\n",
      "1205 [Discriminator loss: 0.529288, acc.: 76.56%] [Generator loss: 2.454895]\n",
      "1206 [Discriminator loss: 0.339980, acc.: 82.81%] [Generator loss: 3.858228]\n",
      "1207 [Discriminator loss: 0.397961, acc.: 79.69%] [Generator loss: 3.240601]\n",
      "1208 [Discriminator loss: 0.538941, acc.: 75.00%] [Generator loss: 2.617075]\n",
      "1209 [Discriminator loss: 0.652248, acc.: 68.75%] [Generator loss: 4.139696]\n",
      "1210 [Discriminator loss: 0.498419, acc.: 76.56%] [Generator loss: 3.445475]\n",
      "1211 [Discriminator loss: 0.456164, acc.: 79.69%] [Generator loss: 3.638692]\n",
      "1212 [Discriminator loss: 0.280985, acc.: 87.50%] [Generator loss: 4.410299]\n",
      "1213 [Discriminator loss: 0.609044, acc.: 73.44%] [Generator loss: 3.349582]\n",
      "1214 [Discriminator loss: 0.468026, acc.: 76.56%] [Generator loss: 2.862556]\n",
      "1215 [Discriminator loss: 0.743904, acc.: 70.31%] [Generator loss: 4.084715]\n",
      "1216 [Discriminator loss: 0.392392, acc.: 87.50%] [Generator loss: 3.692153]\n",
      "1217 [Discriminator loss: 0.361480, acc.: 81.25%] [Generator loss: 3.045984]\n",
      "1218 [Discriminator loss: 0.549675, acc.: 76.56%] [Generator loss: 4.456940]\n",
      "1219 [Discriminator loss: 0.613256, acc.: 71.88%] [Generator loss: 3.503307]\n",
      "1220 [Discriminator loss: 0.495033, acc.: 78.12%] [Generator loss: 3.044032]\n",
      "1221 [Discriminator loss: 0.902882, acc.: 54.69%] [Generator loss: 4.873537]\n",
      "1222 [Discriminator loss: 0.416187, acc.: 82.81%] [Generator loss: 4.089447]\n",
      "1223 [Discriminator loss: 0.627716, acc.: 70.31%] [Generator loss: 4.119853]\n",
      "1224 [Discriminator loss: 0.474011, acc.: 81.25%] [Generator loss: 3.849788]\n",
      "1225 [Discriminator loss: 0.629968, acc.: 68.75%] [Generator loss: 4.155733]\n",
      "1226 [Discriminator loss: 0.780599, acc.: 67.19%] [Generator loss: 3.294140]\n",
      "1227 [Discriminator loss: 0.392557, acc.: 79.69%] [Generator loss: 3.935133]\n",
      "1228 [Discriminator loss: 0.840207, acc.: 62.50%] [Generator loss: 2.881224]\n",
      "1229 [Discriminator loss: 0.969502, acc.: 50.00%] [Generator loss: 3.485888]\n",
      "1230 [Discriminator loss: 0.499626, acc.: 75.00%] [Generator loss: 3.533520]\n",
      "1231 [Discriminator loss: 0.635983, acc.: 65.62%] [Generator loss: 2.934078]\n",
      "1232 [Discriminator loss: 0.752911, acc.: 62.50%] [Generator loss: 4.054464]\n",
      "1233 [Discriminator loss: 0.254692, acc.: 90.62%] [Generator loss: 3.339430]\n",
      "1234 [Discriminator loss: 0.542498, acc.: 75.00%] [Generator loss: 4.060552]\n",
      "1235 [Discriminator loss: 0.510384, acc.: 78.12%] [Generator loss: 3.459406]\n",
      "1236 [Discriminator loss: 0.593354, acc.: 70.31%] [Generator loss: 2.912367]\n",
      "1237 [Discriminator loss: 0.337521, acc.: 85.94%] [Generator loss: 3.720752]\n",
      "1238 [Discriminator loss: 0.499500, acc.: 78.12%] [Generator loss: 3.314394]\n",
      "1239 [Discriminator loss: 0.533996, acc.: 75.00%] [Generator loss: 2.987169]\n",
      "1240 [Discriminator loss: 0.649968, acc.: 71.88%] [Generator loss: 3.866244]\n",
      "1241 [Discriminator loss: 0.798252, acc.: 60.94%] [Generator loss: 2.399040]\n",
      "1242 [Discriminator loss: 0.633533, acc.: 67.19%] [Generator loss: 5.040116]\n",
      "1243 [Discriminator loss: 0.356026, acc.: 85.94%] [Generator loss: 3.843699]\n",
      "1244 [Discriminator loss: 0.529854, acc.: 79.69%] [Generator loss: 3.347870]\n",
      "1245 [Discriminator loss: 0.427412, acc.: 79.69%] [Generator loss: 3.793035]\n",
      "1246 [Discriminator loss: 0.655329, acc.: 73.44%] [Generator loss: 3.856052]\n",
      "1247 [Discriminator loss: 0.304134, acc.: 85.94%] [Generator loss: 2.816912]\n",
      "1248 [Discriminator loss: 0.463881, acc.: 73.44%] [Generator loss: 3.353556]\n",
      "1249 [Discriminator loss: 0.402584, acc.: 81.25%] [Generator loss: 3.753499]\n",
      "1250 [Discriminator loss: 0.311914, acc.: 85.94%] [Generator loss: 3.226187]\n",
      "1251 [Discriminator loss: 0.633383, acc.: 67.19%] [Generator loss: 3.543476]\n",
      "1252 [Discriminator loss: 0.650646, acc.: 71.88%] [Generator loss: 2.890726]\n",
      "1253 [Discriminator loss: 0.328424, acc.: 82.81%] [Generator loss: 3.607352]\n",
      "1254 [Discriminator loss: 0.436069, acc.: 81.25%] [Generator loss: 3.660923]\n",
      "1255 [Discriminator loss: 0.685396, acc.: 59.38%] [Generator loss: 3.907291]\n",
      "1256 [Discriminator loss: 0.461488, acc.: 84.38%] [Generator loss: 2.866898]\n",
      "1257 [Discriminator loss: 0.746071, acc.: 57.81%] [Generator loss: 4.784754]\n",
      "1258 [Discriminator loss: 0.465946, acc.: 79.69%] [Generator loss: 3.882670]\n",
      "1259 [Discriminator loss: 0.502427, acc.: 76.56%] [Generator loss: 3.032554]\n",
      "1260 [Discriminator loss: 0.300932, acc.: 87.50%] [Generator loss: 3.794030]\n",
      "1261 [Discriminator loss: 0.497278, acc.: 78.12%] [Generator loss: 3.266459]\n",
      "1262 [Discriminator loss: 0.638037, acc.: 68.75%] [Generator loss: 4.734028]\n",
      "1263 [Discriminator loss: 0.296528, acc.: 89.06%] [Generator loss: 3.151906]\n",
      "1264 [Discriminator loss: 0.596743, acc.: 71.88%] [Generator loss: 3.411681]\n",
      "1265 [Discriminator loss: 0.641624, acc.: 70.31%] [Generator loss: 3.743564]\n",
      "1266 [Discriminator loss: 0.633747, acc.: 65.62%] [Generator loss: 3.934242]\n",
      "1267 [Discriminator loss: 0.440175, acc.: 81.25%] [Generator loss: 3.394955]\n",
      "1268 [Discriminator loss: 0.565934, acc.: 70.31%] [Generator loss: 2.875956]\n",
      "1269 [Discriminator loss: 0.678854, acc.: 67.19%] [Generator loss: 4.094353]\n",
      "1270 [Discriminator loss: 0.450224, acc.: 79.69%] [Generator loss: 3.501171]\n",
      "1271 [Discriminator loss: 0.666305, acc.: 70.31%] [Generator loss: 3.640852]\n",
      "1272 [Discriminator loss: 0.444694, acc.: 75.00%] [Generator loss: 4.062843]\n",
      "1273 [Discriminator loss: 0.837612, acc.: 65.62%] [Generator loss: 3.917394]\n",
      "1274 [Discriminator loss: 0.401306, acc.: 81.25%] [Generator loss: 3.642387]\n",
      "1275 [Discriminator loss: 0.715673, acc.: 62.50%] [Generator loss: 3.911745]\n",
      "1276 [Discriminator loss: 0.504609, acc.: 73.44%] [Generator loss: 4.185114]\n",
      "1277 [Discriminator loss: 0.314928, acc.: 87.50%] [Generator loss: 3.895336]\n",
      "1278 [Discriminator loss: 0.749011, acc.: 64.06%] [Generator loss: 3.355203]\n",
      "1279 [Discriminator loss: 0.166166, acc.: 93.75%] [Generator loss: 3.179956]\n",
      "1280 [Discriminator loss: 0.337411, acc.: 84.38%] [Generator loss: 3.425256]\n",
      "1281 [Discriminator loss: 0.403909, acc.: 81.25%] [Generator loss: 3.869630]\n",
      "1282 [Discriminator loss: 0.433661, acc.: 76.56%] [Generator loss: 2.971048]\n",
      "1283 [Discriminator loss: 0.570076, acc.: 78.12%] [Generator loss: 3.667240]\n",
      "1284 [Discriminator loss: 0.489749, acc.: 79.69%] [Generator loss: 3.132900]\n",
      "1285 [Discriminator loss: 0.699041, acc.: 67.19%] [Generator loss: 3.864324]\n",
      "1286 [Discriminator loss: 0.456962, acc.: 73.44%] [Generator loss: 2.911278]\n",
      "1287 [Discriminator loss: 0.312282, acc.: 84.38%] [Generator loss: 2.076816]\n",
      "1288 [Discriminator loss: 0.667441, acc.: 68.75%] [Generator loss: 2.226872]\n",
      "1289 [Discriminator loss: 0.683213, acc.: 67.19%] [Generator loss: 4.562725]\n",
      "1290 [Discriminator loss: 0.647741, acc.: 71.88%] [Generator loss: 3.019924]\n",
      "1291 [Discriminator loss: 0.387031, acc.: 81.25%] [Generator loss: 2.563159]\n",
      "1292 [Discriminator loss: 0.652158, acc.: 70.31%] [Generator loss: 3.750285]\n",
      "1293 [Discriminator loss: 0.531712, acc.: 73.44%] [Generator loss: 4.024926]\n",
      "1294 [Discriminator loss: 0.692920, acc.: 67.19%] [Generator loss: 3.673922]\n",
      "1295 [Discriminator loss: 0.958769, acc.: 56.25%] [Generator loss: 3.529697]\n",
      "1296 [Discriminator loss: 0.368522, acc.: 82.81%] [Generator loss: 4.891944]\n",
      "1297 [Discriminator loss: 0.746356, acc.: 67.19%] [Generator loss: 3.540301]\n",
      "1298 [Discriminator loss: 0.791057, acc.: 71.88%] [Generator loss: 5.567518]\n",
      "1299 [Discriminator loss: 0.496332, acc.: 71.88%] [Generator loss: 3.568737]\n",
      "1300 [Discriminator loss: 0.494540, acc.: 81.25%] [Generator loss: 3.305746]\n",
      "1301 [Discriminator loss: 0.279014, acc.: 85.94%] [Generator loss: 4.839942]\n",
      "1302 [Discriminator loss: 0.624425, acc.: 73.44%] [Generator loss: 3.457844]\n",
      "1303 [Discriminator loss: 0.259402, acc.: 87.50%] [Generator loss: 4.507843]\n",
      "1304 [Discriminator loss: 0.443328, acc.: 78.12%] [Generator loss: 3.991004]\n",
      "1305 [Discriminator loss: 0.237899, acc.: 92.19%] [Generator loss: 4.321789]\n",
      "1306 [Discriminator loss: 0.697195, acc.: 68.75%] [Generator loss: 3.566269]\n",
      "1307 [Discriminator loss: 0.312614, acc.: 87.50%] [Generator loss: 3.991460]\n",
      "1308 [Discriminator loss: 0.531218, acc.: 73.44%] [Generator loss: 2.931038]\n",
      "1309 [Discriminator loss: 0.379366, acc.: 84.38%] [Generator loss: 2.852081]\n",
      "1310 [Discriminator loss: 0.619716, acc.: 71.88%] [Generator loss: 3.709696]\n",
      "1311 [Discriminator loss: 0.494005, acc.: 82.81%] [Generator loss: 2.819483]\n",
      "1312 [Discriminator loss: 0.552176, acc.: 75.00%] [Generator loss: 4.016896]\n",
      "1313 [Discriminator loss: 0.479839, acc.: 78.12%] [Generator loss: 4.027686]\n",
      "1314 [Discriminator loss: 0.520786, acc.: 71.88%] [Generator loss: 4.319301]\n",
      "1315 [Discriminator loss: 0.233822, acc.: 93.75%] [Generator loss: 3.358023]\n",
      "1316 [Discriminator loss: 0.625659, acc.: 70.31%] [Generator loss: 3.442027]\n",
      "1317 [Discriminator loss: 0.351264, acc.: 84.38%] [Generator loss: 3.642976]\n",
      "1318 [Discriminator loss: 0.553930, acc.: 75.00%] [Generator loss: 3.694132]\n",
      "1319 [Discriminator loss: 0.627455, acc.: 73.44%] [Generator loss: 3.394055]\n",
      "1320 [Discriminator loss: 0.500306, acc.: 75.00%] [Generator loss: 3.825605]\n",
      "1321 [Discriminator loss: 0.659844, acc.: 70.31%] [Generator loss: 3.170593]\n",
      "1322 [Discriminator loss: 0.421807, acc.: 78.12%] [Generator loss: 4.082009]\n",
      "1323 [Discriminator loss: 0.388049, acc.: 81.25%] [Generator loss: 4.467357]\n",
      "1324 [Discriminator loss: 0.239212, acc.: 90.62%] [Generator loss: 3.866463]\n",
      "1325 [Discriminator loss: 0.421976, acc.: 79.69%] [Generator loss: 3.821432]\n",
      "1326 [Discriminator loss: 0.148243, acc.: 95.31%] [Generator loss: 3.771362]\n",
      "1327 [Discriminator loss: 0.429562, acc.: 81.25%] [Generator loss: 2.580682]\n",
      "1328 [Discriminator loss: 0.335247, acc.: 84.38%] [Generator loss: 3.760475]\n",
      "1329 [Discriminator loss: 0.445176, acc.: 84.38%] [Generator loss: 2.546461]\n",
      "1330 [Discriminator loss: 0.631002, acc.: 75.00%] [Generator loss: 3.823435]\n",
      "1331 [Discriminator loss: 0.496121, acc.: 79.69%] [Generator loss: 2.750585]\n",
      "1332 [Discriminator loss: 0.661921, acc.: 70.31%] [Generator loss: 3.569546]\n",
      "1333 [Discriminator loss: 0.638411, acc.: 67.19%] [Generator loss: 2.862003]\n",
      "1334 [Discriminator loss: 0.558861, acc.: 70.31%] [Generator loss: 4.054590]\n",
      "1335 [Discriminator loss: 0.418823, acc.: 82.81%] [Generator loss: 3.476609]\n",
      "1336 [Discriminator loss: 0.583450, acc.: 65.62%] [Generator loss: 3.819417]\n",
      "1337 [Discriminator loss: 0.465083, acc.: 79.69%] [Generator loss: 3.080536]\n",
      "1338 [Discriminator loss: 0.407866, acc.: 82.81%] [Generator loss: 2.788277]\n",
      "1339 [Discriminator loss: 0.390501, acc.: 81.25%] [Generator loss: 3.634791]\n",
      "1340 [Discriminator loss: 0.231601, acc.: 87.50%] [Generator loss: 3.856877]\n",
      "1341 [Discriminator loss: 0.670865, acc.: 62.50%] [Generator loss: 4.911692]\n",
      "1342 [Discriminator loss: 0.415041, acc.: 81.25%] [Generator loss: 2.699706]\n",
      "1343 [Discriminator loss: 0.884480, acc.: 57.81%] [Generator loss: 4.432961]\n",
      "1344 [Discriminator loss: 0.741855, acc.: 65.62%] [Generator loss: 3.697330]\n",
      "1345 [Discriminator loss: 0.435906, acc.: 81.25%] [Generator loss: 3.897093]\n",
      "1346 [Discriminator loss: 0.276381, acc.: 92.19%] [Generator loss: 4.131959]\n",
      "1347 [Discriminator loss: 0.609769, acc.: 71.88%] [Generator loss: 3.332819]\n",
      "1348 [Discriminator loss: 0.528035, acc.: 68.75%] [Generator loss: 2.981483]\n",
      "1349 [Discriminator loss: 0.241871, acc.: 92.19%] [Generator loss: 3.318594]\n",
      "1350 [Discriminator loss: 0.453833, acc.: 79.69%] [Generator loss: 3.175527]\n",
      "1351 [Discriminator loss: 0.604643, acc.: 78.12%] [Generator loss: 3.594681]\n",
      "1352 [Discriminator loss: 0.556075, acc.: 73.44%] [Generator loss: 3.546608]\n",
      "1353 [Discriminator loss: 0.445501, acc.: 79.69%] [Generator loss: 3.704734]\n",
      "1354 [Discriminator loss: 0.429374, acc.: 79.69%] [Generator loss: 3.662526]\n",
      "1355 [Discriminator loss: 0.770839, acc.: 62.50%] [Generator loss: 3.218196]\n",
      "1356 [Discriminator loss: 0.361448, acc.: 79.69%] [Generator loss: 2.699036]\n",
      "1357 [Discriminator loss: 0.607792, acc.: 70.31%] [Generator loss: 3.827471]\n",
      "1358 [Discriminator loss: 0.402208, acc.: 79.69%] [Generator loss: 3.383533]\n",
      "1359 [Discriminator loss: 0.437114, acc.: 75.00%] [Generator loss: 3.209307]\n",
      "1360 [Discriminator loss: 0.373638, acc.: 84.38%] [Generator loss: 3.679187]\n",
      "1361 [Discriminator loss: 0.322825, acc.: 85.94%] [Generator loss: 3.471835]\n",
      "1362 [Discriminator loss: 0.450162, acc.: 73.44%] [Generator loss: 3.271425]\n",
      "1363 [Discriminator loss: 0.419318, acc.: 79.69%] [Generator loss: 3.910603]\n",
      "1364 [Discriminator loss: 0.225987, acc.: 87.50%] [Generator loss: 3.191632]\n",
      "1365 [Discriminator loss: 0.573778, acc.: 67.19%] [Generator loss: 4.030728]\n",
      "1366 [Discriminator loss: 0.396999, acc.: 84.38%] [Generator loss: 3.626070]\n",
      "1367 [Discriminator loss: 0.394939, acc.: 81.25%] [Generator loss: 2.791432]\n",
      "1368 [Discriminator loss: 0.493006, acc.: 76.56%] [Generator loss: 2.935927]\n",
      "1369 [Discriminator loss: 0.312454, acc.: 87.50%] [Generator loss: 4.592510]\n",
      "1370 [Discriminator loss: 0.535877, acc.: 76.56%] [Generator loss: 2.872585]\n",
      "1371 [Discriminator loss: 0.514933, acc.: 79.69%] [Generator loss: 3.781261]\n",
      "1372 [Discriminator loss: 0.526678, acc.: 79.69%] [Generator loss: 3.200318]\n",
      "1373 [Discriminator loss: 1.053963, acc.: 45.31%] [Generator loss: 4.086221]\n",
      "1374 [Discriminator loss: 0.675686, acc.: 64.06%] [Generator loss: 3.361696]\n",
      "1375 [Discriminator loss: 0.558421, acc.: 76.56%] [Generator loss: 3.787516]\n",
      "1376 [Discriminator loss: 0.474186, acc.: 79.69%] [Generator loss: 3.628196]\n",
      "1377 [Discriminator loss: 0.309982, acc.: 84.38%] [Generator loss: 4.124784]\n",
      "1378 [Discriminator loss: 0.304203, acc.: 87.50%] [Generator loss: 3.819855]\n",
      "1379 [Discriminator loss: 0.762415, acc.: 62.50%] [Generator loss: 3.666267]\n",
      "1380 [Discriminator loss: 0.699297, acc.: 65.62%] [Generator loss: 3.852015]\n",
      "1381 [Discriminator loss: 0.360899, acc.: 81.25%] [Generator loss: 3.335826]\n",
      "1382 [Discriminator loss: 0.558458, acc.: 68.75%] [Generator loss: 4.070631]\n",
      "1383 [Discriminator loss: 0.286249, acc.: 89.06%] [Generator loss: 3.910641]\n",
      "1384 [Discriminator loss: 0.459125, acc.: 78.12%] [Generator loss: 3.302586]\n",
      "1385 [Discriminator loss: 0.645294, acc.: 65.62%] [Generator loss: 3.985426]\n",
      "1386 [Discriminator loss: 0.721831, acc.: 62.50%] [Generator loss: 3.728358]\n",
      "1387 [Discriminator loss: 0.896598, acc.: 60.94%] [Generator loss: 3.759659]\n",
      "1388 [Discriminator loss: 0.727351, acc.: 73.44%] [Generator loss: 3.528919]\n",
      "1389 [Discriminator loss: 0.729057, acc.: 62.50%] [Generator loss: 3.938523]\n",
      "1390 [Discriminator loss: 0.325193, acc.: 89.06%] [Generator loss: 3.108774]\n",
      "1391 [Discriminator loss: 0.486525, acc.: 81.25%] [Generator loss: 3.602217]\n",
      "1392 [Discriminator loss: 0.343808, acc.: 81.25%] [Generator loss: 3.768369]\n",
      "1393 [Discriminator loss: 0.443880, acc.: 78.12%] [Generator loss: 3.314117]\n",
      "1394 [Discriminator loss: 0.547282, acc.: 75.00%] [Generator loss: 3.774908]\n",
      "1395 [Discriminator loss: 0.479365, acc.: 75.00%] [Generator loss: 3.682061]\n",
      "1396 [Discriminator loss: 0.389887, acc.: 79.69%] [Generator loss: 3.310587]\n",
      "1397 [Discriminator loss: 0.421650, acc.: 78.12%] [Generator loss: 2.932455]\n",
      "1398 [Discriminator loss: 0.488067, acc.: 76.56%] [Generator loss: 3.986930]\n",
      "1399 [Discriminator loss: 0.471979, acc.: 76.56%] [Generator loss: 3.252483]\n",
      "1400 [Discriminator loss: 0.422323, acc.: 85.94%] [Generator loss: 3.258947]\n",
      "1401 [Discriminator loss: 0.271225, acc.: 89.06%] [Generator loss: 3.791967]\n",
      "1402 [Discriminator loss: 0.496116, acc.: 71.88%] [Generator loss: 3.613057]\n",
      "1403 [Discriminator loss: 0.721635, acc.: 68.75%] [Generator loss: 3.749541]\n",
      "1404 [Discriminator loss: 0.670278, acc.: 68.75%] [Generator loss: 3.174732]\n",
      "1405 [Discriminator loss: 0.404294, acc.: 76.56%] [Generator loss: 4.035665]\n",
      "1406 [Discriminator loss: 0.552417, acc.: 70.31%] [Generator loss: 3.273646]\n",
      "1407 [Discriminator loss: 0.437702, acc.: 78.12%] [Generator loss: 2.429302]\n",
      "1408 [Discriminator loss: 0.390919, acc.: 79.69%] [Generator loss: 3.855797]\n",
      "1409 [Discriminator loss: 0.320686, acc.: 87.50%] [Generator loss: 3.139315]\n",
      "1410 [Discriminator loss: 0.153632, acc.: 100.00%] [Generator loss: 3.752017]\n",
      "1411 [Discriminator loss: 0.426340, acc.: 75.00%] [Generator loss: 4.272614]\n",
      "1412 [Discriminator loss: 0.492122, acc.: 75.00%] [Generator loss: 3.289899]\n",
      "1413 [Discriminator loss: 1.507258, acc.: 34.38%] [Generator loss: 5.137059]\n",
      "1414 [Discriminator loss: 0.497115, acc.: 78.12%] [Generator loss: 4.001625]\n",
      "1415 [Discriminator loss: 0.408718, acc.: 76.56%] [Generator loss: 3.178002]\n",
      "1416 [Discriminator loss: 0.327899, acc.: 87.50%] [Generator loss: 3.647852]\n",
      "1417 [Discriminator loss: 0.810516, acc.: 62.50%] [Generator loss: 3.547499]\n",
      "1418 [Discriminator loss: 0.317604, acc.: 87.50%] [Generator loss: 3.585618]\n",
      "1419 [Discriminator loss: 0.531931, acc.: 75.00%] [Generator loss: 3.755654]\n",
      "1420 [Discriminator loss: 0.232379, acc.: 89.06%] [Generator loss: 3.547660]\n",
      "1421 [Discriminator loss: 0.512126, acc.: 79.69%] [Generator loss: 4.107552]\n",
      "1422 [Discriminator loss: 0.532388, acc.: 75.00%] [Generator loss: 3.176584]\n",
      "1423 [Discriminator loss: 0.303159, acc.: 89.06%] [Generator loss: 3.512190]\n",
      "1424 [Discriminator loss: 0.167194, acc.: 96.88%] [Generator loss: 2.843743]\n",
      "1425 [Discriminator loss: 0.628698, acc.: 70.31%] [Generator loss: 3.186229]\n",
      "1426 [Discriminator loss: 0.511350, acc.: 75.00%] [Generator loss: 3.908372]\n",
      "1427 [Discriminator loss: 0.360537, acc.: 82.81%] [Generator loss: 3.836470]\n",
      "1428 [Discriminator loss: 0.593306, acc.: 73.44%] [Generator loss: 3.321641]\n",
      "1429 [Discriminator loss: 0.564903, acc.: 70.31%] [Generator loss: 3.599850]\n",
      "1430 [Discriminator loss: 0.348588, acc.: 82.81%] [Generator loss: 3.519604]\n",
      "1431 [Discriminator loss: 0.420859, acc.: 79.69%] [Generator loss: 3.023942]\n",
      "1432 [Discriminator loss: 0.412235, acc.: 84.38%] [Generator loss: 4.522963]\n",
      "1433 [Discriminator loss: 0.861973, acc.: 56.25%] [Generator loss: 3.638372]\n",
      "1434 [Discriminator loss: 0.358193, acc.: 82.81%] [Generator loss: 3.616459]\n",
      "1435 [Discriminator loss: 0.493491, acc.: 79.69%] [Generator loss: 3.286417]\n",
      "1436 [Discriminator loss: 0.407753, acc.: 82.81%] [Generator loss: 3.983714]\n",
      "1437 [Discriminator loss: 0.656325, acc.: 71.88%] [Generator loss: 2.826655]\n",
      "1438 [Discriminator loss: 0.593718, acc.: 73.44%] [Generator loss: 3.234885]\n",
      "1439 [Discriminator loss: 0.366924, acc.: 87.50%] [Generator loss: 3.584470]\n",
      "1440 [Discriminator loss: 0.501551, acc.: 75.00%] [Generator loss: 4.147445]\n",
      "1441 [Discriminator loss: 0.357241, acc.: 85.94%] [Generator loss: 3.465289]\n",
      "1442 [Discriminator loss: 0.319748, acc.: 84.38%] [Generator loss: 4.154822]\n",
      "1443 [Discriminator loss: 0.408433, acc.: 81.25%] [Generator loss: 4.509951]\n",
      "1444 [Discriminator loss: 0.478795, acc.: 75.00%] [Generator loss: 4.001701]\n",
      "1445 [Discriminator loss: 0.327733, acc.: 84.38%] [Generator loss: 3.340031]\n",
      "1446 [Discriminator loss: 0.427211, acc.: 79.69%] [Generator loss: 3.638872]\n",
      "1447 [Discriminator loss: 0.464883, acc.: 79.69%] [Generator loss: 5.021100]\n",
      "1448 [Discriminator loss: 0.698231, acc.: 67.19%] [Generator loss: 2.266802]\n",
      "1449 [Discriminator loss: 0.386410, acc.: 78.12%] [Generator loss: 3.675912]\n",
      "1450 [Discriminator loss: 0.551968, acc.: 76.56%] [Generator loss: 3.796570]\n",
      "1451 [Discriminator loss: 0.412721, acc.: 85.94%] [Generator loss: 4.432672]\n",
      "1452 [Discriminator loss: 0.336075, acc.: 82.81%] [Generator loss: 4.169619]\n",
      "1453 [Discriminator loss: 0.351035, acc.: 85.94%] [Generator loss: 2.895036]\n",
      "1454 [Discriminator loss: 0.685851, acc.: 64.06%] [Generator loss: 4.636096]\n",
      "1455 [Discriminator loss: 0.463021, acc.: 81.25%] [Generator loss: 3.128366]\n",
      "1456 [Discriminator loss: 0.194383, acc.: 92.19%] [Generator loss: 2.725265]\n",
      "1457 [Discriminator loss: 0.615192, acc.: 75.00%] [Generator loss: 4.534769]\n",
      "1458 [Discriminator loss: 0.456125, acc.: 78.12%] [Generator loss: 3.125109]\n",
      "1459 [Discriminator loss: 0.522589, acc.: 73.44%] [Generator loss: 2.946016]\n",
      "1460 [Discriminator loss: 0.396016, acc.: 79.69%] [Generator loss: 3.034518]\n",
      "1461 [Discriminator loss: 0.351210, acc.: 79.69%] [Generator loss: 3.753498]\n",
      "1462 [Discriminator loss: 0.537819, acc.: 76.56%] [Generator loss: 3.176019]\n",
      "1463 [Discriminator loss: 0.341031, acc.: 82.81%] [Generator loss: 3.174354]\n",
      "1464 [Discriminator loss: 0.370187, acc.: 79.69%] [Generator loss: 4.188626]\n",
      "1465 [Discriminator loss: 0.557103, acc.: 73.44%] [Generator loss: 2.610283]\n",
      "1466 [Discriminator loss: 0.660090, acc.: 71.88%] [Generator loss: 4.591832]\n",
      "1467 [Discriminator loss: 0.426621, acc.: 81.25%] [Generator loss: 3.225123]\n",
      "1468 [Discriminator loss: 0.605687, acc.: 68.75%] [Generator loss: 3.404806]\n",
      "1469 [Discriminator loss: 0.628164, acc.: 75.00%] [Generator loss: 2.897906]\n",
      "1470 [Discriminator loss: 0.438848, acc.: 78.12%] [Generator loss: 4.011242]\n",
      "1471 [Discriminator loss: 0.679005, acc.: 64.06%] [Generator loss: 3.328274]\n",
      "1472 [Discriminator loss: 0.541713, acc.: 70.31%] [Generator loss: 3.826427]\n",
      "1473 [Discriminator loss: 0.308590, acc.: 85.94%] [Generator loss: 3.915309]\n",
      "1474 [Discriminator loss: 0.241005, acc.: 87.50%] [Generator loss: 4.866339]\n",
      "1475 [Discriminator loss: 0.558019, acc.: 71.88%] [Generator loss: 2.963777]\n",
      "1476 [Discriminator loss: 0.451105, acc.: 76.56%] [Generator loss: 3.355481]\n",
      "1477 [Discriminator loss: 0.203257, acc.: 92.19%] [Generator loss: 3.401547]\n",
      "1478 [Discriminator loss: 0.717988, acc.: 64.06%] [Generator loss: 3.507061]\n",
      "1479 [Discriminator loss: 0.400866, acc.: 82.81%] [Generator loss: 2.099671]\n",
      "1480 [Discriminator loss: 0.391688, acc.: 82.81%] [Generator loss: 3.470958]\n",
      "1481 [Discriminator loss: 0.337552, acc.: 79.69%] [Generator loss: 3.241979]\n",
      "1482 [Discriminator loss: 0.754835, acc.: 64.06%] [Generator loss: 3.705850]\n",
      "1483 [Discriminator loss: 0.382213, acc.: 90.62%] [Generator loss: 5.284822]\n",
      "1484 [Discriminator loss: 0.723214, acc.: 65.62%] [Generator loss: 2.878711]\n",
      "1485 [Discriminator loss: 0.260123, acc.: 87.50%] [Generator loss: 3.932881]\n",
      "1486 [Discriminator loss: 0.415027, acc.: 76.56%] [Generator loss: 3.829216]\n",
      "1487 [Discriminator loss: 0.554515, acc.: 73.44%] [Generator loss: 3.372629]\n",
      "1488 [Discriminator loss: 0.434675, acc.: 84.38%] [Generator loss: 3.231824]\n",
      "1489 [Discriminator loss: 0.293246, acc.: 90.62%] [Generator loss: 2.773485]\n",
      "1490 [Discriminator loss: 0.623375, acc.: 71.88%] [Generator loss: 3.507920]\n",
      "1491 [Discriminator loss: 0.381403, acc.: 87.50%] [Generator loss: 3.058831]\n",
      "1492 [Discriminator loss: 0.287563, acc.: 90.62%] [Generator loss: 3.419398]\n",
      "1493 [Discriminator loss: 0.386592, acc.: 85.94%] [Generator loss: 2.778268]\n",
      "1494 [Discriminator loss: 0.388397, acc.: 82.81%] [Generator loss: 2.513024]\n",
      "1495 [Discriminator loss: 0.530209, acc.: 71.88%] [Generator loss: 3.858556]\n",
      "1496 [Discriminator loss: 0.317410, acc.: 87.50%] [Generator loss: 4.089760]\n",
      "1497 [Discriminator loss: 0.439900, acc.: 79.69%] [Generator loss: 3.530200]\n",
      "1498 [Discriminator loss: 0.401689, acc.: 81.25%] [Generator loss: 3.705016]\n",
      "1499 [Discriminator loss: 0.442434, acc.: 84.38%] [Generator loss: 3.449043]\n",
      "1500 [Discriminator loss: 0.418441, acc.: 79.69%] [Generator loss: 3.848067]\n",
      "1501 [Discriminator loss: 0.516039, acc.: 73.44%] [Generator loss: 3.434220]\n",
      "1502 [Discriminator loss: 0.253459, acc.: 90.62%] [Generator loss: 3.988582]\n",
      "1503 [Discriminator loss: 0.293396, acc.: 90.62%] [Generator loss: 3.747590]\n",
      "1504 [Discriminator loss: 0.490380, acc.: 75.00%] [Generator loss: 3.879464]\n",
      "1505 [Discriminator loss: 0.332335, acc.: 82.81%] [Generator loss: 4.250327]\n",
      "1506 [Discriminator loss: 0.518757, acc.: 78.12%] [Generator loss: 3.001101]\n",
      "1507 [Discriminator loss: 0.750732, acc.: 62.50%] [Generator loss: 4.942224]\n",
      "1508 [Discriminator loss: 0.237053, acc.: 89.06%] [Generator loss: 3.181615]\n",
      "1509 [Discriminator loss: 0.900265, acc.: 60.94%] [Generator loss: 4.656467]\n",
      "1510 [Discriminator loss: 0.308930, acc.: 89.06%] [Generator loss: 3.622512]\n",
      "1511 [Discriminator loss: 0.308593, acc.: 87.50%] [Generator loss: 3.616670]\n",
      "1512 [Discriminator loss: 0.355322, acc.: 82.81%] [Generator loss: 5.294252]\n",
      "1513 [Discriminator loss: 0.244934, acc.: 90.62%] [Generator loss: 3.769310]\n",
      "1514 [Discriminator loss: 0.471664, acc.: 78.12%] [Generator loss: 2.234630]\n",
      "1515 [Discriminator loss: 0.336985, acc.: 87.50%] [Generator loss: 3.146399]\n",
      "1516 [Discriminator loss: 0.182428, acc.: 93.75%] [Generator loss: 3.257786]\n",
      "1517 [Discriminator loss: 0.394579, acc.: 82.81%] [Generator loss: 3.348664]\n",
      "1518 [Discriminator loss: 0.179872, acc.: 92.19%] [Generator loss: 2.756465]\n",
      "1519 [Discriminator loss: 0.344622, acc.: 84.38%] [Generator loss: 3.033933]\n",
      "1520 [Discriminator loss: 0.440761, acc.: 82.81%] [Generator loss: 3.181345]\n",
      "1521 [Discriminator loss: 0.482337, acc.: 75.00%] [Generator loss: 3.820139]\n",
      "1522 [Discriminator loss: 0.308236, acc.: 85.94%] [Generator loss: 3.580781]\n",
      "1523 [Discriminator loss: 0.582346, acc.: 78.12%] [Generator loss: 3.828768]\n",
      "1524 [Discriminator loss: 0.306210, acc.: 90.62%] [Generator loss: 2.242307]\n",
      "1525 [Discriminator loss: 0.369133, acc.: 85.94%] [Generator loss: 3.254698]\n",
      "1526 [Discriminator loss: 0.217872, acc.: 90.62%] [Generator loss: 2.919576]\n",
      "1527 [Discriminator loss: 0.487080, acc.: 78.12%] [Generator loss: 2.266387]\n",
      "1528 [Discriminator loss: 0.601950, acc.: 73.44%] [Generator loss: 3.533001]\n",
      "1529 [Discriminator loss: 0.479533, acc.: 81.25%] [Generator loss: 3.251336]\n",
      "1530 [Discriminator loss: 0.517087, acc.: 75.00%] [Generator loss: 3.860445]\n",
      "1531 [Discriminator loss: 0.381513, acc.: 81.25%] [Generator loss: 3.483532]\n",
      "1532 [Discriminator loss: 0.540891, acc.: 79.69%] [Generator loss: 4.475537]\n",
      "1533 [Discriminator loss: 0.264489, acc.: 90.62%] [Generator loss: 4.115786]\n",
      "1534 [Discriminator loss: 0.492451, acc.: 79.69%] [Generator loss: 3.711075]\n",
      "1535 [Discriminator loss: 0.641558, acc.: 67.19%] [Generator loss: 3.964984]\n",
      "1536 [Discriminator loss: 0.815656, acc.: 64.06%] [Generator loss: 3.753688]\n",
      "1537 [Discriminator loss: 0.781543, acc.: 59.38%] [Generator loss: 3.945082]\n",
      "1538 [Discriminator loss: 0.196140, acc.: 85.94%] [Generator loss: 4.862479]\n",
      "1539 [Discriminator loss: 0.645876, acc.: 73.44%] [Generator loss: 5.134499]\n",
      "1540 [Discriminator loss: 0.194494, acc.: 92.19%] [Generator loss: 4.430591]\n",
      "1541 [Discriminator loss: 0.713993, acc.: 64.06%] [Generator loss: 3.437309]\n",
      "1542 [Discriminator loss: 0.405365, acc.: 82.81%] [Generator loss: 4.503833]\n",
      "1543 [Discriminator loss: 0.395325, acc.: 84.38%] [Generator loss: 3.094762]\n",
      "1544 [Discriminator loss: 0.359956, acc.: 82.81%] [Generator loss: 3.855220]\n",
      "1545 [Discriminator loss: 0.357066, acc.: 84.38%] [Generator loss: 3.533663]\n",
      "1546 [Discriminator loss: 0.991266, acc.: 56.25%] [Generator loss: 3.839581]\n",
      "1547 [Discriminator loss: 0.495911, acc.: 78.12%] [Generator loss: 4.093900]\n",
      "1548 [Discriminator loss: 0.439173, acc.: 81.25%] [Generator loss: 3.353992]\n",
      "1549 [Discriminator loss: 0.435180, acc.: 82.81%] [Generator loss: 3.066419]\n",
      "1550 [Discriminator loss: 0.311748, acc.: 89.06%] [Generator loss: 3.762807]\n",
      "1551 [Discriminator loss: 0.377086, acc.: 82.81%] [Generator loss: 4.786864]\n",
      "1552 [Discriminator loss: 0.589935, acc.: 71.88%] [Generator loss: 3.255275]\n",
      "1553 [Discriminator loss: 0.393212, acc.: 81.25%] [Generator loss: 4.080510]\n",
      "1554 [Discriminator loss: 0.551489, acc.: 75.00%] [Generator loss: 4.119850]\n",
      "1555 [Discriminator loss: 0.815681, acc.: 62.50%] [Generator loss: 4.766202]\n",
      "1556 [Discriminator loss: 0.387270, acc.: 85.94%] [Generator loss: 3.602226]\n",
      "1557 [Discriminator loss: 0.199848, acc.: 92.19%] [Generator loss: 3.473373]\n",
      "1558 [Discriminator loss: 0.423670, acc.: 79.69%] [Generator loss: 3.635210]\n",
      "1559 [Discriminator loss: 0.837413, acc.: 57.81%] [Generator loss: 3.083640]\n",
      "1560 [Discriminator loss: 0.295409, acc.: 84.38%] [Generator loss: 4.705384]\n",
      "1561 [Discriminator loss: 0.502339, acc.: 75.00%] [Generator loss: 3.983058]\n",
      "1562 [Discriminator loss: 0.447782, acc.: 82.81%] [Generator loss: 2.770216]\n",
      "1563 [Discriminator loss: 0.748627, acc.: 67.19%] [Generator loss: 4.115632]\n",
      "1564 [Discriminator loss: 0.480346, acc.: 76.56%] [Generator loss: 3.587631]\n",
      "1565 [Discriminator loss: 0.781962, acc.: 56.25%] [Generator loss: 5.124187]\n",
      "1566 [Discriminator loss: 0.460004, acc.: 79.69%] [Generator loss: 2.441635]\n",
      "1567 [Discriminator loss: 0.512951, acc.: 78.12%] [Generator loss: 5.277110]\n",
      "1568 [Discriminator loss: 0.457341, acc.: 78.12%] [Generator loss: 4.041018]\n",
      "1569 [Discriminator loss: 0.572880, acc.: 71.88%] [Generator loss: 3.709661]\n",
      "1570 [Discriminator loss: 0.602519, acc.: 71.88%] [Generator loss: 3.095341]\n",
      "1571 [Discriminator loss: 0.442571, acc.: 79.69%] [Generator loss: 2.553864]\n",
      "1572 [Discriminator loss: 0.458357, acc.: 73.44%] [Generator loss: 3.261876]\n",
      "1573 [Discriminator loss: 0.394616, acc.: 79.69%] [Generator loss: 4.370214]\n",
      "1574 [Discriminator loss: 0.530280, acc.: 76.56%] [Generator loss: 3.331091]\n",
      "1575 [Discriminator loss: 0.524824, acc.: 71.88%] [Generator loss: 4.110907]\n",
      "1576 [Discriminator loss: 0.307959, acc.: 84.38%] [Generator loss: 3.108072]\n",
      "1577 [Discriminator loss: 0.269088, acc.: 89.06%] [Generator loss: 3.729251]\n",
      "1578 [Discriminator loss: 0.861607, acc.: 75.00%] [Generator loss: 5.041926]\n",
      "1579 [Discriminator loss: 0.367666, acc.: 82.81%] [Generator loss: 4.478340]\n",
      "1580 [Discriminator loss: 0.447142, acc.: 81.25%] [Generator loss: 3.679265]\n",
      "1581 [Discriminator loss: 0.210366, acc.: 93.75%] [Generator loss: 3.754219]\n",
      "1582 [Discriminator loss: 0.208433, acc.: 92.19%] [Generator loss: 3.681272]\n",
      "1583 [Discriminator loss: 0.373972, acc.: 81.25%] [Generator loss: 2.787590]\n",
      "1584 [Discriminator loss: 0.451961, acc.: 79.69%] [Generator loss: 3.216336]\n",
      "1585 [Discriminator loss: 0.284506, acc.: 89.06%] [Generator loss: 2.813953]\n",
      "1586 [Discriminator loss: 0.739142, acc.: 65.62%] [Generator loss: 3.367416]\n",
      "1587 [Discriminator loss: 0.301505, acc.: 87.50%] [Generator loss: 3.850680]\n",
      "1588 [Discriminator loss: 0.797643, acc.: 67.19%] [Generator loss: 4.089993]\n",
      "1589 [Discriminator loss: 0.540297, acc.: 68.75%] [Generator loss: 4.891810]\n",
      "1590 [Discriminator loss: 0.640538, acc.: 68.75%] [Generator loss: 5.086327]\n",
      "1591 [Discriminator loss: 0.271397, acc.: 89.06%] [Generator loss: 4.569710]\n",
      "1592 [Discriminator loss: 0.354481, acc.: 84.38%] [Generator loss: 4.641537]\n",
      "1593 [Discriminator loss: 0.561702, acc.: 76.56%] [Generator loss: 4.028337]\n",
      "1594 [Discriminator loss: 0.437899, acc.: 75.00%] [Generator loss: 2.970897]\n",
      "1595 [Discriminator loss: 0.249430, acc.: 90.62%] [Generator loss: 3.816680]\n",
      "1596 [Discriminator loss: 0.252731, acc.: 89.06%] [Generator loss: 4.730109]\n",
      "1597 [Discriminator loss: 0.478752, acc.: 75.00%] [Generator loss: 3.340789]\n",
      "1598 [Discriminator loss: 0.693583, acc.: 68.75%] [Generator loss: 3.509542]\n",
      "1599 [Discriminator loss: 0.438701, acc.: 81.25%] [Generator loss: 4.367846]\n",
      "1600 [Discriminator loss: 0.408363, acc.: 82.81%] [Generator loss: 3.569189]\n",
      "1601 [Discriminator loss: 0.621416, acc.: 67.19%] [Generator loss: 4.052838]\n",
      "1602 [Discriminator loss: 0.294993, acc.: 90.62%] [Generator loss: 3.917411]\n",
      "1603 [Discriminator loss: 0.985028, acc.: 54.69%] [Generator loss: 2.749699]\n",
      "1604 [Discriminator loss: 0.434176, acc.: 78.12%] [Generator loss: 3.478689]\n",
      "1605 [Discriminator loss: 0.352901, acc.: 84.38%] [Generator loss: 3.230584]\n",
      "1606 [Discriminator loss: 0.472202, acc.: 76.56%] [Generator loss: 3.760308]\n",
      "1607 [Discriminator loss: 0.553056, acc.: 70.31%] [Generator loss: 4.375049]\n",
      "1608 [Discriminator loss: 0.481088, acc.: 79.69%] [Generator loss: 3.553829]\n",
      "1609 [Discriminator loss: 0.435268, acc.: 81.25%] [Generator loss: 4.179729]\n",
      "1610 [Discriminator loss: 0.249941, acc.: 90.62%] [Generator loss: 3.851808]\n",
      "1611 [Discriminator loss: 0.571877, acc.: 76.56%] [Generator loss: 4.572132]\n",
      "1612 [Discriminator loss: 0.532327, acc.: 79.69%] [Generator loss: 3.609027]\n",
      "1613 [Discriminator loss: 0.683588, acc.: 71.88%] [Generator loss: 5.095670]\n",
      "1614 [Discriminator loss: 0.370381, acc.: 85.94%] [Generator loss: 4.227208]\n",
      "1615 [Discriminator loss: 0.554056, acc.: 68.75%] [Generator loss: 4.167437]\n",
      "1616 [Discriminator loss: 0.400028, acc.: 78.12%] [Generator loss: 3.163137]\n",
      "1617 [Discriminator loss: 0.605192, acc.: 67.19%] [Generator loss: 3.506666]\n",
      "1618 [Discriminator loss: 0.664870, acc.: 64.06%] [Generator loss: 3.744326]\n",
      "1619 [Discriminator loss: 0.421801, acc.: 82.81%] [Generator loss: 5.044347]\n",
      "1620 [Discriminator loss: 0.353449, acc.: 82.81%] [Generator loss: 3.612122]\n",
      "1621 [Discriminator loss: 0.264488, acc.: 87.50%] [Generator loss: 3.354826]\n",
      "1622 [Discriminator loss: 0.328463, acc.: 84.38%] [Generator loss: 3.468527]\n",
      "1623 [Discriminator loss: 0.232202, acc.: 92.19%] [Generator loss: 3.517498]\n",
      "1624 [Discriminator loss: 0.424084, acc.: 82.81%] [Generator loss: 4.335347]\n",
      "1625 [Discriminator loss: 0.390118, acc.: 82.81%] [Generator loss: 2.772468]\n",
      "1626 [Discriminator loss: 0.271328, acc.: 84.38%] [Generator loss: 3.946500]\n",
      "1627 [Discriminator loss: 0.627059, acc.: 71.88%] [Generator loss: 5.448749]\n",
      "1628 [Discriminator loss: 0.517019, acc.: 79.69%] [Generator loss: 3.461483]\n",
      "1629 [Discriminator loss: 0.412610, acc.: 76.56%] [Generator loss: 2.995550]\n",
      "1630 [Discriminator loss: 0.532476, acc.: 70.31%] [Generator loss: 5.045770]\n",
      "1631 [Discriminator loss: 0.651079, acc.: 71.88%] [Generator loss: 2.602863]\n",
      "1632 [Discriminator loss: 0.473488, acc.: 76.56%] [Generator loss: 4.659102]\n",
      "1633 [Discriminator loss: 0.411641, acc.: 81.25%] [Generator loss: 3.691623]\n",
      "1634 [Discriminator loss: 0.470461, acc.: 76.56%] [Generator loss: 3.496802]\n",
      "1635 [Discriminator loss: 0.663462, acc.: 64.06%] [Generator loss: 4.960252]\n",
      "1636 [Discriminator loss: 0.423532, acc.: 82.81%] [Generator loss: 3.764949]\n",
      "1637 [Discriminator loss: 0.400457, acc.: 79.69%] [Generator loss: 2.511893]\n",
      "1638 [Discriminator loss: 0.280828, acc.: 84.38%] [Generator loss: 3.521290]\n",
      "1639 [Discriminator loss: 0.340505, acc.: 85.94%] [Generator loss: 2.731364]\n",
      "1640 [Discriminator loss: 0.373513, acc.: 78.12%] [Generator loss: 4.186380]\n",
      "1641 [Discriminator loss: 0.351263, acc.: 82.81%] [Generator loss: 3.579301]\n",
      "1642 [Discriminator loss: 0.544315, acc.: 79.69%] [Generator loss: 4.595615]\n",
      "1643 [Discriminator loss: 0.490973, acc.: 79.69%] [Generator loss: 2.954406]\n",
      "1644 [Discriminator loss: 0.260009, acc.: 87.50%] [Generator loss: 2.687887]\n",
      "1645 [Discriminator loss: 0.260135, acc.: 95.31%] [Generator loss: 3.098533]\n",
      "1646 [Discriminator loss: 0.384090, acc.: 82.81%] [Generator loss: 3.704005]\n",
      "1647 [Discriminator loss: 0.342232, acc.: 84.38%] [Generator loss: 4.142442]\n",
      "1648 [Discriminator loss: 0.480009, acc.: 76.56%] [Generator loss: 3.223128]\n",
      "1649 [Discriminator loss: 0.212868, acc.: 90.62%] [Generator loss: 2.729222]\n",
      "1650 [Discriminator loss: 0.342773, acc.: 85.94%] [Generator loss: 4.188269]\n",
      "1651 [Discriminator loss: 0.264297, acc.: 87.50%] [Generator loss: 4.170047]\n",
      "1652 [Discriminator loss: 0.371322, acc.: 87.50%] [Generator loss: 2.932189]\n",
      "1653 [Discriminator loss: 0.587450, acc.: 75.00%] [Generator loss: 3.477505]\n",
      "1654 [Discriminator loss: 0.332438, acc.: 85.94%] [Generator loss: 4.319663]\n",
      "1655 [Discriminator loss: 0.438956, acc.: 79.69%] [Generator loss: 3.137473]\n",
      "1656 [Discriminator loss: 0.538523, acc.: 81.25%] [Generator loss: 3.936844]\n",
      "1657 [Discriminator loss: 0.441036, acc.: 79.69%] [Generator loss: 3.209843]\n",
      "1658 [Discriminator loss: 0.262035, acc.: 89.06%] [Generator loss: 4.195950]\n",
      "1659 [Discriminator loss: 0.444173, acc.: 81.25%] [Generator loss: 2.483959]\n",
      "1660 [Discriminator loss: 0.506475, acc.: 75.00%] [Generator loss: 4.815047]\n",
      "1661 [Discriminator loss: 0.816919, acc.: 65.62%] [Generator loss: 3.722322]\n",
      "1662 [Discriminator loss: 0.389005, acc.: 79.69%] [Generator loss: 2.911432]\n",
      "1663 [Discriminator loss: 0.443544, acc.: 78.12%] [Generator loss: 4.313260]\n",
      "1664 [Discriminator loss: 0.196606, acc.: 95.31%] [Generator loss: 3.763624]\n",
      "1665 [Discriminator loss: 0.828131, acc.: 67.19%] [Generator loss: 4.704101]\n",
      "1666 [Discriminator loss: 0.598137, acc.: 75.00%] [Generator loss: 3.100530]\n",
      "1667 [Discriminator loss: 0.565720, acc.: 75.00%] [Generator loss: 4.525085]\n",
      "1668 [Discriminator loss: 0.322155, acc.: 89.06%] [Generator loss: 4.390103]\n",
      "1669 [Discriminator loss: 0.507170, acc.: 79.69%] [Generator loss: 2.589525]\n",
      "1670 [Discriminator loss: 0.503344, acc.: 75.00%] [Generator loss: 4.490712]\n",
      "1671 [Discriminator loss: 1.049131, acc.: 48.44%] [Generator loss: 3.625371]\n",
      "1672 [Discriminator loss: 0.681305, acc.: 64.06%] [Generator loss: 3.856562]\n",
      "1673 [Discriminator loss: 0.484185, acc.: 78.12%] [Generator loss: 4.090248]\n",
      "1674 [Discriminator loss: 0.177025, acc.: 92.19%] [Generator loss: 4.142481]\n",
      "1675 [Discriminator loss: 0.481509, acc.: 79.69%] [Generator loss: 4.346865]\n",
      "1676 [Discriminator loss: 0.297706, acc.: 85.94%] [Generator loss: 3.848780]\n",
      "1677 [Discriminator loss: 0.415042, acc.: 79.69%] [Generator loss: 3.933190]\n",
      "1678 [Discriminator loss: 0.524525, acc.: 75.00%] [Generator loss: 5.086495]\n",
      "1679 [Discriminator loss: 0.351962, acc.: 82.81%] [Generator loss: 3.357274]\n",
      "1680 [Discriminator loss: 0.456037, acc.: 76.56%] [Generator loss: 4.204817]\n",
      "1681 [Discriminator loss: 0.649497, acc.: 73.44%] [Generator loss: 2.765166]\n",
      "1682 [Discriminator loss: 0.498327, acc.: 79.69%] [Generator loss: 2.634164]\n",
      "1683 [Discriminator loss: 0.260877, acc.: 93.75%] [Generator loss: 3.997077]\n",
      "1684 [Discriminator loss: 0.445703, acc.: 85.94%] [Generator loss: 2.318768]\n",
      "1685 [Discriminator loss: 0.325029, acc.: 84.38%] [Generator loss: 3.081333]\n",
      "1686 [Discriminator loss: 0.416077, acc.: 76.56%] [Generator loss: 2.993466]\n",
      "1687 [Discriminator loss: 0.407927, acc.: 81.25%] [Generator loss: 3.676565]\n",
      "1688 [Discriminator loss: 0.379447, acc.: 81.25%] [Generator loss: 3.549885]\n",
      "1689 [Discriminator loss: 0.465820, acc.: 79.69%] [Generator loss: 2.622010]\n",
      "1690 [Discriminator loss: 0.300912, acc.: 90.62%] [Generator loss: 4.225165]\n",
      "1691 [Discriminator loss: 0.325912, acc.: 81.25%] [Generator loss: 2.978955]\n",
      "1692 [Discriminator loss: 0.601952, acc.: 71.88%] [Generator loss: 3.754986]\n",
      "1693 [Discriminator loss: 0.252943, acc.: 87.50%] [Generator loss: 4.971644]\n",
      "1694 [Discriminator loss: 0.406510, acc.: 79.69%] [Generator loss: 3.004943]\n",
      "1695 [Discriminator loss: 0.421506, acc.: 81.25%] [Generator loss: 3.637744]\n",
      "1696 [Discriminator loss: 0.292611, acc.: 85.94%] [Generator loss: 3.908192]\n",
      "1697 [Discriminator loss: 0.311096, acc.: 85.94%] [Generator loss: 3.085567]\n",
      "1698 [Discriminator loss: 0.497193, acc.: 78.12%] [Generator loss: 3.849309]\n",
      "1699 [Discriminator loss: 0.345543, acc.: 85.94%] [Generator loss: 3.998329]\n",
      "1700 [Discriminator loss: 0.392154, acc.: 81.25%] [Generator loss: 3.061074]\n",
      "1701 [Discriminator loss: 0.208569, acc.: 95.31%] [Generator loss: 3.987988]\n",
      "1702 [Discriminator loss: 0.728281, acc.: 67.19%] [Generator loss: 3.459228]\n",
      "1703 [Discriminator loss: 0.349919, acc.: 81.25%] [Generator loss: 3.062898]\n",
      "1704 [Discriminator loss: 0.615796, acc.: 76.56%] [Generator loss: 5.291427]\n",
      "1705 [Discriminator loss: 0.809965, acc.: 67.19%] [Generator loss: 3.091378]\n",
      "1706 [Discriminator loss: 0.437108, acc.: 82.81%] [Generator loss: 2.772110]\n",
      "1707 [Discriminator loss: 0.583985, acc.: 73.44%] [Generator loss: 3.857681]\n",
      "1708 [Discriminator loss: 0.377735, acc.: 85.94%] [Generator loss: 3.955168]\n",
      "1709 [Discriminator loss: 0.413139, acc.: 73.44%] [Generator loss: 3.346964]\n",
      "1710 [Discriminator loss: 0.715330, acc.: 67.19%] [Generator loss: 4.266397]\n",
      "1711 [Discriminator loss: 0.659197, acc.: 70.31%] [Generator loss: 3.678646]\n",
      "1712 [Discriminator loss: 0.689924, acc.: 73.44%] [Generator loss: 3.695656]\n",
      "1713 [Discriminator loss: 0.248753, acc.: 92.19%] [Generator loss: 3.974707]\n",
      "1714 [Discriminator loss: 0.500625, acc.: 73.44%] [Generator loss: 3.175710]\n",
      "1715 [Discriminator loss: 0.553376, acc.: 76.56%] [Generator loss: 2.722336]\n",
      "1716 [Discriminator loss: 0.409916, acc.: 85.94%] [Generator loss: 3.811579]\n",
      "1717 [Discriminator loss: 0.636748, acc.: 75.00%] [Generator loss: 3.947846]\n",
      "1718 [Discriminator loss: 0.486387, acc.: 76.56%] [Generator loss: 2.731889]\n",
      "1719 [Discriminator loss: 0.558775, acc.: 71.88%] [Generator loss: 4.544945]\n",
      "1720 [Discriminator loss: 0.446305, acc.: 76.56%] [Generator loss: 3.075758]\n",
      "1721 [Discriminator loss: 0.428723, acc.: 76.56%] [Generator loss: 4.925403]\n",
      "1722 [Discriminator loss: 0.435330, acc.: 79.69%] [Generator loss: 3.734446]\n",
      "1723 [Discriminator loss: 0.265125, acc.: 90.62%] [Generator loss: 3.301445]\n",
      "1724 [Discriminator loss: 0.523204, acc.: 81.25%] [Generator loss: 3.967157]\n",
      "1725 [Discriminator loss: 0.375062, acc.: 84.38%] [Generator loss: 4.696506]\n",
      "1726 [Discriminator loss: 0.394928, acc.: 84.38%] [Generator loss: 3.876788]\n",
      "1727 [Discriminator loss: 0.536465, acc.: 70.31%] [Generator loss: 3.786409]\n",
      "1728 [Discriminator loss: 0.352975, acc.: 85.94%] [Generator loss: 3.111991]\n",
      "1729 [Discriminator loss: 0.282038, acc.: 85.94%] [Generator loss: 2.883646]\n",
      "1730 [Discriminator loss: 0.266961, acc.: 90.62%] [Generator loss: 3.459084]\n",
      "1731 [Discriminator loss: 0.236114, acc.: 90.62%] [Generator loss: 3.777652]\n",
      "1732 [Discriminator loss: 0.853726, acc.: 62.50%] [Generator loss: 3.495753]\n",
      "1733 [Discriminator loss: 0.506030, acc.: 68.75%] [Generator loss: 3.332784]\n",
      "1734 [Discriminator loss: 0.330904, acc.: 84.38%] [Generator loss: 4.020262]\n",
      "1735 [Discriminator loss: 0.292099, acc.: 85.94%] [Generator loss: 4.312639]\n",
      "1736 [Discriminator loss: 0.456500, acc.: 78.12%] [Generator loss: 3.845490]\n",
      "1737 [Discriminator loss: 0.338788, acc.: 85.94%] [Generator loss: 3.694151]\n",
      "1738 [Discriminator loss: 0.665531, acc.: 67.19%] [Generator loss: 4.694996]\n",
      "1739 [Discriminator loss: 0.424238, acc.: 82.81%] [Generator loss: 4.172040]\n",
      "1740 [Discriminator loss: 0.360113, acc.: 85.94%] [Generator loss: 3.670464]\n",
      "1741 [Discriminator loss: 0.560032, acc.: 73.44%] [Generator loss: 3.490628]\n",
      "1742 [Discriminator loss: 0.397589, acc.: 82.81%] [Generator loss: 3.433652]\n",
      "1743 [Discriminator loss: 0.504620, acc.: 68.75%] [Generator loss: 4.870711]\n",
      "1744 [Discriminator loss: 0.386480, acc.: 85.94%] [Generator loss: 3.170008]\n",
      "1745 [Discriminator loss: 0.272014, acc.: 89.06%] [Generator loss: 3.098823]\n",
      "1746 [Discriminator loss: 0.522360, acc.: 78.12%] [Generator loss: 4.070580]\n",
      "1747 [Discriminator loss: 0.377541, acc.: 79.69%] [Generator loss: 4.020920]\n",
      "1748 [Discriminator loss: 0.525831, acc.: 79.69%] [Generator loss: 4.116659]\n",
      "1749 [Discriminator loss: 0.797658, acc.: 64.06%] [Generator loss: 3.817743]\n",
      "1750 [Discriminator loss: 0.495480, acc.: 75.00%] [Generator loss: 3.786586]\n",
      "1751 [Discriminator loss: 0.484463, acc.: 81.25%] [Generator loss: 3.789447]\n",
      "1752 [Discriminator loss: 0.449036, acc.: 81.25%] [Generator loss: 3.739859]\n",
      "1753 [Discriminator loss: 0.821768, acc.: 64.06%] [Generator loss: 4.897656]\n",
      "1754 [Discriminator loss: 0.292038, acc.: 87.50%] [Generator loss: 4.398821]\n",
      "1755 [Discriminator loss: 0.500657, acc.: 76.56%] [Generator loss: 3.041758]\n",
      "1756 [Discriminator loss: 0.525062, acc.: 79.69%] [Generator loss: 2.910343]\n",
      "1757 [Discriminator loss: 0.192474, acc.: 92.19%] [Generator loss: 3.923254]\n",
      "1758 [Discriminator loss: 0.503535, acc.: 79.69%] [Generator loss: 3.768786]\n",
      "1759 [Discriminator loss: 0.253413, acc.: 87.50%] [Generator loss: 3.355445]\n",
      "1760 [Discriminator loss: 0.249868, acc.: 87.50%] [Generator loss: 3.532500]\n",
      "1761 [Discriminator loss: 1.058933, acc.: 56.25%] [Generator loss: 5.231298]\n",
      "1762 [Discriminator loss: 0.406438, acc.: 76.56%] [Generator loss: 3.947361]\n",
      "1763 [Discriminator loss: 0.682926, acc.: 68.75%] [Generator loss: 3.937807]\n",
      "1764 [Discriminator loss: 0.304333, acc.: 87.50%] [Generator loss: 4.593141]\n",
      "1765 [Discriminator loss: 0.431032, acc.: 78.12%] [Generator loss: 3.318946]\n",
      "1766 [Discriminator loss: 0.308909, acc.: 84.38%] [Generator loss: 4.247612]\n",
      "1767 [Discriminator loss: 0.564682, acc.: 75.00%] [Generator loss: 3.431608]\n",
      "1768 [Discriminator loss: 0.199057, acc.: 92.19%] [Generator loss: 4.340590]\n",
      "1769 [Discriminator loss: 0.713331, acc.: 59.38%] [Generator loss: 4.057891]\n",
      "1770 [Discriminator loss: 0.185988, acc.: 92.19%] [Generator loss: 3.975462]\n",
      "1771 [Discriminator loss: 0.594827, acc.: 68.75%] [Generator loss: 3.720143]\n",
      "1772 [Discriminator loss: 0.449064, acc.: 79.69%] [Generator loss: 4.098251]\n",
      "1773 [Discriminator loss: 0.373117, acc.: 78.12%] [Generator loss: 3.482591]\n",
      "1774 [Discriminator loss: 0.537993, acc.: 78.12%] [Generator loss: 3.826773]\n",
      "1775 [Discriminator loss: 0.366053, acc.: 81.25%] [Generator loss: 3.810203]\n",
      "1776 [Discriminator loss: 0.528063, acc.: 78.12%] [Generator loss: 4.140795]\n",
      "1777 [Discriminator loss: 0.519298, acc.: 79.69%] [Generator loss: 3.352034]\n",
      "1778 [Discriminator loss: 0.355503, acc.: 82.81%] [Generator loss: 3.264600]\n",
      "1779 [Discriminator loss: 0.515909, acc.: 76.56%] [Generator loss: 4.227152]\n",
      "1780 [Discriminator loss: 0.384161, acc.: 79.69%] [Generator loss: 4.641817]\n",
      "1781 [Discriminator loss: 0.442631, acc.: 79.69%] [Generator loss: 2.441157]\n",
      "1782 [Discriminator loss: 0.304861, acc.: 87.50%] [Generator loss: 4.667477]\n",
      "1783 [Discriminator loss: 0.441125, acc.: 82.81%] [Generator loss: 3.547412]\n",
      "1784 [Discriminator loss: 0.398367, acc.: 76.56%] [Generator loss: 3.384360]\n",
      "1785 [Discriminator loss: 0.333595, acc.: 85.94%] [Generator loss: 2.984770]\n",
      "1786 [Discriminator loss: 0.250784, acc.: 89.06%] [Generator loss: 3.758777]\n",
      "1787 [Discriminator loss: 0.284961, acc.: 90.62%] [Generator loss: 3.265284]\n",
      "1788 [Discriminator loss: 0.266061, acc.: 89.06%] [Generator loss: 3.911103]\n",
      "1789 [Discriminator loss: 0.360535, acc.: 82.81%] [Generator loss: 2.810542]\n",
      "1790 [Discriminator loss: 0.360103, acc.: 82.81%] [Generator loss: 3.117958]\n",
      "1791 [Discriminator loss: 0.287373, acc.: 89.06%] [Generator loss: 3.337113]\n",
      "1792 [Discriminator loss: 0.739647, acc.: 67.19%] [Generator loss: 3.382518]\n",
      "1793 [Discriminator loss: 0.366630, acc.: 81.25%] [Generator loss: 4.879274]\n",
      "1794 [Discriminator loss: 0.388842, acc.: 82.81%] [Generator loss: 3.408997]\n",
      "1795 [Discriminator loss: 0.711997, acc.: 67.19%] [Generator loss: 3.427514]\n",
      "1796 [Discriminator loss: 0.384989, acc.: 78.12%] [Generator loss: 3.743156]\n",
      "1797 [Discriminator loss: 0.567499, acc.: 73.44%] [Generator loss: 4.330125]\n",
      "1798 [Discriminator loss: 0.345428, acc.: 84.38%] [Generator loss: 4.281711]\n",
      "1799 [Discriminator loss: 0.650150, acc.: 68.75%] [Generator loss: 3.978764]\n",
      "1800 [Discriminator loss: 0.423637, acc.: 78.12%] [Generator loss: 4.435406]\n",
      "1801 [Discriminator loss: 0.233414, acc.: 87.50%] [Generator loss: 3.813113]\n",
      "1802 [Discriminator loss: 0.254763, acc.: 89.06%] [Generator loss: 3.915474]\n",
      "1803 [Discriminator loss: 0.388817, acc.: 79.69%] [Generator loss: 3.791603]\n",
      "1804 [Discriminator loss: 0.300347, acc.: 90.62%] [Generator loss: 4.605873]\n",
      "1805 [Discriminator loss: 0.420114, acc.: 82.81%] [Generator loss: 4.127543]\n",
      "1806 [Discriminator loss: 0.184050, acc.: 93.75%] [Generator loss: 3.819326]\n",
      "1807 [Discriminator loss: 0.566070, acc.: 68.75%] [Generator loss: 3.685438]\n",
      "1808 [Discriminator loss: 0.329712, acc.: 85.94%] [Generator loss: 3.999984]\n",
      "1809 [Discriminator loss: 0.435255, acc.: 78.12%] [Generator loss: 2.926499]\n",
      "1810 [Discriminator loss: 0.203981, acc.: 93.75%] [Generator loss: 4.372550]\n",
      "1811 [Discriminator loss: 0.611692, acc.: 67.19%] [Generator loss: 4.169390]\n",
      "1812 [Discriminator loss: 0.408533, acc.: 84.38%] [Generator loss: 3.557713]\n",
      "1813 [Discriminator loss: 0.324860, acc.: 81.25%] [Generator loss: 4.098680]\n",
      "1814 [Discriminator loss: 0.194492, acc.: 93.75%] [Generator loss: 5.293247]\n",
      "1815 [Discriminator loss: 0.513116, acc.: 78.12%] [Generator loss: 4.255791]\n",
      "1816 [Discriminator loss: 0.176871, acc.: 95.31%] [Generator loss: 4.265777]\n",
      "1817 [Discriminator loss: 0.737189, acc.: 67.19%] [Generator loss: 5.063596]\n",
      "1818 [Discriminator loss: 0.771071, acc.: 65.62%] [Generator loss: 4.316783]\n",
      "1819 [Discriminator loss: 0.326176, acc.: 85.94%] [Generator loss: 3.647731]\n",
      "1820 [Discriminator loss: 0.200117, acc.: 90.62%] [Generator loss: 3.724707]\n",
      "1821 [Discriminator loss: 0.742086, acc.: 67.19%] [Generator loss: 4.621093]\n",
      "1822 [Discriminator loss: 0.446974, acc.: 87.50%] [Generator loss: 3.572320]\n",
      "1823 [Discriminator loss: 0.298339, acc.: 90.62%] [Generator loss: 3.467505]\n",
      "1824 [Discriminator loss: 0.481927, acc.: 82.81%] [Generator loss: 4.157153]\n",
      "1825 [Discriminator loss: 0.245999, acc.: 87.50%] [Generator loss: 3.883462]\n",
      "1826 [Discriminator loss: 0.459763, acc.: 81.25%] [Generator loss: 3.818619]\n",
      "1827 [Discriminator loss: 0.555819, acc.: 71.88%] [Generator loss: 3.634616]\n",
      "1828 [Discriminator loss: 0.428353, acc.: 78.12%] [Generator loss: 4.349826]\n",
      "1829 [Discriminator loss: 0.512937, acc.: 76.56%] [Generator loss: 5.186485]\n",
      "1830 [Discriminator loss: 0.601209, acc.: 71.88%] [Generator loss: 3.389856]\n",
      "1831 [Discriminator loss: 0.346125, acc.: 84.38%] [Generator loss: 4.597944]\n",
      "1832 [Discriminator loss: 0.250954, acc.: 89.06%] [Generator loss: 4.126593]\n",
      "1833 [Discriminator loss: 0.492642, acc.: 70.31%] [Generator loss: 3.050951]\n",
      "1834 [Discriminator loss: 0.390266, acc.: 79.69%] [Generator loss: 4.954453]\n",
      "1835 [Discriminator loss: 0.483682, acc.: 78.12%] [Generator loss: 4.303230]\n",
      "1836 [Discriminator loss: 0.402315, acc.: 79.69%] [Generator loss: 3.685893]\n",
      "1837 [Discriminator loss: 0.178422, acc.: 95.31%] [Generator loss: 3.691338]\n",
      "1838 [Discriminator loss: 0.382532, acc.: 81.25%] [Generator loss: 4.574886]\n",
      "1839 [Discriminator loss: 0.220243, acc.: 90.62%] [Generator loss: 4.004617]\n",
      "1840 [Discriminator loss: 0.421508, acc.: 84.38%] [Generator loss: 5.177307]\n",
      "1841 [Discriminator loss: 0.353253, acc.: 87.50%] [Generator loss: 3.996143]\n",
      "1842 [Discriminator loss: 0.392826, acc.: 81.25%] [Generator loss: 4.166247]\n",
      "1843 [Discriminator loss: 0.271244, acc.: 90.62%] [Generator loss: 4.615138]\n",
      "1844 [Discriminator loss: 0.335935, acc.: 85.94%] [Generator loss: 3.652006]\n",
      "1845 [Discriminator loss: 0.265691, acc.: 90.62%] [Generator loss: 3.263574]\n",
      "1846 [Discriminator loss: 0.405342, acc.: 81.25%] [Generator loss: 3.454114]\n",
      "1847 [Discriminator loss: 0.644851, acc.: 68.75%] [Generator loss: 3.061073]\n",
      "1848 [Discriminator loss: 0.234444, acc.: 87.50%] [Generator loss: 3.826771]\n",
      "1849 [Discriminator loss: 0.489924, acc.: 75.00%] [Generator loss: 3.495215]\n",
      "1850 [Discriminator loss: 0.432001, acc.: 78.12%] [Generator loss: 2.230070]\n",
      "1851 [Discriminator loss: 0.294445, acc.: 85.94%] [Generator loss: 3.264824]\n",
      "1852 [Discriminator loss: 0.306924, acc.: 85.94%] [Generator loss: 2.943480]\n",
      "1853 [Discriminator loss: 0.618354, acc.: 67.19%] [Generator loss: 3.801375]\n",
      "1854 [Discriminator loss: 0.410340, acc.: 81.25%] [Generator loss: 2.417776]\n",
      "1855 [Discriminator loss: 0.530662, acc.: 73.44%] [Generator loss: 5.435834]\n",
      "1856 [Discriminator loss: 0.523998, acc.: 71.88%] [Generator loss: 2.516298]\n",
      "1857 [Discriminator loss: 0.813238, acc.: 62.50%] [Generator loss: 5.450067]\n",
      "1858 [Discriminator loss: 0.268714, acc.: 90.62%] [Generator loss: 4.628384]\n",
      "1859 [Discriminator loss: 0.490236, acc.: 76.56%] [Generator loss: 3.804903]\n",
      "1860 [Discriminator loss: 0.522341, acc.: 76.56%] [Generator loss: 4.218888]\n",
      "1861 [Discriminator loss: 0.369770, acc.: 89.06%] [Generator loss: 2.755713]\n",
      "1862 [Discriminator loss: 0.745603, acc.: 60.94%] [Generator loss: 4.333272]\n",
      "1863 [Discriminator loss: 0.265870, acc.: 84.38%] [Generator loss: 4.086740]\n",
      "1864 [Discriminator loss: 0.709151, acc.: 71.88%] [Generator loss: 3.079049]\n",
      "1865 [Discriminator loss: 0.413391, acc.: 75.00%] [Generator loss: 4.105840]\n",
      "1866 [Discriminator loss: 0.759061, acc.: 68.75%] [Generator loss: 3.091285]\n",
      "1867 [Discriminator loss: 0.290846, acc.: 85.94%] [Generator loss: 2.703341]\n",
      "1868 [Discriminator loss: 0.303661, acc.: 87.50%] [Generator loss: 2.777695]\n",
      "1869 [Discriminator loss: 0.177536, acc.: 95.31%] [Generator loss: 2.540977]\n",
      "1870 [Discriminator loss: 0.490459, acc.: 76.56%] [Generator loss: 5.627866]\n",
      "1871 [Discriminator loss: 0.305766, acc.: 87.50%] [Generator loss: 3.558651]\n",
      "1872 [Discriminator loss: 0.666698, acc.: 70.31%] [Generator loss: 4.055171]\n",
      "1873 [Discriminator loss: 0.192952, acc.: 92.19%] [Generator loss: 4.566844]\n",
      "1874 [Discriminator loss: 0.529238, acc.: 73.44%] [Generator loss: 4.021389]\n",
      "1875 [Discriminator loss: 0.179787, acc.: 92.19%] [Generator loss: 3.664063]\n",
      "1876 [Discriminator loss: 0.243551, acc.: 92.19%] [Generator loss: 3.746168]\n",
      "1877 [Discriminator loss: 0.394432, acc.: 87.50%] [Generator loss: 3.105633]\n",
      "1878 [Discriminator loss: 0.455437, acc.: 79.69%] [Generator loss: 3.622809]\n",
      "1879 [Discriminator loss: 0.406630, acc.: 82.81%] [Generator loss: 3.517050]\n",
      "1880 [Discriminator loss: 0.366308, acc.: 84.38%] [Generator loss: 3.093556]\n",
      "1881 [Discriminator loss: 0.329953, acc.: 87.50%] [Generator loss: 3.965384]\n",
      "1882 [Discriminator loss: 0.479437, acc.: 76.56%] [Generator loss: 3.644165]\n",
      "1883 [Discriminator loss: 0.345756, acc.: 84.38%] [Generator loss: 3.324867]\n",
      "1884 [Discriminator loss: 0.728675, acc.: 71.88%] [Generator loss: 4.487604]\n",
      "1885 [Discriminator loss: 0.352373, acc.: 76.56%] [Generator loss: 4.072971]\n",
      "1886 [Discriminator loss: 0.346774, acc.: 84.38%] [Generator loss: 4.977651]\n",
      "1887 [Discriminator loss: 0.504182, acc.: 78.12%] [Generator loss: 4.489543]\n",
      "1888 [Discriminator loss: 0.387020, acc.: 81.25%] [Generator loss: 4.711429]\n",
      "1889 [Discriminator loss: 0.346530, acc.: 84.38%] [Generator loss: 3.877517]\n",
      "1890 [Discriminator loss: 0.514116, acc.: 73.44%] [Generator loss: 4.174752]\n",
      "1891 [Discriminator loss: 0.139777, acc.: 95.31%] [Generator loss: 4.022690]\n",
      "1892 [Discriminator loss: 0.293687, acc.: 82.81%] [Generator loss: 3.064454]\n",
      "1893 [Discriminator loss: 0.328916, acc.: 87.50%] [Generator loss: 4.085325]\n",
      "1894 [Discriminator loss: 0.380860, acc.: 82.81%] [Generator loss: 4.910693]\n",
      "1895 [Discriminator loss: 0.486547, acc.: 75.00%] [Generator loss: 3.386643]\n",
      "1896 [Discriminator loss: 0.347790, acc.: 82.81%] [Generator loss: 2.552952]\n",
      "1897 [Discriminator loss: 0.457492, acc.: 75.00%] [Generator loss: 3.802651]\n",
      "1898 [Discriminator loss: 0.226704, acc.: 93.75%] [Generator loss: 5.067758]\n",
      "1899 [Discriminator loss: 0.470467, acc.: 78.12%] [Generator loss: 3.371328]\n",
      "1900 [Discriminator loss: 0.633945, acc.: 78.12%] [Generator loss: 3.810862]\n",
      "1901 [Discriminator loss: 0.359631, acc.: 89.06%] [Generator loss: 3.645514]\n",
      "1902 [Discriminator loss: 0.345325, acc.: 81.25%] [Generator loss: 3.578107]\n",
      "1903 [Discriminator loss: 0.315370, acc.: 84.38%] [Generator loss: 4.324472]\n",
      "1904 [Discriminator loss: 0.283197, acc.: 82.81%] [Generator loss: 3.177485]\n",
      "1905 [Discriminator loss: 0.602259, acc.: 71.88%] [Generator loss: 4.984962]\n",
      "1906 [Discriminator loss: 0.558502, acc.: 73.44%] [Generator loss: 4.019581]\n",
      "1907 [Discriminator loss: 0.343380, acc.: 85.94%] [Generator loss: 4.127604]\n",
      "1908 [Discriminator loss: 0.377071, acc.: 84.38%] [Generator loss: 4.392708]\n",
      "1909 [Discriminator loss: 0.418012, acc.: 82.81%] [Generator loss: 4.088662]\n",
      "1910 [Discriminator loss: 0.502046, acc.: 78.12%] [Generator loss: 3.795535]\n",
      "1911 [Discriminator loss: 0.907974, acc.: 57.81%] [Generator loss: 4.825125]\n",
      "1912 [Discriminator loss: 0.607463, acc.: 76.56%] [Generator loss: 3.759840]\n",
      "1913 [Discriminator loss: 0.551426, acc.: 75.00%] [Generator loss: 3.677959]\n",
      "1914 [Discriminator loss: 0.233495, acc.: 90.62%] [Generator loss: 3.767637]\n",
      "1915 [Discriminator loss: 0.340828, acc.: 85.94%] [Generator loss: 3.291533]\n",
      "1916 [Discriminator loss: 0.404340, acc.: 81.25%] [Generator loss: 4.931672]\n",
      "1917 [Discriminator loss: 0.385038, acc.: 85.94%] [Generator loss: 4.543303]\n",
      "1918 [Discriminator loss: 0.356941, acc.: 85.94%] [Generator loss: 3.441045]\n",
      "1919 [Discriminator loss: 0.494170, acc.: 75.00%] [Generator loss: 3.513843]\n",
      "1920 [Discriminator loss: 0.557777, acc.: 78.12%] [Generator loss: 3.341915]\n",
      "1921 [Discriminator loss: 0.347782, acc.: 85.94%] [Generator loss: 3.687887]\n",
      "1922 [Discriminator loss: 0.647258, acc.: 68.75%] [Generator loss: 4.192834]\n",
      "1923 [Discriminator loss: 0.240046, acc.: 89.06%] [Generator loss: 3.643947]\n",
      "1924 [Discriminator loss: 0.478583, acc.: 75.00%] [Generator loss: 3.377428]\n",
      "1925 [Discriminator loss: 0.402217, acc.: 82.81%] [Generator loss: 3.171812]\n",
      "1926 [Discriminator loss: 0.395926, acc.: 78.12%] [Generator loss: 3.235100]\n",
      "1927 [Discriminator loss: 0.625669, acc.: 65.62%] [Generator loss: 4.033342]\n",
      "1928 [Discriminator loss: 0.256976, acc.: 87.50%] [Generator loss: 4.195901]\n",
      "1929 [Discriminator loss: 0.419960, acc.: 82.81%] [Generator loss: 3.866863]\n",
      "1930 [Discriminator loss: 0.375386, acc.: 81.25%] [Generator loss: 3.743537]\n",
      "1931 [Discriminator loss: 0.568130, acc.: 75.00%] [Generator loss: 3.931463]\n",
      "1932 [Discriminator loss: 0.223348, acc.: 93.75%] [Generator loss: 4.100253]\n",
      "1933 [Discriminator loss: 0.321431, acc.: 82.81%] [Generator loss: 4.142923]\n",
      "1934 [Discriminator loss: 0.318996, acc.: 89.06%] [Generator loss: 4.082508]\n",
      "1935 [Discriminator loss: 0.415838, acc.: 82.81%] [Generator loss: 3.256810]\n",
      "1936 [Discriminator loss: 0.395145, acc.: 84.38%] [Generator loss: 4.810051]\n",
      "1937 [Discriminator loss: 0.117602, acc.: 96.88%] [Generator loss: 4.354899]\n",
      "1938 [Discriminator loss: 0.646494, acc.: 67.19%] [Generator loss: 3.561235]\n",
      "1939 [Discriminator loss: 0.450463, acc.: 79.69%] [Generator loss: 4.578413]\n",
      "1940 [Discriminator loss: 0.648814, acc.: 70.31%] [Generator loss: 3.304203]\n",
      "1941 [Discriminator loss: 0.184324, acc.: 93.75%] [Generator loss: 3.779161]\n",
      "1942 [Discriminator loss: 0.440799, acc.: 84.38%] [Generator loss: 3.754061]\n",
      "1943 [Discriminator loss: 0.279584, acc.: 82.81%] [Generator loss: 4.392043]\n",
      "1944 [Discriminator loss: 0.307889, acc.: 85.94%] [Generator loss: 3.946268]\n",
      "1945 [Discriminator loss: 0.410705, acc.: 78.12%] [Generator loss: 3.592975]\n",
      "1946 [Discriminator loss: 0.310735, acc.: 84.38%] [Generator loss: 4.788126]\n",
      "1947 [Discriminator loss: 0.571279, acc.: 71.88%] [Generator loss: 3.141736]\n",
      "1948 [Discriminator loss: 0.536320, acc.: 79.69%] [Generator loss: 3.702639]\n",
      "1949 [Discriminator loss: 0.573694, acc.: 75.00%] [Generator loss: 3.312284]\n",
      "1950 [Discriminator loss: 0.315415, acc.: 85.94%] [Generator loss: 4.584854]\n",
      "1951 [Discriminator loss: 0.379775, acc.: 81.25%] [Generator loss: 2.914153]\n",
      "1952 [Discriminator loss: 0.330073, acc.: 87.50%] [Generator loss: 4.453192]\n",
      "1953 [Discriminator loss: 0.559399, acc.: 78.12%] [Generator loss: 4.194103]\n",
      "1954 [Discriminator loss: 0.290107, acc.: 89.06%] [Generator loss: 4.232590]\n",
      "1955 [Discriminator loss: 0.376824, acc.: 87.50%] [Generator loss: 3.009748]\n",
      "1956 [Discriminator loss: 0.552465, acc.: 79.69%] [Generator loss: 3.721288]\n",
      "1957 [Discriminator loss: 0.319020, acc.: 82.81%] [Generator loss: 3.160948]\n",
      "1958 [Discriminator loss: 0.571706, acc.: 79.69%] [Generator loss: 3.594516]\n",
      "1959 [Discriminator loss: 0.887157, acc.: 62.50%] [Generator loss: 3.997733]\n",
      "1960 [Discriminator loss: 0.809555, acc.: 68.75%] [Generator loss: 3.331409]\n",
      "1961 [Discriminator loss: 0.406308, acc.: 82.81%] [Generator loss: 3.658870]\n",
      "1962 [Discriminator loss: 0.482524, acc.: 81.25%] [Generator loss: 4.106949]\n",
      "1963 [Discriminator loss: 0.833110, acc.: 67.19%] [Generator loss: 3.889165]\n",
      "1964 [Discriminator loss: 0.423904, acc.: 76.56%] [Generator loss: 4.637043]\n",
      "1965 [Discriminator loss: 0.172824, acc.: 93.75%] [Generator loss: 4.918401]\n",
      "1966 [Discriminator loss: 0.418064, acc.: 76.56%] [Generator loss: 3.489067]\n",
      "1967 [Discriminator loss: 0.435064, acc.: 79.69%] [Generator loss: 3.370659]\n",
      "1968 [Discriminator loss: 0.320196, acc.: 85.94%] [Generator loss: 4.164505]\n",
      "1969 [Discriminator loss: 0.237952, acc.: 90.62%] [Generator loss: 3.977436]\n",
      "1970 [Discriminator loss: 0.542395, acc.: 78.12%] [Generator loss: 3.211525]\n",
      "1971 [Discriminator loss: 0.308609, acc.: 85.94%] [Generator loss: 3.505304]\n",
      "1972 [Discriminator loss: 0.282301, acc.: 87.50%] [Generator loss: 3.701681]\n",
      "1973 [Discriminator loss: 0.431187, acc.: 87.50%] [Generator loss: 3.710724]\n",
      "1974 [Discriminator loss: 0.607269, acc.: 68.75%] [Generator loss: 2.939820]\n",
      "1975 [Discriminator loss: 0.557627, acc.: 71.88%] [Generator loss: 3.025480]\n",
      "1976 [Discriminator loss: 0.330328, acc.: 87.50%] [Generator loss: 3.708980]\n",
      "1977 [Discriminator loss: 0.270616, acc.: 93.75%] [Generator loss: 3.357451]\n",
      "1978 [Discriminator loss: 0.290250, acc.: 90.62%] [Generator loss: 4.708119]\n",
      "1979 [Discriminator loss: 0.369209, acc.: 84.38%] [Generator loss: 3.348445]\n",
      "1980 [Discriminator loss: 0.166446, acc.: 96.88%] [Generator loss: 2.921151]\n",
      "1981 [Discriminator loss: 0.487789, acc.: 71.88%] [Generator loss: 5.024403]\n",
      "1982 [Discriminator loss: 0.514488, acc.: 73.44%] [Generator loss: 3.674376]\n",
      "1983 [Discriminator loss: 0.295625, acc.: 84.38%] [Generator loss: 3.959863]\n",
      "1984 [Discriminator loss: 0.331072, acc.: 81.25%] [Generator loss: 3.675949]\n",
      "1985 [Discriminator loss: 0.504253, acc.: 76.56%] [Generator loss: 4.939918]\n",
      "1986 [Discriminator loss: 0.440837, acc.: 79.69%] [Generator loss: 3.751161]\n",
      "1987 [Discriminator loss: 0.514710, acc.: 75.00%] [Generator loss: 2.985241]\n",
      "1988 [Discriminator loss: 0.648486, acc.: 73.44%] [Generator loss: 4.519912]\n",
      "1989 [Discriminator loss: 0.491329, acc.: 81.25%] [Generator loss: 3.249335]\n",
      "1990 [Discriminator loss: 0.538134, acc.: 73.44%] [Generator loss: 3.368319]\n",
      "1991 [Discriminator loss: 0.671677, acc.: 70.31%] [Generator loss: 4.011366]\n",
      "1992 [Discriminator loss: 0.551982, acc.: 75.00%] [Generator loss: 4.624017]\n",
      "1993 [Discriminator loss: 0.605602, acc.: 73.44%] [Generator loss: 3.848609]\n",
      "1994 [Discriminator loss: 0.373487, acc.: 85.94%] [Generator loss: 4.117450]\n",
      "1995 [Discriminator loss: 0.378864, acc.: 84.38%] [Generator loss: 3.973659]\n",
      "1996 [Discriminator loss: 0.202272, acc.: 90.62%] [Generator loss: 4.226439]\n",
      "1997 [Discriminator loss: 0.571913, acc.: 81.25%] [Generator loss: 4.437947]\n",
      "1998 [Discriminator loss: 0.538011, acc.: 76.56%] [Generator loss: 3.684270]\n",
      "1999 [Discriminator loss: 0.302409, acc.: 90.62%] [Generator loss: 4.505910]\n",
      "2000 [Discriminator loss: 0.365191, acc.: 81.25%] [Generator loss: 3.793939]\n",
      "2001 [Discriminator loss: 0.681201, acc.: 65.62%] [Generator loss: 3.881591]\n",
      "2002 [Discriminator loss: 0.380300, acc.: 81.25%] [Generator loss: 3.841668]\n",
      "2003 [Discriminator loss: 0.399414, acc.: 85.94%] [Generator loss: 2.464235]\n",
      "2004 [Discriminator loss: 0.318202, acc.: 85.94%] [Generator loss: 3.301192]\n",
      "2005 [Discriminator loss: 0.547749, acc.: 76.56%] [Generator loss: 4.027173]\n",
      "2006 [Discriminator loss: 0.463596, acc.: 75.00%] [Generator loss: 3.617525]\n",
      "2007 [Discriminator loss: 0.645854, acc.: 71.88%] [Generator loss: 4.284696]\n",
      "2008 [Discriminator loss: 0.608453, acc.: 67.19%] [Generator loss: 4.189672]\n",
      "2009 [Discriminator loss: 0.499371, acc.: 81.25%] [Generator loss: 4.721949]\n",
      "2010 [Discriminator loss: 0.791316, acc.: 65.62%] [Generator loss: 4.303834]\n",
      "2011 [Discriminator loss: 0.338547, acc.: 82.81%] [Generator loss: 3.508074]\n",
      "2012 [Discriminator loss: 0.319309, acc.: 87.50%] [Generator loss: 4.122949]\n",
      "2013 [Discriminator loss: 0.370389, acc.: 81.25%] [Generator loss: 4.303904]\n",
      "2014 [Discriminator loss: 0.326231, acc.: 90.62%] [Generator loss: 3.398538]\n",
      "2015 [Discriminator loss: 0.384345, acc.: 78.12%] [Generator loss: 3.453211]\n",
      "2016 [Discriminator loss: 0.337822, acc.: 85.94%] [Generator loss: 3.762913]\n",
      "2017 [Discriminator loss: 0.422651, acc.: 81.25%] [Generator loss: 2.731081]\n",
      "2018 [Discriminator loss: 0.235148, acc.: 92.19%] [Generator loss: 3.360723]\n",
      "2019 [Discriminator loss: 0.485121, acc.: 79.69%] [Generator loss: 3.666114]\n",
      "2020 [Discriminator loss: 0.507392, acc.: 79.69%] [Generator loss: 2.709942]\n",
      "2021 [Discriminator loss: 0.359851, acc.: 84.38%] [Generator loss: 3.064934]\n",
      "2022 [Discriminator loss: 0.364048, acc.: 85.94%] [Generator loss: 4.350365]\n",
      "2023 [Discriminator loss: 0.583742, acc.: 71.88%] [Generator loss: 3.727108]\n",
      "2024 [Discriminator loss: 0.234697, acc.: 87.50%] [Generator loss: 4.077367]\n",
      "2025 [Discriminator loss: 0.485098, acc.: 78.12%] [Generator loss: 4.214451]\n",
      "2026 [Discriminator loss: 0.332229, acc.: 82.81%] [Generator loss: 2.698360]\n",
      "2027 [Discriminator loss: 0.535816, acc.: 75.00%] [Generator loss: 5.284211]\n",
      "2028 [Discriminator loss: 0.159582, acc.: 92.19%] [Generator loss: 4.254578]\n",
      "2029 [Discriminator loss: 0.519212, acc.: 76.56%] [Generator loss: 4.261008]\n",
      "2030 [Discriminator loss: 0.396573, acc.: 79.69%] [Generator loss: 5.407592]\n",
      "2031 [Discriminator loss: 0.333111, acc.: 82.81%] [Generator loss: 4.079241]\n",
      "2032 [Discriminator loss: 0.271273, acc.: 90.62%] [Generator loss: 4.401916]\n",
      "2033 [Discriminator loss: 0.193790, acc.: 95.31%] [Generator loss: 4.537692]\n",
      "2034 [Discriminator loss: 0.351440, acc.: 84.38%] [Generator loss: 2.497116]\n",
      "2035 [Discriminator loss: 0.386937, acc.: 85.94%] [Generator loss: 3.547504]\n",
      "2036 [Discriminator loss: 0.346377, acc.: 85.94%] [Generator loss: 4.289931]\n",
      "2037 [Discriminator loss: 0.630936, acc.: 71.88%] [Generator loss: 3.497550]\n",
      "2038 [Discriminator loss: 0.204437, acc.: 93.75%] [Generator loss: 3.691345]\n",
      "2039 [Discriminator loss: 0.588545, acc.: 70.31%] [Generator loss: 4.295327]\n",
      "2040 [Discriminator loss: 0.224579, acc.: 89.06%] [Generator loss: 3.325085]\n",
      "2041 [Discriminator loss: 0.330376, acc.: 87.50%] [Generator loss: 3.061792]\n",
      "2042 [Discriminator loss: 0.797493, acc.: 53.12%] [Generator loss: 3.215495]\n",
      "2043 [Discriminator loss: 0.293733, acc.: 87.50%] [Generator loss: 4.287084]\n",
      "2044 [Discriminator loss: 0.271854, acc.: 89.06%] [Generator loss: 3.833941]\n",
      "2045 [Discriminator loss: 0.389515, acc.: 82.81%] [Generator loss: 2.999654]\n",
      "2046 [Discriminator loss: 0.314551, acc.: 85.94%] [Generator loss: 4.177213]\n",
      "2047 [Discriminator loss: 0.317195, acc.: 89.06%] [Generator loss: 3.084583]\n",
      "2048 [Discriminator loss: 0.639258, acc.: 70.31%] [Generator loss: 4.574293]\n",
      "2049 [Discriminator loss: 0.795906, acc.: 56.25%] [Generator loss: 3.973570]\n",
      "2050 [Discriminator loss: 0.506344, acc.: 78.12%] [Generator loss: 2.797674]\n",
      "2051 [Discriminator loss: 0.308608, acc.: 82.81%] [Generator loss: 3.898107]\n",
      "2052 [Discriminator loss: 0.183333, acc.: 89.06%] [Generator loss: 4.299708]\n",
      "2053 [Discriminator loss: 0.318661, acc.: 81.25%] [Generator loss: 4.211130]\n",
      "2054 [Discriminator loss: 0.235993, acc.: 90.62%] [Generator loss: 3.772557]\n",
      "2055 [Discriminator loss: 0.361655, acc.: 81.25%] [Generator loss: 4.106520]\n",
      "2056 [Discriminator loss: 0.338672, acc.: 82.81%] [Generator loss: 3.142619]\n",
      "2057 [Discriminator loss: 0.327229, acc.: 89.06%] [Generator loss: 3.941953]\n",
      "2058 [Discriminator loss: 0.225862, acc.: 85.94%] [Generator loss: 2.964401]\n",
      "2059 [Discriminator loss: 0.170204, acc.: 92.19%] [Generator loss: 3.348288]\n",
      "2060 [Discriminator loss: 0.425139, acc.: 82.81%] [Generator loss: 2.988063]\n",
      "2061 [Discriminator loss: 0.192056, acc.: 93.75%] [Generator loss: 4.011926]\n",
      "2062 [Discriminator loss: 0.690176, acc.: 64.06%] [Generator loss: 3.936101]\n",
      "2063 [Discriminator loss: 0.338310, acc.: 87.50%] [Generator loss: 2.807311]\n",
      "2064 [Discriminator loss: 0.305260, acc.: 82.81%] [Generator loss: 3.821441]\n",
      "2065 [Discriminator loss: 0.165951, acc.: 96.88%] [Generator loss: 2.865697]\n",
      "2066 [Discriminator loss: 0.504827, acc.: 76.56%] [Generator loss: 3.804088]\n",
      "2067 [Discriminator loss: 0.685028, acc.: 64.06%] [Generator loss: 4.946002]\n",
      "2068 [Discriminator loss: 0.532920, acc.: 76.56%] [Generator loss: 3.037674]\n",
      "2069 [Discriminator loss: 0.458626, acc.: 75.00%] [Generator loss: 3.684722]\n",
      "2070 [Discriminator loss: 0.349522, acc.: 84.38%] [Generator loss: 4.630014]\n",
      "2071 [Discriminator loss: 0.478221, acc.: 84.38%] [Generator loss: 2.985248]\n",
      "2072 [Discriminator loss: 0.480958, acc.: 78.12%] [Generator loss: 4.558433]\n",
      "2073 [Discriminator loss: 0.676071, acc.: 73.44%] [Generator loss: 3.891804]\n",
      "2074 [Discriminator loss: 0.158408, acc.: 96.88%] [Generator loss: 3.401753]\n",
      "2075 [Discriminator loss: 0.268981, acc.: 85.94%] [Generator loss: 3.953043]\n",
      "2076 [Discriminator loss: 0.188550, acc.: 92.19%] [Generator loss: 3.842145]\n",
      "2077 [Discriminator loss: 0.444974, acc.: 78.12%] [Generator loss: 3.233439]\n",
      "2078 [Discriminator loss: 0.266036, acc.: 82.81%] [Generator loss: 3.514972]\n",
      "2079 [Discriminator loss: 0.393980, acc.: 85.94%] [Generator loss: 3.191539]\n",
      "2080 [Discriminator loss: 0.250517, acc.: 89.06%] [Generator loss: 3.875308]\n",
      "2081 [Discriminator loss: 0.441060, acc.: 78.12%] [Generator loss: 5.028759]\n",
      "2082 [Discriminator loss: 0.159024, acc.: 95.31%] [Generator loss: 3.508071]\n",
      "2083 [Discriminator loss: 0.496877, acc.: 76.56%] [Generator loss: 3.474043]\n",
      "2084 [Discriminator loss: 0.523235, acc.: 78.12%] [Generator loss: 3.438472]\n",
      "2085 [Discriminator loss: 0.402728, acc.: 81.25%] [Generator loss: 3.736675]\n",
      "2086 [Discriminator loss: 0.311635, acc.: 87.50%] [Generator loss: 2.609738]\n",
      "2087 [Discriminator loss: 0.887983, acc.: 59.38%] [Generator loss: 4.668732]\n",
      "2088 [Discriminator loss: 0.473072, acc.: 82.81%] [Generator loss: 4.295564]\n",
      "2089 [Discriminator loss: 0.436519, acc.: 79.69%] [Generator loss: 4.226309]\n",
      "2090 [Discriminator loss: 0.204489, acc.: 89.06%] [Generator loss: 3.277035]\n",
      "2091 [Discriminator loss: 0.382364, acc.: 87.50%] [Generator loss: 3.943050]\n",
      "2092 [Discriminator loss: 0.423878, acc.: 84.38%] [Generator loss: 3.735190]\n",
      "2093 [Discriminator loss: 0.343998, acc.: 85.94%] [Generator loss: 3.423384]\n",
      "2094 [Discriminator loss: 0.529362, acc.: 70.31%] [Generator loss: 5.017814]\n",
      "2095 [Discriminator loss: 0.304584, acc.: 89.06%] [Generator loss: 4.646244]\n",
      "2096 [Discriminator loss: 0.487072, acc.: 78.12%] [Generator loss: 2.924994]\n",
      "2097 [Discriminator loss: 0.237202, acc.: 90.62%] [Generator loss: 3.763583]\n",
      "2098 [Discriminator loss: 0.325822, acc.: 82.81%] [Generator loss: 3.484796]\n",
      "2099 [Discriminator loss: 0.442444, acc.: 78.12%] [Generator loss: 3.650784]\n",
      "2100 [Discriminator loss: 0.484050, acc.: 73.44%] [Generator loss: 3.517114]\n",
      "2101 [Discriminator loss: 0.218704, acc.: 90.62%] [Generator loss: 3.996520]\n",
      "2102 [Discriminator loss: 0.347040, acc.: 84.38%] [Generator loss: 3.787798]\n",
      "2103 [Discriminator loss: 0.508860, acc.: 79.69%] [Generator loss: 3.422177]\n",
      "2104 [Discriminator loss: 0.321145, acc.: 87.50%] [Generator loss: 3.578825]\n",
      "2105 [Discriminator loss: 0.306183, acc.: 89.06%] [Generator loss: 3.234378]\n",
      "2106 [Discriminator loss: 0.460742, acc.: 79.69%] [Generator loss: 3.784680]\n",
      "2107 [Discriminator loss: 0.246186, acc.: 87.50%] [Generator loss: 4.190764]\n",
      "2108 [Discriminator loss: 0.604975, acc.: 76.56%] [Generator loss: 4.086487]\n",
      "2109 [Discriminator loss: 0.518770, acc.: 82.81%] [Generator loss: 3.830012]\n",
      "2110 [Discriminator loss: 0.402654, acc.: 81.25%] [Generator loss: 3.607871]\n",
      "2111 [Discriminator loss: 0.547588, acc.: 70.31%] [Generator loss: 3.815519]\n",
      "2112 [Discriminator loss: 0.606148, acc.: 73.44%] [Generator loss: 4.070076]\n",
      "2113 [Discriminator loss: 0.631085, acc.: 70.31%] [Generator loss: 4.165406]\n",
      "2114 [Discriminator loss: 0.654150, acc.: 75.00%] [Generator loss: 3.567798]\n",
      "2115 [Discriminator loss: 0.652603, acc.: 67.19%] [Generator loss: 3.817719]\n",
      "2116 [Discriminator loss: 0.231556, acc.: 85.94%] [Generator loss: 4.429935]\n",
      "2117 [Discriminator loss: 0.442172, acc.: 75.00%] [Generator loss: 4.484149]\n",
      "2118 [Discriminator loss: 0.487355, acc.: 73.44%] [Generator loss: 4.104643]\n",
      "2119 [Discriminator loss: 0.363263, acc.: 84.38%] [Generator loss: 3.617972]\n",
      "2120 [Discriminator loss: 0.366866, acc.: 79.69%] [Generator loss: 4.466649]\n",
      "2121 [Discriminator loss: 0.464352, acc.: 71.88%] [Generator loss: 3.949916]\n",
      "2122 [Discriminator loss: 0.440674, acc.: 79.69%] [Generator loss: 4.772459]\n",
      "2123 [Discriminator loss: 0.343021, acc.: 79.69%] [Generator loss: 3.682844]\n",
      "2124 [Discriminator loss: 0.514802, acc.: 76.56%] [Generator loss: 3.599561]\n",
      "2125 [Discriminator loss: 0.379500, acc.: 84.38%] [Generator loss: 3.972574]\n",
      "2126 [Discriminator loss: 0.293821, acc.: 84.38%] [Generator loss: 3.816676]\n",
      "2127 [Discriminator loss: 0.382522, acc.: 87.50%] [Generator loss: 2.810276]\n",
      "2128 [Discriminator loss: 0.476181, acc.: 82.81%] [Generator loss: 5.076145]\n",
      "2129 [Discriminator loss: 0.323324, acc.: 87.50%] [Generator loss: 3.904284]\n",
      "2130 [Discriminator loss: 0.420285, acc.: 78.12%] [Generator loss: 3.203613]\n",
      "2131 [Discriminator loss: 0.245078, acc.: 87.50%] [Generator loss: 2.576807]\n",
      "2132 [Discriminator loss: 0.311714, acc.: 89.06%] [Generator loss: 4.068048]\n",
      "2133 [Discriminator loss: 0.536887, acc.: 78.12%] [Generator loss: 4.443388]\n",
      "2134 [Discriminator loss: 0.520020, acc.: 75.00%] [Generator loss: 3.563883]\n",
      "2135 [Discriminator loss: 0.267238, acc.: 87.50%] [Generator loss: 2.811494]\n",
      "2136 [Discriminator loss: 0.436738, acc.: 79.69%] [Generator loss: 3.174555]\n",
      "2137 [Discriminator loss: 0.592213, acc.: 70.31%] [Generator loss: 4.716610]\n",
      "2138 [Discriminator loss: 0.279315, acc.: 87.50%] [Generator loss: 4.726497]\n",
      "2139 [Discriminator loss: 0.720465, acc.: 65.62%] [Generator loss: 3.597875]\n",
      "2140 [Discriminator loss: 0.150941, acc.: 95.31%] [Generator loss: 4.503457]\n",
      "2141 [Discriminator loss: 0.322525, acc.: 89.06%] [Generator loss: 2.842328]\n",
      "2142 [Discriminator loss: 0.299947, acc.: 81.25%] [Generator loss: 4.166772]\n",
      "2143 [Discriminator loss: 0.196695, acc.: 95.31%] [Generator loss: 4.232320]\n",
      "2144 [Discriminator loss: 0.383060, acc.: 84.38%] [Generator loss: 2.942959]\n",
      "2145 [Discriminator loss: 0.520511, acc.: 76.56%] [Generator loss: 4.421101]\n",
      "2146 [Discriminator loss: 0.607134, acc.: 70.31%] [Generator loss: 2.892470]\n",
      "2147 [Discriminator loss: 0.443179, acc.: 79.69%] [Generator loss: 3.891140]\n",
      "2148 [Discriminator loss: 0.625926, acc.: 73.44%] [Generator loss: 4.143906]\n",
      "2149 [Discriminator loss: 0.270436, acc.: 87.50%] [Generator loss: 3.595809]\n",
      "2150 [Discriminator loss: 0.481651, acc.: 75.00%] [Generator loss: 3.731001]\n",
      "2151 [Discriminator loss: 0.286137, acc.: 87.50%] [Generator loss: 3.471492]\n",
      "2152 [Discriminator loss: 0.627064, acc.: 75.00%] [Generator loss: 4.582696]\n",
      "2153 [Discriminator loss: 0.412662, acc.: 79.69%] [Generator loss: 3.848438]\n",
      "2154 [Discriminator loss: 0.220352, acc.: 92.19%] [Generator loss: 3.543237]\n",
      "2155 [Discriminator loss: 0.514161, acc.: 75.00%] [Generator loss: 3.912647]\n",
      "2156 [Discriminator loss: 0.365471, acc.: 79.69%] [Generator loss: 3.583148]\n",
      "2157 [Discriminator loss: 0.502899, acc.: 78.12%] [Generator loss: 4.464772]\n",
      "2158 [Discriminator loss: 0.623566, acc.: 67.19%] [Generator loss: 3.631450]\n",
      "2159 [Discriminator loss: 0.330171, acc.: 82.81%] [Generator loss: 3.885633]\n",
      "2160 [Discriminator loss: 0.808903, acc.: 71.88%] [Generator loss: 4.322393]\n",
      "2161 [Discriminator loss: 0.389617, acc.: 81.25%] [Generator loss: 3.192971]\n",
      "2162 [Discriminator loss: 0.429881, acc.: 84.38%] [Generator loss: 4.682259]\n",
      "2163 [Discriminator loss: 0.525404, acc.: 76.56%] [Generator loss: 3.631832]\n",
      "2164 [Discriminator loss: 0.332668, acc.: 82.81%] [Generator loss: 3.938987]\n",
      "2165 [Discriminator loss: 0.441379, acc.: 81.25%] [Generator loss: 3.437788]\n",
      "2166 [Discriminator loss: 0.307242, acc.: 87.50%] [Generator loss: 3.507227]\n",
      "2167 [Discriminator loss: 0.426340, acc.: 79.69%] [Generator loss: 4.825933]\n",
      "2168 [Discriminator loss: 0.447470, acc.: 73.44%] [Generator loss: 3.578771]\n",
      "2169 [Discriminator loss: 0.336285, acc.: 87.50%] [Generator loss: 3.165823]\n",
      "2170 [Discriminator loss: 0.597042, acc.: 73.44%] [Generator loss: 3.526581]\n",
      "2171 [Discriminator loss: 0.279175, acc.: 89.06%] [Generator loss: 4.070288]\n",
      "2172 [Discriminator loss: 0.632687, acc.: 67.19%] [Generator loss: 4.440160]\n",
      "2173 [Discriminator loss: 0.289297, acc.: 85.94%] [Generator loss: 4.227633]\n",
      "2174 [Discriminator loss: 0.367213, acc.: 81.25%] [Generator loss: 4.695243]\n",
      "2175 [Discriminator loss: 0.382503, acc.: 82.81%] [Generator loss: 4.104795]\n",
      "2176 [Discriminator loss: 0.245832, acc.: 87.50%] [Generator loss: 3.525881]\n",
      "2177 [Discriminator loss: 0.314212, acc.: 87.50%] [Generator loss: 5.155910]\n",
      "2178 [Discriminator loss: 0.446862, acc.: 78.12%] [Generator loss: 3.237726]\n",
      "2179 [Discriminator loss: 0.175493, acc.: 93.75%] [Generator loss: 3.717986]\n",
      "2180 [Discriminator loss: 0.466364, acc.: 81.25%] [Generator loss: 4.405252]\n",
      "2181 [Discriminator loss: 0.617740, acc.: 73.44%] [Generator loss: 3.316711]\n",
      "2182 [Discriminator loss: 0.373840, acc.: 81.25%] [Generator loss: 3.286985]\n",
      "2183 [Discriminator loss: 0.198132, acc.: 93.75%] [Generator loss: 4.673440]\n",
      "2184 [Discriminator loss: 0.434128, acc.: 76.56%] [Generator loss: 3.123693]\n",
      "2185 [Discriminator loss: 0.400339, acc.: 79.69%] [Generator loss: 3.703183]\n",
      "2186 [Discriminator loss: 0.248335, acc.: 87.50%] [Generator loss: 3.655766]\n",
      "2187 [Discriminator loss: 0.434422, acc.: 79.69%] [Generator loss: 3.823863]\n",
      "2188 [Discriminator loss: 0.311085, acc.: 85.94%] [Generator loss: 3.587382]\n",
      "2189 [Discriminator loss: 0.586356, acc.: 67.19%] [Generator loss: 5.205116]\n",
      "2190 [Discriminator loss: 0.482590, acc.: 76.56%] [Generator loss: 4.800902]\n",
      "2191 [Discriminator loss: 0.445781, acc.: 79.69%] [Generator loss: 4.205391]\n",
      "2192 [Discriminator loss: 0.544940, acc.: 71.88%] [Generator loss: 5.766575]\n",
      "2193 [Discriminator loss: 0.364804, acc.: 84.38%] [Generator loss: 5.016604]\n",
      "2194 [Discriminator loss: 0.423039, acc.: 79.69%] [Generator loss: 4.559066]\n",
      "2195 [Discriminator loss: 0.318988, acc.: 87.50%] [Generator loss: 3.135174]\n",
      "2196 [Discriminator loss: 0.454720, acc.: 78.12%] [Generator loss: 4.053045]\n",
      "2197 [Discriminator loss: 0.445717, acc.: 82.81%] [Generator loss: 4.130626]\n",
      "2198 [Discriminator loss: 0.482577, acc.: 78.12%] [Generator loss: 3.759063]\n",
      "2199 [Discriminator loss: 0.337321, acc.: 84.38%] [Generator loss: 3.238851]\n",
      "2200 [Discriminator loss: 0.335335, acc.: 85.94%] [Generator loss: 3.304793]\n",
      "2201 [Discriminator loss: 0.436835, acc.: 76.56%] [Generator loss: 4.460217]\n",
      "2202 [Discriminator loss: 0.298481, acc.: 90.62%] [Generator loss: 4.320626]\n",
      "2203 [Discriminator loss: 0.391665, acc.: 84.38%] [Generator loss: 3.768226]\n",
      "2204 [Discriminator loss: 0.464809, acc.: 73.44%] [Generator loss: 3.208087]\n",
      "2205 [Discriminator loss: 0.324012, acc.: 85.94%] [Generator loss: 3.466886]\n",
      "2206 [Discriminator loss: 0.347796, acc.: 87.50%] [Generator loss: 4.198867]\n",
      "2207 [Discriminator loss: 0.539758, acc.: 79.69%] [Generator loss: 2.891835]\n",
      "2208 [Discriminator loss: 0.306541, acc.: 84.38%] [Generator loss: 3.199763]\n",
      "2209 [Discriminator loss: 0.224429, acc.: 93.75%] [Generator loss: 3.694730]\n",
      "2210 [Discriminator loss: 0.147575, acc.: 95.31%] [Generator loss: 2.555959]\n",
      "2211 [Discriminator loss: 0.665611, acc.: 67.19%] [Generator loss: 4.718725]\n",
      "2212 [Discriminator loss: 0.407394, acc.: 81.25%] [Generator loss: 3.626588]\n",
      "2213 [Discriminator loss: 0.509122, acc.: 79.69%] [Generator loss: 4.990831]\n",
      "2214 [Discriminator loss: 0.207656, acc.: 92.19%] [Generator loss: 3.560971]\n",
      "2215 [Discriminator loss: 0.700137, acc.: 67.19%] [Generator loss: 4.510701]\n",
      "2216 [Discriminator loss: 0.271931, acc.: 90.62%] [Generator loss: 4.107784]\n",
      "2217 [Discriminator loss: 0.570233, acc.: 76.56%] [Generator loss: 4.305835]\n",
      "2218 [Discriminator loss: 0.224717, acc.: 92.19%] [Generator loss: 4.424014]\n",
      "2219 [Discriminator loss: 0.195366, acc.: 92.19%] [Generator loss: 3.969418]\n",
      "2220 [Discriminator loss: 0.455927, acc.: 78.12%] [Generator loss: 3.970828]\n",
      "2221 [Discriminator loss: 0.146808, acc.: 96.88%] [Generator loss: 4.398420]\n",
      "2222 [Discriminator loss: 0.271815, acc.: 89.06%] [Generator loss: 3.856883]\n",
      "2223 [Discriminator loss: 0.350052, acc.: 82.81%] [Generator loss: 3.407015]\n",
      "2224 [Discriminator loss: 0.592194, acc.: 68.75%] [Generator loss: 3.702391]\n",
      "2225 [Discriminator loss: 0.452663, acc.: 84.38%] [Generator loss: 3.169618]\n",
      "2226 [Discriminator loss: 0.319300, acc.: 85.94%] [Generator loss: 4.751640]\n",
      "2227 [Discriminator loss: 0.334246, acc.: 82.81%] [Generator loss: 3.614493]\n",
      "2228 [Discriminator loss: 0.602137, acc.: 68.75%] [Generator loss: 4.546995]\n",
      "2229 [Discriminator loss: 0.299894, acc.: 92.19%] [Generator loss: 4.604574]\n",
      "2230 [Discriminator loss: 0.643734, acc.: 68.75%] [Generator loss: 4.411373]\n",
      "2231 [Discriminator loss: 0.284552, acc.: 87.50%] [Generator loss: 4.402676]\n",
      "2232 [Discriminator loss: 0.818187, acc.: 59.38%] [Generator loss: 6.384826]\n",
      "2233 [Discriminator loss: 0.603034, acc.: 70.31%] [Generator loss: 4.060232]\n",
      "2234 [Discriminator loss: 0.182618, acc.: 93.75%] [Generator loss: 3.510934]\n",
      "2235 [Discriminator loss: 0.332265, acc.: 87.50%] [Generator loss: 4.110413]\n",
      "2236 [Discriminator loss: 0.257045, acc.: 89.06%] [Generator loss: 4.084171]\n",
      "2237 [Discriminator loss: 0.434320, acc.: 82.81%] [Generator loss: 4.003719]\n",
      "2238 [Discriminator loss: 0.407065, acc.: 85.94%] [Generator loss: 3.698836]\n",
      "2239 [Discriminator loss: 0.380103, acc.: 81.25%] [Generator loss: 3.468822]\n",
      "2240 [Discriminator loss: 0.270258, acc.: 92.19%] [Generator loss: 3.262003]\n",
      "2241 [Discriminator loss: 0.276643, acc.: 84.38%] [Generator loss: 3.933318]\n",
      "2242 [Discriminator loss: 0.147563, acc.: 93.75%] [Generator loss: 4.576064]\n",
      "2243 [Discriminator loss: 0.161276, acc.: 92.19%] [Generator loss: 3.861059]\n",
      "2244 [Discriminator loss: 0.179226, acc.: 92.19%] [Generator loss: 3.229011]\n",
      "2245 [Discriminator loss: 0.253209, acc.: 90.62%] [Generator loss: 3.758036]\n",
      "2246 [Discriminator loss: 0.367443, acc.: 79.69%] [Generator loss: 4.522479]\n",
      "2247 [Discriminator loss: 0.221736, acc.: 92.19%] [Generator loss: 3.809415]\n",
      "2248 [Discriminator loss: 0.524931, acc.: 76.56%] [Generator loss: 3.300500]\n",
      "2249 [Discriminator loss: 0.206612, acc.: 95.31%] [Generator loss: 3.834299]\n",
      "2250 [Discriminator loss: 0.504401, acc.: 79.69%] [Generator loss: 4.892914]\n",
      "2251 [Discriminator loss: 0.132363, acc.: 93.75%] [Generator loss: 4.895689]\n",
      "2252 [Discriminator loss: 0.414238, acc.: 85.94%] [Generator loss: 3.661025]\n",
      "2253 [Discriminator loss: 0.448121, acc.: 79.69%] [Generator loss: 3.172301]\n",
      "2254 [Discriminator loss: 0.212400, acc.: 89.06%] [Generator loss: 3.988986]\n",
      "2255 [Discriminator loss: 0.347829, acc.: 84.38%] [Generator loss: 2.501069]\n",
      "2256 [Discriminator loss: 0.399983, acc.: 81.25%] [Generator loss: 3.894335]\n",
      "2257 [Discriminator loss: 0.072777, acc.: 98.44%] [Generator loss: 4.925256]\n",
      "2258 [Discriminator loss: 0.299302, acc.: 87.50%] [Generator loss: 3.189505]\n",
      "2259 [Discriminator loss: 0.373898, acc.: 82.81%] [Generator loss: 3.128949]\n",
      "2260 [Discriminator loss: 0.363336, acc.: 89.06%] [Generator loss: 3.383570]\n",
      "2261 [Discriminator loss: 0.686332, acc.: 71.88%] [Generator loss: 4.931344]\n",
      "2262 [Discriminator loss: 0.158372, acc.: 95.31%] [Generator loss: 5.664270]\n",
      "2263 [Discriminator loss: 0.382206, acc.: 84.38%] [Generator loss: 4.313986]\n",
      "2264 [Discriminator loss: 0.163826, acc.: 93.75%] [Generator loss: 4.178563]\n",
      "2265 [Discriminator loss: 0.326967, acc.: 84.38%] [Generator loss: 3.892254]\n",
      "2266 [Discriminator loss: 0.085956, acc.: 98.44%] [Generator loss: 4.488895]\n",
      "2267 [Discriminator loss: 0.490392, acc.: 73.44%] [Generator loss: 3.543262]\n",
      "2268 [Discriminator loss: 0.198950, acc.: 90.62%] [Generator loss: 4.615987]\n",
      "2269 [Discriminator loss: 0.162237, acc.: 93.75%] [Generator loss: 4.666761]\n",
      "2270 [Discriminator loss: 0.389846, acc.: 79.69%] [Generator loss: 3.554210]\n",
      "2271 [Discriminator loss: 0.623592, acc.: 71.88%] [Generator loss: 4.748889]\n",
      "2272 [Discriminator loss: 0.308778, acc.: 87.50%] [Generator loss: 3.567858]\n",
      "2273 [Discriminator loss: 0.263648, acc.: 87.50%] [Generator loss: 3.979457]\n",
      "2274 [Discriminator loss: 0.428555, acc.: 78.12%] [Generator loss: 5.413226]\n",
      "2275 [Discriminator loss: 0.889393, acc.: 60.94%] [Generator loss: 4.911565]\n",
      "2276 [Discriminator loss: 0.313672, acc.: 82.81%] [Generator loss: 4.873818]\n",
      "2277 [Discriminator loss: 0.291483, acc.: 85.94%] [Generator loss: 4.463931]\n",
      "2278 [Discriminator loss: 0.172250, acc.: 92.19%] [Generator loss: 4.338618]\n",
      "2279 [Discriminator loss: 0.414368, acc.: 81.25%] [Generator loss: 6.454595]\n",
      "2280 [Discriminator loss: 0.393644, acc.: 84.38%] [Generator loss: 4.384327]\n",
      "2281 [Discriminator loss: 0.702019, acc.: 65.62%] [Generator loss: 3.945894]\n",
      "2282 [Discriminator loss: 0.231901, acc.: 92.19%] [Generator loss: 4.945993]\n",
      "2283 [Discriminator loss: 0.476171, acc.: 79.69%] [Generator loss: 4.168741]\n",
      "2284 [Discriminator loss: 0.339753, acc.: 87.50%] [Generator loss: 3.319104]\n",
      "2285 [Discriminator loss: 0.426172, acc.: 85.94%] [Generator loss: 4.079536]\n",
      "2286 [Discriminator loss: 0.317998, acc.: 87.50%] [Generator loss: 3.626761]\n",
      "2287 [Discriminator loss: 0.766944, acc.: 67.19%] [Generator loss: 4.404033]\n",
      "2288 [Discriminator loss: 0.191836, acc.: 93.75%] [Generator loss: 4.504086]\n",
      "2289 [Discriminator loss: 0.271249, acc.: 87.50%] [Generator loss: 3.392477]\n",
      "2290 [Discriminator loss: 0.229498, acc.: 92.19%] [Generator loss: 3.549458]\n",
      "2291 [Discriminator loss: 0.426523, acc.: 79.69%] [Generator loss: 3.491078]\n",
      "2292 [Discriminator loss: 0.346667, acc.: 85.94%] [Generator loss: 4.812440]\n",
      "2293 [Discriminator loss: 0.541302, acc.: 73.44%] [Generator loss: 4.686157]\n",
      "2294 [Discriminator loss: 0.406334, acc.: 84.38%] [Generator loss: 3.947053]\n",
      "2295 [Discriminator loss: 0.211168, acc.: 89.06%] [Generator loss: 3.484004]\n",
      "2296 [Discriminator loss: 0.485082, acc.: 76.56%] [Generator loss: 3.754153]\n",
      "2297 [Discriminator loss: 0.163942, acc.: 93.75%] [Generator loss: 3.160476]\n",
      "2298 [Discriminator loss: 0.153237, acc.: 93.75%] [Generator loss: 3.684436]\n",
      "2299 [Discriminator loss: 0.291015, acc.: 85.94%] [Generator loss: 3.650697]\n",
      "2300 [Discriminator loss: 0.139876, acc.: 96.88%] [Generator loss: 4.896710]\n",
      "2301 [Discriminator loss: 0.286080, acc.: 87.50%] [Generator loss: 4.417625]\n",
      "2302 [Discriminator loss: 0.481391, acc.: 75.00%] [Generator loss: 3.547189]\n",
      "2303 [Discriminator loss: 0.169474, acc.: 92.19%] [Generator loss: 5.980453]\n",
      "2304 [Discriminator loss: 0.519629, acc.: 79.69%] [Generator loss: 3.175606]\n",
      "2305 [Discriminator loss: 0.371810, acc.: 79.69%] [Generator loss: 3.974489]\n",
      "2306 [Discriminator loss: 0.358212, acc.: 81.25%] [Generator loss: 4.508137]\n",
      "2307 [Discriminator loss: 0.347471, acc.: 84.38%] [Generator loss: 4.826640]\n",
      "2308 [Discriminator loss: 0.300275, acc.: 85.94%] [Generator loss: 4.570436]\n",
      "2309 [Discriminator loss: 0.710274, acc.: 70.31%] [Generator loss: 5.120831]\n",
      "2310 [Discriminator loss: 0.313650, acc.: 87.50%] [Generator loss: 3.599942]\n",
      "2311 [Discriminator loss: 0.348478, acc.: 81.25%] [Generator loss: 5.743996]\n",
      "2312 [Discriminator loss: 0.239329, acc.: 92.19%] [Generator loss: 5.482664]\n",
      "2313 [Discriminator loss: 0.609332, acc.: 65.62%] [Generator loss: 4.330916]\n",
      "2314 [Discriminator loss: 0.649504, acc.: 73.44%] [Generator loss: 4.700428]\n",
      "2315 [Discriminator loss: 0.343356, acc.: 85.94%] [Generator loss: 3.782340]\n",
      "2316 [Discriminator loss: 0.223779, acc.: 92.19%] [Generator loss: 4.186059]\n",
      "2317 [Discriminator loss: 0.422562, acc.: 81.25%] [Generator loss: 3.831972]\n",
      "2318 [Discriminator loss: 0.238283, acc.: 92.19%] [Generator loss: 3.504047]\n",
      "2319 [Discriminator loss: 0.563279, acc.: 70.31%] [Generator loss: 4.454176]\n",
      "2320 [Discriminator loss: 0.890908, acc.: 68.75%] [Generator loss: 3.563290]\n",
      "2321 [Discriminator loss: 0.646796, acc.: 76.56%] [Generator loss: 4.941249]\n",
      "2322 [Discriminator loss: 0.230424, acc.: 90.62%] [Generator loss: 4.163264]\n",
      "2323 [Discriminator loss: 0.157869, acc.: 95.31%] [Generator loss: 4.696238]\n",
      "2324 [Discriminator loss: 0.406560, acc.: 82.81%] [Generator loss: 4.269963]\n",
      "2325 [Discriminator loss: 0.285753, acc.: 85.94%] [Generator loss: 3.617500]\n",
      "2326 [Discriminator loss: 0.196398, acc.: 93.75%] [Generator loss: 2.892003]\n",
      "2327 [Discriminator loss: 0.626041, acc.: 78.12%] [Generator loss: 5.434300]\n",
      "2328 [Discriminator loss: 1.147844, acc.: 56.25%] [Generator loss: 3.145645]\n",
      "2329 [Discriminator loss: 0.267230, acc.: 85.94%] [Generator loss: 4.756212]\n",
      "2330 [Discriminator loss: 0.183912, acc.: 90.62%] [Generator loss: 4.668728]\n",
      "2331 [Discriminator loss: 0.391599, acc.: 84.38%] [Generator loss: 5.053194]\n",
      "2332 [Discriminator loss: 0.214600, acc.: 89.06%] [Generator loss: 4.486592]\n",
      "2333 [Discriminator loss: 0.353809, acc.: 84.38%] [Generator loss: 5.115138]\n",
      "2334 [Discriminator loss: 0.408785, acc.: 79.69%] [Generator loss: 4.389966]\n",
      "2335 [Discriminator loss: 0.483598, acc.: 76.56%] [Generator loss: 4.412141]\n",
      "2336 [Discriminator loss: 0.325586, acc.: 84.38%] [Generator loss: 4.566522]\n",
      "2337 [Discriminator loss: 0.448459, acc.: 79.69%] [Generator loss: 4.435854]\n",
      "2338 [Discriminator loss: 0.243368, acc.: 90.62%] [Generator loss: 4.017776]\n",
      "2339 [Discriminator loss: 0.660434, acc.: 65.62%] [Generator loss: 4.930479]\n",
      "2340 [Discriminator loss: 0.306954, acc.: 89.06%] [Generator loss: 4.497247]\n",
      "2341 [Discriminator loss: 0.775526, acc.: 64.06%] [Generator loss: 4.382962]\n",
      "2342 [Discriminator loss: 0.296775, acc.: 85.94%] [Generator loss: 4.710020]\n",
      "2343 [Discriminator loss: 0.550367, acc.: 78.12%] [Generator loss: 3.170938]\n",
      "2344 [Discriminator loss: 0.300469, acc.: 85.94%] [Generator loss: 4.773557]\n",
      "2345 [Discriminator loss: 0.297895, acc.: 87.50%] [Generator loss: 3.066239]\n",
      "2346 [Discriminator loss: 0.655883, acc.: 70.31%] [Generator loss: 5.753102]\n",
      "2347 [Discriminator loss: 0.325767, acc.: 87.50%] [Generator loss: 4.454728]\n",
      "2348 [Discriminator loss: 0.751204, acc.: 62.50%] [Generator loss: 4.161592]\n",
      "2349 [Discriminator loss: 0.294949, acc.: 84.38%] [Generator loss: 4.433088]\n",
      "2350 [Discriminator loss: 0.419624, acc.: 84.38%] [Generator loss: 3.322202]\n",
      "2351 [Discriminator loss: 0.348059, acc.: 87.50%] [Generator loss: 3.686860]\n",
      "2352 [Discriminator loss: 0.364475, acc.: 81.25%] [Generator loss: 3.700228]\n",
      "2353 [Discriminator loss: 0.234976, acc.: 90.62%] [Generator loss: 3.278488]\n",
      "2354 [Discriminator loss: 0.374322, acc.: 84.38%] [Generator loss: 3.984563]\n",
      "2355 [Discriminator loss: 0.594999, acc.: 73.44%] [Generator loss: 4.277967]\n",
      "2356 [Discriminator loss: 0.250236, acc.: 89.06%] [Generator loss: 4.120000]\n",
      "2357 [Discriminator loss: 0.298743, acc.: 81.25%] [Generator loss: 4.423742]\n",
      "2358 [Discriminator loss: 0.216036, acc.: 89.06%] [Generator loss: 2.619723]\n",
      "2359 [Discriminator loss: 0.694324, acc.: 75.00%] [Generator loss: 5.362980]\n",
      "2360 [Discriminator loss: 0.538938, acc.: 75.00%] [Generator loss: 3.213678]\n",
      "2361 [Discriminator loss: 0.351232, acc.: 82.81%] [Generator loss: 4.469586]\n",
      "2362 [Discriminator loss: 0.375269, acc.: 81.25%] [Generator loss: 4.736058]\n",
      "2363 [Discriminator loss: 0.332126, acc.: 87.50%] [Generator loss: 4.307432]\n",
      "2364 [Discriminator loss: 0.390516, acc.: 89.06%] [Generator loss: 2.838052]\n",
      "2365 [Discriminator loss: 0.405871, acc.: 84.38%] [Generator loss: 4.850392]\n",
      "2366 [Discriminator loss: 0.560195, acc.: 68.75%] [Generator loss: 3.893095]\n",
      "2367 [Discriminator loss: 0.438243, acc.: 81.25%] [Generator loss: 3.722880]\n",
      "2368 [Discriminator loss: 0.336684, acc.: 89.06%] [Generator loss: 3.485128]\n",
      "2369 [Discriminator loss: 0.225117, acc.: 92.19%] [Generator loss: 4.407365]\n",
      "2370 [Discriminator loss: 0.471565, acc.: 76.56%] [Generator loss: 4.083915]\n",
      "2371 [Discriminator loss: 0.572571, acc.: 65.62%] [Generator loss: 4.564722]\n",
      "2372 [Discriminator loss: 0.391671, acc.: 78.12%] [Generator loss: 3.091997]\n",
      "2373 [Discriminator loss: 0.200691, acc.: 90.62%] [Generator loss: 3.284031]\n",
      "2374 [Discriminator loss: 0.536928, acc.: 73.44%] [Generator loss: 5.191168]\n",
      "2375 [Discriminator loss: 0.373205, acc.: 84.38%] [Generator loss: 4.683437]\n",
      "2376 [Discriminator loss: 0.473922, acc.: 78.12%] [Generator loss: 3.702788]\n",
      "2377 [Discriminator loss: 0.353649, acc.: 82.81%] [Generator loss: 4.540385]\n",
      "2378 [Discriminator loss: 0.186549, acc.: 95.31%] [Generator loss: 4.513892]\n",
      "2379 [Discriminator loss: 0.299609, acc.: 85.94%] [Generator loss: 4.500500]\n",
      "2380 [Discriminator loss: 0.309483, acc.: 85.94%] [Generator loss: 4.148918]\n",
      "2381 [Discriminator loss: 0.372249, acc.: 84.38%] [Generator loss: 4.210090]\n",
      "2382 [Discriminator loss: 0.195864, acc.: 90.62%] [Generator loss: 3.637136]\n",
      "2383 [Discriminator loss: 0.280255, acc.: 89.06%] [Generator loss: 3.556727]\n",
      "2384 [Discriminator loss: 0.290274, acc.: 87.50%] [Generator loss: 4.146568]\n",
      "2385 [Discriminator loss: 0.464903, acc.: 85.94%] [Generator loss: 5.113194]\n",
      "2386 [Discriminator loss: 0.429498, acc.: 78.12%] [Generator loss: 4.042520]\n",
      "2387 [Discriminator loss: 0.409685, acc.: 79.69%] [Generator loss: 4.373110]\n",
      "2388 [Discriminator loss: 0.482551, acc.: 75.00%] [Generator loss: 2.821871]\n",
      "2389 [Discriminator loss: 0.694355, acc.: 71.88%] [Generator loss: 3.756327]\n",
      "2390 [Discriminator loss: 0.122624, acc.: 95.31%] [Generator loss: 6.485481]\n",
      "2391 [Discriminator loss: 0.514214, acc.: 76.56%] [Generator loss: 3.232856]\n",
      "2392 [Discriminator loss: 0.331984, acc.: 89.06%] [Generator loss: 4.205904]\n",
      "2393 [Discriminator loss: 0.230329, acc.: 89.06%] [Generator loss: 3.762261]\n",
      "2394 [Discriminator loss: 0.491107, acc.: 76.56%] [Generator loss: 3.379461]\n",
      "2395 [Discriminator loss: 0.181956, acc.: 92.19%] [Generator loss: 4.639181]\n",
      "2396 [Discriminator loss: 0.303376, acc.: 87.50%] [Generator loss: 4.546399]\n",
      "2397 [Discriminator loss: 0.383242, acc.: 79.69%] [Generator loss: 4.391412]\n",
      "2398 [Discriminator loss: 0.362442, acc.: 79.69%] [Generator loss: 3.195883]\n",
      "2399 [Discriminator loss: 0.339013, acc.: 85.94%] [Generator loss: 4.201571]\n",
      "2400 [Discriminator loss: 0.396094, acc.: 81.25%] [Generator loss: 2.999715]\n",
      "2401 [Discriminator loss: 0.839290, acc.: 64.06%] [Generator loss: 4.632488]\n",
      "2402 [Discriminator loss: 0.320902, acc.: 82.81%] [Generator loss: 4.113545]\n",
      "2403 [Discriminator loss: 0.354103, acc.: 87.50%] [Generator loss: 2.643319]\n",
      "2404 [Discriminator loss: 0.517242, acc.: 78.12%] [Generator loss: 5.067978]\n",
      "2405 [Discriminator loss: 0.393085, acc.: 82.81%] [Generator loss: 3.677403]\n",
      "2406 [Discriminator loss: 0.925387, acc.: 59.38%] [Generator loss: 4.251176]\n",
      "2407 [Discriminator loss: 0.296092, acc.: 92.19%] [Generator loss: 4.331476]\n",
      "2408 [Discriminator loss: 0.393699, acc.: 79.69%] [Generator loss: 4.762280]\n",
      "2409 [Discriminator loss: 0.275067, acc.: 90.62%] [Generator loss: 4.622659]\n",
      "2410 [Discriminator loss: 0.486714, acc.: 79.69%] [Generator loss: 4.340808]\n",
      "2411 [Discriminator loss: 0.342346, acc.: 85.94%] [Generator loss: 4.983176]\n",
      "2412 [Discriminator loss: 0.474124, acc.: 78.12%] [Generator loss: 4.786248]\n",
      "2413 [Discriminator loss: 0.305137, acc.: 84.38%] [Generator loss: 4.286181]\n",
      "2414 [Discriminator loss: 0.350642, acc.: 87.50%] [Generator loss: 3.779925]\n",
      "2415 [Discriminator loss: 0.318485, acc.: 87.50%] [Generator loss: 5.309615]\n",
      "2416 [Discriminator loss: 0.647439, acc.: 65.62%] [Generator loss: 3.851121]\n",
      "2417 [Discriminator loss: 0.141835, acc.: 96.88%] [Generator loss: 4.652517]\n",
      "2418 [Discriminator loss: 0.262717, acc.: 84.38%] [Generator loss: 3.776949]\n",
      "2419 [Discriminator loss: 0.486737, acc.: 79.69%] [Generator loss: 4.626988]\n",
      "2420 [Discriminator loss: 0.349032, acc.: 90.62%] [Generator loss: 4.546133]\n",
      "2421 [Discriminator loss: 0.457816, acc.: 84.38%] [Generator loss: 3.316399]\n",
      "2422 [Discriminator loss: 0.359022, acc.: 84.38%] [Generator loss: 4.277066]\n",
      "2423 [Discriminator loss: 0.322195, acc.: 89.06%] [Generator loss: 4.144455]\n",
      "2424 [Discriminator loss: 0.596124, acc.: 68.75%] [Generator loss: 4.002594]\n",
      "2425 [Discriminator loss: 0.104406, acc.: 96.88%] [Generator loss: 5.422283]\n",
      "2426 [Discriminator loss: 0.428691, acc.: 81.25%] [Generator loss: 2.828033]\n",
      "2427 [Discriminator loss: 0.381260, acc.: 82.81%] [Generator loss: 4.360280]\n",
      "2428 [Discriminator loss: 0.441292, acc.: 78.12%] [Generator loss: 5.191834]\n",
      "2429 [Discriminator loss: 0.342837, acc.: 82.81%] [Generator loss: 4.893674]\n",
      "2430 [Discriminator loss: 0.240761, acc.: 90.62%] [Generator loss: 3.474285]\n",
      "2431 [Discriminator loss: 0.323347, acc.: 89.06%] [Generator loss: 4.386358]\n",
      "2432 [Discriminator loss: 0.492202, acc.: 76.56%] [Generator loss: 3.604115]\n",
      "2433 [Discriminator loss: 0.182378, acc.: 90.62%] [Generator loss: 4.960720]\n",
      "2434 [Discriminator loss: 0.282866, acc.: 89.06%] [Generator loss: 3.712102]\n",
      "2435 [Discriminator loss: 0.407471, acc.: 84.38%] [Generator loss: 3.987269]\n",
      "2436 [Discriminator loss: 0.366016, acc.: 78.12%] [Generator loss: 4.713567]\n",
      "2437 [Discriminator loss: 0.388067, acc.: 81.25%] [Generator loss: 4.334136]\n",
      "2438 [Discriminator loss: 0.180356, acc.: 93.75%] [Generator loss: 3.459650]\n",
      "2439 [Discriminator loss: 0.571015, acc.: 73.44%] [Generator loss: 4.811465]\n",
      "2440 [Discriminator loss: 0.305622, acc.: 84.38%] [Generator loss: 4.145512]\n",
      "2441 [Discriminator loss: 0.209731, acc.: 90.62%] [Generator loss: 4.674484]\n",
      "2442 [Discriminator loss: 0.381715, acc.: 82.81%] [Generator loss: 3.464076]\n",
      "2443 [Discriminator loss: 0.170753, acc.: 93.75%] [Generator loss: 3.882713]\n",
      "2444 [Discriminator loss: 0.445738, acc.: 76.56%] [Generator loss: 3.785721]\n",
      "2445 [Discriminator loss: 0.435043, acc.: 76.56%] [Generator loss: 4.257483]\n",
      "2446 [Discriminator loss: 0.395051, acc.: 84.38%] [Generator loss: 3.650941]\n",
      "2447 [Discriminator loss: 0.599178, acc.: 68.75%] [Generator loss: 3.850388]\n",
      "2448 [Discriminator loss: 0.106822, acc.: 98.44%] [Generator loss: 3.474974]\n",
      "2449 [Discriminator loss: 0.553858, acc.: 71.88%] [Generator loss: 3.737462]\n",
      "2450 [Discriminator loss: 0.243907, acc.: 87.50%] [Generator loss: 4.388737]\n",
      "2451 [Discriminator loss: 0.374822, acc.: 82.81%] [Generator loss: 3.655274]\n",
      "2452 [Discriminator loss: 0.253886, acc.: 87.50%] [Generator loss: 4.349251]\n",
      "2453 [Discriminator loss: 0.474766, acc.: 76.56%] [Generator loss: 3.874801]\n",
      "2454 [Discriminator loss: 0.384877, acc.: 84.38%] [Generator loss: 4.107189]\n",
      "2455 [Discriminator loss: 0.216176, acc.: 93.75%] [Generator loss: 3.578105]\n",
      "2456 [Discriminator loss: 0.670612, acc.: 70.31%] [Generator loss: 4.519926]\n",
      "2457 [Discriminator loss: 0.381139, acc.: 84.38%] [Generator loss: 5.382419]\n",
      "2458 [Discriminator loss: 0.591045, acc.: 78.12%] [Generator loss: 2.829673]\n",
      "2459 [Discriminator loss: 0.620228, acc.: 68.75%] [Generator loss: 4.520926]\n",
      "2460 [Discriminator loss: 0.172648, acc.: 90.62%] [Generator loss: 4.590821]\n",
      "2461 [Discriminator loss: 0.385000, acc.: 81.25%] [Generator loss: 3.086322]\n",
      "2462 [Discriminator loss: 0.390714, acc.: 84.38%] [Generator loss: 3.448026]\n",
      "2463 [Discriminator loss: 0.299687, acc.: 82.81%] [Generator loss: 4.409253]\n",
      "2464 [Discriminator loss: 0.580437, acc.: 71.88%] [Generator loss: 4.056837]\n",
      "2465 [Discriminator loss: 0.487990, acc.: 81.25%] [Generator loss: 3.485164]\n",
      "2466 [Discriminator loss: 0.599210, acc.: 70.31%] [Generator loss: 3.422447]\n",
      "2467 [Discriminator loss: 0.529271, acc.: 71.88%] [Generator loss: 3.688612]\n",
      "2468 [Discriminator loss: 0.360869, acc.: 85.94%] [Generator loss: 3.928017]\n",
      "2469 [Discriminator loss: 0.696661, acc.: 67.19%] [Generator loss: 4.039688]\n",
      "2470 [Discriminator loss: 0.379298, acc.: 81.25%] [Generator loss: 4.380379]\n",
      "2471 [Discriminator loss: 0.505884, acc.: 79.69%] [Generator loss: 3.262323]\n",
      "2472 [Discriminator loss: 0.517567, acc.: 70.31%] [Generator loss: 4.174759]\n",
      "2473 [Discriminator loss: 0.289854, acc.: 87.50%] [Generator loss: 4.687585]\n",
      "2474 [Discriminator loss: 0.406380, acc.: 79.69%] [Generator loss: 3.630341]\n",
      "2475 [Discriminator loss: 0.446446, acc.: 82.81%] [Generator loss: 5.225784]\n",
      "2476 [Discriminator loss: 0.290778, acc.: 85.94%] [Generator loss: 4.728113]\n",
      "2477 [Discriminator loss: 0.699986, acc.: 64.06%] [Generator loss: 3.706688]\n",
      "2478 [Discriminator loss: 0.271787, acc.: 85.94%] [Generator loss: 4.182141]\n",
      "2479 [Discriminator loss: 0.387232, acc.: 84.38%] [Generator loss: 3.935240]\n",
      "2480 [Discriminator loss: 0.399380, acc.: 82.81%] [Generator loss: 3.729055]\n",
      "2481 [Discriminator loss: 0.247964, acc.: 87.50%] [Generator loss: 2.810809]\n",
      "2482 [Discriminator loss: 0.486930, acc.: 79.69%] [Generator loss: 3.509530]\n",
      "2483 [Discriminator loss: 0.521303, acc.: 75.00%] [Generator loss: 3.534574]\n",
      "2484 [Discriminator loss: 0.415015, acc.: 82.81%] [Generator loss: 3.865276]\n",
      "2485 [Discriminator loss: 0.372133, acc.: 82.81%] [Generator loss: 3.958851]\n",
      "2486 [Discriminator loss: 0.319668, acc.: 87.50%] [Generator loss: 4.895644]\n",
      "2487 [Discriminator loss: 0.618547, acc.: 67.19%] [Generator loss: 4.055532]\n",
      "2488 [Discriminator loss: 0.341945, acc.: 89.06%] [Generator loss: 3.563131]\n",
      "2489 [Discriminator loss: 0.337608, acc.: 87.50%] [Generator loss: 3.577881]\n",
      "2490 [Discriminator loss: 0.359166, acc.: 82.81%] [Generator loss: 3.435520]\n",
      "2491 [Discriminator loss: 0.281586, acc.: 89.06%] [Generator loss: 3.558818]\n",
      "2492 [Discriminator loss: 0.275393, acc.: 89.06%] [Generator loss: 3.084277]\n",
      "2493 [Discriminator loss: 0.473931, acc.: 82.81%] [Generator loss: 3.732080]\n",
      "2494 [Discriminator loss: 0.455891, acc.: 85.94%] [Generator loss: 4.258585]\n",
      "2495 [Discriminator loss: 0.304509, acc.: 87.50%] [Generator loss: 3.992983]\n",
      "2496 [Discriminator loss: 0.496109, acc.: 71.88%] [Generator loss: 4.130397]\n",
      "2497 [Discriminator loss: 0.502821, acc.: 76.56%] [Generator loss: 4.150677]\n",
      "2498 [Discriminator loss: 0.214396, acc.: 87.50%] [Generator loss: 3.999715]\n",
      "2499 [Discriminator loss: 0.506122, acc.: 75.00%] [Generator loss: 4.728150]\n",
      "2500 [Discriminator loss: 0.568761, acc.: 71.88%] [Generator loss: 3.186953]\n",
      "2501 [Discriminator loss: 0.231014, acc.: 90.62%] [Generator loss: 3.655283]\n",
      "2502 [Discriminator loss: 0.528039, acc.: 73.44%] [Generator loss: 5.464792]\n",
      "2503 [Discriminator loss: 0.499652, acc.: 76.56%] [Generator loss: 3.852979]\n",
      "2504 [Discriminator loss: 0.857728, acc.: 59.38%] [Generator loss: 5.367499]\n",
      "2505 [Discriminator loss: 0.273348, acc.: 85.94%] [Generator loss: 4.375837]\n",
      "2506 [Discriminator loss: 0.166787, acc.: 96.88%] [Generator loss: 3.566996]\n",
      "2507 [Discriminator loss: 0.240529, acc.: 92.19%] [Generator loss: 3.109612]\n",
      "2508 [Discriminator loss: 0.145736, acc.: 98.44%] [Generator loss: 3.631968]\n",
      "2509 [Discriminator loss: 0.734605, acc.: 62.50%] [Generator loss: 6.052108]\n",
      "2510 [Discriminator loss: 0.581951, acc.: 76.56%] [Generator loss: 4.459492]\n",
      "2511 [Discriminator loss: 0.287694, acc.: 85.94%] [Generator loss: 4.326109]\n",
      "2512 [Discriminator loss: 0.247685, acc.: 85.94%] [Generator loss: 3.308441]\n",
      "2513 [Discriminator loss: 0.370094, acc.: 89.06%] [Generator loss: 3.888587]\n",
      "2514 [Discriminator loss: 0.215704, acc.: 89.06%] [Generator loss: 3.352129]\n",
      "2515 [Discriminator loss: 0.260216, acc.: 82.81%] [Generator loss: 4.256361]\n",
      "2516 [Discriminator loss: 0.314462, acc.: 85.94%] [Generator loss: 3.995162]\n",
      "2517 [Discriminator loss: 0.684718, acc.: 73.44%] [Generator loss: 3.256036]\n",
      "2518 [Discriminator loss: 0.362294, acc.: 85.94%] [Generator loss: 3.803937]\n",
      "2519 [Discriminator loss: 0.461721, acc.: 79.69%] [Generator loss: 3.839980]\n",
      "2520 [Discriminator loss: 0.308255, acc.: 84.38%] [Generator loss: 4.548703]\n",
      "2521 [Discriminator loss: 0.308652, acc.: 79.69%] [Generator loss: 3.588973]\n",
      "2522 [Discriminator loss: 0.264979, acc.: 87.50%] [Generator loss: 3.969064]\n",
      "2523 [Discriminator loss: 0.774612, acc.: 68.75%] [Generator loss: 3.820161]\n",
      "2524 [Discriminator loss: 0.511863, acc.: 79.69%] [Generator loss: 2.723487]\n",
      "2525 [Discriminator loss: 0.441515, acc.: 76.56%] [Generator loss: 3.242988]\n",
      "2526 [Discriminator loss: 0.284138, acc.: 85.94%] [Generator loss: 3.497691]\n",
      "2527 [Discriminator loss: 0.741489, acc.: 68.75%] [Generator loss: 4.592216]\n",
      "2528 [Discriminator loss: 0.414004, acc.: 85.94%] [Generator loss: 4.189267]\n",
      "2529 [Discriminator loss: 0.665982, acc.: 67.19%] [Generator loss: 4.682661]\n",
      "2530 [Discriminator loss: 0.212962, acc.: 90.62%] [Generator loss: 3.872629]\n",
      "2531 [Discriminator loss: 0.907396, acc.: 60.94%] [Generator loss: 5.517519]\n",
      "2532 [Discriminator loss: 0.506453, acc.: 79.69%] [Generator loss: 3.901734]\n",
      "2533 [Discriminator loss: 0.339788, acc.: 85.94%] [Generator loss: 3.776215]\n",
      "2534 [Discriminator loss: 0.374687, acc.: 81.25%] [Generator loss: 4.709970]\n",
      "2535 [Discriminator loss: 0.340338, acc.: 82.81%] [Generator loss: 4.170851]\n",
      "2536 [Discriminator loss: 0.327407, acc.: 81.25%] [Generator loss: 3.580001]\n",
      "2537 [Discriminator loss: 0.454607, acc.: 78.12%] [Generator loss: 4.605000]\n",
      "2538 [Discriminator loss: 0.210853, acc.: 93.75%] [Generator loss: 4.894242]\n",
      "2539 [Discriminator loss: 0.314441, acc.: 84.38%] [Generator loss: 2.357809]\n",
      "2540 [Discriminator loss: 0.283887, acc.: 90.62%] [Generator loss: 2.466228]\n",
      "2541 [Discriminator loss: 0.396079, acc.: 78.12%] [Generator loss: 3.214150]\n",
      "2542 [Discriminator loss: 0.187611, acc.: 90.62%] [Generator loss: 4.209227]\n",
      "2543 [Discriminator loss: 0.636701, acc.: 68.75%] [Generator loss: 3.913979]\n",
      "2544 [Discriminator loss: 0.388303, acc.: 76.56%] [Generator loss: 3.881500]\n",
      "2545 [Discriminator loss: 0.412599, acc.: 79.69%] [Generator loss: 3.223611]\n",
      "2546 [Discriminator loss: 0.407423, acc.: 82.81%] [Generator loss: 3.674469]\n",
      "2547 [Discriminator loss: 0.268150, acc.: 89.06%] [Generator loss: 4.975665]\n",
      "2548 [Discriminator loss: 0.478911, acc.: 82.81%] [Generator loss: 3.355143]\n",
      "2549 [Discriminator loss: 0.398504, acc.: 81.25%] [Generator loss: 4.322654]\n",
      "2550 [Discriminator loss: 0.329100, acc.: 87.50%] [Generator loss: 4.179996]\n",
      "2551 [Discriminator loss: 0.685386, acc.: 70.31%] [Generator loss: 4.571015]\n",
      "2552 [Discriminator loss: 0.604126, acc.: 76.56%] [Generator loss: 3.437481]\n",
      "2553 [Discriminator loss: 0.341910, acc.: 85.94%] [Generator loss: 3.504447]\n",
      "2554 [Discriminator loss: 0.562656, acc.: 76.56%] [Generator loss: 4.324904]\n",
      "2555 [Discriminator loss: 0.375534, acc.: 84.38%] [Generator loss: 4.383859]\n",
      "2556 [Discriminator loss: 0.413075, acc.: 79.69%] [Generator loss: 4.730816]\n",
      "2557 [Discriminator loss: 0.383597, acc.: 82.81%] [Generator loss: 3.619346]\n",
      "2558 [Discriminator loss: 0.375063, acc.: 85.94%] [Generator loss: 3.545427]\n",
      "2559 [Discriminator loss: 0.379522, acc.: 84.38%] [Generator loss: 3.918118]\n",
      "2560 [Discriminator loss: 0.515164, acc.: 78.12%] [Generator loss: 3.818912]\n",
      "2561 [Discriminator loss: 0.353258, acc.: 87.50%] [Generator loss: 3.892406]\n",
      "2562 [Discriminator loss: 0.232189, acc.: 90.62%] [Generator loss: 4.114598]\n",
      "2563 [Discriminator loss: 0.388993, acc.: 81.25%] [Generator loss: 4.346485]\n",
      "2564 [Discriminator loss: 0.319600, acc.: 84.38%] [Generator loss: 4.157529]\n",
      "2565 [Discriminator loss: 0.349626, acc.: 84.38%] [Generator loss: 4.360275]\n",
      "2566 [Discriminator loss: 0.619832, acc.: 65.62%] [Generator loss: 3.558112]\n",
      "2567 [Discriminator loss: 0.221475, acc.: 92.19%] [Generator loss: 4.798624]\n",
      "2568 [Discriminator loss: 0.564173, acc.: 73.44%] [Generator loss: 3.418906]\n",
      "2569 [Discriminator loss: 0.595109, acc.: 73.44%] [Generator loss: 4.233695]\n",
      "2570 [Discriminator loss: 0.169092, acc.: 93.75%] [Generator loss: 4.480862]\n",
      "2571 [Discriminator loss: 0.553030, acc.: 68.75%] [Generator loss: 3.657657]\n",
      "2572 [Discriminator loss: 0.249463, acc.: 90.62%] [Generator loss: 3.987512]\n",
      "2573 [Discriminator loss: 0.434832, acc.: 82.81%] [Generator loss: 4.196627]\n",
      "2574 [Discriminator loss: 0.413506, acc.: 82.81%] [Generator loss: 3.878457]\n",
      "2575 [Discriminator loss: 0.265674, acc.: 89.06%] [Generator loss: 3.373777]\n",
      "2576 [Discriminator loss: 0.384809, acc.: 81.25%] [Generator loss: 4.459023]\n",
      "2577 [Discriminator loss: 0.412880, acc.: 78.12%] [Generator loss: 4.193895]\n",
      "2578 [Discriminator loss: 0.720052, acc.: 59.38%] [Generator loss: 4.106979]\n",
      "2579 [Discriminator loss: 0.169671, acc.: 95.31%] [Generator loss: 3.765874]\n",
      "2580 [Discriminator loss: 0.472606, acc.: 78.12%] [Generator loss: 3.527192]\n",
      "2581 [Discriminator loss: 0.378187, acc.: 82.81%] [Generator loss: 4.558084]\n",
      "2582 [Discriminator loss: 0.593294, acc.: 70.31%] [Generator loss: 3.373562]\n",
      "2583 [Discriminator loss: 0.557009, acc.: 75.00%] [Generator loss: 4.211992]\n",
      "2584 [Discriminator loss: 0.214426, acc.: 85.94%] [Generator loss: 3.513481]\n",
      "2585 [Discriminator loss: 0.407968, acc.: 79.69%] [Generator loss: 4.261749]\n",
      "2586 [Discriminator loss: 0.345319, acc.: 90.62%] [Generator loss: 3.065575]\n",
      "2587 [Discriminator loss: 0.429035, acc.: 79.69%] [Generator loss: 5.091883]\n",
      "2588 [Discriminator loss: 0.417065, acc.: 79.69%] [Generator loss: 2.796475]\n",
      "2589 [Discriminator loss: 0.357506, acc.: 82.81%] [Generator loss: 4.029106]\n",
      "2590 [Discriminator loss: 0.257809, acc.: 89.06%] [Generator loss: 3.528513]\n",
      "2591 [Discriminator loss: 0.400487, acc.: 81.25%] [Generator loss: 3.846377]\n",
      "2592 [Discriminator loss: 0.339917, acc.: 87.50%] [Generator loss: 3.226675]\n",
      "2593 [Discriminator loss: 0.528549, acc.: 75.00%] [Generator loss: 4.217345]\n",
      "2594 [Discriminator loss: 0.388530, acc.: 82.81%] [Generator loss: 3.982310]\n",
      "2595 [Discriminator loss: 0.327966, acc.: 84.38%] [Generator loss: 2.590490]\n",
      "2596 [Discriminator loss: 0.397133, acc.: 76.56%] [Generator loss: 3.717797]\n",
      "2597 [Discriminator loss: 0.162168, acc.: 92.19%] [Generator loss: 4.187131]\n",
      "2598 [Discriminator loss: 0.354661, acc.: 81.25%] [Generator loss: 3.516122]\n",
      "2599 [Discriminator loss: 0.630410, acc.: 70.31%] [Generator loss: 2.798834]\n",
      "2600 [Discriminator loss: 0.220901, acc.: 90.62%] [Generator loss: 3.626175]\n",
      "2601 [Discriminator loss: 0.417786, acc.: 76.56%] [Generator loss: 4.950878]\n",
      "2602 [Discriminator loss: 0.181600, acc.: 93.75%] [Generator loss: 4.660877]\n",
      "2603 [Discriminator loss: 0.637616, acc.: 73.44%] [Generator loss: 2.798900]\n",
      "2604 [Discriminator loss: 0.112203, acc.: 96.88%] [Generator loss: 2.873124]\n",
      "2605 [Discriminator loss: 0.346435, acc.: 82.81%] [Generator loss: 4.223551]\n",
      "2606 [Discriminator loss: 0.289859, acc.: 87.50%] [Generator loss: 4.101575]\n",
      "2607 [Discriminator loss: 0.700960, acc.: 78.12%] [Generator loss: 4.334623]\n",
      "2608 [Discriminator loss: 0.273650, acc.: 85.94%] [Generator loss: 4.214005]\n",
      "2609 [Discriminator loss: 0.266049, acc.: 85.94%] [Generator loss: 3.730575]\n",
      "2610 [Discriminator loss: 0.280302, acc.: 87.50%] [Generator loss: 4.392559]\n",
      "2611 [Discriminator loss: 0.449314, acc.: 79.69%] [Generator loss: 4.063970]\n",
      "2612 [Discriminator loss: 0.444454, acc.: 75.00%] [Generator loss: 5.445744]\n",
      "2613 [Discriminator loss: 0.489581, acc.: 76.56%] [Generator loss: 4.099392]\n",
      "2614 [Discriminator loss: 0.503615, acc.: 75.00%] [Generator loss: 3.757223]\n",
      "2615 [Discriminator loss: 0.592093, acc.: 78.12%] [Generator loss: 3.715304]\n",
      "2616 [Discriminator loss: 0.313642, acc.: 82.81%] [Generator loss: 3.146301]\n",
      "2617 [Discriminator loss: 0.348748, acc.: 79.69%] [Generator loss: 3.400289]\n",
      "2618 [Discriminator loss: 0.407248, acc.: 78.12%] [Generator loss: 3.900605]\n",
      "2619 [Discriminator loss: 0.433626, acc.: 79.69%] [Generator loss: 2.666111]\n",
      "2620 [Discriminator loss: 0.728402, acc.: 67.19%] [Generator loss: 4.972537]\n",
      "2621 [Discriminator loss: 0.627327, acc.: 67.19%] [Generator loss: 3.383963]\n",
      "2622 [Discriminator loss: 0.426667, acc.: 81.25%] [Generator loss: 4.324708]\n",
      "2623 [Discriminator loss: 0.464560, acc.: 78.12%] [Generator loss: 5.231776]\n",
      "2624 [Discriminator loss: 0.473295, acc.: 79.69%] [Generator loss: 4.331425]\n",
      "2625 [Discriminator loss: 0.278686, acc.: 89.06%] [Generator loss: 3.631580]\n",
      "2626 [Discriminator loss: 0.569895, acc.: 68.75%] [Generator loss: 3.580452]\n",
      "2627 [Discriminator loss: 0.197724, acc.: 92.19%] [Generator loss: 3.818306]\n",
      "2628 [Discriminator loss: 0.488716, acc.: 82.81%] [Generator loss: 3.574479]\n",
      "2629 [Discriminator loss: 0.395271, acc.: 79.69%] [Generator loss: 2.752118]\n",
      "2630 [Discriminator loss: 0.478938, acc.: 79.69%] [Generator loss: 3.874425]\n",
      "2631 [Discriminator loss: 0.354767, acc.: 85.94%] [Generator loss: 3.867953]\n",
      "2632 [Discriminator loss: 0.474105, acc.: 82.81%] [Generator loss: 3.670616]\n",
      "2633 [Discriminator loss: 0.337319, acc.: 82.81%] [Generator loss: 3.037944]\n",
      "2634 [Discriminator loss: 0.268079, acc.: 87.50%] [Generator loss: 2.786101]\n",
      "2635 [Discriminator loss: 0.221661, acc.: 92.19%] [Generator loss: 3.885375]\n",
      "2636 [Discriminator loss: 0.498626, acc.: 79.69%] [Generator loss: 3.408071]\n",
      "2637 [Discriminator loss: 0.277799, acc.: 89.06%] [Generator loss: 4.300886]\n",
      "2638 [Discriminator loss: 0.368866, acc.: 85.94%] [Generator loss: 3.478174]\n",
      "2639 [Discriminator loss: 0.489426, acc.: 81.25%] [Generator loss: 3.393242]\n",
      "2640 [Discriminator loss: 0.907620, acc.: 56.25%] [Generator loss: 4.220707]\n",
      "2641 [Discriminator loss: 0.453757, acc.: 76.56%] [Generator loss: 3.847120]\n",
      "2642 [Discriminator loss: 0.490888, acc.: 76.56%] [Generator loss: 4.335353]\n",
      "2643 [Discriminator loss: 0.253925, acc.: 89.06%] [Generator loss: 4.104073]\n",
      "2644 [Discriminator loss: 0.378166, acc.: 85.94%] [Generator loss: 3.904376]\n",
      "2645 [Discriminator loss: 0.210748, acc.: 93.75%] [Generator loss: 3.735132]\n",
      "2646 [Discriminator loss: 0.310054, acc.: 82.81%] [Generator loss: 3.372783]\n",
      "2647 [Discriminator loss: 0.771903, acc.: 71.88%] [Generator loss: 4.127938]\n",
      "2648 [Discriminator loss: 0.253203, acc.: 84.38%] [Generator loss: 3.572554]\n",
      "2649 [Discriminator loss: 0.872192, acc.: 65.62%] [Generator loss: 4.099146]\n",
      "2650 [Discriminator loss: 0.225069, acc.: 95.31%] [Generator loss: 5.201166]\n",
      "2651 [Discriminator loss: 0.432950, acc.: 78.12%] [Generator loss: 3.555603]\n",
      "2652 [Discriminator loss: 0.282098, acc.: 89.06%] [Generator loss: 4.137557]\n",
      "2653 [Discriminator loss: 0.395538, acc.: 78.12%] [Generator loss: 3.732805]\n",
      "2654 [Discriminator loss: 0.486892, acc.: 79.69%] [Generator loss: 4.361898]\n",
      "2655 [Discriminator loss: 0.348021, acc.: 82.81%] [Generator loss: 3.699330]\n",
      "2656 [Discriminator loss: 0.502091, acc.: 70.31%] [Generator loss: 3.896630]\n",
      "2657 [Discriminator loss: 0.257194, acc.: 89.06%] [Generator loss: 4.268910]\n",
      "2658 [Discriminator loss: 0.698928, acc.: 71.88%] [Generator loss: 5.757792]\n",
      "2659 [Discriminator loss: 0.256115, acc.: 87.50%] [Generator loss: 4.252725]\n",
      "2660 [Discriminator loss: 0.382716, acc.: 84.38%] [Generator loss: 3.696434]\n",
      "2661 [Discriminator loss: 0.543185, acc.: 78.12%] [Generator loss: 3.610229]\n",
      "2662 [Discriminator loss: 0.148822, acc.: 92.19%] [Generator loss: 4.614556]\n",
      "2663 [Discriminator loss: 0.253034, acc.: 81.25%] [Generator loss: 3.704174]\n",
      "2664 [Discriminator loss: 0.542213, acc.: 78.12%] [Generator loss: 4.149199]\n",
      "2665 [Discriminator loss: 0.460437, acc.: 84.38%] [Generator loss: 4.613590]\n",
      "2666 [Discriminator loss: 0.176326, acc.: 92.19%] [Generator loss: 3.721406]\n",
      "2667 [Discriminator loss: 0.571906, acc.: 71.88%] [Generator loss: 3.237814]\n",
      "2668 [Discriminator loss: 0.419918, acc.: 81.25%] [Generator loss: 4.628847]\n",
      "2669 [Discriminator loss: 0.516283, acc.: 75.00%] [Generator loss: 3.395430]\n",
      "2670 [Discriminator loss: 0.435114, acc.: 78.12%] [Generator loss: 3.786190]\n",
      "2671 [Discriminator loss: 0.401583, acc.: 79.69%] [Generator loss: 5.014924]\n",
      "2672 [Discriminator loss: 0.350768, acc.: 84.38%] [Generator loss: 4.594530]\n",
      "2673 [Discriminator loss: 0.561543, acc.: 73.44%] [Generator loss: 3.443117]\n",
      "2674 [Discriminator loss: 0.320556, acc.: 84.38%] [Generator loss: 4.050738]\n",
      "2675 [Discriminator loss: 0.178658, acc.: 90.62%] [Generator loss: 4.187960]\n",
      "2676 [Discriminator loss: 0.279519, acc.: 89.06%] [Generator loss: 3.120799]\n",
      "2677 [Discriminator loss: 0.616105, acc.: 71.88%] [Generator loss: 5.540459]\n",
      "2678 [Discriminator loss: 0.482261, acc.: 78.12%] [Generator loss: 4.633277]\n",
      "2679 [Discriminator loss: 0.181237, acc.: 95.31%] [Generator loss: 3.866268]\n",
      "2680 [Discriminator loss: 0.391708, acc.: 85.94%] [Generator loss: 4.383334]\n",
      "2681 [Discriminator loss: 0.454032, acc.: 81.25%] [Generator loss: 4.023765]\n",
      "2682 [Discriminator loss: 0.407802, acc.: 81.25%] [Generator loss: 4.131775]\n",
      "2683 [Discriminator loss: 0.358762, acc.: 81.25%] [Generator loss: 3.095652]\n",
      "2684 [Discriminator loss: 0.271205, acc.: 92.19%] [Generator loss: 3.211727]\n",
      "2685 [Discriminator loss: 0.298988, acc.: 87.50%] [Generator loss: 3.455136]\n",
      "2686 [Discriminator loss: 0.673030, acc.: 76.56%] [Generator loss: 4.415193]\n",
      "2687 [Discriminator loss: 0.262873, acc.: 85.94%] [Generator loss: 3.972364]\n",
      "2688 [Discriminator loss: 0.347349, acc.: 89.06%] [Generator loss: 3.830947]\n",
      "2689 [Discriminator loss: 0.508657, acc.: 75.00%] [Generator loss: 4.540411]\n",
      "2690 [Discriminator loss: 0.237188, acc.: 92.19%] [Generator loss: 4.206355]\n",
      "2691 [Discriminator loss: 0.459554, acc.: 71.88%] [Generator loss: 4.270680]\n",
      "2692 [Discriminator loss: 0.308878, acc.: 87.50%] [Generator loss: 3.654673]\n",
      "2693 [Discriminator loss: 0.290069, acc.: 87.50%] [Generator loss: 4.208997]\n",
      "2694 [Discriminator loss: 0.297509, acc.: 89.06%] [Generator loss: 3.813816]\n",
      "2695 [Discriminator loss: 0.298800, acc.: 82.81%] [Generator loss: 4.031651]\n",
      "2696 [Discriminator loss: 0.439623, acc.: 82.81%] [Generator loss: 4.457863]\n",
      "2697 [Discriminator loss: 0.488867, acc.: 81.25%] [Generator loss: 4.514443]\n",
      "2698 [Discriminator loss: 0.375732, acc.: 84.38%] [Generator loss: 3.299708]\n",
      "2699 [Discriminator loss: 0.371477, acc.: 79.69%] [Generator loss: 3.987972]\n",
      "2700 [Discriminator loss: 0.410227, acc.: 81.25%] [Generator loss: 4.158364]\n",
      "2701 [Discriminator loss: 0.657070, acc.: 70.31%] [Generator loss: 3.579666]\n",
      "2702 [Discriminator loss: 0.438222, acc.: 79.69%] [Generator loss: 3.493117]\n",
      "2703 [Discriminator loss: 0.302052, acc.: 90.62%] [Generator loss: 3.834610]\n",
      "2704 [Discriminator loss: 0.417050, acc.: 75.00%] [Generator loss: 4.609799]\n",
      "2705 [Discriminator loss: 0.519749, acc.: 79.69%] [Generator loss: 4.217676]\n",
      "2706 [Discriminator loss: 0.532679, acc.: 75.00%] [Generator loss: 4.839261]\n",
      "2707 [Discriminator loss: 0.159214, acc.: 95.31%] [Generator loss: 4.382922]\n",
      "2708 [Discriminator loss: 0.295008, acc.: 87.50%] [Generator loss: 2.911367]\n",
      "2709 [Discriminator loss: 0.252453, acc.: 89.06%] [Generator loss: 3.052107]\n",
      "2710 [Discriminator loss: 0.548831, acc.: 71.88%] [Generator loss: 3.955881]\n",
      "2711 [Discriminator loss: 0.348314, acc.: 84.38%] [Generator loss: 4.290870]\n",
      "2712 [Discriminator loss: 0.593504, acc.: 71.88%] [Generator loss: 5.237366]\n",
      "2713 [Discriminator loss: 0.323020, acc.: 81.25%] [Generator loss: 3.485794]\n",
      "2714 [Discriminator loss: 0.359088, acc.: 84.38%] [Generator loss: 4.539123]\n",
      "2715 [Discriminator loss: 0.291020, acc.: 87.50%] [Generator loss: 4.096007]\n",
      "2716 [Discriminator loss: 0.340273, acc.: 87.50%] [Generator loss: 3.273330]\n",
      "2717 [Discriminator loss: 0.620512, acc.: 65.62%] [Generator loss: 4.607882]\n",
      "2718 [Discriminator loss: 0.301594, acc.: 89.06%] [Generator loss: 3.849145]\n",
      "2719 [Discriminator loss: 0.403918, acc.: 79.69%] [Generator loss: 4.606615]\n",
      "2720 [Discriminator loss: 0.602043, acc.: 76.56%] [Generator loss: 4.134828]\n",
      "2721 [Discriminator loss: 0.323348, acc.: 87.50%] [Generator loss: 4.831160]\n",
      "2722 [Discriminator loss: 0.380420, acc.: 81.25%] [Generator loss: 3.973817]\n",
      "2723 [Discriminator loss: 0.278461, acc.: 87.50%] [Generator loss: 4.142622]\n",
      "2724 [Discriminator loss: 0.410129, acc.: 79.69%] [Generator loss: 4.550419]\n",
      "2725 [Discriminator loss: 0.414239, acc.: 85.94%] [Generator loss: 3.934875]\n",
      "2726 [Discriminator loss: 0.527952, acc.: 76.56%] [Generator loss: 3.260625]\n",
      "2727 [Discriminator loss: 0.191273, acc.: 93.75%] [Generator loss: 4.261465]\n",
      "2728 [Discriminator loss: 0.518595, acc.: 79.69%] [Generator loss: 3.113014]\n",
      "2729 [Discriminator loss: 0.297621, acc.: 87.50%] [Generator loss: 3.784511]\n",
      "2730 [Discriminator loss: 0.382529, acc.: 82.81%] [Generator loss: 3.451542]\n",
      "2731 [Discriminator loss: 0.273254, acc.: 84.38%] [Generator loss: 3.733029]\n",
      "2732 [Discriminator loss: 0.449965, acc.: 78.12%] [Generator loss: 3.612051]\n",
      "2733 [Discriminator loss: 0.301051, acc.: 89.06%] [Generator loss: 2.363077]\n",
      "2734 [Discriminator loss: 0.328909, acc.: 85.94%] [Generator loss: 3.612285]\n",
      "2735 [Discriminator loss: 0.540825, acc.: 75.00%] [Generator loss: 3.935929]\n",
      "2736 [Discriminator loss: 0.433897, acc.: 78.12%] [Generator loss: 3.943239]\n",
      "2737 [Discriminator loss: 0.370823, acc.: 79.69%] [Generator loss: 4.770767]\n",
      "2738 [Discriminator loss: 0.406881, acc.: 81.25%] [Generator loss: 5.132558]\n",
      "2739 [Discriminator loss: 0.573037, acc.: 78.12%] [Generator loss: 3.026421]\n",
      "2740 [Discriminator loss: 0.666041, acc.: 60.94%] [Generator loss: 4.474989]\n",
      "2741 [Discriminator loss: 0.277493, acc.: 87.50%] [Generator loss: 4.752824]\n",
      "2742 [Discriminator loss: 0.584938, acc.: 70.31%] [Generator loss: 2.908524]\n",
      "2743 [Discriminator loss: 0.192978, acc.: 92.19%] [Generator loss: 3.024480]\n",
      "2744 [Discriminator loss: 0.318579, acc.: 84.38%] [Generator loss: 3.238688]\n",
      "2745 [Discriminator loss: 0.337379, acc.: 85.94%] [Generator loss: 4.449894]\n",
      "2746 [Discriminator loss: 0.521837, acc.: 82.81%] [Generator loss: 3.567156]\n",
      "2747 [Discriminator loss: 0.448464, acc.: 75.00%] [Generator loss: 4.695058]\n",
      "2748 [Discriminator loss: 0.316989, acc.: 78.12%] [Generator loss: 4.314325]\n",
      "2749 [Discriminator loss: 0.545886, acc.: 68.75%] [Generator loss: 3.232614]\n",
      "2750 [Discriminator loss: 0.413203, acc.: 76.56%] [Generator loss: 4.427636]\n",
      "2751 [Discriminator loss: 0.560333, acc.: 79.69%] [Generator loss: 3.775959]\n",
      "2752 [Discriminator loss: 0.497319, acc.: 76.56%] [Generator loss: 4.401301]\n",
      "2753 [Discriminator loss: 0.232573, acc.: 89.06%] [Generator loss: 4.290666]\n",
      "2754 [Discriminator loss: 0.332806, acc.: 81.25%] [Generator loss: 3.990798]\n",
      "2755 [Discriminator loss: 0.369415, acc.: 79.69%] [Generator loss: 3.967007]\n",
      "2756 [Discriminator loss: 0.335420, acc.: 90.62%] [Generator loss: 4.220505]\n",
      "2757 [Discriminator loss: 0.402039, acc.: 79.69%] [Generator loss: 3.423308]\n",
      "2758 [Discriminator loss: 0.367954, acc.: 85.94%] [Generator loss: 3.942101]\n",
      "2759 [Discriminator loss: 0.288959, acc.: 90.62%] [Generator loss: 4.331798]\n",
      "2760 [Discriminator loss: 0.312995, acc.: 87.50%] [Generator loss: 2.902645]\n",
      "2761 [Discriminator loss: 0.363521, acc.: 87.50%] [Generator loss: 3.692754]\n",
      "2762 [Discriminator loss: 0.302854, acc.: 92.19%] [Generator loss: 3.507956]\n",
      "2763 [Discriminator loss: 0.952761, acc.: 59.38%] [Generator loss: 3.843238]\n",
      "2764 [Discriminator loss: 0.225264, acc.: 90.62%] [Generator loss: 4.601312]\n",
      "2765 [Discriminator loss: 0.326120, acc.: 87.50%] [Generator loss: 2.829056]\n",
      "2766 [Discriminator loss: 0.419276, acc.: 81.25%] [Generator loss: 2.897068]\n",
      "2767 [Discriminator loss: 0.259955, acc.: 92.19%] [Generator loss: 4.081182]\n",
      "2768 [Discriminator loss: 0.464784, acc.: 73.44%] [Generator loss: 3.903619]\n",
      "2769 [Discriminator loss: 0.497177, acc.: 75.00%] [Generator loss: 3.695892]\n",
      "2770 [Discriminator loss: 0.330837, acc.: 81.25%] [Generator loss: 3.819629]\n",
      "2771 [Discriminator loss: 0.387739, acc.: 85.94%] [Generator loss: 4.370928]\n",
      "2772 [Discriminator loss: 0.130410, acc.: 93.75%] [Generator loss: 3.848246]\n",
      "2773 [Discriminator loss: 0.282895, acc.: 87.50%] [Generator loss: 3.649735]\n",
      "2774 [Discriminator loss: 0.535665, acc.: 70.31%] [Generator loss: 4.777716]\n",
      "2775 [Discriminator loss: 0.286718, acc.: 89.06%] [Generator loss: 3.671282]\n",
      "2776 [Discriminator loss: 0.302713, acc.: 84.38%] [Generator loss: 3.751616]\n",
      "2777 [Discriminator loss: 0.449680, acc.: 75.00%] [Generator loss: 4.993942]\n",
      "2778 [Discriminator loss: 0.503858, acc.: 79.69%] [Generator loss: 3.965482]\n",
      "2779 [Discriminator loss: 0.270191, acc.: 90.62%] [Generator loss: 3.848848]\n",
      "2780 [Discriminator loss: 0.142234, acc.: 96.88%] [Generator loss: 3.686615]\n",
      "2781 [Discriminator loss: 0.241395, acc.: 89.06%] [Generator loss: 4.130353]\n",
      "2782 [Discriminator loss: 0.295309, acc.: 84.38%] [Generator loss: 3.748149]\n",
      "2783 [Discriminator loss: 0.502043, acc.: 84.38%] [Generator loss: 3.307987]\n",
      "2784 [Discriminator loss: 0.389636, acc.: 79.69%] [Generator loss: 3.251795]\n",
      "2785 [Discriminator loss: 0.617222, acc.: 75.00%] [Generator loss: 3.509717]\n",
      "2786 [Discriminator loss: 0.256087, acc.: 87.50%] [Generator loss: 3.920369]\n",
      "2787 [Discriminator loss: 0.348985, acc.: 87.50%] [Generator loss: 3.276977]\n",
      "2788 [Discriminator loss: 0.680204, acc.: 75.00%] [Generator loss: 4.326052]\n",
      "2789 [Discriminator loss: 0.489381, acc.: 76.56%] [Generator loss: 4.479239]\n",
      "2790 [Discriminator loss: 0.350492, acc.: 84.38%] [Generator loss: 3.117239]\n",
      "2791 [Discriminator loss: 0.305366, acc.: 87.50%] [Generator loss: 4.433064]\n",
      "2792 [Discriminator loss: 0.248240, acc.: 89.06%] [Generator loss: 3.246821]\n",
      "2793 [Discriminator loss: 0.188862, acc.: 93.75%] [Generator loss: 3.052149]\n",
      "2794 [Discriminator loss: 0.207187, acc.: 90.62%] [Generator loss: 3.207213]\n",
      "2795 [Discriminator loss: 0.617895, acc.: 73.44%] [Generator loss: 3.787306]\n",
      "2796 [Discriminator loss: 0.259586, acc.: 92.19%] [Generator loss: 4.740649]\n",
      "2797 [Discriminator loss: 0.615317, acc.: 73.44%] [Generator loss: 4.222525]\n",
      "2798 [Discriminator loss: 0.231890, acc.: 92.19%] [Generator loss: 4.488781]\n",
      "2799 [Discriminator loss: 0.311376, acc.: 82.81%] [Generator loss: 4.967476]\n",
      "2800 [Discriminator loss: 0.590174, acc.: 73.44%] [Generator loss: 4.421179]\n",
      "2801 [Discriminator loss: 0.362785, acc.: 82.81%] [Generator loss: 4.433325]\n",
      "2802 [Discriminator loss: 0.260338, acc.: 87.50%] [Generator loss: 3.952547]\n",
      "2803 [Discriminator loss: 0.486065, acc.: 78.12%] [Generator loss: 4.061537]\n",
      "2804 [Discriminator loss: 0.277234, acc.: 90.62%] [Generator loss: 3.487445]\n",
      "2805 [Discriminator loss: 0.907658, acc.: 64.06%] [Generator loss: 4.294849]\n",
      "2806 [Discriminator loss: 0.304604, acc.: 84.38%] [Generator loss: 4.749230]\n",
      "2807 [Discriminator loss: 0.239416, acc.: 89.06%] [Generator loss: 3.382285]\n",
      "2808 [Discriminator loss: 0.214394, acc.: 93.75%] [Generator loss: 4.388311]\n",
      "2809 [Discriminator loss: 0.127275, acc.: 95.31%] [Generator loss: 4.819994]\n",
      "2810 [Discriminator loss: 0.419033, acc.: 82.81%] [Generator loss: 5.428157]\n",
      "2811 [Discriminator loss: 0.331805, acc.: 84.38%] [Generator loss: 4.284049]\n",
      "2812 [Discriminator loss: 0.437772, acc.: 79.69%] [Generator loss: 4.548728]\n",
      "2813 [Discriminator loss: 0.532049, acc.: 76.56%] [Generator loss: 3.573219]\n",
      "2814 [Discriminator loss: 0.476696, acc.: 76.56%] [Generator loss: 3.606079]\n",
      "2815 [Discriminator loss: 0.216254, acc.: 89.06%] [Generator loss: 4.288059]\n",
      "2816 [Discriminator loss: 0.487947, acc.: 78.12%] [Generator loss: 4.359665]\n",
      "2817 [Discriminator loss: 0.396243, acc.: 82.81%] [Generator loss: 3.628456]\n",
      "2818 [Discriminator loss: 0.381612, acc.: 79.69%] [Generator loss: 3.274031]\n",
      "2819 [Discriminator loss: 0.281252, acc.: 90.62%] [Generator loss: 4.214931]\n",
      "2820 [Discriminator loss: 0.281357, acc.: 84.38%] [Generator loss: 3.626434]\n",
      "2821 [Discriminator loss: 0.567854, acc.: 73.44%] [Generator loss: 4.327047]\n",
      "2822 [Discriminator loss: 0.253065, acc.: 85.94%] [Generator loss: 4.029290]\n",
      "2823 [Discriminator loss: 0.474749, acc.: 79.69%] [Generator loss: 4.738206]\n",
      "2824 [Discriminator loss: 0.316992, acc.: 89.06%] [Generator loss: 4.112964]\n",
      "2825 [Discriminator loss: 0.298197, acc.: 84.38%] [Generator loss: 3.350748]\n",
      "2826 [Discriminator loss: 0.224160, acc.: 90.62%] [Generator loss: 4.105326]\n",
      "2827 [Discriminator loss: 0.403421, acc.: 81.25%] [Generator loss: 4.751993]\n",
      "2828 [Discriminator loss: 0.375334, acc.: 82.81%] [Generator loss: 4.112908]\n",
      "2829 [Discriminator loss: 0.505196, acc.: 78.12%] [Generator loss: 3.390017]\n",
      "2830 [Discriminator loss: 0.307614, acc.: 87.50%] [Generator loss: 4.151444]\n",
      "2831 [Discriminator loss: 0.733636, acc.: 62.50%] [Generator loss: 4.848665]\n",
      "2832 [Discriminator loss: 0.535087, acc.: 76.56%] [Generator loss: 3.744090]\n",
      "2833 [Discriminator loss: 0.145045, acc.: 93.75%] [Generator loss: 4.178402]\n",
      "2834 [Discriminator loss: 0.384350, acc.: 85.94%] [Generator loss: 4.096676]\n",
      "2835 [Discriminator loss: 0.494722, acc.: 82.81%] [Generator loss: 3.088181]\n",
      "2836 [Discriminator loss: 0.304748, acc.: 87.50%] [Generator loss: 4.098673]\n",
      "2837 [Discriminator loss: 0.225819, acc.: 93.75%] [Generator loss: 4.252863]\n",
      "2838 [Discriminator loss: 0.486396, acc.: 75.00%] [Generator loss: 4.067183]\n",
      "2839 [Discriminator loss: 0.303482, acc.: 85.94%] [Generator loss: 4.248182]\n",
      "2840 [Discriminator loss: 0.246715, acc.: 89.06%] [Generator loss: 3.998995]\n",
      "2841 [Discriminator loss: 0.451394, acc.: 75.00%] [Generator loss: 3.780797]\n",
      "2842 [Discriminator loss: 0.143377, acc.: 93.75%] [Generator loss: 3.552606]\n",
      "2843 [Discriminator loss: 0.125260, acc.: 100.00%] [Generator loss: 3.367359]\n",
      "2844 [Discriminator loss: 0.275391, acc.: 89.06%] [Generator loss: 3.601825]\n",
      "2845 [Discriminator loss: 0.171688, acc.: 90.62%] [Generator loss: 3.855285]\n",
      "2846 [Discriminator loss: 0.335815, acc.: 85.94%] [Generator loss: 3.232320]\n",
      "2847 [Discriminator loss: 0.604681, acc.: 75.00%] [Generator loss: 5.263508]\n",
      "2848 [Discriminator loss: 0.389656, acc.: 87.50%] [Generator loss: 3.889872]\n",
      "2849 [Discriminator loss: 0.260346, acc.: 89.06%] [Generator loss: 3.656514]\n",
      "2850 [Discriminator loss: 0.222380, acc.: 92.19%] [Generator loss: 4.852697]\n",
      "2851 [Discriminator loss: 0.553663, acc.: 81.25%] [Generator loss: 3.460390]\n",
      "2852 [Discriminator loss: 0.327327, acc.: 90.62%] [Generator loss: 4.289052]\n",
      "2853 [Discriminator loss: 0.371078, acc.: 82.81%] [Generator loss: 3.876073]\n",
      "2854 [Discriminator loss: 0.312391, acc.: 84.38%] [Generator loss: 3.465076]\n",
      "2855 [Discriminator loss: 0.198088, acc.: 93.75%] [Generator loss: 3.446818]\n",
      "2856 [Discriminator loss: 0.320493, acc.: 87.50%] [Generator loss: 3.832757]\n",
      "2857 [Discriminator loss: 0.400066, acc.: 81.25%] [Generator loss: 4.602582]\n",
      "2858 [Discriminator loss: 0.479994, acc.: 71.88%] [Generator loss: 3.515079]\n",
      "2859 [Discriminator loss: 0.338873, acc.: 84.38%] [Generator loss: 3.750882]\n",
      "2860 [Discriminator loss: 0.284477, acc.: 84.38%] [Generator loss: 2.955938]\n",
      "2861 [Discriminator loss: 0.388897, acc.: 87.50%] [Generator loss: 3.969091]\n",
      "2862 [Discriminator loss: 0.198091, acc.: 89.06%] [Generator loss: 4.091833]\n",
      "2863 [Discriminator loss: 0.451099, acc.: 81.25%] [Generator loss: 3.323328]\n",
      "2864 [Discriminator loss: 0.137635, acc.: 96.88%] [Generator loss: 4.233051]\n",
      "2865 [Discriminator loss: 0.280353, acc.: 82.81%] [Generator loss: 3.445427]\n",
      "2866 [Discriminator loss: 0.673212, acc.: 64.06%] [Generator loss: 4.941020]\n",
      "2867 [Discriminator loss: 0.363779, acc.: 79.69%] [Generator loss: 3.299219]\n",
      "2868 [Discriminator loss: 0.244000, acc.: 89.06%] [Generator loss: 3.500373]\n",
      "2869 [Discriminator loss: 0.351242, acc.: 87.50%] [Generator loss: 3.930017]\n",
      "2870 [Discriminator loss: 0.591577, acc.: 75.00%] [Generator loss: 4.349147]\n",
      "2871 [Discriminator loss: 0.530474, acc.: 78.12%] [Generator loss: 3.431988]\n",
      "2872 [Discriminator loss: 0.268068, acc.: 87.50%] [Generator loss: 3.875500]\n",
      "2873 [Discriminator loss: 0.232055, acc.: 90.62%] [Generator loss: 3.251527]\n",
      "2874 [Discriminator loss: 0.545746, acc.: 75.00%] [Generator loss: 5.101320]\n",
      "2875 [Discriminator loss: 0.386872, acc.: 81.25%] [Generator loss: 4.058157]\n",
      "2876 [Discriminator loss: 0.383913, acc.: 79.69%] [Generator loss: 4.338942]\n",
      "2877 [Discriminator loss: 0.263874, acc.: 90.62%] [Generator loss: 3.910215]\n",
      "2878 [Discriminator loss: 0.278655, acc.: 89.06%] [Generator loss: 3.196360]\n",
      "2879 [Discriminator loss: 0.245776, acc.: 90.62%] [Generator loss: 3.778387]\n",
      "2880 [Discriminator loss: 0.217593, acc.: 87.50%] [Generator loss: 3.678914]\n",
      "2881 [Discriminator loss: 0.292518, acc.: 87.50%] [Generator loss: 3.903622]\n",
      "2882 [Discriminator loss: 0.308531, acc.: 85.94%] [Generator loss: 4.016670]\n",
      "2883 [Discriminator loss: 0.474702, acc.: 78.12%] [Generator loss: 3.622429]\n",
      "2884 [Discriminator loss: 0.431868, acc.: 84.38%] [Generator loss: 4.359256]\n",
      "2885 [Discriminator loss: 0.382504, acc.: 84.38%] [Generator loss: 3.253553]\n",
      "2886 [Discriminator loss: 0.460739, acc.: 73.44%] [Generator loss: 4.875064]\n",
      "2887 [Discriminator loss: 0.364757, acc.: 85.94%] [Generator loss: 4.140680]\n",
      "2888 [Discriminator loss: 0.226054, acc.: 90.62%] [Generator loss: 3.334916]\n",
      "2889 [Discriminator loss: 0.481652, acc.: 76.56%] [Generator loss: 4.196432]\n",
      "2890 [Discriminator loss: 0.426328, acc.: 82.81%] [Generator loss: 4.126579]\n",
      "2891 [Discriminator loss: 0.511045, acc.: 79.69%] [Generator loss: 4.627570]\n",
      "2892 [Discriminator loss: 0.398969, acc.: 79.69%] [Generator loss: 3.727573]\n",
      "2893 [Discriminator loss: 0.334069, acc.: 89.06%] [Generator loss: 4.000073]\n",
      "2894 [Discriminator loss: 0.437785, acc.: 85.94%] [Generator loss: 2.972187]\n",
      "2895 [Discriminator loss: 0.353188, acc.: 84.38%] [Generator loss: 4.408660]\n",
      "2896 [Discriminator loss: 0.310338, acc.: 92.19%] [Generator loss: 4.675369]\n",
      "2897 [Discriminator loss: 0.450232, acc.: 76.56%] [Generator loss: 3.521324]\n",
      "2898 [Discriminator loss: 0.399696, acc.: 82.81%] [Generator loss: 3.621354]\n",
      "2899 [Discriminator loss: 0.301076, acc.: 87.50%] [Generator loss: 3.694293]\n",
      "2900 [Discriminator loss: 0.406567, acc.: 79.69%] [Generator loss: 3.820937]\n",
      "2901 [Discriminator loss: 0.383951, acc.: 82.81%] [Generator loss: 3.839041]\n",
      "2902 [Discriminator loss: 0.257396, acc.: 89.06%] [Generator loss: 4.971416]\n",
      "2903 [Discriminator loss: 0.409174, acc.: 81.25%] [Generator loss: 4.248940]\n",
      "2904 [Discriminator loss: 0.251609, acc.: 87.50%] [Generator loss: 4.505404]\n",
      "2905 [Discriminator loss: 0.320236, acc.: 87.50%] [Generator loss: 4.995567]\n",
      "2906 [Discriminator loss: 0.454393, acc.: 79.69%] [Generator loss: 3.346238]\n",
      "2907 [Discriminator loss: 0.452883, acc.: 78.12%] [Generator loss: 5.048839]\n",
      "2908 [Discriminator loss: 0.342759, acc.: 82.81%] [Generator loss: 4.621979]\n",
      "2909 [Discriminator loss: 0.360226, acc.: 85.94%] [Generator loss: 3.032506]\n",
      "2910 [Discriminator loss: 0.206378, acc.: 90.62%] [Generator loss: 3.785981]\n",
      "2911 [Discriminator loss: 0.549734, acc.: 67.19%] [Generator loss: 4.660066]\n",
      "2912 [Discriminator loss: 0.503893, acc.: 78.12%] [Generator loss: 4.011664]\n",
      "2913 [Discriminator loss: 0.571336, acc.: 71.88%] [Generator loss: 4.634068]\n",
      "2914 [Discriminator loss: 0.415669, acc.: 82.81%] [Generator loss: 3.550016]\n",
      "2915 [Discriminator loss: 0.381610, acc.: 81.25%] [Generator loss: 4.165631]\n",
      "2916 [Discriminator loss: 0.393174, acc.: 84.38%] [Generator loss: 4.039710]\n",
      "2917 [Discriminator loss: 0.369196, acc.: 82.81%] [Generator loss: 3.909616]\n",
      "2918 [Discriminator loss: 0.448891, acc.: 82.81%] [Generator loss: 3.179609]\n",
      "2919 [Discriminator loss: 0.381343, acc.: 85.94%] [Generator loss: 4.376442]\n",
      "2920 [Discriminator loss: 0.468381, acc.: 79.69%] [Generator loss: 3.748900]\n",
      "2921 [Discriminator loss: 0.253651, acc.: 93.75%] [Generator loss: 3.928420]\n",
      "2922 [Discriminator loss: 0.436341, acc.: 78.12%] [Generator loss: 4.302974]\n",
      "2923 [Discriminator loss: 0.184262, acc.: 93.75%] [Generator loss: 3.495726]\n",
      "2924 [Discriminator loss: 0.511716, acc.: 76.56%] [Generator loss: 5.077864]\n",
      "2925 [Discriminator loss: 0.200544, acc.: 92.19%] [Generator loss: 5.033725]\n",
      "2926 [Discriminator loss: 0.424524, acc.: 78.12%] [Generator loss: 3.914501]\n",
      "2927 [Discriminator loss: 0.155921, acc.: 92.19%] [Generator loss: 4.334294]\n",
      "2928 [Discriminator loss: 0.379429, acc.: 87.50%] [Generator loss: 2.759869]\n",
      "2929 [Discriminator loss: 0.294322, acc.: 89.06%] [Generator loss: 4.359248]\n",
      "2930 [Discriminator loss: 0.249008, acc.: 87.50%] [Generator loss: 3.473747]\n",
      "2931 [Discriminator loss: 0.442830, acc.: 75.00%] [Generator loss: 3.246686]\n",
      "2932 [Discriminator loss: 0.246570, acc.: 85.94%] [Generator loss: 3.555386]\n",
      "2933 [Discriminator loss: 0.230995, acc.: 93.75%] [Generator loss: 3.151057]\n",
      "2934 [Discriminator loss: 0.444795, acc.: 79.69%] [Generator loss: 4.598354]\n",
      "2935 [Discriminator loss: 0.467001, acc.: 78.12%] [Generator loss: 3.485429]\n",
      "2936 [Discriminator loss: 0.376605, acc.: 81.25%] [Generator loss: 3.480932]\n",
      "2937 [Discriminator loss: 0.321366, acc.: 87.50%] [Generator loss: 4.152016]\n",
      "2938 [Discriminator loss: 0.436914, acc.: 82.81%] [Generator loss: 4.911733]\n",
      "2939 [Discriminator loss: 1.023490, acc.: 54.69%] [Generator loss: 3.910884]\n",
      "2940 [Discriminator loss: 0.224002, acc.: 90.62%] [Generator loss: 4.529569]\n",
      "2941 [Discriminator loss: 0.146733, acc.: 93.75%] [Generator loss: 5.072355]\n",
      "2942 [Discriminator loss: 0.366253, acc.: 81.25%] [Generator loss: 3.977967]\n",
      "2943 [Discriminator loss: 0.250353, acc.: 87.50%] [Generator loss: 4.472334]\n",
      "2944 [Discriminator loss: 0.499229, acc.: 75.00%] [Generator loss: 3.876225]\n",
      "2945 [Discriminator loss: 0.127246, acc.: 96.88%] [Generator loss: 4.175390]\n",
      "2946 [Discriminator loss: 0.374539, acc.: 85.94%] [Generator loss: 4.303130]\n",
      "2947 [Discriminator loss: 0.389133, acc.: 84.38%] [Generator loss: 3.481769]\n",
      "2948 [Discriminator loss: 0.391312, acc.: 82.81%] [Generator loss: 4.650794]\n",
      "2949 [Discriminator loss: 0.236045, acc.: 90.62%] [Generator loss: 4.975715]\n",
      "2950 [Discriminator loss: 0.672045, acc.: 67.19%] [Generator loss: 3.430731]\n",
      "2951 [Discriminator loss: 0.358533, acc.: 87.50%] [Generator loss: 3.506656]\n",
      "2952 [Discriminator loss: 0.369873, acc.: 82.81%] [Generator loss: 4.189782]\n",
      "2953 [Discriminator loss: 0.272452, acc.: 89.06%] [Generator loss: 3.842135]\n",
      "2954 [Discriminator loss: 0.201579, acc.: 92.19%] [Generator loss: 3.038471]\n",
      "2955 [Discriminator loss: 0.412755, acc.: 84.38%] [Generator loss: 3.495301]\n",
      "2956 [Discriminator loss: 0.341361, acc.: 81.25%] [Generator loss: 3.975633]\n",
      "2957 [Discriminator loss: 0.369867, acc.: 92.19%] [Generator loss: 2.823038]\n",
      "2958 [Discriminator loss: 0.597457, acc.: 75.00%] [Generator loss: 4.868383]\n",
      "2959 [Discriminator loss: 0.533874, acc.: 75.00%] [Generator loss: 4.104536]\n",
      "2960 [Discriminator loss: 0.175374, acc.: 93.75%] [Generator loss: 4.507731]\n",
      "2961 [Discriminator loss: 0.568637, acc.: 73.44%] [Generator loss: 4.398187]\n",
      "2962 [Discriminator loss: 0.252279, acc.: 89.06%] [Generator loss: 4.254793]\n",
      "2963 [Discriminator loss: 0.631682, acc.: 70.31%] [Generator loss: 3.683473]\n",
      "2964 [Discriminator loss: 0.342222, acc.: 89.06%] [Generator loss: 4.639110]\n",
      "2965 [Discriminator loss: 0.161086, acc.: 95.31%] [Generator loss: 4.972975]\n",
      "2966 [Discriminator loss: 0.390951, acc.: 79.69%] [Generator loss: 4.230501]\n",
      "2967 [Discriminator loss: 0.428476, acc.: 79.69%] [Generator loss: 3.242573]\n",
      "2968 [Discriminator loss: 0.661550, acc.: 71.88%] [Generator loss: 4.069978]\n",
      "2969 [Discriminator loss: 0.333045, acc.: 85.94%] [Generator loss: 3.702079]\n",
      "2970 [Discriminator loss: 0.418448, acc.: 82.81%] [Generator loss: 4.789539]\n",
      "2971 [Discriminator loss: 0.381915, acc.: 84.38%] [Generator loss: 3.703516]\n",
      "2972 [Discriminator loss: 0.316456, acc.: 87.50%] [Generator loss: 3.286549]\n",
      "2973 [Discriminator loss: 0.238318, acc.: 85.94%] [Generator loss: 3.782845]\n",
      "2974 [Discriminator loss: 0.367167, acc.: 81.25%] [Generator loss: 3.910171]\n",
      "2975 [Discriminator loss: 0.190017, acc.: 92.19%] [Generator loss: 3.373902]\n",
      "2976 [Discriminator loss: 0.498137, acc.: 79.69%] [Generator loss: 4.484100]\n",
      "2977 [Discriminator loss: 0.402880, acc.: 76.56%] [Generator loss: 4.749117]\n",
      "2978 [Discriminator loss: 0.219000, acc.: 92.19%] [Generator loss: 2.630854]\n",
      "2979 [Discriminator loss: 0.373334, acc.: 81.25%] [Generator loss: 4.416467]\n",
      "2980 [Discriminator loss: 0.364694, acc.: 82.81%] [Generator loss: 2.893661]\n",
      "2981 [Discriminator loss: 0.410419, acc.: 75.00%] [Generator loss: 3.154659]\n",
      "2982 [Discriminator loss: 0.452590, acc.: 82.81%] [Generator loss: 4.340366]\n",
      "2983 [Discriminator loss: 0.473148, acc.: 79.69%] [Generator loss: 4.677347]\n",
      "2984 [Discriminator loss: 0.629497, acc.: 70.31%] [Generator loss: 3.602302]\n",
      "2985 [Discriminator loss: 0.133003, acc.: 93.75%] [Generator loss: 3.476290]\n",
      "2986 [Discriminator loss: 0.347677, acc.: 84.38%] [Generator loss: 3.431062]\n",
      "2987 [Discriminator loss: 0.259260, acc.: 89.06%] [Generator loss: 4.645719]\n",
      "2988 [Discriminator loss: 0.310063, acc.: 85.94%] [Generator loss: 3.595641]\n",
      "2989 [Discriminator loss: 0.572525, acc.: 71.88%] [Generator loss: 3.277786]\n",
      "2990 [Discriminator loss: 0.381588, acc.: 85.94%] [Generator loss: 4.557373]\n",
      "2991 [Discriminator loss: 0.367186, acc.: 82.81%] [Generator loss: 3.579556]\n",
      "2992 [Discriminator loss: 0.452068, acc.: 79.69%] [Generator loss: 4.850118]\n",
      "2993 [Discriminator loss: 0.398383, acc.: 81.25%] [Generator loss: 4.168795]\n",
      "2994 [Discriminator loss: 0.438254, acc.: 79.69%] [Generator loss: 3.602096]\n",
      "2995 [Discriminator loss: 0.245078, acc.: 87.50%] [Generator loss: 3.928035]\n",
      "2996 [Discriminator loss: 0.491194, acc.: 79.69%] [Generator loss: 2.857471]\n",
      "2997 [Discriminator loss: 0.343978, acc.: 81.25%] [Generator loss: 4.189228]\n",
      "2998 [Discriminator loss: 0.354512, acc.: 84.38%] [Generator loss: 3.399338]\n",
      "2999 [Discriminator loss: 0.263092, acc.: 89.06%] [Generator loss: 4.233140]\n",
      "3000 [Discriminator loss: 0.329198, acc.: 82.81%] [Generator loss: 3.495426]\n",
      "3001 [Discriminator loss: 0.347405, acc.: 87.50%] [Generator loss: 3.596398]\n",
      "3002 [Discriminator loss: 0.309347, acc.: 87.50%] [Generator loss: 4.225189]\n",
      "3003 [Discriminator loss: 0.289651, acc.: 87.50%] [Generator loss: 4.378268]\n",
      "3004 [Discriminator loss: 0.278231, acc.: 92.19%] [Generator loss: 3.889679]\n",
      "3005 [Discriminator loss: 0.568274, acc.: 75.00%] [Generator loss: 4.962048]\n",
      "3006 [Discriminator loss: 0.230151, acc.: 90.62%] [Generator loss: 3.599135]\n",
      "3007 [Discriminator loss: 0.995689, acc.: 54.69%] [Generator loss: 5.652267]\n",
      "3008 [Discriminator loss: 0.238495, acc.: 89.06%] [Generator loss: 5.001912]\n",
      "3009 [Discriminator loss: 0.558978, acc.: 73.44%] [Generator loss: 3.188335]\n",
      "3010 [Discriminator loss: 0.395157, acc.: 75.00%] [Generator loss: 4.560396]\n",
      "3011 [Discriminator loss: 0.328474, acc.: 85.94%] [Generator loss: 3.654563]\n",
      "3012 [Discriminator loss: 0.292211, acc.: 84.38%] [Generator loss: 3.977765]\n",
      "3013 [Discriminator loss: 0.330886, acc.: 84.38%] [Generator loss: 3.006537]\n",
      "3014 [Discriminator loss: 0.410274, acc.: 81.25%] [Generator loss: 4.343752]\n",
      "3015 [Discriminator loss: 0.224475, acc.: 87.50%] [Generator loss: 4.619832]\n",
      "3016 [Discriminator loss: 0.549651, acc.: 76.56%] [Generator loss: 4.353984]\n",
      "3017 [Discriminator loss: 0.125105, acc.: 95.31%] [Generator loss: 3.960098]\n",
      "3018 [Discriminator loss: 0.350230, acc.: 89.06%] [Generator loss: 3.807040]\n",
      "3019 [Discriminator loss: 0.258684, acc.: 89.06%] [Generator loss: 3.134123]\n",
      "3020 [Discriminator loss: 0.564214, acc.: 81.25%] [Generator loss: 4.219656]\n",
      "3021 [Discriminator loss: 0.410560, acc.: 87.50%] [Generator loss: 3.737445]\n",
      "3022 [Discriminator loss: 0.439665, acc.: 82.81%] [Generator loss: 3.481591]\n",
      "3023 [Discriminator loss: 0.500635, acc.: 73.44%] [Generator loss: 4.181133]\n",
      "3024 [Discriminator loss: 0.461465, acc.: 76.56%] [Generator loss: 3.225658]\n",
      "3025 [Discriminator loss: 0.326521, acc.: 81.25%] [Generator loss: 2.934992]\n",
      "3026 [Discriminator loss: 0.207430, acc.: 93.75%] [Generator loss: 3.473299]\n",
      "3027 [Discriminator loss: 0.319564, acc.: 84.38%] [Generator loss: 3.426261]\n",
      "3028 [Discriminator loss: 0.364527, acc.: 82.81%] [Generator loss: 3.483201]\n",
      "3029 [Discriminator loss: 0.292840, acc.: 89.06%] [Generator loss: 2.915395]\n",
      "3030 [Discriminator loss: 0.352995, acc.: 82.81%] [Generator loss: 3.700559]\n",
      "3031 [Discriminator loss: 0.309590, acc.: 87.50%] [Generator loss: 3.521991]\n",
      "3032 [Discriminator loss: 0.423683, acc.: 87.50%] [Generator loss: 3.072277]\n",
      "3033 [Discriminator loss: 0.451733, acc.: 82.81%] [Generator loss: 3.913225]\n",
      "3034 [Discriminator loss: 0.550700, acc.: 76.56%] [Generator loss: 4.075915]\n",
      "3035 [Discriminator loss: 0.272382, acc.: 89.06%] [Generator loss: 3.856685]\n",
      "3036 [Discriminator loss: 0.368443, acc.: 81.25%] [Generator loss: 4.547844]\n",
      "3037 [Discriminator loss: 0.186455, acc.: 93.75%] [Generator loss: 4.262125]\n",
      "3038 [Discriminator loss: 0.654602, acc.: 67.19%] [Generator loss: 3.958042]\n",
      "3039 [Discriminator loss: 0.384276, acc.: 82.81%] [Generator loss: 4.337344]\n",
      "3040 [Discriminator loss: 0.214304, acc.: 92.19%] [Generator loss: 3.966922]\n",
      "3041 [Discriminator loss: 0.496538, acc.: 73.44%] [Generator loss: 4.438672]\n",
      "3042 [Discriminator loss: 0.223811, acc.: 89.06%] [Generator loss: 4.022264]\n",
      "3043 [Discriminator loss: 0.265813, acc.: 92.19%] [Generator loss: 3.281456]\n",
      "3044 [Discriminator loss: 0.280774, acc.: 92.19%] [Generator loss: 3.768211]\n",
      "3045 [Discriminator loss: 0.373655, acc.: 82.81%] [Generator loss: 3.496400]\n",
      "3046 [Discriminator loss: 0.450010, acc.: 81.25%] [Generator loss: 4.510080]\n",
      "3047 [Discriminator loss: 0.481091, acc.: 75.00%] [Generator loss: 4.238750]\n",
      "3048 [Discriminator loss: 0.400550, acc.: 85.94%] [Generator loss: 3.363996]\n",
      "3049 [Discriminator loss: 0.550761, acc.: 81.25%] [Generator loss: 3.821253]\n",
      "3050 [Discriminator loss: 0.373578, acc.: 85.94%] [Generator loss: 3.223290]\n",
      "3051 [Discriminator loss: 0.332966, acc.: 85.94%] [Generator loss: 3.543925]\n",
      "3052 [Discriminator loss: 0.478744, acc.: 81.25%] [Generator loss: 3.988729]\n",
      "3053 [Discriminator loss: 0.358099, acc.: 82.81%] [Generator loss: 3.519755]\n",
      "3054 [Discriminator loss: 0.216480, acc.: 92.19%] [Generator loss: 3.979580]\n",
      "3055 [Discriminator loss: 0.409711, acc.: 79.69%] [Generator loss: 4.486122]\n",
      "3056 [Discriminator loss: 0.175206, acc.: 92.19%] [Generator loss: 4.034341]\n",
      "3057 [Discriminator loss: 0.289692, acc.: 90.62%] [Generator loss: 3.817554]\n",
      "3058 [Discriminator loss: 0.564873, acc.: 68.75%] [Generator loss: 3.812963]\n",
      "3059 [Discriminator loss: 0.284353, acc.: 85.94%] [Generator loss: 3.794520]\n",
      "3060 [Discriminator loss: 0.245733, acc.: 90.62%] [Generator loss: 4.295519]\n",
      "3061 [Discriminator loss: 0.561806, acc.: 78.12%] [Generator loss: 3.528026]\n",
      "3062 [Discriminator loss: 0.495420, acc.: 76.56%] [Generator loss: 4.206698]\n",
      "3063 [Discriminator loss: 0.222686, acc.: 92.19%] [Generator loss: 4.830409]\n",
      "3064 [Discriminator loss: 0.514596, acc.: 76.56%] [Generator loss: 3.436540]\n",
      "3065 [Discriminator loss: 0.459169, acc.: 78.12%] [Generator loss: 3.140776]\n",
      "3066 [Discriminator loss: 0.135325, acc.: 93.75%] [Generator loss: 3.781057]\n",
      "3067 [Discriminator loss: 0.262188, acc.: 90.62%] [Generator loss: 4.013859]\n",
      "3068 [Discriminator loss: 0.301538, acc.: 84.38%] [Generator loss: 3.603999]\n",
      "3069 [Discriminator loss: 0.298705, acc.: 85.94%] [Generator loss: 4.358227]\n",
      "3070 [Discriminator loss: 0.141597, acc.: 98.44%] [Generator loss: 3.674195]\n",
      "3071 [Discriminator loss: 0.390104, acc.: 79.69%] [Generator loss: 4.325169]\n",
      "3072 [Discriminator loss: 0.341113, acc.: 84.38%] [Generator loss: 4.011834]\n",
      "3073 [Discriminator loss: 0.395833, acc.: 76.56%] [Generator loss: 4.768571]\n",
      "3074 [Discriminator loss: 0.260732, acc.: 89.06%] [Generator loss: 3.034440]\n",
      "3075 [Discriminator loss: 0.335316, acc.: 87.50%] [Generator loss: 3.513701]\n",
      "3076 [Discriminator loss: 0.219441, acc.: 93.75%] [Generator loss: 4.192648]\n",
      "3077 [Discriminator loss: 0.303236, acc.: 84.38%] [Generator loss: 5.125556]\n",
      "3078 [Discriminator loss: 0.177464, acc.: 92.19%] [Generator loss: 4.030981]\n",
      "3079 [Discriminator loss: 0.457669, acc.: 79.69%] [Generator loss: 4.533859]\n",
      "3080 [Discriminator loss: 0.310570, acc.: 84.38%] [Generator loss: 4.631706]\n",
      "3081 [Discriminator loss: 0.137145, acc.: 95.31%] [Generator loss: 4.118362]\n",
      "3082 [Discriminator loss: 0.304052, acc.: 90.62%] [Generator loss: 4.022826]\n",
      "3083 [Discriminator loss: 0.303003, acc.: 90.62%] [Generator loss: 3.295749]\n",
      "3084 [Discriminator loss: 0.268908, acc.: 92.19%] [Generator loss: 4.168211]\n",
      "3085 [Discriminator loss: 0.639749, acc.: 73.44%] [Generator loss: 4.137384]\n",
      "3086 [Discriminator loss: 0.210004, acc.: 87.50%] [Generator loss: 4.201419]\n",
      "3087 [Discriminator loss: 0.370098, acc.: 82.81%] [Generator loss: 4.022136]\n",
      "3088 [Discriminator loss: 0.414327, acc.: 79.69%] [Generator loss: 4.275862]\n",
      "3089 [Discriminator loss: 0.386557, acc.: 79.69%] [Generator loss: 3.519393]\n",
      "3090 [Discriminator loss: 0.346376, acc.: 82.81%] [Generator loss: 4.674223]\n",
      "3091 [Discriminator loss: 0.362850, acc.: 84.38%] [Generator loss: 3.997974]\n",
      "3092 [Discriminator loss: 0.330449, acc.: 79.69%] [Generator loss: 3.496648]\n",
      "3093 [Discriminator loss: 0.335065, acc.: 85.94%] [Generator loss: 3.861564]\n",
      "3094 [Discriminator loss: 0.360847, acc.: 84.38%] [Generator loss: 4.840919]\n",
      "3095 [Discriminator loss: 0.351912, acc.: 84.38%] [Generator loss: 3.998701]\n",
      "3096 [Discriminator loss: 0.335728, acc.: 85.94%] [Generator loss: 3.583636]\n",
      "3097 [Discriminator loss: 0.523644, acc.: 79.69%] [Generator loss: 3.807775]\n",
      "3098 [Discriminator loss: 0.356263, acc.: 85.94%] [Generator loss: 3.656176]\n",
      "3099 [Discriminator loss: 0.496655, acc.: 75.00%] [Generator loss: 3.644839]\n",
      "3100 [Discriminator loss: 0.247797, acc.: 87.50%] [Generator loss: 4.172426]\n",
      "3101 [Discriminator loss: 0.745577, acc.: 65.62%] [Generator loss: 3.560030]\n",
      "3102 [Discriminator loss: 0.330170, acc.: 87.50%] [Generator loss: 4.519725]\n",
      "3103 [Discriminator loss: 0.332221, acc.: 89.06%] [Generator loss: 3.288857]\n",
      "3104 [Discriminator loss: 0.717675, acc.: 65.62%] [Generator loss: 4.545215]\n",
      "3105 [Discriminator loss: 0.338615, acc.: 82.81%] [Generator loss: 4.057967]\n",
      "3106 [Discriminator loss: 0.324386, acc.: 84.38%] [Generator loss: 4.483332]\n",
      "3107 [Discriminator loss: 0.270534, acc.: 85.94%] [Generator loss: 4.579326]\n",
      "3108 [Discriminator loss: 0.332535, acc.: 84.38%] [Generator loss: 3.558021]\n",
      "3109 [Discriminator loss: 0.261279, acc.: 87.50%] [Generator loss: 4.432827]\n",
      "3110 [Discriminator loss: 0.325215, acc.: 84.38%] [Generator loss: 4.280962]\n",
      "3111 [Discriminator loss: 0.299705, acc.: 87.50%] [Generator loss: 4.507491]\n",
      "3112 [Discriminator loss: 0.281476, acc.: 85.94%] [Generator loss: 3.602414]\n",
      "3113 [Discriminator loss: 0.170570, acc.: 93.75%] [Generator loss: 3.699024]\n",
      "3114 [Discriminator loss: 0.310724, acc.: 85.94%] [Generator loss: 3.710255]\n",
      "3115 [Discriminator loss: 0.283664, acc.: 90.62%] [Generator loss: 4.028452]\n",
      "3116 [Discriminator loss: 0.212786, acc.: 89.06%] [Generator loss: 3.479235]\n",
      "3117 [Discriminator loss: 0.417581, acc.: 78.12%] [Generator loss: 4.622316]\n",
      "3118 [Discriminator loss: 0.308819, acc.: 87.50%] [Generator loss: 4.351056]\n",
      "3119 [Discriminator loss: 0.444279, acc.: 79.69%] [Generator loss: 3.639785]\n",
      "3120 [Discriminator loss: 0.177048, acc.: 93.75%] [Generator loss: 3.491991]\n",
      "3121 [Discriminator loss: 0.355015, acc.: 78.12%] [Generator loss: 3.610464]\n",
      "3122 [Discriminator loss: 0.368641, acc.: 87.50%] [Generator loss: 3.516047]\n",
      "3123 [Discriminator loss: 0.294829, acc.: 87.50%] [Generator loss: 4.086525]\n",
      "3124 [Discriminator loss: 0.248873, acc.: 84.38%] [Generator loss: 4.450461]\n",
      "3125 [Discriminator loss: 0.484621, acc.: 73.44%] [Generator loss: 4.261639]\n",
      "3126 [Discriminator loss: 0.288598, acc.: 84.38%] [Generator loss: 5.725096]\n",
      "3127 [Discriminator loss: 0.524706, acc.: 76.56%] [Generator loss: 3.928586]\n",
      "3128 [Discriminator loss: 0.354926, acc.: 84.38%] [Generator loss: 4.437010]\n",
      "3129 [Discriminator loss: 0.154635, acc.: 95.31%] [Generator loss: 4.414857]\n",
      "3130 [Discriminator loss: 0.438868, acc.: 78.12%] [Generator loss: 3.533363]\n",
      "3131 [Discriminator loss: 0.351180, acc.: 84.38%] [Generator loss: 3.864497]\n",
      "3132 [Discriminator loss: 0.387573, acc.: 81.25%] [Generator loss: 4.499313]\n",
      "3133 [Discriminator loss: 0.402990, acc.: 85.94%] [Generator loss: 4.057457]\n",
      "3134 [Discriminator loss: 0.298065, acc.: 89.06%] [Generator loss: 3.735909]\n",
      "3135 [Discriminator loss: 0.355217, acc.: 82.81%] [Generator loss: 3.318252]\n",
      "3136 [Discriminator loss: 0.447114, acc.: 81.25%] [Generator loss: 3.981025]\n",
      "3137 [Discriminator loss: 0.324692, acc.: 85.94%] [Generator loss: 3.451099]\n",
      "3138 [Discriminator loss: 0.285660, acc.: 85.94%] [Generator loss: 4.417334]\n",
      "3139 [Discriminator loss: 0.148705, acc.: 93.75%] [Generator loss: 3.832972]\n",
      "3140 [Discriminator loss: 0.511877, acc.: 79.69%] [Generator loss: 3.282637]\n",
      "3141 [Discriminator loss: 0.192604, acc.: 93.75%] [Generator loss: 3.800126]\n",
      "3142 [Discriminator loss: 0.657557, acc.: 68.75%] [Generator loss: 2.620517]\n",
      "3143 [Discriminator loss: 0.229490, acc.: 90.62%] [Generator loss: 3.634682]\n",
      "3144 [Discriminator loss: 0.445224, acc.: 78.12%] [Generator loss: 4.342292]\n",
      "3145 [Discriminator loss: 0.316465, acc.: 85.94%] [Generator loss: 4.058052]\n",
      "3146 [Discriminator loss: 0.288522, acc.: 87.50%] [Generator loss: 4.499206]\n",
      "3147 [Discriminator loss: 0.471497, acc.: 82.81%] [Generator loss: 5.103693]\n",
      "3148 [Discriminator loss: 0.318161, acc.: 89.06%] [Generator loss: 3.929858]\n",
      "3149 [Discriminator loss: 0.390843, acc.: 81.25%] [Generator loss: 2.843410]\n",
      "3150 [Discriminator loss: 0.388426, acc.: 79.69%] [Generator loss: 3.720473]\n",
      "3151 [Discriminator loss: 0.245205, acc.: 85.94%] [Generator loss: 3.627103]\n",
      "3152 [Discriminator loss: 0.404248, acc.: 78.12%] [Generator loss: 3.958034]\n",
      "3153 [Discriminator loss: 0.199937, acc.: 93.75%] [Generator loss: 3.789644]\n",
      "3154 [Discriminator loss: 0.215088, acc.: 92.19%] [Generator loss: 4.024161]\n",
      "3155 [Discriminator loss: 0.298749, acc.: 85.94%] [Generator loss: 4.055084]\n",
      "3156 [Discriminator loss: 0.262861, acc.: 89.06%] [Generator loss: 3.903343]\n",
      "3157 [Discriminator loss: 0.577869, acc.: 78.12%] [Generator loss: 4.651859]\n",
      "3158 [Discriminator loss: 0.365127, acc.: 90.62%] [Generator loss: 3.928097]\n",
      "3159 [Discriminator loss: 0.687356, acc.: 68.75%] [Generator loss: 4.581460]\n",
      "3160 [Discriminator loss: 0.224957, acc.: 90.62%] [Generator loss: 4.693891]\n",
      "3161 [Discriminator loss: 0.337582, acc.: 82.81%] [Generator loss: 4.160369]\n",
      "3162 [Discriminator loss: 0.280514, acc.: 85.94%] [Generator loss: 4.735178]\n",
      "3163 [Discriminator loss: 0.508427, acc.: 79.69%] [Generator loss: 4.368804]\n",
      "3164 [Discriminator loss: 0.473196, acc.: 82.81%] [Generator loss: 3.574727]\n",
      "3165 [Discriminator loss: 0.689502, acc.: 70.31%] [Generator loss: 5.038544]\n",
      "3166 [Discriminator loss: 0.226063, acc.: 90.62%] [Generator loss: 4.584445]\n",
      "3167 [Discriminator loss: 0.472009, acc.: 76.56%] [Generator loss: 2.941924]\n",
      "3168 [Discriminator loss: 0.221386, acc.: 89.06%] [Generator loss: 4.601624]\n",
      "3169 [Discriminator loss: 0.238676, acc.: 87.50%] [Generator loss: 3.527681]\n",
      "3170 [Discriminator loss: 0.483694, acc.: 81.25%] [Generator loss: 2.697866]\n",
      "3171 [Discriminator loss: 0.396590, acc.: 81.25%] [Generator loss: 4.538784]\n",
      "3172 [Discriminator loss: 0.502692, acc.: 81.25%] [Generator loss: 3.130334]\n",
      "3173 [Discriminator loss: 0.609446, acc.: 75.00%] [Generator loss: 4.599806]\n",
      "3174 [Discriminator loss: 0.535568, acc.: 81.25%] [Generator loss: 5.816170]\n",
      "3175 [Discriminator loss: 0.449064, acc.: 81.25%] [Generator loss: 3.500214]\n",
      "3176 [Discriminator loss: 0.280269, acc.: 87.50%] [Generator loss: 3.737125]\n",
      "3177 [Discriminator loss: 0.326226, acc.: 84.38%] [Generator loss: 5.157829]\n",
      "3178 [Discriminator loss: 0.333778, acc.: 81.25%] [Generator loss: 4.334064]\n",
      "3179 [Discriminator loss: 0.291661, acc.: 89.06%] [Generator loss: 3.623068]\n",
      "3180 [Discriminator loss: 0.349096, acc.: 82.81%] [Generator loss: 4.214160]\n",
      "3181 [Discriminator loss: 0.371568, acc.: 79.69%] [Generator loss: 3.625979]\n",
      "3182 [Discriminator loss: 0.292901, acc.: 85.94%] [Generator loss: 4.142653]\n",
      "3183 [Discriminator loss: 0.299998, acc.: 87.50%] [Generator loss: 3.855380]\n",
      "3184 [Discriminator loss: 0.269745, acc.: 90.62%] [Generator loss: 3.756872]\n",
      "3185 [Discriminator loss: 0.382209, acc.: 82.81%] [Generator loss: 3.626571]\n",
      "3186 [Discriminator loss: 0.311977, acc.: 87.50%] [Generator loss: 3.706984]\n",
      "3187 [Discriminator loss: 0.290598, acc.: 84.38%] [Generator loss: 3.823635]\n",
      "3188 [Discriminator loss: 0.464924, acc.: 75.00%] [Generator loss: 3.541134]\n",
      "3189 [Discriminator loss: 0.456193, acc.: 78.12%] [Generator loss: 5.374672]\n",
      "3190 [Discriminator loss: 0.629045, acc.: 70.31%] [Generator loss: 4.053924]\n",
      "3191 [Discriminator loss: 0.338090, acc.: 85.94%] [Generator loss: 4.022140]\n",
      "3192 [Discriminator loss: 0.279617, acc.: 87.50%] [Generator loss: 5.258919]\n",
      "3193 [Discriminator loss: 0.422134, acc.: 81.25%] [Generator loss: 4.775753]\n",
      "3194 [Discriminator loss: 0.386983, acc.: 84.38%] [Generator loss: 4.087299]\n",
      "3195 [Discriminator loss: 0.632244, acc.: 73.44%] [Generator loss: 4.174138]\n",
      "3196 [Discriminator loss: 0.187020, acc.: 98.44%] [Generator loss: 5.285786]\n",
      "3197 [Discriminator loss: 0.606751, acc.: 71.88%] [Generator loss: 4.300814]\n",
      "3198 [Discriminator loss: 0.406048, acc.: 84.38%] [Generator loss: 3.992066]\n",
      "3199 [Discriminator loss: 0.349274, acc.: 82.81%] [Generator loss: 4.903360]\n",
      "3200 [Discriminator loss: 0.234482, acc.: 87.50%] [Generator loss: 4.854210]\n",
      "3201 [Discriminator loss: 0.363404, acc.: 89.06%] [Generator loss: 5.009059]\n",
      "3202 [Discriminator loss: 0.369849, acc.: 85.94%] [Generator loss: 3.799344]\n",
      "3203 [Discriminator loss: 0.687469, acc.: 71.88%] [Generator loss: 4.253529]\n",
      "3204 [Discriminator loss: 0.300810, acc.: 84.38%] [Generator loss: 4.425406]\n",
      "3205 [Discriminator loss: 0.486505, acc.: 75.00%] [Generator loss: 4.187611]\n",
      "3206 [Discriminator loss: 0.381782, acc.: 84.38%] [Generator loss: 4.533567]\n",
      "3207 [Discriminator loss: 0.288950, acc.: 85.94%] [Generator loss: 4.281391]\n",
      "3208 [Discriminator loss: 0.455163, acc.: 82.81%] [Generator loss: 4.362829]\n",
      "3209 [Discriminator loss: 0.303167, acc.: 82.81%] [Generator loss: 3.577086]\n",
      "3210 [Discriminator loss: 0.187943, acc.: 93.75%] [Generator loss: 4.334612]\n",
      "3211 [Discriminator loss: 0.312827, acc.: 85.94%] [Generator loss: 4.276786]\n",
      "3212 [Discriminator loss: 0.236451, acc.: 89.06%] [Generator loss: 4.624717]\n",
      "3213 [Discriminator loss: 0.271379, acc.: 84.38%] [Generator loss: 3.380783]\n",
      "3214 [Discriminator loss: 0.316483, acc.: 84.38%] [Generator loss: 3.851810]\n",
      "3215 [Discriminator loss: 0.672748, acc.: 67.19%] [Generator loss: 4.099145]\n",
      "3216 [Discriminator loss: 0.297078, acc.: 92.19%] [Generator loss: 3.711919]\n",
      "3217 [Discriminator loss: 0.222093, acc.: 93.75%] [Generator loss: 4.280711]\n",
      "3218 [Discriminator loss: 0.445095, acc.: 75.00%] [Generator loss: 4.138705]\n",
      "3219 [Discriminator loss: 0.414860, acc.: 78.12%] [Generator loss: 4.585226]\n",
      "3220 [Discriminator loss: 0.328603, acc.: 89.06%] [Generator loss: 4.410209]\n",
      "3221 [Discriminator loss: 0.274260, acc.: 89.06%] [Generator loss: 3.886241]\n",
      "3222 [Discriminator loss: 0.418729, acc.: 79.69%] [Generator loss: 4.300481]\n",
      "3223 [Discriminator loss: 0.297162, acc.: 90.62%] [Generator loss: 4.081937]\n",
      "3224 [Discriminator loss: 0.267620, acc.: 87.50%] [Generator loss: 3.932443]\n",
      "3225 [Discriminator loss: 0.414740, acc.: 79.69%] [Generator loss: 3.755483]\n",
      "3226 [Discriminator loss: 0.385150, acc.: 85.94%] [Generator loss: 5.367941]\n",
      "3227 [Discriminator loss: 0.486403, acc.: 79.69%] [Generator loss: 4.096407]\n",
      "3228 [Discriminator loss: 0.190965, acc.: 90.62%] [Generator loss: 4.616381]\n",
      "3229 [Discriminator loss: 0.219007, acc.: 90.62%] [Generator loss: 4.264498]\n",
      "3230 [Discriminator loss: 0.241693, acc.: 90.62%] [Generator loss: 3.262597]\n",
      "3231 [Discriminator loss: 0.355771, acc.: 85.94%] [Generator loss: 3.966153]\n",
      "3232 [Discriminator loss: 0.308656, acc.: 90.62%] [Generator loss: 4.933355]\n",
      "3233 [Discriminator loss: 0.240188, acc.: 92.19%] [Generator loss: 4.516813]\n",
      "3234 [Discriminator loss: 0.384003, acc.: 81.25%] [Generator loss: 4.552112]\n",
      "3235 [Discriminator loss: 0.613435, acc.: 70.31%] [Generator loss: 3.912459]\n",
      "3236 [Discriminator loss: 0.232409, acc.: 89.06%] [Generator loss: 4.720345]\n",
      "3237 [Discriminator loss: 0.517085, acc.: 78.12%] [Generator loss: 3.447127]\n",
      "3238 [Discriminator loss: 0.332096, acc.: 82.81%] [Generator loss: 5.205197]\n",
      "3239 [Discriminator loss: 0.394405, acc.: 82.81%] [Generator loss: 2.888314]\n",
      "3240 [Discriminator loss: 0.359931, acc.: 85.94%] [Generator loss: 4.295298]\n",
      "3241 [Discriminator loss: 0.376299, acc.: 87.50%] [Generator loss: 3.345930]\n",
      "3242 [Discriminator loss: 0.231439, acc.: 93.75%] [Generator loss: 3.360977]\n",
      "3243 [Discriminator loss: 0.332058, acc.: 85.94%] [Generator loss: 3.316572]\n",
      "3244 [Discriminator loss: 0.437151, acc.: 73.44%] [Generator loss: 3.332137]\n",
      "3245 [Discriminator loss: 0.164137, acc.: 93.75%] [Generator loss: 3.617425]\n",
      "3246 [Discriminator loss: 0.537664, acc.: 79.69%] [Generator loss: 4.368194]\n",
      "3247 [Discriminator loss: 0.412581, acc.: 78.12%] [Generator loss: 2.636202]\n",
      "3248 [Discriminator loss: 0.517766, acc.: 76.56%] [Generator loss: 4.095748]\n",
      "3249 [Discriminator loss: 0.484203, acc.: 76.56%] [Generator loss: 3.499187]\n",
      "3250 [Discriminator loss: 0.600389, acc.: 73.44%] [Generator loss: 4.569159]\n",
      "3251 [Discriminator loss: 0.233643, acc.: 92.19%] [Generator loss: 4.246240]\n",
      "3252 [Discriminator loss: 0.312052, acc.: 87.50%] [Generator loss: 4.711699]\n",
      "3253 [Discriminator loss: 0.348565, acc.: 82.81%] [Generator loss: 4.605209]\n",
      "3254 [Discriminator loss: 0.136750, acc.: 95.31%] [Generator loss: 4.156991]\n",
      "3255 [Discriminator loss: 0.472528, acc.: 79.69%] [Generator loss: 3.978908]\n",
      "3256 [Discriminator loss: 0.432031, acc.: 82.81%] [Generator loss: 4.831760]\n",
      "3257 [Discriminator loss: 0.459697, acc.: 85.94%] [Generator loss: 3.992419]\n",
      "3258 [Discriminator loss: 0.369796, acc.: 81.25%] [Generator loss: 4.474299]\n",
      "3259 [Discriminator loss: 0.516398, acc.: 76.56%] [Generator loss: 4.312946]\n",
      "3260 [Discriminator loss: 0.287945, acc.: 87.50%] [Generator loss: 3.763237]\n",
      "3261 [Discriminator loss: 0.336329, acc.: 85.94%] [Generator loss: 3.827708]\n",
      "3262 [Discriminator loss: 0.374203, acc.: 85.94%] [Generator loss: 3.719182]\n",
      "3263 [Discriminator loss: 0.260812, acc.: 90.62%] [Generator loss: 3.957664]\n",
      "3264 [Discriminator loss: 0.538202, acc.: 75.00%] [Generator loss: 3.565210]\n",
      "3265 [Discriminator loss: 0.352963, acc.: 87.50%] [Generator loss: 4.827650]\n",
      "3266 [Discriminator loss: 0.302801, acc.: 92.19%] [Generator loss: 4.765333]\n",
      "3267 [Discriminator loss: 0.448297, acc.: 78.12%] [Generator loss: 3.747012]\n",
      "3268 [Discriminator loss: 0.179252, acc.: 90.62%] [Generator loss: 4.987280]\n",
      "3269 [Discriminator loss: 0.384158, acc.: 84.38%] [Generator loss: 3.936172]\n",
      "3270 [Discriminator loss: 0.499980, acc.: 76.56%] [Generator loss: 3.743031]\n",
      "3271 [Discriminator loss: 0.394686, acc.: 78.12%] [Generator loss: 3.538381]\n",
      "3272 [Discriminator loss: 0.277728, acc.: 89.06%] [Generator loss: 4.701657]\n",
      "3273 [Discriminator loss: 0.301797, acc.: 90.62%] [Generator loss: 3.082985]\n",
      "3274 [Discriminator loss: 0.450352, acc.: 78.12%] [Generator loss: 4.865392]\n",
      "3275 [Discriminator loss: 0.523088, acc.: 76.56%] [Generator loss: 3.646319]\n",
      "3276 [Discriminator loss: 0.299379, acc.: 87.50%] [Generator loss: 3.916451]\n",
      "3277 [Discriminator loss: 0.391731, acc.: 84.38%] [Generator loss: 4.945479]\n",
      "3278 [Discriminator loss: 0.288058, acc.: 87.50%] [Generator loss: 4.937611]\n",
      "3279 [Discriminator loss: 0.421029, acc.: 76.56%] [Generator loss: 3.718070]\n",
      "3280 [Discriminator loss: 0.387821, acc.: 89.06%] [Generator loss: 3.239924]\n",
      "3281 [Discriminator loss: 0.292387, acc.: 84.38%] [Generator loss: 3.911423]\n",
      "3282 [Discriminator loss: 0.268703, acc.: 87.50%] [Generator loss: 4.528650]\n",
      "3283 [Discriminator loss: 0.394111, acc.: 84.38%] [Generator loss: 3.745212]\n",
      "3284 [Discriminator loss: 0.342572, acc.: 82.81%] [Generator loss: 3.511959]\n",
      "3285 [Discriminator loss: 0.267319, acc.: 84.38%] [Generator loss: 3.242730]\n",
      "3286 [Discriminator loss: 0.390076, acc.: 81.25%] [Generator loss: 3.455441]\n",
      "3287 [Discriminator loss: 0.239347, acc.: 87.50%] [Generator loss: 4.165409]\n",
      "3288 [Discriminator loss: 0.163431, acc.: 92.19%] [Generator loss: 3.116757]\n",
      "3289 [Discriminator loss: 0.456641, acc.: 78.12%] [Generator loss: 4.213937]\n",
      "3290 [Discriminator loss: 0.433685, acc.: 84.38%] [Generator loss: 3.454668]\n",
      "3291 [Discriminator loss: 0.306834, acc.: 87.50%] [Generator loss: 4.436750]\n",
      "3292 [Discriminator loss: 0.385711, acc.: 81.25%] [Generator loss: 3.936049]\n",
      "3293 [Discriminator loss: 0.409954, acc.: 78.12%] [Generator loss: 3.965104]\n",
      "3294 [Discriminator loss: 0.200304, acc.: 92.19%] [Generator loss: 3.347254]\n",
      "3295 [Discriminator loss: 0.355221, acc.: 84.38%] [Generator loss: 3.699219]\n",
      "3296 [Discriminator loss: 0.426290, acc.: 82.81%] [Generator loss: 4.586226]\n",
      "3297 [Discriminator loss: 0.391342, acc.: 85.94%] [Generator loss: 3.756944]\n",
      "3298 [Discriminator loss: 0.349264, acc.: 82.81%] [Generator loss: 5.004268]\n",
      "3299 [Discriminator loss: 0.277128, acc.: 85.94%] [Generator loss: 3.692316]\n",
      "3300 [Discriminator loss: 0.402837, acc.: 82.81%] [Generator loss: 3.883862]\n",
      "3301 [Discriminator loss: 0.219685, acc.: 92.19%] [Generator loss: 3.612402]\n",
      "3302 [Discriminator loss: 0.566917, acc.: 75.00%] [Generator loss: 4.318261]\n",
      "3303 [Discriminator loss: 0.336415, acc.: 87.50%] [Generator loss: 3.543628]\n",
      "3304 [Discriminator loss: 0.120617, acc.: 95.31%] [Generator loss: 3.792634]\n",
      "3305 [Discriminator loss: 0.395720, acc.: 82.81%] [Generator loss: 3.550212]\n",
      "3306 [Discriminator loss: 0.364763, acc.: 76.56%] [Generator loss: 4.083628]\n",
      "3307 [Discriminator loss: 0.236957, acc.: 89.06%] [Generator loss: 4.132531]\n",
      "3308 [Discriminator loss: 0.395730, acc.: 81.25%] [Generator loss: 3.321777]\n",
      "3309 [Discriminator loss: 0.581566, acc.: 73.44%] [Generator loss: 4.652790]\n",
      "3310 [Discriminator loss: 0.349833, acc.: 87.50%] [Generator loss: 3.853836]\n",
      "3311 [Discriminator loss: 0.491822, acc.: 79.69%] [Generator loss: 3.454742]\n",
      "3312 [Discriminator loss: 0.202543, acc.: 90.62%] [Generator loss: 4.178247]\n",
      "3313 [Discriminator loss: 0.442981, acc.: 82.81%] [Generator loss: 5.115151]\n",
      "3314 [Discriminator loss: 0.195411, acc.: 90.62%] [Generator loss: 4.886121]\n",
      "3315 [Discriminator loss: 0.471780, acc.: 81.25%] [Generator loss: 3.271621]\n",
      "3316 [Discriminator loss: 0.466694, acc.: 78.12%] [Generator loss: 3.948427]\n",
      "3317 [Discriminator loss: 0.465563, acc.: 81.25%] [Generator loss: 3.624495]\n",
      "3318 [Discriminator loss: 0.321632, acc.: 87.50%] [Generator loss: 4.467649]\n",
      "3319 [Discriminator loss: 0.220807, acc.: 90.62%] [Generator loss: 4.369718]\n",
      "3320 [Discriminator loss: 0.190266, acc.: 95.31%] [Generator loss: 4.068995]\n",
      "3321 [Discriminator loss: 0.537139, acc.: 71.88%] [Generator loss: 4.151445]\n",
      "3322 [Discriminator loss: 0.329375, acc.: 82.81%] [Generator loss: 4.440665]\n",
      "3323 [Discriminator loss: 0.648156, acc.: 71.88%] [Generator loss: 3.765111]\n",
      "3324 [Discriminator loss: 0.309282, acc.: 79.69%] [Generator loss: 4.337977]\n",
      "3325 [Discriminator loss: 0.155713, acc.: 93.75%] [Generator loss: 4.118150]\n",
      "3326 [Discriminator loss: 0.279563, acc.: 89.06%] [Generator loss: 4.089006]\n",
      "3327 [Discriminator loss: 0.286107, acc.: 87.50%] [Generator loss: 4.369089]\n",
      "3328 [Discriminator loss: 0.262703, acc.: 85.94%] [Generator loss: 4.771692]\n",
      "3329 [Discriminator loss: 0.362027, acc.: 82.81%] [Generator loss: 3.267445]\n",
      "3330 [Discriminator loss: 0.403764, acc.: 84.38%] [Generator loss: 3.881057]\n",
      "3331 [Discriminator loss: 0.222314, acc.: 92.19%] [Generator loss: 4.140108]\n",
      "3332 [Discriminator loss: 0.544175, acc.: 73.44%] [Generator loss: 3.088168]\n",
      "3333 [Discriminator loss: 0.313332, acc.: 81.25%] [Generator loss: 4.898459]\n",
      "3334 [Discriminator loss: 0.454443, acc.: 76.56%] [Generator loss: 4.174937]\n",
      "3335 [Discriminator loss: 0.334433, acc.: 79.69%] [Generator loss: 4.422978]\n",
      "3336 [Discriminator loss: 0.258195, acc.: 90.62%] [Generator loss: 4.339150]\n",
      "3337 [Discriminator loss: 0.498645, acc.: 79.69%] [Generator loss: 3.787210]\n",
      "3338 [Discriminator loss: 0.339604, acc.: 85.94%] [Generator loss: 4.539131]\n",
      "3339 [Discriminator loss: 0.094322, acc.: 96.88%] [Generator loss: 4.449186]\n",
      "3340 [Discriminator loss: 0.365576, acc.: 81.25%] [Generator loss: 3.097847]\n",
      "3341 [Discriminator loss: 0.308426, acc.: 87.50%] [Generator loss: 3.892743]\n",
      "3342 [Discriminator loss: 0.261539, acc.: 92.19%] [Generator loss: 3.250197]\n",
      "3343 [Discriminator loss: 0.267323, acc.: 89.06%] [Generator loss: 4.261923]\n",
      "3344 [Discriminator loss: 0.410523, acc.: 87.50%] [Generator loss: 4.451381]\n",
      "3345 [Discriminator loss: 0.749092, acc.: 62.50%] [Generator loss: 4.303045]\n",
      "3346 [Discriminator loss: 0.214574, acc.: 90.62%] [Generator loss: 4.584489]\n",
      "3347 [Discriminator loss: 0.353644, acc.: 89.06%] [Generator loss: 4.995086]\n",
      "3348 [Discriminator loss: 0.240642, acc.: 89.06%] [Generator loss: 4.798061]\n",
      "3349 [Discriminator loss: 0.389989, acc.: 79.69%] [Generator loss: 3.704612]\n",
      "3350 [Discriminator loss: 0.340088, acc.: 81.25%] [Generator loss: 4.092456]\n",
      "3351 [Discriminator loss: 0.316180, acc.: 84.38%] [Generator loss: 4.496559]\n",
      "3352 [Discriminator loss: 0.396232, acc.: 79.69%] [Generator loss: 4.556870]\n",
      "3353 [Discriminator loss: 0.520887, acc.: 76.56%] [Generator loss: 3.559740]\n",
      "3354 [Discriminator loss: 0.258021, acc.: 89.06%] [Generator loss: 4.326130]\n",
      "3355 [Discriminator loss: 0.411244, acc.: 81.25%] [Generator loss: 4.366467]\n",
      "3356 [Discriminator loss: 0.291968, acc.: 90.62%] [Generator loss: 4.348606]\n",
      "3357 [Discriminator loss: 0.318133, acc.: 84.38%] [Generator loss: 4.481928]\n",
      "3358 [Discriminator loss: 0.380203, acc.: 84.38%] [Generator loss: 3.524544]\n",
      "3359 [Discriminator loss: 0.336917, acc.: 81.25%] [Generator loss: 3.494286]\n",
      "3360 [Discriminator loss: 0.480224, acc.: 78.12%] [Generator loss: 3.921184]\n",
      "3361 [Discriminator loss: 0.323485, acc.: 84.38%] [Generator loss: 3.937133]\n",
      "3362 [Discriminator loss: 0.223667, acc.: 90.62%] [Generator loss: 3.706243]\n",
      "3363 [Discriminator loss: 0.532465, acc.: 78.12%] [Generator loss: 3.368846]\n",
      "3364 [Discriminator loss: 0.230512, acc.: 87.50%] [Generator loss: 5.043507]\n",
      "3365 [Discriminator loss: 0.222829, acc.: 90.62%] [Generator loss: 2.682975]\n",
      "3366 [Discriminator loss: 0.310147, acc.: 84.38%] [Generator loss: 3.578986]\n",
      "3367 [Discriminator loss: 0.358138, acc.: 82.81%] [Generator loss: 3.965533]\n",
      "3368 [Discriminator loss: 0.424261, acc.: 82.81%] [Generator loss: 5.229695]\n",
      "3369 [Discriminator loss: 0.351651, acc.: 89.06%] [Generator loss: 3.337925]\n",
      "3370 [Discriminator loss: 0.362128, acc.: 84.38%] [Generator loss: 4.010509]\n",
      "3371 [Discriminator loss: 0.338219, acc.: 87.50%] [Generator loss: 4.477806]\n",
      "3372 [Discriminator loss: 0.371696, acc.: 84.38%] [Generator loss: 3.873957]\n",
      "3373 [Discriminator loss: 0.237421, acc.: 92.19%] [Generator loss: 3.553239]\n",
      "3374 [Discriminator loss: 0.620263, acc.: 65.62%] [Generator loss: 4.520912]\n",
      "3375 [Discriminator loss: 0.197048, acc.: 92.19%] [Generator loss: 4.486294]\n",
      "3376 [Discriminator loss: 0.504836, acc.: 76.56%] [Generator loss: 3.909734]\n",
      "3377 [Discriminator loss: 0.316126, acc.: 87.50%] [Generator loss: 4.764423]\n",
      "3378 [Discriminator loss: 0.287303, acc.: 89.06%] [Generator loss: 4.258431]\n",
      "3379 [Discriminator loss: 0.272413, acc.: 89.06%] [Generator loss: 3.730494]\n",
      "3380 [Discriminator loss: 0.225440, acc.: 89.06%] [Generator loss: 2.924939]\n",
      "3381 [Discriminator loss: 0.416778, acc.: 82.81%] [Generator loss: 3.134144]\n",
      "3382 [Discriminator loss: 0.284549, acc.: 85.94%] [Generator loss: 5.208084]\n",
      "3383 [Discriminator loss: 0.409070, acc.: 81.25%] [Generator loss: 3.985408]\n",
      "3384 [Discriminator loss: 0.517897, acc.: 79.69%] [Generator loss: 3.565762]\n",
      "3385 [Discriminator loss: 0.534617, acc.: 71.88%] [Generator loss: 4.191549]\n",
      "3386 [Discriminator loss: 0.500148, acc.: 76.56%] [Generator loss: 3.473609]\n",
      "3387 [Discriminator loss: 0.157590, acc.: 95.31%] [Generator loss: 4.561733]\n",
      "3388 [Discriminator loss: 0.218007, acc.: 93.75%] [Generator loss: 4.879512]\n",
      "3389 [Discriminator loss: 0.454963, acc.: 78.12%] [Generator loss: 3.772447]\n",
      "3390 [Discriminator loss: 0.373738, acc.: 85.94%] [Generator loss: 4.132016]\n",
      "3391 [Discriminator loss: 0.349507, acc.: 84.38%] [Generator loss: 3.285990]\n",
      "3392 [Discriminator loss: 0.399976, acc.: 84.38%] [Generator loss: 4.643052]\n",
      "3393 [Discriminator loss: 0.206961, acc.: 92.19%] [Generator loss: 4.690050]\n",
      "3394 [Discriminator loss: 0.443189, acc.: 81.25%] [Generator loss: 3.061115]\n",
      "3395 [Discriminator loss: 0.283789, acc.: 90.62%] [Generator loss: 3.968823]\n",
      "3396 [Discriminator loss: 0.196405, acc.: 93.75%] [Generator loss: 4.039460]\n",
      "3397 [Discriminator loss: 0.427612, acc.: 79.69%] [Generator loss: 3.788904]\n",
      "3398 [Discriminator loss: 0.575375, acc.: 76.56%] [Generator loss: 3.365821]\n",
      "3399 [Discriminator loss: 0.288439, acc.: 84.38%] [Generator loss: 4.025785]\n",
      "3400 [Discriminator loss: 0.284312, acc.: 84.38%] [Generator loss: 3.146776]\n",
      "3401 [Discriminator loss: 0.313316, acc.: 89.06%] [Generator loss: 4.655005]\n",
      "3402 [Discriminator loss: 0.309137, acc.: 90.62%] [Generator loss: 4.417305]\n",
      "3403 [Discriminator loss: 0.545760, acc.: 71.88%] [Generator loss: 3.902393]\n",
      "3404 [Discriminator loss: 0.204736, acc.: 93.75%] [Generator loss: 4.277565]\n",
      "3405 [Discriminator loss: 0.556134, acc.: 70.31%] [Generator loss: 3.911790]\n",
      "3406 [Discriminator loss: 0.239688, acc.: 90.62%] [Generator loss: 5.362497]\n",
      "3407 [Discriminator loss: 0.321544, acc.: 84.38%] [Generator loss: 4.076662]\n",
      "3408 [Discriminator loss: 0.248756, acc.: 89.06%] [Generator loss: 3.985449]\n",
      "3409 [Discriminator loss: 0.711548, acc.: 70.31%] [Generator loss: 3.092946]\n",
      "3410 [Discriminator loss: 0.403250, acc.: 82.81%] [Generator loss: 4.291608]\n",
      "3411 [Discriminator loss: 0.327075, acc.: 84.38%] [Generator loss: 3.523357]\n",
      "3412 [Discriminator loss: 0.459152, acc.: 79.69%] [Generator loss: 4.439793]\n",
      "3413 [Discriminator loss: 0.534785, acc.: 76.56%] [Generator loss: 3.785859]\n",
      "3414 [Discriminator loss: 0.213456, acc.: 90.62%] [Generator loss: 3.185022]\n",
      "3415 [Discriminator loss: 0.423727, acc.: 78.12%] [Generator loss: 4.386395]\n",
      "3416 [Discriminator loss: 0.534635, acc.: 78.12%] [Generator loss: 4.248675]\n",
      "3417 [Discriminator loss: 0.367432, acc.: 84.38%] [Generator loss: 3.580253]\n",
      "3418 [Discriminator loss: 0.097249, acc.: 98.44%] [Generator loss: 4.966440]\n",
      "3419 [Discriminator loss: 0.231747, acc.: 92.19%] [Generator loss: 4.123084]\n",
      "3420 [Discriminator loss: 0.670922, acc.: 67.19%] [Generator loss: 6.220102]\n",
      "3421 [Discriminator loss: 0.212180, acc.: 93.75%] [Generator loss: 4.729118]\n",
      "3422 [Discriminator loss: 0.219389, acc.: 92.19%] [Generator loss: 3.723594]\n",
      "3423 [Discriminator loss: 0.398607, acc.: 78.12%] [Generator loss: 3.450229]\n",
      "3424 [Discriminator loss: 0.271460, acc.: 84.38%] [Generator loss: 3.790061]\n",
      "3425 [Discriminator loss: 0.424897, acc.: 81.25%] [Generator loss: 3.334187]\n",
      "3426 [Discriminator loss: 0.273776, acc.: 87.50%] [Generator loss: 3.570534]\n",
      "3427 [Discriminator loss: 0.263428, acc.: 90.62%] [Generator loss: 3.897600]\n",
      "3428 [Discriminator loss: 0.365401, acc.: 79.69%] [Generator loss: 4.226426]\n",
      "3429 [Discriminator loss: 0.278763, acc.: 85.94%] [Generator loss: 3.993335]\n",
      "3430 [Discriminator loss: 0.373341, acc.: 84.38%] [Generator loss: 4.018054]\n",
      "3431 [Discriminator loss: 0.299473, acc.: 87.50%] [Generator loss: 4.671962]\n",
      "3432 [Discriminator loss: 0.430529, acc.: 84.38%] [Generator loss: 3.573459]\n",
      "3433 [Discriminator loss: 0.292887, acc.: 85.94%] [Generator loss: 4.126366]\n",
      "3434 [Discriminator loss: 0.346757, acc.: 87.50%] [Generator loss: 3.066608]\n",
      "3435 [Discriminator loss: 0.452300, acc.: 81.25%] [Generator loss: 3.557634]\n",
      "3436 [Discriminator loss: 0.269131, acc.: 90.62%] [Generator loss: 3.754134]\n",
      "3437 [Discriminator loss: 0.574581, acc.: 75.00%] [Generator loss: 3.686394]\n",
      "3438 [Discriminator loss: 0.251017, acc.: 89.06%] [Generator loss: 4.470739]\n",
      "3439 [Discriminator loss: 0.279153, acc.: 87.50%] [Generator loss: 4.003783]\n",
      "3440 [Discriminator loss: 0.178438, acc.: 95.31%] [Generator loss: 3.874799]\n",
      "3441 [Discriminator loss: 0.204222, acc.: 92.19%] [Generator loss: 4.170956]\n",
      "3442 [Discriminator loss: 0.290264, acc.: 87.50%] [Generator loss: 4.103512]\n",
      "3443 [Discriminator loss: 0.294304, acc.: 85.94%] [Generator loss: 4.017457]\n",
      "3444 [Discriminator loss: 0.307941, acc.: 89.06%] [Generator loss: 3.675668]\n",
      "3445 [Discriminator loss: 0.182267, acc.: 95.31%] [Generator loss: 4.799575]\n",
      "3446 [Discriminator loss: 0.159688, acc.: 93.75%] [Generator loss: 3.521866]\n",
      "3447 [Discriminator loss: 0.195151, acc.: 92.19%] [Generator loss: 2.844671]\n",
      "3448 [Discriminator loss: 0.368087, acc.: 81.25%] [Generator loss: 4.843994]\n",
      "3449 [Discriminator loss: 0.191260, acc.: 95.31%] [Generator loss: 4.888118]\n",
      "3450 [Discriminator loss: 0.399382, acc.: 82.81%] [Generator loss: 3.162247]\n",
      "3451 [Discriminator loss: 0.397856, acc.: 84.38%] [Generator loss: 3.186933]\n",
      "3452 [Discriminator loss: 0.169600, acc.: 92.19%] [Generator loss: 3.838359]\n",
      "3453 [Discriminator loss: 0.348192, acc.: 81.25%] [Generator loss: 4.026328]\n",
      "3454 [Discriminator loss: 0.305748, acc.: 85.94%] [Generator loss: 5.174939]\n",
      "3455 [Discriminator loss: 0.496205, acc.: 79.69%] [Generator loss: 3.763011]\n",
      "3456 [Discriminator loss: 0.274596, acc.: 89.06%] [Generator loss: 3.818406]\n",
      "3457 [Discriminator loss: 0.498599, acc.: 78.12%] [Generator loss: 4.696942]\n",
      "3458 [Discriminator loss: 0.157530, acc.: 93.75%] [Generator loss: 4.071532]\n",
      "3459 [Discriminator loss: 0.169479, acc.: 96.88%] [Generator loss: 3.816094]\n",
      "3460 [Discriminator loss: 0.452391, acc.: 76.56%] [Generator loss: 5.403604]\n",
      "3461 [Discriminator loss: 0.513773, acc.: 71.88%] [Generator loss: 4.106690]\n",
      "3462 [Discriminator loss: 0.316714, acc.: 87.50%] [Generator loss: 4.262179]\n",
      "3463 [Discriminator loss: 0.250970, acc.: 87.50%] [Generator loss: 3.946754]\n",
      "3464 [Discriminator loss: 0.257637, acc.: 89.06%] [Generator loss: 3.957227]\n",
      "3465 [Discriminator loss: 0.189557, acc.: 89.06%] [Generator loss: 3.209278]\n",
      "3466 [Discriminator loss: 0.126131, acc.: 95.31%] [Generator loss: 4.757168]\n",
      "3467 [Discriminator loss: 0.352575, acc.: 85.94%] [Generator loss: 3.799102]\n",
      "3468 [Discriminator loss: 0.241142, acc.: 93.75%] [Generator loss: 4.813303]\n",
      "3469 [Discriminator loss: 0.429189, acc.: 81.25%] [Generator loss: 4.384112]\n",
      "3470 [Discriminator loss: 0.343346, acc.: 84.38%] [Generator loss: 3.175415]\n",
      "3471 [Discriminator loss: 0.269748, acc.: 96.88%] [Generator loss: 4.112985]\n",
      "3472 [Discriminator loss: 0.280414, acc.: 85.94%] [Generator loss: 4.422829]\n",
      "3473 [Discriminator loss: 0.584898, acc.: 70.31%] [Generator loss: 4.229742]\n",
      "3474 [Discriminator loss: 0.362920, acc.: 85.94%] [Generator loss: 3.972022]\n",
      "3475 [Discriminator loss: 0.170396, acc.: 93.75%] [Generator loss: 3.640666]\n",
      "3476 [Discriminator loss: 0.229773, acc.: 89.06%] [Generator loss: 4.240830]\n",
      "3477 [Discriminator loss: 0.398266, acc.: 82.81%] [Generator loss: 4.322125]\n",
      "3478 [Discriminator loss: 0.242506, acc.: 89.06%] [Generator loss: 3.869471]\n",
      "3479 [Discriminator loss: 0.292243, acc.: 89.06%] [Generator loss: 4.778656]\n",
      "3480 [Discriminator loss: 0.306391, acc.: 84.38%] [Generator loss: 3.678781]\n",
      "3481 [Discriminator loss: 0.229399, acc.: 87.50%] [Generator loss: 4.265647]\n",
      "3482 [Discriminator loss: 0.411775, acc.: 81.25%] [Generator loss: 5.310357]\n",
      "3483 [Discriminator loss: 0.262868, acc.: 89.06%] [Generator loss: 4.315896]\n",
      "3484 [Discriminator loss: 0.577096, acc.: 75.00%] [Generator loss: 4.292883]\n",
      "3485 [Discriminator loss: 0.419579, acc.: 85.94%] [Generator loss: 4.426828]\n",
      "3486 [Discriminator loss: 0.370918, acc.: 81.25%] [Generator loss: 4.699447]\n",
      "3487 [Discriminator loss: 0.228521, acc.: 90.62%] [Generator loss: 4.140065]\n",
      "3488 [Discriminator loss: 0.234718, acc.: 90.62%] [Generator loss: 3.424696]\n",
      "3489 [Discriminator loss: 0.298445, acc.: 82.81%] [Generator loss: 3.198948]\n",
      "3490 [Discriminator loss: 0.516669, acc.: 76.56%] [Generator loss: 4.599562]\n",
      "3491 [Discriminator loss: 0.310494, acc.: 89.06%] [Generator loss: 3.454768]\n",
      "3492 [Discriminator loss: 0.421961, acc.: 76.56%] [Generator loss: 5.174355]\n",
      "3493 [Discriminator loss: 0.380287, acc.: 79.69%] [Generator loss: 3.439743]\n",
      "3494 [Discriminator loss: 0.290753, acc.: 92.19%] [Generator loss: 4.649750]\n",
      "3495 [Discriminator loss: 0.145443, acc.: 92.19%] [Generator loss: 4.420672]\n",
      "3496 [Discriminator loss: 0.325069, acc.: 85.94%] [Generator loss: 3.998127]\n",
      "3497 [Discriminator loss: 0.292673, acc.: 87.50%] [Generator loss: 3.753096]\n",
      "3498 [Discriminator loss: 0.225277, acc.: 87.50%] [Generator loss: 4.827753]\n",
      "3499 [Discriminator loss: 0.282890, acc.: 81.25%] [Generator loss: 4.250935]\n",
      "3500 [Discriminator loss: 0.285554, acc.: 90.62%] [Generator loss: 5.167538]\n",
      "3501 [Discriminator loss: 0.120559, acc.: 98.44%] [Generator loss: 4.556885]\n",
      "3502 [Discriminator loss: 0.687608, acc.: 65.62%] [Generator loss: 3.422995]\n",
      "3503 [Discriminator loss: 0.252137, acc.: 89.06%] [Generator loss: 4.666952]\n",
      "3504 [Discriminator loss: 0.328793, acc.: 84.38%] [Generator loss: 3.465454]\n",
      "3505 [Discriminator loss: 0.159459, acc.: 93.75%] [Generator loss: 4.184793]\n",
      "3506 [Discriminator loss: 0.310236, acc.: 89.06%] [Generator loss: 3.992672]\n",
      "3507 [Discriminator loss: 0.148417, acc.: 95.31%] [Generator loss: 3.539816]\n",
      "3508 [Discriminator loss: 0.142043, acc.: 96.88%] [Generator loss: 3.959891]\n",
      "3509 [Discriminator loss: 0.556259, acc.: 71.88%] [Generator loss: 3.520705]\n",
      "3510 [Discriminator loss: 0.165594, acc.: 90.62%] [Generator loss: 4.649107]\n",
      "3511 [Discriminator loss: 0.585275, acc.: 76.56%] [Generator loss: 3.423467]\n",
      "3512 [Discriminator loss: 0.250803, acc.: 92.19%] [Generator loss: 3.733772]\n",
      "3513 [Discriminator loss: 0.524610, acc.: 85.94%] [Generator loss: 3.872608]\n",
      "3514 [Discriminator loss: 0.497853, acc.: 71.88%] [Generator loss: 3.761452]\n",
      "3515 [Discriminator loss: 0.248571, acc.: 92.19%] [Generator loss: 4.062841]\n",
      "3516 [Discriminator loss: 0.333510, acc.: 85.94%] [Generator loss: 4.930171]\n",
      "3517 [Discriminator loss: 0.244657, acc.: 89.06%] [Generator loss: 4.033035]\n",
      "3518 [Discriminator loss: 0.400823, acc.: 82.81%] [Generator loss: 4.867372]\n",
      "3519 [Discriminator loss: 0.319019, acc.: 81.25%] [Generator loss: 4.769086]\n",
      "3520 [Discriminator loss: 0.279884, acc.: 90.62%] [Generator loss: 5.423716]\n",
      "3521 [Discriminator loss: 0.372147, acc.: 89.06%] [Generator loss: 3.723507]\n",
      "3522 [Discriminator loss: 0.339997, acc.: 82.81%] [Generator loss: 3.686508]\n",
      "3523 [Discriminator loss: 0.147981, acc.: 95.31%] [Generator loss: 4.454912]\n",
      "3524 [Discriminator loss: 0.376261, acc.: 85.94%] [Generator loss: 4.387102]\n",
      "3525 [Discriminator loss: 0.288271, acc.: 85.94%] [Generator loss: 4.086635]\n",
      "3526 [Discriminator loss: 0.328698, acc.: 89.06%] [Generator loss: 3.443037]\n",
      "3527 [Discriminator loss: 0.363654, acc.: 84.38%] [Generator loss: 4.205897]\n",
      "3528 [Discriminator loss: 0.376923, acc.: 82.81%] [Generator loss: 4.718580]\n",
      "3529 [Discriminator loss: 0.406035, acc.: 79.69%] [Generator loss: 4.007170]\n",
      "3530 [Discriminator loss: 0.249537, acc.: 87.50%] [Generator loss: 4.130921]\n",
      "3531 [Discriminator loss: 0.635882, acc.: 67.19%] [Generator loss: 3.880086]\n",
      "3532 [Discriminator loss: 0.204831, acc.: 89.06%] [Generator loss: 5.383881]\n",
      "3533 [Discriminator loss: 0.884342, acc.: 62.50%] [Generator loss: 3.396244]\n",
      "3534 [Discriminator loss: 0.184977, acc.: 93.75%] [Generator loss: 4.255813]\n",
      "3535 [Discriminator loss: 0.373506, acc.: 82.81%] [Generator loss: 4.910252]\n",
      "3536 [Discriminator loss: 0.328245, acc.: 85.94%] [Generator loss: 3.898879]\n",
      "3537 [Discriminator loss: 0.514855, acc.: 78.12%] [Generator loss: 4.223825]\n",
      "3538 [Discriminator loss: 0.425895, acc.: 81.25%] [Generator loss: 5.496234]\n",
      "3539 [Discriminator loss: 0.445830, acc.: 81.25%] [Generator loss: 2.933984]\n",
      "3540 [Discriminator loss: 0.287871, acc.: 92.19%] [Generator loss: 2.861011]\n",
      "3541 [Discriminator loss: 0.211664, acc.: 90.62%] [Generator loss: 3.897066]\n",
      "3542 [Discriminator loss: 0.203437, acc.: 92.19%] [Generator loss: 3.720393]\n",
      "3543 [Discriminator loss: 0.321581, acc.: 90.62%] [Generator loss: 3.852316]\n",
      "3544 [Discriminator loss: 0.461890, acc.: 76.56%] [Generator loss: 4.351421]\n",
      "3545 [Discriminator loss: 0.260462, acc.: 87.50%] [Generator loss: 3.715822]\n",
      "3546 [Discriminator loss: 0.292686, acc.: 89.06%] [Generator loss: 4.833636]\n",
      "3547 [Discriminator loss: 0.257922, acc.: 90.62%] [Generator loss: 3.833348]\n",
      "3548 [Discriminator loss: 0.327319, acc.: 90.62%] [Generator loss: 4.980846]\n",
      "3549 [Discriminator loss: 0.329626, acc.: 84.38%] [Generator loss: 4.306848]\n",
      "3550 [Discriminator loss: 0.464358, acc.: 82.81%] [Generator loss: 4.996333]\n",
      "3551 [Discriminator loss: 0.310860, acc.: 85.94%] [Generator loss: 4.102755]\n",
      "3552 [Discriminator loss: 0.279509, acc.: 90.62%] [Generator loss: 4.049695]\n",
      "3553 [Discriminator loss: 0.473942, acc.: 78.12%] [Generator loss: 4.533751]\n",
      "3554 [Discriminator loss: 0.281533, acc.: 87.50%] [Generator loss: 3.169595]\n",
      "3555 [Discriminator loss: 0.333114, acc.: 85.94%] [Generator loss: 3.912340]\n",
      "3556 [Discriminator loss: 0.295302, acc.: 90.62%] [Generator loss: 3.933312]\n",
      "3557 [Discriminator loss: 0.414738, acc.: 78.12%] [Generator loss: 3.613163]\n",
      "3558 [Discriminator loss: 0.628688, acc.: 75.00%] [Generator loss: 4.897919]\n",
      "3559 [Discriminator loss: 0.502589, acc.: 71.88%] [Generator loss: 5.694733]\n",
      "3560 [Discriminator loss: 0.556339, acc.: 78.12%] [Generator loss: 4.670763]\n",
      "3561 [Discriminator loss: 0.358604, acc.: 84.38%] [Generator loss: 4.976129]\n",
      "3562 [Discriminator loss: 0.210271, acc.: 92.19%] [Generator loss: 4.755739]\n",
      "3563 [Discriminator loss: 0.252200, acc.: 89.06%] [Generator loss: 3.903770]\n",
      "3564 [Discriminator loss: 0.408838, acc.: 79.69%] [Generator loss: 4.378326]\n",
      "3565 [Discriminator loss: 0.356884, acc.: 85.94%] [Generator loss: 4.543800]\n",
      "3566 [Discriminator loss: 0.217925, acc.: 90.62%] [Generator loss: 4.687456]\n",
      "3567 [Discriminator loss: 0.277503, acc.: 89.06%] [Generator loss: 3.839504]\n",
      "3568 [Discriminator loss: 0.137755, acc.: 93.75%] [Generator loss: 3.799288]\n",
      "3569 [Discriminator loss: 0.409542, acc.: 81.25%] [Generator loss: 3.991538]\n",
      "3570 [Discriminator loss: 0.245343, acc.: 92.19%] [Generator loss: 3.552989]\n",
      "3571 [Discriminator loss: 0.377132, acc.: 81.25%] [Generator loss: 3.338377]\n",
      "3572 [Discriminator loss: 0.473846, acc.: 78.12%] [Generator loss: 4.352522]\n",
      "3573 [Discriminator loss: 0.550119, acc.: 73.44%] [Generator loss: 3.385144]\n",
      "3574 [Discriminator loss: 0.167758, acc.: 95.31%] [Generator loss: 4.255827]\n",
      "3575 [Discriminator loss: 0.166794, acc.: 90.62%] [Generator loss: 3.324122]\n",
      "3576 [Discriminator loss: 0.299229, acc.: 85.94%] [Generator loss: 2.990875]\n",
      "3577 [Discriminator loss: 0.264686, acc.: 89.06%] [Generator loss: 4.074724]\n",
      "3578 [Discriminator loss: 0.293107, acc.: 90.62%] [Generator loss: 5.626832]\n",
      "3579 [Discriminator loss: 0.455922, acc.: 75.00%] [Generator loss: 4.041038]\n",
      "3580 [Discriminator loss: 0.180911, acc.: 93.75%] [Generator loss: 3.741123]\n",
      "3581 [Discriminator loss: 0.183631, acc.: 90.62%] [Generator loss: 3.350961]\n",
      "3582 [Discriminator loss: 0.460584, acc.: 78.12%] [Generator loss: 3.663707]\n",
      "3583 [Discriminator loss: 0.200684, acc.: 92.19%] [Generator loss: 3.587473]\n",
      "3584 [Discriminator loss: 0.405009, acc.: 84.38%] [Generator loss: 3.578358]\n",
      "3585 [Discriminator loss: 0.187878, acc.: 93.75%] [Generator loss: 4.139085]\n",
      "3586 [Discriminator loss: 0.438904, acc.: 76.56%] [Generator loss: 3.190183]\n",
      "3587 [Discriminator loss: 0.236851, acc.: 87.50%] [Generator loss: 3.561697]\n",
      "3588 [Discriminator loss: 0.231014, acc.: 87.50%] [Generator loss: 4.420789]\n",
      "3589 [Discriminator loss: 0.183940, acc.: 93.75%] [Generator loss: 4.367347]\n",
      "3590 [Discriminator loss: 0.470917, acc.: 78.12%] [Generator loss: 4.757607]\n",
      "3591 [Discriminator loss: 0.231437, acc.: 85.94%] [Generator loss: 4.625808]\n",
      "3592 [Discriminator loss: 0.226469, acc.: 95.31%] [Generator loss: 3.979673]\n",
      "3593 [Discriminator loss: 0.527738, acc.: 81.25%] [Generator loss: 3.384927]\n",
      "3594 [Discriminator loss: 0.321309, acc.: 85.94%] [Generator loss: 4.066719]\n",
      "3595 [Discriminator loss: 0.186479, acc.: 93.75%] [Generator loss: 3.707066]\n",
      "3596 [Discriminator loss: 0.166369, acc.: 93.75%] [Generator loss: 3.573670]\n",
      "3597 [Discriminator loss: 0.169221, acc.: 92.19%] [Generator loss: 4.491020]\n",
      "3598 [Discriminator loss: 0.325135, acc.: 85.94%] [Generator loss: 3.964074]\n",
      "3599 [Discriminator loss: 0.557650, acc.: 71.88%] [Generator loss: 4.202672]\n",
      "3600 [Discriminator loss: 0.337543, acc.: 89.06%] [Generator loss: 4.354723]\n",
      "3601 [Discriminator loss: 0.457708, acc.: 81.25%] [Generator loss: 3.152819]\n",
      "3602 [Discriminator loss: 0.170131, acc.: 95.31%] [Generator loss: 3.911122]\n",
      "3603 [Discriminator loss: 0.251720, acc.: 89.06%] [Generator loss: 3.925075]\n",
      "3604 [Discriminator loss: 0.240042, acc.: 93.75%] [Generator loss: 4.359469]\n",
      "3605 [Discriminator loss: 0.415984, acc.: 78.12%] [Generator loss: 3.734130]\n",
      "3606 [Discriminator loss: 0.344686, acc.: 85.94%] [Generator loss: 4.213531]\n",
      "3607 [Discriminator loss: 0.604658, acc.: 71.88%] [Generator loss: 4.221094]\n",
      "3608 [Discriminator loss: 0.243750, acc.: 89.06%] [Generator loss: 3.850228]\n",
      "3609 [Discriminator loss: 0.413356, acc.: 78.12%] [Generator loss: 4.664920]\n",
      "3610 [Discriminator loss: 0.238453, acc.: 85.94%] [Generator loss: 4.272885]\n",
      "3611 [Discriminator loss: 0.445916, acc.: 81.25%] [Generator loss: 3.955471]\n",
      "3612 [Discriminator loss: 0.205851, acc.: 95.31%] [Generator loss: 4.560851]\n",
      "3613 [Discriminator loss: 0.166937, acc.: 95.31%] [Generator loss: 3.372661]\n",
      "3614 [Discriminator loss: 0.644437, acc.: 68.75%] [Generator loss: 5.145218]\n",
      "3615 [Discriminator loss: 0.377888, acc.: 85.94%] [Generator loss: 4.066360]\n",
      "3616 [Discriminator loss: 0.326195, acc.: 82.81%] [Generator loss: 4.558151]\n",
      "3617 [Discriminator loss: 0.197920, acc.: 93.75%] [Generator loss: 4.305525]\n",
      "3618 [Discriminator loss: 0.118386, acc.: 96.88%] [Generator loss: 4.036183]\n",
      "3619 [Discriminator loss: 0.245326, acc.: 89.06%] [Generator loss: 3.909438]\n",
      "3620 [Discriminator loss: 0.223683, acc.: 92.19%] [Generator loss: 4.108904]\n",
      "3621 [Discriminator loss: 0.704381, acc.: 67.19%] [Generator loss: 3.641753]\n",
      "3622 [Discriminator loss: 0.141398, acc.: 95.31%] [Generator loss: 4.649045]\n",
      "3623 [Discriminator loss: 0.295066, acc.: 87.50%] [Generator loss: 3.836925]\n",
      "3624 [Discriminator loss: 0.262802, acc.: 89.06%] [Generator loss: 3.677278]\n",
      "3625 [Discriminator loss: 0.384545, acc.: 85.94%] [Generator loss: 4.826838]\n",
      "3626 [Discriminator loss: 0.287396, acc.: 90.62%] [Generator loss: 3.928316]\n",
      "3627 [Discriminator loss: 0.204556, acc.: 92.19%] [Generator loss: 4.935398]\n",
      "3628 [Discriminator loss: 0.430267, acc.: 79.69%] [Generator loss: 4.238269]\n",
      "3629 [Discriminator loss: 0.435454, acc.: 76.56%] [Generator loss: 3.286463]\n",
      "3630 [Discriminator loss: 0.435318, acc.: 78.12%] [Generator loss: 3.384047]\n",
      "3631 [Discriminator loss: 0.220092, acc.: 89.06%] [Generator loss: 4.896106]\n",
      "3632 [Discriminator loss: 0.225345, acc.: 93.75%] [Generator loss: 4.659633]\n",
      "3633 [Discriminator loss: 0.366689, acc.: 81.25%] [Generator loss: 4.785369]\n",
      "3634 [Discriminator loss: 0.568672, acc.: 70.31%] [Generator loss: 4.632303]\n",
      "3635 [Discriminator loss: 0.269343, acc.: 87.50%] [Generator loss: 3.967676]\n",
      "3636 [Discriminator loss: 0.408664, acc.: 79.69%] [Generator loss: 4.200191]\n",
      "3637 [Discriminator loss: 0.386421, acc.: 84.38%] [Generator loss: 4.039454]\n",
      "3638 [Discriminator loss: 0.484574, acc.: 76.56%] [Generator loss: 3.625410]\n",
      "3639 [Discriminator loss: 0.415689, acc.: 79.69%] [Generator loss: 4.512575]\n",
      "3640 [Discriminator loss: 0.384651, acc.: 78.12%] [Generator loss: 4.757541]\n",
      "3641 [Discriminator loss: 0.433731, acc.: 81.25%] [Generator loss: 3.828954]\n",
      "3642 [Discriminator loss: 0.153444, acc.: 92.19%] [Generator loss: 4.387762]\n",
      "3643 [Discriminator loss: 0.358518, acc.: 81.25%] [Generator loss: 4.650359]\n",
      "3644 [Discriminator loss: 0.435636, acc.: 78.12%] [Generator loss: 4.802159]\n",
      "3645 [Discriminator loss: 0.497766, acc.: 75.00%] [Generator loss: 4.581880]\n",
      "3646 [Discriminator loss: 0.288704, acc.: 85.94%] [Generator loss: 5.267283]\n",
      "3647 [Discriminator loss: 0.302998, acc.: 81.25%] [Generator loss: 4.909189]\n",
      "3648 [Discriminator loss: 0.193837, acc.: 95.31%] [Generator loss: 4.702277]\n",
      "3649 [Discriminator loss: 0.450334, acc.: 78.12%] [Generator loss: 4.343493]\n",
      "3650 [Discriminator loss: 0.272797, acc.: 89.06%] [Generator loss: 3.703793]\n",
      "3651 [Discriminator loss: 0.251568, acc.: 87.50%] [Generator loss: 4.191301]\n",
      "3652 [Discriminator loss: 0.309421, acc.: 82.81%] [Generator loss: 4.532544]\n",
      "3653 [Discriminator loss: 0.147263, acc.: 93.75%] [Generator loss: 3.759051]\n",
      "3654 [Discriminator loss: 0.411599, acc.: 84.38%] [Generator loss: 5.314956]\n",
      "3655 [Discriminator loss: 0.238222, acc.: 85.94%] [Generator loss: 3.739431]\n",
      "3656 [Discriminator loss: 0.479370, acc.: 78.12%] [Generator loss: 5.702992]\n",
      "3657 [Discriminator loss: 0.487103, acc.: 82.81%] [Generator loss: 3.622185]\n",
      "3658 [Discriminator loss: 0.294497, acc.: 85.94%] [Generator loss: 4.644094]\n",
      "3659 [Discriminator loss: 0.231710, acc.: 90.62%] [Generator loss: 5.099281]\n",
      "3660 [Discriminator loss: 0.366491, acc.: 79.69%] [Generator loss: 3.205930]\n",
      "3661 [Discriminator loss: 0.419344, acc.: 84.38%] [Generator loss: 3.551034]\n",
      "3662 [Discriminator loss: 0.228160, acc.: 93.75%] [Generator loss: 4.305192]\n",
      "3663 [Discriminator loss: 0.467501, acc.: 78.12%] [Generator loss: 3.710018]\n",
      "3664 [Discriminator loss: 0.304077, acc.: 87.50%] [Generator loss: 5.319548]\n",
      "3665 [Discriminator loss: 0.363150, acc.: 81.25%] [Generator loss: 3.467648]\n",
      "3666 [Discriminator loss: 0.303442, acc.: 84.38%] [Generator loss: 5.608124]\n",
      "3667 [Discriminator loss: 0.287993, acc.: 87.50%] [Generator loss: 3.842527]\n",
      "3668 [Discriminator loss: 0.306966, acc.: 82.81%] [Generator loss: 5.444914]\n",
      "3669 [Discriminator loss: 0.335601, acc.: 81.25%] [Generator loss: 4.962291]\n",
      "3670 [Discriminator loss: 0.408857, acc.: 81.25%] [Generator loss: 4.620226]\n",
      "3671 [Discriminator loss: 0.612860, acc.: 68.75%] [Generator loss: 4.553128]\n",
      "3672 [Discriminator loss: 0.415875, acc.: 85.94%] [Generator loss: 4.050210]\n",
      "3673 [Discriminator loss: 0.292065, acc.: 89.06%] [Generator loss: 3.540015]\n",
      "3674 [Discriminator loss: 0.385660, acc.: 84.38%] [Generator loss: 3.516974]\n",
      "3675 [Discriminator loss: 0.380620, acc.: 81.25%] [Generator loss: 5.684184]\n",
      "3676 [Discriminator loss: 0.613172, acc.: 68.75%] [Generator loss: 3.712291]\n",
      "3677 [Discriminator loss: 0.185129, acc.: 90.62%] [Generator loss: 3.550044]\n",
      "3678 [Discriminator loss: 0.234776, acc.: 90.62%] [Generator loss: 4.426794]\n",
      "3679 [Discriminator loss: 0.418147, acc.: 81.25%] [Generator loss: 4.155686]\n",
      "3680 [Discriminator loss: 0.296745, acc.: 87.50%] [Generator loss: 4.863974]\n",
      "3681 [Discriminator loss: 0.228762, acc.: 93.75%] [Generator loss: 4.010529]\n",
      "3682 [Discriminator loss: 0.187751, acc.: 93.75%] [Generator loss: 3.880305]\n",
      "3683 [Discriminator loss: 0.236102, acc.: 89.06%] [Generator loss: 4.759520]\n",
      "3684 [Discriminator loss: 0.352977, acc.: 84.38%] [Generator loss: 3.715932]\n",
      "3685 [Discriminator loss: 0.237329, acc.: 89.06%] [Generator loss: 4.244900]\n",
      "3686 [Discriminator loss: 0.243363, acc.: 87.50%] [Generator loss: 4.583111]\n",
      "3687 [Discriminator loss: 0.554330, acc.: 71.88%] [Generator loss: 2.771357]\n",
      "3688 [Discriminator loss: 0.423046, acc.: 84.38%] [Generator loss: 5.262325]\n",
      "3689 [Discriminator loss: 0.447682, acc.: 73.44%] [Generator loss: 5.287151]\n",
      "3690 [Discriminator loss: 0.292110, acc.: 87.50%] [Generator loss: 4.569422]\n",
      "3691 [Discriminator loss: 0.237422, acc.: 90.62%] [Generator loss: 4.502632]\n",
      "3692 [Discriminator loss: 0.314200, acc.: 85.94%] [Generator loss: 4.121136]\n",
      "3693 [Discriminator loss: 0.295424, acc.: 89.06%] [Generator loss: 4.461761]\n",
      "3694 [Discriminator loss: 0.323440, acc.: 87.50%] [Generator loss: 3.848100]\n",
      "3695 [Discriminator loss: 0.484124, acc.: 79.69%] [Generator loss: 4.234006]\n",
      "3696 [Discriminator loss: 0.217561, acc.: 92.19%] [Generator loss: 5.055981]\n",
      "3697 [Discriminator loss: 0.301196, acc.: 85.94%] [Generator loss: 3.694354]\n",
      "3698 [Discriminator loss: 0.445192, acc.: 85.94%] [Generator loss: 4.569575]\n",
      "3699 [Discriminator loss: 0.313212, acc.: 84.38%] [Generator loss: 4.733809]\n",
      "3700 [Discriminator loss: 0.294047, acc.: 87.50%] [Generator loss: 3.781466]\n",
      "3701 [Discriminator loss: 0.145144, acc.: 95.31%] [Generator loss: 4.212455]\n",
      "3702 [Discriminator loss: 0.281994, acc.: 89.06%] [Generator loss: 4.571012]\n",
      "3703 [Discriminator loss: 0.356978, acc.: 81.25%] [Generator loss: 3.262184]\n",
      "3704 [Discriminator loss: 0.228419, acc.: 93.75%] [Generator loss: 4.758365]\n",
      "3705 [Discriminator loss: 0.207633, acc.: 92.19%] [Generator loss: 5.066496]\n",
      "3706 [Discriminator loss: 0.177823, acc.: 90.62%] [Generator loss: 4.102283]\n",
      "3707 [Discriminator loss: 0.383604, acc.: 78.12%] [Generator loss: 5.112986]\n",
      "3708 [Discriminator loss: 0.171684, acc.: 92.19%] [Generator loss: 5.300101]\n",
      "3709 [Discriminator loss: 0.321146, acc.: 84.38%] [Generator loss: 4.611317]\n",
      "3710 [Discriminator loss: 0.414236, acc.: 81.25%] [Generator loss: 4.398235]\n",
      "3711 [Discriminator loss: 0.461069, acc.: 82.81%] [Generator loss: 5.623259]\n",
      "3712 [Discriminator loss: 0.371972, acc.: 87.50%] [Generator loss: 3.611515]\n",
      "3713 [Discriminator loss: 0.349783, acc.: 85.94%] [Generator loss: 3.094090]\n",
      "3714 [Discriminator loss: 0.293719, acc.: 90.62%] [Generator loss: 4.335913]\n",
      "3715 [Discriminator loss: 0.403611, acc.: 81.25%] [Generator loss: 4.301750]\n",
      "3716 [Discriminator loss: 0.351391, acc.: 84.38%] [Generator loss: 4.555327]\n",
      "3717 [Discriminator loss: 0.394885, acc.: 82.81%] [Generator loss: 4.076166]\n",
      "3718 [Discriminator loss: 0.330765, acc.: 89.06%] [Generator loss: 3.527310]\n",
      "3719 [Discriminator loss: 0.274143, acc.: 90.62%] [Generator loss: 3.626778]\n",
      "3720 [Discriminator loss: 0.307859, acc.: 82.81%] [Generator loss: 3.604695]\n",
      "3721 [Discriminator loss: 0.191937, acc.: 89.06%] [Generator loss: 3.322865]\n",
      "3722 [Discriminator loss: 0.363042, acc.: 85.94%] [Generator loss: 4.698390]\n",
      "3723 [Discriminator loss: 0.452130, acc.: 79.69%] [Generator loss: 4.765630]\n",
      "3724 [Discriminator loss: 0.386284, acc.: 84.38%] [Generator loss: 3.551084]\n",
      "3725 [Discriminator loss: 0.345346, acc.: 85.94%] [Generator loss: 3.571372]\n",
      "3726 [Discriminator loss: 0.395110, acc.: 82.81%] [Generator loss: 3.847110]\n",
      "3727 [Discriminator loss: 0.324863, acc.: 90.62%] [Generator loss: 4.421160]\n",
      "3728 [Discriminator loss: 0.241965, acc.: 90.62%] [Generator loss: 4.006169]\n",
      "3729 [Discriminator loss: 0.216017, acc.: 89.06%] [Generator loss: 3.530752]\n",
      "3730 [Discriminator loss: 0.380339, acc.: 82.81%] [Generator loss: 4.178096]\n",
      "3731 [Discriminator loss: 0.330294, acc.: 82.81%] [Generator loss: 4.141939]\n",
      "3732 [Discriminator loss: 0.240637, acc.: 90.62%] [Generator loss: 5.509598]\n",
      "3733 [Discriminator loss: 0.225563, acc.: 89.06%] [Generator loss: 3.820399]\n",
      "3734 [Discriminator loss: 0.278276, acc.: 90.62%] [Generator loss: 4.377484]\n",
      "3735 [Discriminator loss: 0.317804, acc.: 85.94%] [Generator loss: 3.808972]\n",
      "3736 [Discriminator loss: 0.402490, acc.: 85.94%] [Generator loss: 3.537572]\n",
      "3737 [Discriminator loss: 0.343998, acc.: 87.50%] [Generator loss: 3.966478]\n",
      "3738 [Discriminator loss: 0.272677, acc.: 85.94%] [Generator loss: 3.466104]\n",
      "3739 [Discriminator loss: 0.469264, acc.: 79.69%] [Generator loss: 3.124898]\n",
      "3740 [Discriminator loss: 0.215624, acc.: 89.06%] [Generator loss: 3.654266]\n",
      "3741 [Discriminator loss: 0.248794, acc.: 89.06%] [Generator loss: 4.743555]\n",
      "3742 [Discriminator loss: 0.206261, acc.: 93.75%] [Generator loss: 4.512176]\n",
      "3743 [Discriminator loss: 0.255613, acc.: 89.06%] [Generator loss: 4.441036]\n",
      "3744 [Discriminator loss: 0.276519, acc.: 89.06%] [Generator loss: 3.646094]\n",
      "3745 [Discriminator loss: 0.214215, acc.: 92.19%] [Generator loss: 4.841073]\n",
      "3746 [Discriminator loss: 0.161308, acc.: 92.19%] [Generator loss: 4.046999]\n",
      "3747 [Discriminator loss: 0.475112, acc.: 78.12%] [Generator loss: 3.706451]\n",
      "3748 [Discriminator loss: 0.315759, acc.: 85.94%] [Generator loss: 5.082754]\n",
      "3749 [Discriminator loss: 0.411626, acc.: 82.81%] [Generator loss: 3.717041]\n",
      "3750 [Discriminator loss: 0.427591, acc.: 81.25%] [Generator loss: 4.626297]\n",
      "3751 [Discriminator loss: 0.260575, acc.: 89.06%] [Generator loss: 5.118746]\n",
      "3752 [Discriminator loss: 0.339243, acc.: 87.50%] [Generator loss: 4.375415]\n",
      "3753 [Discriminator loss: 0.232761, acc.: 89.06%] [Generator loss: 3.332605]\n",
      "3754 [Discriminator loss: 0.278614, acc.: 84.38%] [Generator loss: 3.421425]\n",
      "3755 [Discriminator loss: 0.271713, acc.: 89.06%] [Generator loss: 5.232432]\n",
      "3756 [Discriminator loss: 0.507137, acc.: 76.56%] [Generator loss: 3.982118]\n",
      "3757 [Discriminator loss: 0.165841, acc.: 95.31%] [Generator loss: 3.889024]\n",
      "3758 [Discriminator loss: 0.466678, acc.: 76.56%] [Generator loss: 3.906352]\n",
      "3759 [Discriminator loss: 0.314570, acc.: 89.06%] [Generator loss: 4.329776]\n",
      "3760 [Discriminator loss: 0.202110, acc.: 95.31%] [Generator loss: 4.135318]\n",
      "3761 [Discriminator loss: 0.249567, acc.: 92.19%] [Generator loss: 3.866254]\n",
      "3762 [Discriminator loss: 0.168088, acc.: 93.75%] [Generator loss: 3.307102]\n",
      "3763 [Discriminator loss: 0.362590, acc.: 82.81%] [Generator loss: 3.777594]\n",
      "3764 [Discriminator loss: 0.125489, acc.: 95.31%] [Generator loss: 4.845794]\n",
      "3765 [Discriminator loss: 0.280111, acc.: 90.62%] [Generator loss: 3.544124]\n",
      "3766 [Discriminator loss: 0.350355, acc.: 84.38%] [Generator loss: 4.324527]\n",
      "3767 [Discriminator loss: 0.314633, acc.: 87.50%] [Generator loss: 4.687112]\n",
      "3768 [Discriminator loss: 0.223159, acc.: 89.06%] [Generator loss: 3.955028]\n",
      "3769 [Discriminator loss: 0.313710, acc.: 89.06%] [Generator loss: 3.468346]\n",
      "3770 [Discriminator loss: 0.314264, acc.: 90.62%] [Generator loss: 3.733243]\n",
      "3771 [Discriminator loss: 0.240198, acc.: 89.06%] [Generator loss: 3.737497]\n",
      "3772 [Discriminator loss: 0.304862, acc.: 85.94%] [Generator loss: 4.465357]\n",
      "3773 [Discriminator loss: 0.326728, acc.: 87.50%] [Generator loss: 3.615137]\n",
      "3774 [Discriminator loss: 0.267112, acc.: 87.50%] [Generator loss: 4.442626]\n",
      "3775 [Discriminator loss: 0.244082, acc.: 89.06%] [Generator loss: 4.409626]\n",
      "3776 [Discriminator loss: 0.568688, acc.: 67.19%] [Generator loss: 4.116115]\n",
      "3777 [Discriminator loss: 0.272082, acc.: 87.50%] [Generator loss: 4.129916]\n",
      "3778 [Discriminator loss: 0.266464, acc.: 93.75%] [Generator loss: 4.083002]\n",
      "3779 [Discriminator loss: 0.351249, acc.: 79.69%] [Generator loss: 3.092239]\n",
      "3780 [Discriminator loss: 0.330196, acc.: 84.38%] [Generator loss: 4.882994]\n",
      "3781 [Discriminator loss: 0.368682, acc.: 81.25%] [Generator loss: 3.431204]\n",
      "3782 [Discriminator loss: 0.302319, acc.: 89.06%] [Generator loss: 4.519889]\n",
      "3783 [Discriminator loss: 0.138472, acc.: 98.44%] [Generator loss: 4.722207]\n",
      "3784 [Discriminator loss: 0.292408, acc.: 90.62%] [Generator loss: 3.322764]\n",
      "3785 [Discriminator loss: 0.270251, acc.: 89.06%] [Generator loss: 4.278803]\n",
      "3786 [Discriminator loss: 0.449731, acc.: 81.25%] [Generator loss: 4.255725]\n",
      "3787 [Discriminator loss: 0.475677, acc.: 75.00%] [Generator loss: 4.385622]\n",
      "3788 [Discriminator loss: 0.566427, acc.: 76.56%] [Generator loss: 4.371071]\n",
      "3789 [Discriminator loss: 0.532819, acc.: 76.56%] [Generator loss: 3.856947]\n",
      "3790 [Discriminator loss: 0.249968, acc.: 89.06%] [Generator loss: 3.743227]\n",
      "3791 [Discriminator loss: 0.308981, acc.: 87.50%] [Generator loss: 3.790679]\n",
      "3792 [Discriminator loss: 0.332761, acc.: 81.25%] [Generator loss: 4.224890]\n",
      "3793 [Discriminator loss: 0.194683, acc.: 92.19%] [Generator loss: 3.749380]\n",
      "3794 [Discriminator loss: 0.534664, acc.: 78.12%] [Generator loss: 4.384532]\n",
      "3795 [Discriminator loss: 0.238037, acc.: 85.94%] [Generator loss: 3.618402]\n",
      "3796 [Discriminator loss: 0.282371, acc.: 85.94%] [Generator loss: 3.462694]\n",
      "3797 [Discriminator loss: 0.223666, acc.: 90.62%] [Generator loss: 4.388433]\n",
      "3798 [Discriminator loss: 0.398318, acc.: 82.81%] [Generator loss: 4.188783]\n",
      "3799 [Discriminator loss: 0.204110, acc.: 96.88%] [Generator loss: 4.367224]\n",
      "3800 [Discriminator loss: 0.361238, acc.: 87.50%] [Generator loss: 4.354443]\n",
      "3801 [Discriminator loss: 0.579422, acc.: 73.44%] [Generator loss: 3.222834]\n",
      "3802 [Discriminator loss: 0.148388, acc.: 95.31%] [Generator loss: 3.927474]\n",
      "3803 [Discriminator loss: 0.261919, acc.: 89.06%] [Generator loss: 4.540478]\n",
      "3804 [Discriminator loss: 0.401804, acc.: 82.81%] [Generator loss: 4.519359]\n",
      "3805 [Discriminator loss: 0.166572, acc.: 93.75%] [Generator loss: 4.677460]\n",
      "3806 [Discriminator loss: 0.437690, acc.: 81.25%] [Generator loss: 4.102036]\n",
      "3807 [Discriminator loss: 0.169083, acc.: 93.75%] [Generator loss: 3.867975]\n",
      "3808 [Discriminator loss: 0.177363, acc.: 93.75%] [Generator loss: 3.700297]\n",
      "3809 [Discriminator loss: 0.303387, acc.: 89.06%] [Generator loss: 3.734667]\n",
      "3810 [Discriminator loss: 0.406003, acc.: 79.69%] [Generator loss: 4.006201]\n",
      "3811 [Discriminator loss: 0.264630, acc.: 92.19%] [Generator loss: 4.002868]\n",
      "3812 [Discriminator loss: 0.493779, acc.: 84.38%] [Generator loss: 3.671998]\n",
      "3813 [Discriminator loss: 0.289718, acc.: 90.62%] [Generator loss: 4.385935]\n",
      "3814 [Discriminator loss: 0.222746, acc.: 89.06%] [Generator loss: 5.017322]\n",
      "3815 [Discriminator loss: 0.308069, acc.: 84.38%] [Generator loss: 3.969434]\n",
      "3816 [Discriminator loss: 0.589818, acc.: 76.56%] [Generator loss: 3.267812]\n",
      "3817 [Discriminator loss: 0.318195, acc.: 82.81%] [Generator loss: 4.610468]\n",
      "3818 [Discriminator loss: 0.340666, acc.: 79.69%] [Generator loss: 2.988415]\n",
      "3819 [Discriminator loss: 0.284785, acc.: 85.94%] [Generator loss: 4.095200]\n",
      "3820 [Discriminator loss: 0.330934, acc.: 85.94%] [Generator loss: 4.174720]\n",
      "3821 [Discriminator loss: 0.268705, acc.: 87.50%] [Generator loss: 3.842342]\n",
      "3822 [Discriminator loss: 0.241181, acc.: 89.06%] [Generator loss: 4.133001]\n",
      "3823 [Discriminator loss: 0.167144, acc.: 92.19%] [Generator loss: 4.417170]\n",
      "3824 [Discriminator loss: 0.440450, acc.: 76.56%] [Generator loss: 3.936350]\n",
      "3825 [Discriminator loss: 0.239557, acc.: 93.75%] [Generator loss: 3.897665]\n",
      "3826 [Discriminator loss: 0.332057, acc.: 89.06%] [Generator loss: 3.859169]\n",
      "3827 [Discriminator loss: 0.310416, acc.: 84.38%] [Generator loss: 4.801687]\n",
      "3828 [Discriminator loss: 0.337566, acc.: 84.38%] [Generator loss: 3.968326]\n",
      "3829 [Discriminator loss: 0.429114, acc.: 79.69%] [Generator loss: 4.517862]\n",
      "3830 [Discriminator loss: 0.556407, acc.: 73.44%] [Generator loss: 3.498739]\n",
      "3831 [Discriminator loss: 0.183710, acc.: 95.31%] [Generator loss: 4.347139]\n",
      "3832 [Discriminator loss: 0.265318, acc.: 85.94%] [Generator loss: 3.931481]\n",
      "3833 [Discriminator loss: 0.317674, acc.: 85.94%] [Generator loss: 3.307762]\n",
      "3834 [Discriminator loss: 0.328134, acc.: 87.50%] [Generator loss: 4.328070]\n",
      "3835 [Discriminator loss: 0.245997, acc.: 87.50%] [Generator loss: 4.937239]\n",
      "3836 [Discriminator loss: 0.477331, acc.: 70.31%] [Generator loss: 4.580125]\n",
      "3837 [Discriminator loss: 0.230798, acc.: 90.62%] [Generator loss: 5.158003]\n",
      "3838 [Discriminator loss: 0.437742, acc.: 76.56%] [Generator loss: 2.966303]\n",
      "3839 [Discriminator loss: 0.243126, acc.: 84.38%] [Generator loss: 3.767101]\n",
      "3840 [Discriminator loss: 0.208512, acc.: 92.19%] [Generator loss: 4.823610]\n",
      "3841 [Discriminator loss: 0.465832, acc.: 75.00%] [Generator loss: 3.855523]\n",
      "3842 [Discriminator loss: 0.438922, acc.: 84.38%] [Generator loss: 4.497116]\n",
      "3843 [Discriminator loss: 0.179131, acc.: 90.62%] [Generator loss: 5.875398]\n",
      "3844 [Discriminator loss: 0.171210, acc.: 93.75%] [Generator loss: 3.862941]\n",
      "3845 [Discriminator loss: 0.298177, acc.: 90.62%] [Generator loss: 3.926972]\n",
      "3846 [Discriminator loss: 0.252057, acc.: 90.62%] [Generator loss: 4.376732]\n",
      "3847 [Discriminator loss: 0.444852, acc.: 81.25%] [Generator loss: 4.178646]\n",
      "3848 [Discriminator loss: 0.363803, acc.: 82.81%] [Generator loss: 4.565396]\n",
      "3849 [Discriminator loss: 0.205748, acc.: 93.75%] [Generator loss: 3.983157]\n",
      "3850 [Discriminator loss: 0.157170, acc.: 93.75%] [Generator loss: 3.575746]\n",
      "3851 [Discriminator loss: 0.347588, acc.: 82.81%] [Generator loss: 4.183980]\n",
      "3852 [Discriminator loss: 0.506940, acc.: 75.00%] [Generator loss: 4.218645]\n",
      "3853 [Discriminator loss: 0.484128, acc.: 78.12%] [Generator loss: 4.573943]\n",
      "3854 [Discriminator loss: 0.277767, acc.: 89.06%] [Generator loss: 4.387431]\n",
      "3855 [Discriminator loss: 0.308919, acc.: 84.38%] [Generator loss: 4.637767]\n",
      "3856 [Discriminator loss: 0.236842, acc.: 90.62%] [Generator loss: 4.717050]\n",
      "3857 [Discriminator loss: 0.299164, acc.: 90.62%] [Generator loss: 4.596839]\n",
      "3858 [Discriminator loss: 0.129865, acc.: 93.75%] [Generator loss: 5.217312]\n",
      "3859 [Discriminator loss: 0.413373, acc.: 85.94%] [Generator loss: 5.150877]\n",
      "3860 [Discriminator loss: 0.494532, acc.: 78.12%] [Generator loss: 3.679800]\n",
      "3861 [Discriminator loss: 0.296440, acc.: 87.50%] [Generator loss: 3.679215]\n",
      "3862 [Discriminator loss: 0.237426, acc.: 92.19%] [Generator loss: 4.596423]\n",
      "3863 [Discriminator loss: 0.177414, acc.: 92.19%] [Generator loss: 4.291404]\n",
      "3864 [Discriminator loss: 0.375766, acc.: 82.81%] [Generator loss: 4.400463]\n",
      "3865 [Discriminator loss: 0.361837, acc.: 84.38%] [Generator loss: 4.396836]\n",
      "3866 [Discriminator loss: 0.320264, acc.: 90.62%] [Generator loss: 4.288497]\n",
      "3867 [Discriminator loss: 0.317566, acc.: 87.50%] [Generator loss: 3.804286]\n",
      "3868 [Discriminator loss: 0.342204, acc.: 84.38%] [Generator loss: 3.718271]\n",
      "3869 [Discriminator loss: 0.286668, acc.: 87.50%] [Generator loss: 4.815641]\n",
      "3870 [Discriminator loss: 0.328678, acc.: 87.50%] [Generator loss: 5.307090]\n",
      "3871 [Discriminator loss: 0.334272, acc.: 84.38%] [Generator loss: 4.098416]\n",
      "3872 [Discriminator loss: 0.709545, acc.: 67.19%] [Generator loss: 3.216662]\n",
      "3873 [Discriminator loss: 0.172628, acc.: 93.75%] [Generator loss: 4.469075]\n",
      "3874 [Discriminator loss: 0.283273, acc.: 84.38%] [Generator loss: 3.715603]\n",
      "3875 [Discriminator loss: 0.337729, acc.: 84.38%] [Generator loss: 4.727210]\n",
      "3876 [Discriminator loss: 0.127397, acc.: 96.88%] [Generator loss: 4.144348]\n",
      "3877 [Discriminator loss: 0.167345, acc.: 92.19%] [Generator loss: 4.461618]\n",
      "3878 [Discriminator loss: 0.138514, acc.: 93.75%] [Generator loss: 4.723652]\n",
      "3879 [Discriminator loss: 0.424597, acc.: 84.38%] [Generator loss: 4.414905]\n",
      "3880 [Discriminator loss: 0.347256, acc.: 78.12%] [Generator loss: 3.971848]\n",
      "3881 [Discriminator loss: 0.209657, acc.: 90.62%] [Generator loss: 3.757268]\n",
      "3882 [Discriminator loss: 0.402329, acc.: 81.25%] [Generator loss: 3.366089]\n",
      "3883 [Discriminator loss: 0.156467, acc.: 96.88%] [Generator loss: 3.894684]\n",
      "3884 [Discriminator loss: 0.209941, acc.: 89.06%] [Generator loss: 3.847064]\n",
      "3885 [Discriminator loss: 0.672400, acc.: 62.50%] [Generator loss: 4.734750]\n",
      "3886 [Discriminator loss: 0.672507, acc.: 70.31%] [Generator loss: 4.989497]\n",
      "3887 [Discriminator loss: 0.158314, acc.: 92.19%] [Generator loss: 5.725434]\n",
      "3888 [Discriminator loss: 0.236754, acc.: 90.62%] [Generator loss: 4.014374]\n",
      "3889 [Discriminator loss: 0.414303, acc.: 79.69%] [Generator loss: 5.394855]\n",
      "3890 [Discriminator loss: 0.199160, acc.: 90.62%] [Generator loss: 4.932583]\n",
      "3891 [Discriminator loss: 0.347358, acc.: 78.12%] [Generator loss: 4.313584]\n",
      "3892 [Discriminator loss: 0.304178, acc.: 87.50%] [Generator loss: 5.236609]\n",
      "3893 [Discriminator loss: 0.283073, acc.: 89.06%] [Generator loss: 4.321926]\n",
      "3894 [Discriminator loss: 0.202913, acc.: 93.75%] [Generator loss: 3.811877]\n",
      "3895 [Discriminator loss: 0.252971, acc.: 89.06%] [Generator loss: 3.231875]\n",
      "3896 [Discriminator loss: 0.198664, acc.: 93.75%] [Generator loss: 4.892032]\n",
      "3897 [Discriminator loss: 0.278498, acc.: 90.62%] [Generator loss: 2.767802]\n",
      "3898 [Discriminator loss: 0.477778, acc.: 75.00%] [Generator loss: 5.240260]\n",
      "3899 [Discriminator loss: 0.260255, acc.: 90.62%] [Generator loss: 5.024514]\n",
      "3900 [Discriminator loss: 0.371944, acc.: 84.38%] [Generator loss: 4.658844]\n",
      "3901 [Discriminator loss: 0.235942, acc.: 89.06%] [Generator loss: 4.103008]\n",
      "3902 [Discriminator loss: 0.289870, acc.: 85.94%] [Generator loss: 4.388168]\n",
      "3903 [Discriminator loss: 0.098968, acc.: 96.88%] [Generator loss: 4.424750]\n",
      "3904 [Discriminator loss: 0.390359, acc.: 81.25%] [Generator loss: 3.881322]\n",
      "3905 [Discriminator loss: 0.296230, acc.: 87.50%] [Generator loss: 3.724135]\n",
      "3906 [Discriminator loss: 0.339641, acc.: 85.94%] [Generator loss: 3.954129]\n",
      "3907 [Discriminator loss: 0.271281, acc.: 90.62%] [Generator loss: 5.889067]\n",
      "3908 [Discriminator loss: 0.446347, acc.: 79.69%] [Generator loss: 3.436222]\n",
      "3909 [Discriminator loss: 0.401664, acc.: 81.25%] [Generator loss: 4.591681]\n",
      "3910 [Discriminator loss: 0.252590, acc.: 89.06%] [Generator loss: 3.716748]\n",
      "3911 [Discriminator loss: 0.210175, acc.: 90.62%] [Generator loss: 4.034184]\n",
      "3912 [Discriminator loss: 0.357576, acc.: 84.38%] [Generator loss: 5.502911]\n",
      "3913 [Discriminator loss: 0.527998, acc.: 79.69%] [Generator loss: 3.555547]\n",
      "3914 [Discriminator loss: 0.136811, acc.: 96.88%] [Generator loss: 3.549024]\n",
      "3915 [Discriminator loss: 0.239341, acc.: 89.06%] [Generator loss: 3.619880]\n",
      "3916 [Discriminator loss: 0.394420, acc.: 75.00%] [Generator loss: 4.126153]\n",
      "3917 [Discriminator loss: 0.136878, acc.: 93.75%] [Generator loss: 3.963869]\n",
      "3918 [Discriminator loss: 0.326428, acc.: 85.94%] [Generator loss: 4.342648]\n",
      "3919 [Discriminator loss: 0.272672, acc.: 85.94%] [Generator loss: 4.113444]\n",
      "3920 [Discriminator loss: 0.311501, acc.: 87.50%] [Generator loss: 4.867749]\n",
      "3921 [Discriminator loss: 0.310062, acc.: 87.50%] [Generator loss: 5.098912]\n",
      "3922 [Discriminator loss: 0.363753, acc.: 81.25%] [Generator loss: 4.572633]\n",
      "3923 [Discriminator loss: 0.216947, acc.: 93.75%] [Generator loss: 3.455652]\n",
      "3924 [Discriminator loss: 0.253614, acc.: 87.50%] [Generator loss: 4.210865]\n",
      "3925 [Discriminator loss: 0.465595, acc.: 76.56%] [Generator loss: 3.969580]\n",
      "3926 [Discriminator loss: 0.272253, acc.: 89.06%] [Generator loss: 4.133055]\n",
      "3927 [Discriminator loss: 0.478083, acc.: 81.25%] [Generator loss: 3.751842]\n",
      "3928 [Discriminator loss: 0.184913, acc.: 89.06%] [Generator loss: 4.834334]\n",
      "3929 [Discriminator loss: 0.159515, acc.: 95.31%] [Generator loss: 4.778314]\n",
      "3930 [Discriminator loss: 0.240982, acc.: 90.62%] [Generator loss: 3.804399]\n",
      "3931 [Discriminator loss: 0.334050, acc.: 85.94%] [Generator loss: 3.847769]\n",
      "3932 [Discriminator loss: 0.782591, acc.: 59.38%] [Generator loss: 4.658933]\n",
      "3933 [Discriminator loss: 0.730924, acc.: 75.00%] [Generator loss: 4.389821]\n",
      "3934 [Discriminator loss: 0.191113, acc.: 92.19%] [Generator loss: 4.685879]\n",
      "3935 [Discriminator loss: 0.279307, acc.: 85.94%] [Generator loss: 3.774603]\n",
      "3936 [Discriminator loss: 0.433542, acc.: 84.38%] [Generator loss: 4.954665]\n",
      "3937 [Discriminator loss: 0.433984, acc.: 75.00%] [Generator loss: 3.666637]\n",
      "3938 [Discriminator loss: 0.151307, acc.: 93.75%] [Generator loss: 4.295648]\n",
      "3939 [Discriminator loss: 0.217928, acc.: 87.50%] [Generator loss: 3.725388]\n",
      "3940 [Discriminator loss: 0.270868, acc.: 85.94%] [Generator loss: 4.151654]\n",
      "3941 [Discriminator loss: 0.350526, acc.: 82.81%] [Generator loss: 4.149649]\n",
      "3942 [Discriminator loss: 0.493378, acc.: 73.44%] [Generator loss: 4.337734]\n",
      "3943 [Discriminator loss: 0.384953, acc.: 85.94%] [Generator loss: 5.390507]\n",
      "3944 [Discriminator loss: 0.347466, acc.: 85.94%] [Generator loss: 5.213715]\n",
      "3945 [Discriminator loss: 0.341014, acc.: 85.94%] [Generator loss: 3.354470]\n",
      "3946 [Discriminator loss: 0.213462, acc.: 93.75%] [Generator loss: 4.433670]\n",
      "3947 [Discriminator loss: 0.547075, acc.: 73.44%] [Generator loss: 3.483644]\n",
      "3948 [Discriminator loss: 0.284019, acc.: 87.50%] [Generator loss: 5.141362]\n",
      "3949 [Discriminator loss: 0.392752, acc.: 81.25%] [Generator loss: 3.947492]\n",
      "3950 [Discriminator loss: 0.192081, acc.: 95.31%] [Generator loss: 3.703014]\n",
      "3951 [Discriminator loss: 0.386245, acc.: 76.56%] [Generator loss: 4.153403]\n",
      "3952 [Discriminator loss: 0.354902, acc.: 87.50%] [Generator loss: 4.311805]\n",
      "3953 [Discriminator loss: 0.399911, acc.: 87.50%] [Generator loss: 4.441417]\n",
      "3954 [Discriminator loss: 0.287513, acc.: 85.94%] [Generator loss: 4.830388]\n",
      "3955 [Discriminator loss: 0.314787, acc.: 85.94%] [Generator loss: 4.080802]\n",
      "3956 [Discriminator loss: 0.378951, acc.: 81.25%] [Generator loss: 4.889733]\n",
      "3957 [Discriminator loss: 0.266838, acc.: 84.38%] [Generator loss: 5.036423]\n",
      "3958 [Discriminator loss: 0.229851, acc.: 89.06%] [Generator loss: 4.325473]\n",
      "3959 [Discriminator loss: 0.184898, acc.: 90.62%] [Generator loss: 4.232635]\n",
      "3960 [Discriminator loss: 0.368298, acc.: 84.38%] [Generator loss: 4.060187]\n",
      "3961 [Discriminator loss: 0.288025, acc.: 87.50%] [Generator loss: 3.616121]\n",
      "3962 [Discriminator loss: 0.284643, acc.: 90.62%] [Generator loss: 4.756824]\n",
      "3963 [Discriminator loss: 0.271680, acc.: 89.06%] [Generator loss: 5.283286]\n",
      "3964 [Discriminator loss: 0.302762, acc.: 85.94%] [Generator loss: 3.784925]\n",
      "3965 [Discriminator loss: 0.322263, acc.: 87.50%] [Generator loss: 5.532696]\n",
      "3966 [Discriminator loss: 0.253282, acc.: 89.06%] [Generator loss: 3.556067]\n",
      "3967 [Discriminator loss: 0.501010, acc.: 75.00%] [Generator loss: 3.737870]\n",
      "3968 [Discriminator loss: 0.199938, acc.: 90.62%] [Generator loss: 4.276738]\n",
      "3969 [Discriminator loss: 0.272248, acc.: 85.94%] [Generator loss: 4.389134]\n",
      "3970 [Discriminator loss: 0.215675, acc.: 93.75%] [Generator loss: 4.713945]\n",
      "3971 [Discriminator loss: 0.197332, acc.: 93.75%] [Generator loss: 5.290440]\n",
      "3972 [Discriminator loss: 0.452968, acc.: 84.38%] [Generator loss: 4.608141]\n",
      "3973 [Discriminator loss: 0.331239, acc.: 81.25%] [Generator loss: 3.966164]\n",
      "3974 [Discriminator loss: 0.375687, acc.: 81.25%] [Generator loss: 3.701095]\n",
      "3975 [Discriminator loss: 0.304161, acc.: 89.06%] [Generator loss: 5.281040]\n",
      "3976 [Discriminator loss: 0.471693, acc.: 84.38%] [Generator loss: 3.444432]\n",
      "3977 [Discriminator loss: 0.398448, acc.: 85.94%] [Generator loss: 4.084334]\n",
      "3978 [Discriminator loss: 0.212240, acc.: 85.94%] [Generator loss: 3.813584]\n",
      "3979 [Discriminator loss: 0.292464, acc.: 87.50%] [Generator loss: 4.153205]\n",
      "3980 [Discriminator loss: 0.332175, acc.: 87.50%] [Generator loss: 3.912989]\n",
      "3981 [Discriminator loss: 0.438967, acc.: 82.81%] [Generator loss: 4.471078]\n",
      "3982 [Discriminator loss: 0.297887, acc.: 84.38%] [Generator loss: 4.456265]\n",
      "3983 [Discriminator loss: 0.114256, acc.: 96.88%] [Generator loss: 4.853251]\n",
      "3984 [Discriminator loss: 0.283311, acc.: 85.94%] [Generator loss: 4.152065]\n",
      "3985 [Discriminator loss: 0.207865, acc.: 90.62%] [Generator loss: 3.328236]\n",
      "3986 [Discriminator loss: 0.392770, acc.: 85.94%] [Generator loss: 4.300237]\n",
      "3987 [Discriminator loss: 0.158407, acc.: 93.75%] [Generator loss: 4.448888]\n",
      "3988 [Discriminator loss: 0.154883, acc.: 93.75%] [Generator loss: 2.931014]\n",
      "3989 [Discriminator loss: 0.250490, acc.: 90.62%] [Generator loss: 4.198948]\n",
      "3990 [Discriminator loss: 0.276560, acc.: 90.62%] [Generator loss: 3.776636]\n",
      "3991 [Discriminator loss: 0.267050, acc.: 89.06%] [Generator loss: 4.503140]\n",
      "3992 [Discriminator loss: 0.225016, acc.: 92.19%] [Generator loss: 4.416892]\n",
      "3993 [Discriminator loss: 0.245967, acc.: 89.06%] [Generator loss: 4.400554]\n",
      "3994 [Discriminator loss: 0.294332, acc.: 87.50%] [Generator loss: 4.762640]\n",
      "3995 [Discriminator loss: 0.196040, acc.: 95.31%] [Generator loss: 3.536050]\n",
      "3996 [Discriminator loss: 0.447003, acc.: 75.00%] [Generator loss: 4.699605]\n",
      "3997 [Discriminator loss: 0.257474, acc.: 87.50%] [Generator loss: 4.918345]\n",
      "3998 [Discriminator loss: 0.197463, acc.: 90.62%] [Generator loss: 4.717358]\n",
      "3999 [Discriminator loss: 0.413786, acc.: 81.25%] [Generator loss: 4.056956]\n",
      "4000 [Discriminator loss: 0.134344, acc.: 95.31%] [Generator loss: 4.680902]\n",
      "4001 [Discriminator loss: 0.338962, acc.: 89.06%] [Generator loss: 4.539114]\n",
      "4002 [Discriminator loss: 0.241831, acc.: 85.94%] [Generator loss: 3.977787]\n",
      "4003 [Discriminator loss: 0.299182, acc.: 85.94%] [Generator loss: 4.657779]\n",
      "4004 [Discriminator loss: 0.331059, acc.: 89.06%] [Generator loss: 4.226600]\n",
      "4005 [Discriminator loss: 0.250294, acc.: 90.62%] [Generator loss: 3.467255]\n",
      "4006 [Discriminator loss: 0.411324, acc.: 78.12%] [Generator loss: 5.512286]\n",
      "4007 [Discriminator loss: 0.411146, acc.: 85.94%] [Generator loss: 4.597416]\n",
      "4008 [Discriminator loss: 0.472295, acc.: 81.25%] [Generator loss: 4.072724]\n",
      "4009 [Discriminator loss: 0.116546, acc.: 93.75%] [Generator loss: 4.960216]\n",
      "4010 [Discriminator loss: 0.173772, acc.: 95.31%] [Generator loss: 4.915508]\n",
      "4011 [Discriminator loss: 0.314223, acc.: 85.94%] [Generator loss: 4.706020]\n",
      "4012 [Discriminator loss: 0.213174, acc.: 93.75%] [Generator loss: 4.161379]\n",
      "4013 [Discriminator loss: 0.275504, acc.: 85.94%] [Generator loss: 4.169864]\n",
      "4014 [Discriminator loss: 0.422325, acc.: 82.81%] [Generator loss: 4.235361]\n",
      "4015 [Discriminator loss: 0.208479, acc.: 92.19%] [Generator loss: 3.854070]\n",
      "4016 [Discriminator loss: 0.430191, acc.: 79.69%] [Generator loss: 4.217979]\n",
      "4017 [Discriminator loss: 0.400300, acc.: 79.69%] [Generator loss: 4.408642]\n",
      "4018 [Discriminator loss: 0.399391, acc.: 79.69%] [Generator loss: 3.627586]\n",
      "4019 [Discriminator loss: 0.162111, acc.: 93.75%] [Generator loss: 3.741093]\n",
      "4020 [Discriminator loss: 0.298724, acc.: 89.06%] [Generator loss: 4.706446]\n",
      "4021 [Discriminator loss: 0.354174, acc.: 87.50%] [Generator loss: 3.974476]\n",
      "4022 [Discriminator loss: 0.198980, acc.: 92.19%] [Generator loss: 4.167525]\n",
      "4023 [Discriminator loss: 0.326665, acc.: 85.94%] [Generator loss: 4.362227]\n",
      "4024 [Discriminator loss: 0.548969, acc.: 75.00%] [Generator loss: 3.871974]\n",
      "4025 [Discriminator loss: 0.231305, acc.: 89.06%] [Generator loss: 4.292181]\n",
      "4026 [Discriminator loss: 0.598323, acc.: 65.62%] [Generator loss: 5.432220]\n",
      "4027 [Discriminator loss: 0.309611, acc.: 85.94%] [Generator loss: 5.069093]\n",
      "4028 [Discriminator loss: 0.250329, acc.: 89.06%] [Generator loss: 3.465232]\n",
      "4029 [Discriminator loss: 0.144936, acc.: 95.31%] [Generator loss: 3.679855]\n",
      "4030 [Discriminator loss: 0.248964, acc.: 90.62%] [Generator loss: 4.626607]\n",
      "4031 [Discriminator loss: 0.240331, acc.: 89.06%] [Generator loss: 4.084380]\n",
      "4032 [Discriminator loss: 0.365232, acc.: 81.25%] [Generator loss: 3.117969]\n",
      "4033 [Discriminator loss: 0.230981, acc.: 90.62%] [Generator loss: 4.415108]\n",
      "4034 [Discriminator loss: 0.216466, acc.: 92.19%] [Generator loss: 4.761413]\n",
      "4035 [Discriminator loss: 0.460210, acc.: 78.12%] [Generator loss: 3.363936]\n",
      "4036 [Discriminator loss: 0.172877, acc.: 90.62%] [Generator loss: 5.038061]\n",
      "4037 [Discriminator loss: 0.303220, acc.: 87.50%] [Generator loss: 4.214038]\n",
      "4038 [Discriminator loss: 0.251221, acc.: 89.06%] [Generator loss: 4.541741]\n",
      "4039 [Discriminator loss: 0.360336, acc.: 79.69%] [Generator loss: 3.633258]\n",
      "4040 [Discriminator loss: 0.299264, acc.: 84.38%] [Generator loss: 4.011360]\n",
      "4041 [Discriminator loss: 0.264426, acc.: 85.94%] [Generator loss: 3.617367]\n",
      "4042 [Discriminator loss: 0.168170, acc.: 95.31%] [Generator loss: 4.273357]\n",
      "4043 [Discriminator loss: 0.393148, acc.: 82.81%] [Generator loss: 3.208725]\n",
      "4044 [Discriminator loss: 0.177731, acc.: 92.19%] [Generator loss: 4.353971]\n",
      "4045 [Discriminator loss: 0.265306, acc.: 84.38%] [Generator loss: 4.191402]\n",
      "4046 [Discriminator loss: 0.201302, acc.: 92.19%] [Generator loss: 4.306316]\n",
      "4047 [Discriminator loss: 0.314378, acc.: 85.94%] [Generator loss: 3.406435]\n",
      "4048 [Discriminator loss: 0.657241, acc.: 65.62%] [Generator loss: 5.152215]\n",
      "4049 [Discriminator loss: 0.308070, acc.: 84.38%] [Generator loss: 4.311595]\n",
      "4050 [Discriminator loss: 0.314883, acc.: 89.06%] [Generator loss: 4.344876]\n",
      "4051 [Discriminator loss: 0.189883, acc.: 95.31%] [Generator loss: 4.357211]\n",
      "4052 [Discriminator loss: 0.190675, acc.: 93.75%] [Generator loss: 4.328777]\n",
      "4053 [Discriminator loss: 0.152161, acc.: 92.19%] [Generator loss: 3.964264]\n",
      "4054 [Discriminator loss: 0.338439, acc.: 81.25%] [Generator loss: 5.332649]\n",
      "4055 [Discriminator loss: 0.341866, acc.: 79.69%] [Generator loss: 3.694913]\n",
      "4056 [Discriminator loss: 0.326329, acc.: 84.38%] [Generator loss: 4.347528]\n",
      "4057 [Discriminator loss: 0.223726, acc.: 92.19%] [Generator loss: 5.627830]\n",
      "4058 [Discriminator loss: 0.216496, acc.: 92.19%] [Generator loss: 3.186087]\n",
      "4059 [Discriminator loss: 0.257300, acc.: 87.50%] [Generator loss: 4.067900]\n",
      "4060 [Discriminator loss: 0.298081, acc.: 92.19%] [Generator loss: 4.239314]\n",
      "4061 [Discriminator loss: 0.165696, acc.: 93.75%] [Generator loss: 3.412657]\n",
      "4062 [Discriminator loss: 0.211671, acc.: 95.31%] [Generator loss: 3.572970]\n",
      "4063 [Discriminator loss: 0.373893, acc.: 76.56%] [Generator loss: 5.414311]\n",
      "4064 [Discriminator loss: 0.180272, acc.: 92.19%] [Generator loss: 4.758366]\n",
      "4065 [Discriminator loss: 0.325165, acc.: 84.38%] [Generator loss: 2.934238]\n",
      "4066 [Discriminator loss: 0.444986, acc.: 82.81%] [Generator loss: 5.568855]\n",
      "4067 [Discriminator loss: 0.189109, acc.: 93.75%] [Generator loss: 5.173700]\n",
      "4068 [Discriminator loss: 0.186996, acc.: 93.75%] [Generator loss: 4.640634]\n",
      "4069 [Discriminator loss: 0.321728, acc.: 87.50%] [Generator loss: 3.593374]\n",
      "4070 [Discriminator loss: 0.427350, acc.: 79.69%] [Generator loss: 5.280950]\n",
      "4071 [Discriminator loss: 0.305868, acc.: 84.38%] [Generator loss: 4.524260]\n",
      "4072 [Discriminator loss: 0.471211, acc.: 75.00%] [Generator loss: 3.522159]\n",
      "4073 [Discriminator loss: 0.151810, acc.: 93.75%] [Generator loss: 3.922630]\n",
      "4074 [Discriminator loss: 0.438459, acc.: 79.69%] [Generator loss: 4.094201]\n",
      "4075 [Discriminator loss: 0.142326, acc.: 93.75%] [Generator loss: 4.404960]\n",
      "4076 [Discriminator loss: 0.281779, acc.: 89.06%] [Generator loss: 3.421916]\n",
      "4077 [Discriminator loss: 0.416975, acc.: 82.81%] [Generator loss: 3.943938]\n",
      "4078 [Discriminator loss: 0.222191, acc.: 93.75%] [Generator loss: 4.672192]\n",
      "4079 [Discriminator loss: 0.265531, acc.: 93.75%] [Generator loss: 3.830391]\n",
      "4080 [Discriminator loss: 0.276729, acc.: 89.06%] [Generator loss: 3.768867]\n",
      "4081 [Discriminator loss: 0.280502, acc.: 87.50%] [Generator loss: 3.856130]\n",
      "4082 [Discriminator loss: 0.365927, acc.: 85.94%] [Generator loss: 5.364149]\n",
      "4083 [Discriminator loss: 0.118510, acc.: 95.31%] [Generator loss: 5.133150]\n",
      "4084 [Discriminator loss: 0.340942, acc.: 84.38%] [Generator loss: 4.054435]\n",
      "4085 [Discriminator loss: 0.294529, acc.: 84.38%] [Generator loss: 3.974890]\n",
      "4086 [Discriminator loss: 0.247147, acc.: 90.62%] [Generator loss: 4.342306]\n",
      "4087 [Discriminator loss: 0.221934, acc.: 89.06%] [Generator loss: 4.003284]\n",
      "4088 [Discriminator loss: 0.230915, acc.: 87.50%] [Generator loss: 3.861727]\n",
      "4089 [Discriminator loss: 0.280799, acc.: 87.50%] [Generator loss: 5.154313]\n",
      "4090 [Discriminator loss: 0.329104, acc.: 81.25%] [Generator loss: 5.113145]\n",
      "4091 [Discriminator loss: 0.451016, acc.: 78.12%] [Generator loss: 3.190547]\n",
      "4092 [Discriminator loss: 0.292652, acc.: 85.94%] [Generator loss: 4.071300]\n",
      "4093 [Discriminator loss: 0.115771, acc.: 93.75%] [Generator loss: 5.215297]\n",
      "4094 [Discriminator loss: 0.247143, acc.: 90.62%] [Generator loss: 3.649317]\n",
      "4095 [Discriminator loss: 0.341467, acc.: 85.94%] [Generator loss: 4.354259]\n",
      "4096 [Discriminator loss: 0.346917, acc.: 87.50%] [Generator loss: 4.670280]\n",
      "4097 [Discriminator loss: 0.184582, acc.: 90.62%] [Generator loss: 5.233563]\n",
      "4098 [Discriminator loss: 0.408657, acc.: 85.94%] [Generator loss: 5.572895]\n",
      "4099 [Discriminator loss: 0.473378, acc.: 84.38%] [Generator loss: 3.843277]\n",
      "4100 [Discriminator loss: 0.283748, acc.: 87.50%] [Generator loss: 4.204948]\n",
      "4101 [Discriminator loss: 0.206477, acc.: 89.06%] [Generator loss: 4.585442]\n",
      "4102 [Discriminator loss: 0.331740, acc.: 89.06%] [Generator loss: 4.582991]\n",
      "4103 [Discriminator loss: 0.926514, acc.: 60.94%] [Generator loss: 5.812143]\n",
      "4104 [Discriminator loss: 0.497236, acc.: 85.94%] [Generator loss: 3.775583]\n",
      "4105 [Discriminator loss: 0.239514, acc.: 87.50%] [Generator loss: 3.455606]\n",
      "4106 [Discriminator loss: 0.210787, acc.: 90.62%] [Generator loss: 4.827003]\n",
      "4107 [Discriminator loss: 0.597830, acc.: 71.88%] [Generator loss: 3.724140]\n",
      "4108 [Discriminator loss: 0.250552, acc.: 89.06%] [Generator loss: 5.335093]\n",
      "4109 [Discriminator loss: 0.434963, acc.: 85.94%] [Generator loss: 3.849719]\n",
      "4110 [Discriminator loss: 0.189665, acc.: 92.19%] [Generator loss: 3.021605]\n",
      "4111 [Discriminator loss: 0.394757, acc.: 78.12%] [Generator loss: 4.751773]\n",
      "4112 [Discriminator loss: 0.176228, acc.: 93.75%] [Generator loss: 5.306489]\n",
      "4113 [Discriminator loss: 0.541072, acc.: 70.31%] [Generator loss: 3.082218]\n",
      "4114 [Discriminator loss: 0.269691, acc.: 85.94%] [Generator loss: 5.068459]\n",
      "4115 [Discriminator loss: 0.168128, acc.: 93.75%] [Generator loss: 4.830146]\n",
      "4116 [Discriminator loss: 0.584317, acc.: 75.00%] [Generator loss: 3.876081]\n",
      "4117 [Discriminator loss: 0.170685, acc.: 95.31%] [Generator loss: 4.309802]\n",
      "4118 [Discriminator loss: 0.203819, acc.: 89.06%] [Generator loss: 5.142247]\n",
      "4119 [Discriminator loss: 0.208016, acc.: 92.19%] [Generator loss: 3.827600]\n",
      "4120 [Discriminator loss: 0.233003, acc.: 93.75%] [Generator loss: 3.773550]\n",
      "4121 [Discriminator loss: 0.371497, acc.: 81.25%] [Generator loss: 4.349929]\n",
      "4122 [Discriminator loss: 0.568151, acc.: 76.56%] [Generator loss: 4.973668]\n",
      "4123 [Discriminator loss: 0.397497, acc.: 81.25%] [Generator loss: 4.722399]\n",
      "4124 [Discriminator loss: 0.244207, acc.: 92.19%] [Generator loss: 4.074021]\n",
      "4125 [Discriminator loss: 0.283222, acc.: 87.50%] [Generator loss: 4.456497]\n",
      "4126 [Discriminator loss: 0.226126, acc.: 92.19%] [Generator loss: 4.740531]\n",
      "4127 [Discriminator loss: 0.197940, acc.: 90.62%] [Generator loss: 3.111421]\n",
      "4128 [Discriminator loss: 0.623957, acc.: 70.31%] [Generator loss: 3.729394]\n",
      "4129 [Discriminator loss: 0.257060, acc.: 89.06%] [Generator loss: 5.428528]\n",
      "4130 [Discriminator loss: 0.532859, acc.: 71.88%] [Generator loss: 4.527704]\n",
      "4131 [Discriminator loss: 0.458185, acc.: 79.69%] [Generator loss: 5.812443]\n",
      "4132 [Discriminator loss: 0.290312, acc.: 87.50%] [Generator loss: 3.255894]\n",
      "4133 [Discriminator loss: 0.316681, acc.: 87.50%] [Generator loss: 4.885109]\n",
      "4134 [Discriminator loss: 0.352759, acc.: 85.94%] [Generator loss: 4.485184]\n",
      "4135 [Discriminator loss: 0.208014, acc.: 92.19%] [Generator loss: 4.858296]\n",
      "4136 [Discriminator loss: 0.286872, acc.: 87.50%] [Generator loss: 4.077732]\n",
      "4137 [Discriminator loss: 0.363401, acc.: 89.06%] [Generator loss: 4.685671]\n",
      "4138 [Discriminator loss: 0.439950, acc.: 81.25%] [Generator loss: 4.389480]\n",
      "4139 [Discriminator loss: 0.319487, acc.: 84.38%] [Generator loss: 5.070740]\n",
      "4140 [Discriminator loss: 0.154429, acc.: 95.31%] [Generator loss: 5.727838]\n",
      "4141 [Discriminator loss: 0.304086, acc.: 84.38%] [Generator loss: 4.600035]\n",
      "4142 [Discriminator loss: 0.352038, acc.: 84.38%] [Generator loss: 4.218478]\n",
      "4143 [Discriminator loss: 0.501693, acc.: 78.12%] [Generator loss: 5.048146]\n",
      "4144 [Discriminator loss: 0.169140, acc.: 93.75%] [Generator loss: 4.498400]\n",
      "4145 [Discriminator loss: 0.343217, acc.: 87.50%] [Generator loss: 4.162264]\n",
      "4146 [Discriminator loss: 0.276935, acc.: 92.19%] [Generator loss: 5.445671]\n",
      "4147 [Discriminator loss: 0.134440, acc.: 93.75%] [Generator loss: 5.024787]\n",
      "4148 [Discriminator loss: 0.296716, acc.: 82.81%] [Generator loss: 4.664315]\n",
      "4149 [Discriminator loss: 0.449682, acc.: 78.12%] [Generator loss: 5.604001]\n",
      "4150 [Discriminator loss: 0.229359, acc.: 90.62%] [Generator loss: 4.597410]\n",
      "4151 [Discriminator loss: 0.373427, acc.: 84.38%] [Generator loss: 4.397460]\n",
      "4152 [Discriminator loss: 0.214374, acc.: 93.75%] [Generator loss: 3.820535]\n",
      "4153 [Discriminator loss: 0.230360, acc.: 95.31%] [Generator loss: 4.082346]\n",
      "4154 [Discriminator loss: 0.387923, acc.: 81.25%] [Generator loss: 5.239117]\n",
      "4155 [Discriminator loss: 0.355994, acc.: 82.81%] [Generator loss: 3.997553]\n",
      "4156 [Discriminator loss: 0.531281, acc.: 73.44%] [Generator loss: 5.344309]\n",
      "4157 [Discriminator loss: 0.256591, acc.: 87.50%] [Generator loss: 3.847069]\n",
      "4158 [Discriminator loss: 0.250199, acc.: 89.06%] [Generator loss: 4.615603]\n",
      "4159 [Discriminator loss: 0.277893, acc.: 87.50%] [Generator loss: 4.936896]\n",
      "4160 [Discriminator loss: 0.333808, acc.: 85.94%] [Generator loss: 4.684724]\n",
      "4161 [Discriminator loss: 0.155753, acc.: 90.62%] [Generator loss: 4.536318]\n",
      "4162 [Discriminator loss: 0.220169, acc.: 89.06%] [Generator loss: 3.579277]\n",
      "4163 [Discriminator loss: 0.451047, acc.: 82.81%] [Generator loss: 4.523751]\n",
      "4164 [Discriminator loss: 0.111365, acc.: 96.88%] [Generator loss: 5.888707]\n",
      "4165 [Discriminator loss: 0.198257, acc.: 93.75%] [Generator loss: 4.474881]\n",
      "4166 [Discriminator loss: 0.422666, acc.: 81.25%] [Generator loss: 4.226420]\n",
      "4167 [Discriminator loss: 0.232714, acc.: 87.50%] [Generator loss: 4.200329]\n",
      "4168 [Discriminator loss: 0.195582, acc.: 93.75%] [Generator loss: 4.358393]\n",
      "4169 [Discriminator loss: 0.282674, acc.: 93.75%] [Generator loss: 5.261451]\n",
      "4170 [Discriminator loss: 0.427139, acc.: 81.25%] [Generator loss: 4.601096]\n",
      "4171 [Discriminator loss: 0.369576, acc.: 81.25%] [Generator loss: 4.979875]\n",
      "4172 [Discriminator loss: 0.344696, acc.: 85.94%] [Generator loss: 4.245720]\n",
      "4173 [Discriminator loss: 0.267634, acc.: 93.75%] [Generator loss: 3.951299]\n",
      "4174 [Discriminator loss: 0.930774, acc.: 60.94%] [Generator loss: 6.443377]\n",
      "4175 [Discriminator loss: 0.261832, acc.: 89.06%] [Generator loss: 5.267376]\n",
      "4176 [Discriminator loss: 0.245803, acc.: 93.75%] [Generator loss: 3.394476]\n",
      "4177 [Discriminator loss: 0.428655, acc.: 76.56%] [Generator loss: 5.305277]\n",
      "4178 [Discriminator loss: 0.218642, acc.: 90.62%] [Generator loss: 4.809565]\n",
      "4179 [Discriminator loss: 0.182667, acc.: 93.75%] [Generator loss: 4.429751]\n",
      "4180 [Discriminator loss: 0.288266, acc.: 90.62%] [Generator loss: 4.302444]\n",
      "4181 [Discriminator loss: 0.203734, acc.: 87.50%] [Generator loss: 4.858439]\n",
      "4182 [Discriminator loss: 0.267770, acc.: 92.19%] [Generator loss: 4.003987]\n",
      "4183 [Discriminator loss: 0.333036, acc.: 89.06%] [Generator loss: 5.146753]\n",
      "4184 [Discriminator loss: 0.256893, acc.: 89.06%] [Generator loss: 4.372613]\n",
      "4185 [Discriminator loss: 0.254357, acc.: 89.06%] [Generator loss: 3.993207]\n",
      "4186 [Discriminator loss: 0.506675, acc.: 76.56%] [Generator loss: 5.997612]\n",
      "4187 [Discriminator loss: 0.392992, acc.: 79.69%] [Generator loss: 4.791430]\n",
      "4188 [Discriminator loss: 0.293987, acc.: 85.94%] [Generator loss: 4.577841]\n",
      "4189 [Discriminator loss: 0.147568, acc.: 92.19%] [Generator loss: 5.228076]\n",
      "4190 [Discriminator loss: 0.225446, acc.: 89.06%] [Generator loss: 4.703829]\n",
      "4191 [Discriminator loss: 0.231708, acc.: 87.50%] [Generator loss: 3.496499]\n",
      "4192 [Discriminator loss: 0.240686, acc.: 89.06%] [Generator loss: 4.773295]\n",
      "4193 [Discriminator loss: 0.218450, acc.: 92.19%] [Generator loss: 3.942294]\n",
      "4194 [Discriminator loss: 0.294782, acc.: 87.50%] [Generator loss: 4.555690]\n",
      "4195 [Discriminator loss: 0.227918, acc.: 92.19%] [Generator loss: 3.987228]\n",
      "4196 [Discriminator loss: 0.272226, acc.: 87.50%] [Generator loss: 3.244573]\n",
      "4197 [Discriminator loss: 0.307099, acc.: 89.06%] [Generator loss: 3.658701]\n",
      "4198 [Discriminator loss: 0.220778, acc.: 89.06%] [Generator loss: 4.104681]\n",
      "4199 [Discriminator loss: 0.721243, acc.: 62.50%] [Generator loss: 5.440442]\n",
      "4200 [Discriminator loss: 0.169960, acc.: 98.44%] [Generator loss: 5.272605]\n",
      "4201 [Discriminator loss: 0.193957, acc.: 89.06%] [Generator loss: 4.594594]\n",
      "4202 [Discriminator loss: 0.443975, acc.: 81.25%] [Generator loss: 4.463942]\n",
      "4203 [Discriminator loss: 0.291662, acc.: 85.94%] [Generator loss: 4.649137]\n",
      "4204 [Discriminator loss: 0.222549, acc.: 89.06%] [Generator loss: 4.113540]\n",
      "4205 [Discriminator loss: 0.491862, acc.: 79.69%] [Generator loss: 4.604400]\n",
      "4206 [Discriminator loss: 0.300364, acc.: 89.06%] [Generator loss: 4.436656]\n",
      "4207 [Discriminator loss: 0.304515, acc.: 85.94%] [Generator loss: 4.115410]\n",
      "4208 [Discriminator loss: 0.178033, acc.: 93.75%] [Generator loss: 4.469652]\n",
      "4209 [Discriminator loss: 0.423840, acc.: 81.25%] [Generator loss: 3.867289]\n",
      "4210 [Discriminator loss: 0.349736, acc.: 84.38%] [Generator loss: 4.501139]\n",
      "4211 [Discriminator loss: 0.282254, acc.: 85.94%] [Generator loss: 3.558141]\n",
      "4212 [Discriminator loss: 0.268951, acc.: 87.50%] [Generator loss: 4.597435]\n",
      "4213 [Discriminator loss: 0.239652, acc.: 93.75%] [Generator loss: 3.885722]\n",
      "4214 [Discriminator loss: 0.122252, acc.: 95.31%] [Generator loss: 4.197317]\n",
      "4215 [Discriminator loss: 0.297271, acc.: 87.50%] [Generator loss: 4.057244]\n",
      "4216 [Discriminator loss: 0.282413, acc.: 87.50%] [Generator loss: 4.701303]\n",
      "4217 [Discriminator loss: 0.297067, acc.: 87.50%] [Generator loss: 3.475530]\n",
      "4218 [Discriminator loss: 0.198107, acc.: 90.62%] [Generator loss: 4.549711]\n",
      "4219 [Discriminator loss: 0.123053, acc.: 96.88%] [Generator loss: 4.444284]\n",
      "4220 [Discriminator loss: 0.250927, acc.: 92.19%] [Generator loss: 4.858436]\n",
      "4221 [Discriminator loss: 0.237579, acc.: 92.19%] [Generator loss: 5.245770]\n",
      "4222 [Discriminator loss: 0.384310, acc.: 82.81%] [Generator loss: 4.217138]\n",
      "4223 [Discriminator loss: 0.206078, acc.: 90.62%] [Generator loss: 3.694453]\n",
      "4224 [Discriminator loss: 0.182488, acc.: 93.75%] [Generator loss: 4.006681]\n",
      "4225 [Discriminator loss: 0.365741, acc.: 81.25%] [Generator loss: 4.987467]\n",
      "4226 [Discriminator loss: 0.300341, acc.: 90.62%] [Generator loss: 5.395654]\n",
      "4227 [Discriminator loss: 0.298411, acc.: 85.94%] [Generator loss: 4.506873]\n",
      "4228 [Discriminator loss: 0.237230, acc.: 90.62%] [Generator loss: 4.756199]\n",
      "4229 [Discriminator loss: 0.247227, acc.: 92.19%] [Generator loss: 4.870014]\n",
      "4230 [Discriminator loss: 0.267074, acc.: 90.62%] [Generator loss: 5.014058]\n",
      "4231 [Discriminator loss: 0.373487, acc.: 82.81%] [Generator loss: 4.056833]\n",
      "4232 [Discriminator loss: 0.314680, acc.: 85.94%] [Generator loss: 4.287995]\n",
      "4233 [Discriminator loss: 0.375818, acc.: 81.25%] [Generator loss: 5.109756]\n",
      "4234 [Discriminator loss: 0.366004, acc.: 84.38%] [Generator loss: 3.775112]\n",
      "4235 [Discriminator loss: 0.232415, acc.: 92.19%] [Generator loss: 4.449387]\n",
      "4236 [Discriminator loss: 0.431649, acc.: 79.69%] [Generator loss: 5.302265]\n",
      "4237 [Discriminator loss: 0.180868, acc.: 92.19%] [Generator loss: 4.258143]\n",
      "4238 [Discriminator loss: 0.365265, acc.: 85.94%] [Generator loss: 3.799356]\n",
      "4239 [Discriminator loss: 0.331364, acc.: 85.94%] [Generator loss: 5.697042]\n",
      "4240 [Discriminator loss: 0.448670, acc.: 85.94%] [Generator loss: 5.064735]\n",
      "4241 [Discriminator loss: 0.371141, acc.: 85.94%] [Generator loss: 3.542803]\n",
      "4242 [Discriminator loss: 0.302288, acc.: 84.38%] [Generator loss: 4.148404]\n",
      "4243 [Discriminator loss: 0.294179, acc.: 87.50%] [Generator loss: 4.378219]\n",
      "4244 [Discriminator loss: 0.523742, acc.: 76.56%] [Generator loss: 3.832294]\n",
      "4245 [Discriminator loss: 0.221322, acc.: 85.94%] [Generator loss: 4.394713]\n",
      "4246 [Discriminator loss: 0.339419, acc.: 81.25%] [Generator loss: 4.890504]\n",
      "4247 [Discriminator loss: 0.218607, acc.: 92.19%] [Generator loss: 4.973142]\n",
      "4248 [Discriminator loss: 0.283930, acc.: 84.38%] [Generator loss: 3.674790]\n",
      "4249 [Discriminator loss: 0.363302, acc.: 82.81%] [Generator loss: 5.050570]\n",
      "4250 [Discriminator loss: 0.201946, acc.: 92.19%] [Generator loss: 3.800821]\n",
      "4251 [Discriminator loss: 0.315481, acc.: 87.50%] [Generator loss: 4.192229]\n",
      "4252 [Discriminator loss: 0.490531, acc.: 76.56%] [Generator loss: 3.931626]\n",
      "4253 [Discriminator loss: 0.134476, acc.: 96.88%] [Generator loss: 4.781956]\n",
      "4254 [Discriminator loss: 0.432417, acc.: 78.12%] [Generator loss: 4.803408]\n",
      "4255 [Discriminator loss: 0.457804, acc.: 78.12%] [Generator loss: 4.004720]\n",
      "4256 [Discriminator loss: 0.244077, acc.: 89.06%] [Generator loss: 4.530404]\n",
      "4257 [Discriminator loss: 0.473592, acc.: 76.56%] [Generator loss: 4.625626]\n",
      "4258 [Discriminator loss: 0.172032, acc.: 95.31%] [Generator loss: 5.414229]\n",
      "4259 [Discriminator loss: 0.164114, acc.: 93.75%] [Generator loss: 3.750949]\n",
      "4260 [Discriminator loss: 0.689513, acc.: 65.62%] [Generator loss: 5.554808]\n",
      "4261 [Discriminator loss: 0.357166, acc.: 89.06%] [Generator loss: 4.129642]\n",
      "4262 [Discriminator loss: 0.234900, acc.: 90.62%] [Generator loss: 4.033144]\n",
      "4263 [Discriminator loss: 0.134996, acc.: 95.31%] [Generator loss: 4.850875]\n",
      "4264 [Discriminator loss: 0.124186, acc.: 98.44%] [Generator loss: 3.883067]\n",
      "4265 [Discriminator loss: 0.437775, acc.: 76.56%] [Generator loss: 4.663815]\n",
      "4266 [Discriminator loss: 0.416680, acc.: 81.25%] [Generator loss: 3.451050]\n",
      "4267 [Discriminator loss: 0.317307, acc.: 85.94%] [Generator loss: 3.909632]\n",
      "4268 [Discriminator loss: 0.328661, acc.: 82.81%] [Generator loss: 4.498056]\n",
      "4269 [Discriminator loss: 0.322163, acc.: 84.38%] [Generator loss: 5.346051]\n",
      "4270 [Discriminator loss: 0.284918, acc.: 87.50%] [Generator loss: 3.691428]\n",
      "4271 [Discriminator loss: 0.508772, acc.: 81.25%] [Generator loss: 4.233619]\n",
      "4272 [Discriminator loss: 0.187434, acc.: 90.62%] [Generator loss: 4.411804]\n",
      "4273 [Discriminator loss: 0.254993, acc.: 89.06%] [Generator loss: 3.864838]\n",
      "4274 [Discriminator loss: 0.169279, acc.: 90.62%] [Generator loss: 4.690279]\n",
      "4275 [Discriminator loss: 0.432006, acc.: 76.56%] [Generator loss: 4.597435]\n",
      "4276 [Discriminator loss: 0.257861, acc.: 89.06%] [Generator loss: 4.109494]\n",
      "4277 [Discriminator loss: 0.311734, acc.: 82.81%] [Generator loss: 5.428829]\n",
      "4278 [Discriminator loss: 0.398870, acc.: 89.06%] [Generator loss: 5.260219]\n",
      "4279 [Discriminator loss: 0.225422, acc.: 93.75%] [Generator loss: 4.311825]\n",
      "4280 [Discriminator loss: 0.418482, acc.: 87.50%] [Generator loss: 4.647116]\n",
      "4281 [Discriminator loss: 0.284466, acc.: 87.50%] [Generator loss: 4.953255]\n",
      "4282 [Discriminator loss: 0.185857, acc.: 89.06%] [Generator loss: 4.924050]\n",
      "4283 [Discriminator loss: 0.440207, acc.: 82.81%] [Generator loss: 4.596268]\n",
      "4284 [Discriminator loss: 0.117228, acc.: 93.75%] [Generator loss: 4.389225]\n",
      "4285 [Discriminator loss: 0.178440, acc.: 90.62%] [Generator loss: 4.583640]\n",
      "4286 [Discriminator loss: 0.241769, acc.: 87.50%] [Generator loss: 4.094100]\n",
      "4287 [Discriminator loss: 0.470313, acc.: 78.12%] [Generator loss: 5.303514]\n",
      "4288 [Discriminator loss: 0.358743, acc.: 82.81%] [Generator loss: 3.663987]\n",
      "4289 [Discriminator loss: 0.364153, acc.: 87.50%] [Generator loss: 3.980515]\n",
      "4290 [Discriminator loss: 0.138270, acc.: 96.88%] [Generator loss: 4.198900]\n",
      "4291 [Discriminator loss: 0.320795, acc.: 87.50%] [Generator loss: 2.968403]\n",
      "4292 [Discriminator loss: 0.189649, acc.: 89.06%] [Generator loss: 4.883156]\n",
      "4293 [Discriminator loss: 0.294021, acc.: 90.62%] [Generator loss: 4.033680]\n",
      "4294 [Discriminator loss: 0.356565, acc.: 81.25%] [Generator loss: 5.249563]\n",
      "4295 [Discriminator loss: 0.256875, acc.: 87.50%] [Generator loss: 4.253595]\n",
      "4296 [Discriminator loss: 0.353207, acc.: 81.25%] [Generator loss: 4.646112]\n",
      "4297 [Discriminator loss: 0.287219, acc.: 89.06%] [Generator loss: 4.332697]\n",
      "4298 [Discriminator loss: 0.252597, acc.: 89.06%] [Generator loss: 4.485662]\n",
      "4299 [Discriminator loss: 0.246571, acc.: 93.75%] [Generator loss: 4.743377]\n",
      "4300 [Discriminator loss: 0.283427, acc.: 89.06%] [Generator loss: 4.445834]\n",
      "4301 [Discriminator loss: 0.212739, acc.: 90.62%] [Generator loss: 4.256798]\n",
      "4302 [Discriminator loss: 0.195595, acc.: 92.19%] [Generator loss: 3.879925]\n",
      "4303 [Discriminator loss: 0.282384, acc.: 87.50%] [Generator loss: 4.597784]\n",
      "4304 [Discriminator loss: 0.264205, acc.: 87.50%] [Generator loss: 3.796957]\n",
      "4305 [Discriminator loss: 0.425544, acc.: 81.25%] [Generator loss: 4.798673]\n",
      "4306 [Discriminator loss: 0.174371, acc.: 96.88%] [Generator loss: 5.713864]\n",
      "4307 [Discriminator loss: 0.343946, acc.: 82.81%] [Generator loss: 3.540090]\n",
      "4308 [Discriminator loss: 0.175771, acc.: 95.31%] [Generator loss: 4.121888]\n",
      "4309 [Discriminator loss: 0.343648, acc.: 81.25%] [Generator loss: 4.710261]\n",
      "4310 [Discriminator loss: 0.191221, acc.: 92.19%] [Generator loss: 5.223368]\n",
      "4311 [Discriminator loss: 0.451629, acc.: 85.94%] [Generator loss: 3.736662]\n",
      "4312 [Discriminator loss: 0.300623, acc.: 82.81%] [Generator loss: 3.722720]\n",
      "4313 [Discriminator loss: 0.183107, acc.: 92.19%] [Generator loss: 3.323231]\n",
      "4314 [Discriminator loss: 0.292826, acc.: 92.19%] [Generator loss: 4.371749]\n",
      "4315 [Discriminator loss: 0.277729, acc.: 85.94%] [Generator loss: 4.587553]\n",
      "4316 [Discriminator loss: 0.138203, acc.: 92.19%] [Generator loss: 4.031948]\n",
      "4317 [Discriminator loss: 0.325823, acc.: 84.38%] [Generator loss: 3.932665]\n",
      "4318 [Discriminator loss: 0.200157, acc.: 89.06%] [Generator loss: 4.127590]\n",
      "4319 [Discriminator loss: 0.456520, acc.: 79.69%] [Generator loss: 5.383521]\n",
      "4320 [Discriminator loss: 0.138062, acc.: 93.75%] [Generator loss: 4.860496]\n",
      "4321 [Discriminator loss: 0.520953, acc.: 78.12%] [Generator loss: 4.635380]\n",
      "4322 [Discriminator loss: 0.487567, acc.: 79.69%] [Generator loss: 7.031267]\n",
      "4323 [Discriminator loss: 0.558559, acc.: 76.56%] [Generator loss: 4.696682]\n",
      "4324 [Discriminator loss: 0.221503, acc.: 93.75%] [Generator loss: 4.679610]\n",
      "4325 [Discriminator loss: 0.335751, acc.: 81.25%] [Generator loss: 4.097100]\n",
      "4326 [Discriminator loss: 0.125124, acc.: 96.88%] [Generator loss: 5.258820]\n",
      "4327 [Discriminator loss: 0.613617, acc.: 76.56%] [Generator loss: 4.048112]\n",
      "4328 [Discriminator loss: 0.195174, acc.: 92.19%] [Generator loss: 4.180806]\n",
      "4329 [Discriminator loss: 0.300854, acc.: 84.38%] [Generator loss: 3.491873]\n",
      "4330 [Discriminator loss: 0.382609, acc.: 79.69%] [Generator loss: 5.190606]\n",
      "4331 [Discriminator loss: 0.468661, acc.: 76.56%] [Generator loss: 3.827434]\n",
      "4332 [Discriminator loss: 0.138385, acc.: 95.31%] [Generator loss: 5.011395]\n",
      "4333 [Discriminator loss: 0.119229, acc.: 95.31%] [Generator loss: 3.897321]\n",
      "4334 [Discriminator loss: 0.315519, acc.: 84.38%] [Generator loss: 5.206601]\n",
      "4335 [Discriminator loss: 0.294915, acc.: 87.50%] [Generator loss: 4.175948]\n",
      "4336 [Discriminator loss: 0.413295, acc.: 85.94%] [Generator loss: 4.405936]\n",
      "4337 [Discriminator loss: 0.348774, acc.: 82.81%] [Generator loss: 4.409811]\n",
      "4338 [Discriminator loss: 0.212176, acc.: 93.75%] [Generator loss: 4.602318]\n",
      "4339 [Discriminator loss: 0.192633, acc.: 92.19%] [Generator loss: 4.457571]\n",
      "4340 [Discriminator loss: 0.224186, acc.: 93.75%] [Generator loss: 4.582482]\n",
      "4341 [Discriminator loss: 0.188802, acc.: 96.88%] [Generator loss: 4.100677]\n",
      "4342 [Discriminator loss: 0.157820, acc.: 95.31%] [Generator loss: 3.107185]\n",
      "4343 [Discriminator loss: 0.347767, acc.: 85.94%] [Generator loss: 5.404533]\n",
      "4344 [Discriminator loss: 0.210758, acc.: 95.31%] [Generator loss: 4.394337]\n",
      "4345 [Discriminator loss: 0.195857, acc.: 95.31%] [Generator loss: 2.544947]\n",
      "4346 [Discriminator loss: 0.273013, acc.: 89.06%] [Generator loss: 3.927013]\n",
      "4347 [Discriminator loss: 0.248240, acc.: 89.06%] [Generator loss: 3.207355]\n",
      "4348 [Discriminator loss: 0.446165, acc.: 81.25%] [Generator loss: 6.024916]\n",
      "4349 [Discriminator loss: 0.322156, acc.: 89.06%] [Generator loss: 6.023955]\n",
      "4350 [Discriminator loss: 0.556703, acc.: 73.44%] [Generator loss: 4.256540]\n",
      "4351 [Discriminator loss: 0.112052, acc.: 96.88%] [Generator loss: 3.955036]\n",
      "4352 [Discriminator loss: 0.373854, acc.: 84.38%] [Generator loss: 3.542635]\n",
      "4353 [Discriminator loss: 0.408358, acc.: 82.81%] [Generator loss: 4.473118]\n",
      "4354 [Discriminator loss: 0.239965, acc.: 90.62%] [Generator loss: 4.708820]\n",
      "4355 [Discriminator loss: 0.180171, acc.: 92.19%] [Generator loss: 4.388964]\n",
      "4356 [Discriminator loss: 0.400304, acc.: 81.25%] [Generator loss: 4.347770]\n",
      "4357 [Discriminator loss: 0.129116, acc.: 96.88%] [Generator loss: 4.591520]\n",
      "4358 [Discriminator loss: 0.198932, acc.: 90.62%] [Generator loss: 5.177213]\n",
      "4359 [Discriminator loss: 0.521333, acc.: 76.56%] [Generator loss: 4.616245]\n",
      "4360 [Discriminator loss: 0.316802, acc.: 89.06%] [Generator loss: 4.837603]\n",
      "4361 [Discriminator loss: 0.498088, acc.: 81.25%] [Generator loss: 3.749746]\n",
      "4362 [Discriminator loss: 0.207788, acc.: 92.19%] [Generator loss: 3.991782]\n",
      "4363 [Discriminator loss: 0.302740, acc.: 90.62%] [Generator loss: 2.717776]\n",
      "4364 [Discriminator loss: 0.177900, acc.: 95.31%] [Generator loss: 4.811589]\n",
      "4365 [Discriminator loss: 0.163082, acc.: 96.88%] [Generator loss: 3.991189]\n",
      "4366 [Discriminator loss: 0.546106, acc.: 67.19%] [Generator loss: 3.300410]\n",
      "4367 [Discriminator loss: 0.146638, acc.: 95.31%] [Generator loss: 4.934299]\n",
      "4368 [Discriminator loss: 0.292596, acc.: 87.50%] [Generator loss: 5.204830]\n",
      "4369 [Discriminator loss: 0.273626, acc.: 89.06%] [Generator loss: 3.701900]\n",
      "4370 [Discriminator loss: 0.316449, acc.: 90.62%] [Generator loss: 4.173649]\n",
      "4371 [Discriminator loss: 0.179352, acc.: 92.19%] [Generator loss: 4.199115]\n",
      "4372 [Discriminator loss: 0.182747, acc.: 92.19%] [Generator loss: 3.803206]\n",
      "4373 [Discriminator loss: 0.585092, acc.: 71.88%] [Generator loss: 6.920882]\n",
      "4374 [Discriminator loss: 0.268893, acc.: 90.62%] [Generator loss: 6.321967]\n",
      "4375 [Discriminator loss: 0.353515, acc.: 87.50%] [Generator loss: 3.753752]\n",
      "4376 [Discriminator loss: 0.218875, acc.: 92.19%] [Generator loss: 3.811622]\n",
      "4377 [Discriminator loss: 0.262045, acc.: 90.62%] [Generator loss: 4.741946]\n",
      "4378 [Discriminator loss: 0.180791, acc.: 90.62%] [Generator loss: 4.232907]\n",
      "4379 [Discriminator loss: 0.340811, acc.: 84.38%] [Generator loss: 4.812150]\n",
      "4380 [Discriminator loss: 0.577182, acc.: 78.12%] [Generator loss: 5.605755]\n",
      "4381 [Discriminator loss: 0.249041, acc.: 89.06%] [Generator loss: 4.653219]\n",
      "4382 [Discriminator loss: 0.170343, acc.: 92.19%] [Generator loss: 4.688826]\n",
      "4383 [Discriminator loss: 0.370117, acc.: 82.81%] [Generator loss: 6.500219]\n",
      "4384 [Discriminator loss: 0.337911, acc.: 87.50%] [Generator loss: 5.020014]\n",
      "4385 [Discriminator loss: 0.327248, acc.: 85.94%] [Generator loss: 4.017711]\n",
      "4386 [Discriminator loss: 0.219153, acc.: 95.31%] [Generator loss: 5.045286]\n",
      "4387 [Discriminator loss: 0.355889, acc.: 84.38%] [Generator loss: 4.805788]\n",
      "4388 [Discriminator loss: 0.148488, acc.: 96.88%] [Generator loss: 5.345984]\n",
      "4389 [Discriminator loss: 0.400216, acc.: 81.25%] [Generator loss: 4.111228]\n",
      "4390 [Discriminator loss: 0.220695, acc.: 90.62%] [Generator loss: 4.293276]\n",
      "4391 [Discriminator loss: 0.170950, acc.: 93.75%] [Generator loss: 5.206487]\n",
      "4392 [Discriminator loss: 0.202408, acc.: 95.31%] [Generator loss: 4.026882]\n",
      "4393 [Discriminator loss: 0.475332, acc.: 75.00%] [Generator loss: 3.250794]\n",
      "4394 [Discriminator loss: 0.188512, acc.: 93.75%] [Generator loss: 3.776782]\n",
      "4395 [Discriminator loss: 0.405707, acc.: 79.69%] [Generator loss: 5.180951]\n",
      "4396 [Discriminator loss: 0.154819, acc.: 95.31%] [Generator loss: 4.412484]\n",
      "4397 [Discriminator loss: 0.293913, acc.: 87.50%] [Generator loss: 4.428588]\n",
      "4398 [Discriminator loss: 0.254067, acc.: 90.62%] [Generator loss: 4.473739]\n",
      "4399 [Discriminator loss: 0.148929, acc.: 93.75%] [Generator loss: 4.644987]\n",
      "4400 [Discriminator loss: 0.162952, acc.: 92.19%] [Generator loss: 4.685153]\n",
      "4401 [Discriminator loss: 0.486097, acc.: 76.56%] [Generator loss: 3.413436]\n",
      "4402 [Discriminator loss: 0.245659, acc.: 90.62%] [Generator loss: 4.645146]\n",
      "4403 [Discriminator loss: 0.432359, acc.: 85.94%] [Generator loss: 2.879352]\n",
      "4404 [Discriminator loss: 0.237220, acc.: 89.06%] [Generator loss: 3.403359]\n",
      "4405 [Discriminator loss: 0.238552, acc.: 89.06%] [Generator loss: 4.505666]\n",
      "4406 [Discriminator loss: 0.136865, acc.: 95.31%] [Generator loss: 4.593458]\n",
      "4407 [Discriminator loss: 0.319543, acc.: 84.38%] [Generator loss: 3.750664]\n",
      "4408 [Discriminator loss: 0.371268, acc.: 76.56%] [Generator loss: 4.784247]\n",
      "4409 [Discriminator loss: 0.455238, acc.: 84.38%] [Generator loss: 4.231565]\n",
      "4410 [Discriminator loss: 0.168714, acc.: 96.88%] [Generator loss: 4.229575]\n",
      "4411 [Discriminator loss: 0.261508, acc.: 90.62%] [Generator loss: 5.583813]\n",
      "4412 [Discriminator loss: 0.262598, acc.: 92.19%] [Generator loss: 3.297047]\n",
      "4413 [Discriminator loss: 0.410184, acc.: 81.25%] [Generator loss: 4.801602]\n",
      "4414 [Discriminator loss: 0.139705, acc.: 93.75%] [Generator loss: 5.669851]\n",
      "4415 [Discriminator loss: 0.542312, acc.: 70.31%] [Generator loss: 3.265651]\n",
      "4416 [Discriminator loss: 0.185164, acc.: 96.88%] [Generator loss: 4.317668]\n",
      "4417 [Discriminator loss: 0.155756, acc.: 92.19%] [Generator loss: 4.821456]\n",
      "4418 [Discriminator loss: 0.236627, acc.: 90.62%] [Generator loss: 5.200517]\n",
      "4419 [Discriminator loss: 0.354956, acc.: 85.94%] [Generator loss: 4.272064]\n",
      "4420 [Discriminator loss: 0.238250, acc.: 90.62%] [Generator loss: 3.304908]\n",
      "4421 [Discriminator loss: 0.310292, acc.: 79.69%] [Generator loss: 4.822303]\n",
      "4422 [Discriminator loss: 0.290132, acc.: 89.06%] [Generator loss: 4.467727]\n",
      "4423 [Discriminator loss: 0.356130, acc.: 82.81%] [Generator loss: 4.393034]\n",
      "4424 [Discriminator loss: 0.166996, acc.: 95.31%] [Generator loss: 5.091788]\n",
      "4425 [Discriminator loss: 0.191650, acc.: 92.19%] [Generator loss: 4.095298]\n",
      "4426 [Discriminator loss: 0.219203, acc.: 90.62%] [Generator loss: 4.468872]\n",
      "4427 [Discriminator loss: 0.202855, acc.: 87.50%] [Generator loss: 4.708456]\n",
      "4428 [Discriminator loss: 0.282688, acc.: 89.06%] [Generator loss: 5.731121]\n",
      "4429 [Discriminator loss: 0.450682, acc.: 81.25%] [Generator loss: 3.343350]\n",
      "4430 [Discriminator loss: 0.138278, acc.: 95.31%] [Generator loss: 4.302007]\n",
      "4431 [Discriminator loss: 0.286453, acc.: 87.50%] [Generator loss: 4.537380]\n",
      "4432 [Discriminator loss: 0.313344, acc.: 84.38%] [Generator loss: 4.549357]\n",
      "4433 [Discriminator loss: 0.224731, acc.: 92.19%] [Generator loss: 2.968834]\n",
      "4434 [Discriminator loss: 0.374620, acc.: 85.94%] [Generator loss: 5.092721]\n",
      "4435 [Discriminator loss: 0.418056, acc.: 79.69%] [Generator loss: 4.093881]\n",
      "4436 [Discriminator loss: 0.340379, acc.: 84.38%] [Generator loss: 5.734563]\n",
      "4437 [Discriminator loss: 0.307353, acc.: 85.94%] [Generator loss: 5.334098]\n",
      "4438 [Discriminator loss: 0.136948, acc.: 96.88%] [Generator loss: 3.873207]\n",
      "4439 [Discriminator loss: 0.460370, acc.: 85.94%] [Generator loss: 4.723491]\n",
      "4440 [Discriminator loss: 0.298942, acc.: 85.94%] [Generator loss: 4.349980]\n",
      "4441 [Discriminator loss: 0.648845, acc.: 71.88%] [Generator loss: 4.732644]\n",
      "4442 [Discriminator loss: 0.145683, acc.: 93.75%] [Generator loss: 5.329071]\n",
      "4443 [Discriminator loss: 0.386414, acc.: 78.12%] [Generator loss: 3.218816]\n",
      "4444 [Discriminator loss: 0.361085, acc.: 89.06%] [Generator loss: 4.478574]\n",
      "4445 [Discriminator loss: 0.259553, acc.: 90.62%] [Generator loss: 5.081725]\n",
      "4446 [Discriminator loss: 0.225787, acc.: 89.06%] [Generator loss: 3.676812]\n",
      "4447 [Discriminator loss: 0.458917, acc.: 75.00%] [Generator loss: 4.579119]\n",
      "4448 [Discriminator loss: 0.241790, acc.: 90.62%] [Generator loss: 5.805865]\n",
      "4449 [Discriminator loss: 0.284100, acc.: 87.50%] [Generator loss: 3.862312]\n",
      "4450 [Discriminator loss: 0.385412, acc.: 76.56%] [Generator loss: 4.283478]\n",
      "4451 [Discriminator loss: 0.209626, acc.: 93.75%] [Generator loss: 4.332843]\n",
      "4452 [Discriminator loss: 0.207939, acc.: 92.19%] [Generator loss: 4.648721]\n",
      "4453 [Discriminator loss: 0.468122, acc.: 82.81%] [Generator loss: 5.386270]\n",
      "4454 [Discriminator loss: 0.226559, acc.: 89.06%] [Generator loss: 5.142360]\n",
      "4455 [Discriminator loss: 0.355128, acc.: 78.12%] [Generator loss: 4.209529]\n",
      "4456 [Discriminator loss: 0.427901, acc.: 79.69%] [Generator loss: 4.833115]\n",
      "4457 [Discriminator loss: 0.203636, acc.: 90.62%] [Generator loss: 5.554794]\n",
      "4458 [Discriminator loss: 0.297436, acc.: 90.62%] [Generator loss: 5.068763]\n",
      "4459 [Discriminator loss: 0.161474, acc.: 92.19%] [Generator loss: 3.969303]\n",
      "4460 [Discriminator loss: 0.451337, acc.: 81.25%] [Generator loss: 3.632329]\n",
      "4461 [Discriminator loss: 0.189831, acc.: 93.75%] [Generator loss: 5.075321]\n",
      "4462 [Discriminator loss: 0.197345, acc.: 89.06%] [Generator loss: 4.683138]\n",
      "4463 [Discriminator loss: 0.205019, acc.: 89.06%] [Generator loss: 4.650767]\n",
      "4464 [Discriminator loss: 0.757334, acc.: 68.75%] [Generator loss: 4.758976]\n",
      "4465 [Discriminator loss: 0.256294, acc.: 90.62%] [Generator loss: 4.669918]\n",
      "4466 [Discriminator loss: 0.473046, acc.: 75.00%] [Generator loss: 3.705558]\n",
      "4467 [Discriminator loss: 0.128862, acc.: 96.88%] [Generator loss: 4.043247]\n",
      "4468 [Discriminator loss: 0.282570, acc.: 89.06%] [Generator loss: 4.471009]\n",
      "4469 [Discriminator loss: 0.195773, acc.: 90.62%] [Generator loss: 4.425744]\n",
      "4470 [Discriminator loss: 0.186098, acc.: 93.75%] [Generator loss: 3.803477]\n",
      "4471 [Discriminator loss: 0.374125, acc.: 87.50%] [Generator loss: 5.239358]\n",
      "4472 [Discriminator loss: 0.513407, acc.: 75.00%] [Generator loss: 3.927538]\n",
      "4473 [Discriminator loss: 0.183786, acc.: 92.19%] [Generator loss: 4.180577]\n",
      "4474 [Discriminator loss: 0.262611, acc.: 89.06%] [Generator loss: 3.888629]\n",
      "4475 [Discriminator loss: 0.194664, acc.: 92.19%] [Generator loss: 5.666996]\n",
      "4476 [Discriminator loss: 0.168839, acc.: 93.75%] [Generator loss: 4.497906]\n",
      "4477 [Discriminator loss: 0.234064, acc.: 92.19%] [Generator loss: 4.456913]\n",
      "4478 [Discriminator loss: 0.200244, acc.: 95.31%] [Generator loss: 4.064399]\n",
      "4479 [Discriminator loss: 0.207250, acc.: 85.94%] [Generator loss: 5.039804]\n",
      "4480 [Discriminator loss: 0.158441, acc.: 93.75%] [Generator loss: 4.811990]\n",
      "4481 [Discriminator loss: 0.278435, acc.: 93.75%] [Generator loss: 4.687585]\n",
      "4482 [Discriminator loss: 0.303069, acc.: 92.19%] [Generator loss: 4.304671]\n",
      "4483 [Discriminator loss: 0.172731, acc.: 92.19%] [Generator loss: 4.450959]\n",
      "4484 [Discriminator loss: 0.296446, acc.: 84.38%] [Generator loss: 4.505410]\n",
      "4485 [Discriminator loss: 0.254260, acc.: 87.50%] [Generator loss: 3.754193]\n",
      "4486 [Discriminator loss: 0.253285, acc.: 87.50%] [Generator loss: 4.445876]\n",
      "4487 [Discriminator loss: 0.117313, acc.: 98.44%] [Generator loss: 4.703046]\n",
      "4488 [Discriminator loss: 0.480681, acc.: 70.31%] [Generator loss: 4.775541]\n",
      "4489 [Discriminator loss: 0.139145, acc.: 92.19%] [Generator loss: 4.613858]\n",
      "4490 [Discriminator loss: 0.181802, acc.: 90.62%] [Generator loss: 3.931115]\n",
      "4491 [Discriminator loss: 0.300951, acc.: 87.50%] [Generator loss: 4.439705]\n",
      "4492 [Discriminator loss: 0.126054, acc.: 98.44%] [Generator loss: 4.719849]\n",
      "4493 [Discriminator loss: 0.138439, acc.: 96.88%] [Generator loss: 3.665519]\n",
      "4494 [Discriminator loss: 0.216487, acc.: 89.06%] [Generator loss: 4.786376]\n",
      "4495 [Discriminator loss: 0.210463, acc.: 92.19%] [Generator loss: 4.974609]\n",
      "4496 [Discriminator loss: 0.147042, acc.: 95.31%] [Generator loss: 4.430659]\n",
      "4497 [Discriminator loss: 0.208326, acc.: 93.75%] [Generator loss: 4.052529]\n",
      "4498 [Discriminator loss: 0.161278, acc.: 93.75%] [Generator loss: 4.342822]\n",
      "4499 [Discriminator loss: 0.369345, acc.: 84.38%] [Generator loss: 3.418845]\n",
      "4500 [Discriminator loss: 0.202464, acc.: 90.62%] [Generator loss: 4.168335]\n",
      "4501 [Discriminator loss: 0.548970, acc.: 73.44%] [Generator loss: 4.190967]\n",
      "4502 [Discriminator loss: 0.074872, acc.: 98.44%] [Generator loss: 6.134593]\n",
      "4503 [Discriminator loss: 0.193362, acc.: 93.75%] [Generator loss: 4.295245]\n",
      "4504 [Discriminator loss: 0.135779, acc.: 93.75%] [Generator loss: 4.792087]\n",
      "4505 [Discriminator loss: 0.409886, acc.: 81.25%] [Generator loss: 5.819630]\n",
      "4506 [Discriminator loss: 0.128883, acc.: 96.88%] [Generator loss: 5.478814]\n",
      "4507 [Discriminator loss: 0.260332, acc.: 92.19%] [Generator loss: 4.362870]\n",
      "4508 [Discriminator loss: 0.179362, acc.: 95.31%] [Generator loss: 4.896247]\n",
      "4509 [Discriminator loss: 0.226438, acc.: 92.19%] [Generator loss: 4.375925]\n",
      "4510 [Discriminator loss: 0.718660, acc.: 75.00%] [Generator loss: 3.930612]\n",
      "4511 [Discriminator loss: 0.139757, acc.: 96.88%] [Generator loss: 5.129653]\n",
      "4512 [Discriminator loss: 0.204977, acc.: 90.62%] [Generator loss: 4.488851]\n",
      "4513 [Discriminator loss: 0.146481, acc.: 95.31%] [Generator loss: 4.416448]\n",
      "4514 [Discriminator loss: 0.118036, acc.: 96.88%] [Generator loss: 4.852699]\n",
      "4515 [Discriminator loss: 0.234068, acc.: 89.06%] [Generator loss: 4.263722]\n",
      "4516 [Discriminator loss: 0.400078, acc.: 85.94%] [Generator loss: 3.901024]\n",
      "4517 [Discriminator loss: 0.269083, acc.: 87.50%] [Generator loss: 3.967382]\n",
      "4518 [Discriminator loss: 0.387102, acc.: 82.81%] [Generator loss: 5.098081]\n",
      "4519 [Discriminator loss: 0.221574, acc.: 92.19%] [Generator loss: 5.502568]\n",
      "4520 [Discriminator loss: 0.136937, acc.: 95.31%] [Generator loss: 4.896499]\n",
      "4521 [Discriminator loss: 0.287952, acc.: 89.06%] [Generator loss: 4.408582]\n",
      "4522 [Discriminator loss: 0.232293, acc.: 89.06%] [Generator loss: 4.892861]\n",
      "4523 [Discriminator loss: 0.573613, acc.: 79.69%] [Generator loss: 4.626349]\n",
      "4524 [Discriminator loss: 0.182224, acc.: 90.62%] [Generator loss: 4.969810]\n",
      "4525 [Discriminator loss: 0.378080, acc.: 79.69%] [Generator loss: 4.035533]\n",
      "4526 [Discriminator loss: 0.198061, acc.: 92.19%] [Generator loss: 4.213775]\n",
      "4527 [Discriminator loss: 0.245574, acc.: 87.50%] [Generator loss: 4.523995]\n",
      "4528 [Discriminator loss: 0.210607, acc.: 92.19%] [Generator loss: 4.072296]\n",
      "4529 [Discriminator loss: 0.470717, acc.: 79.69%] [Generator loss: 4.757262]\n",
      "4530 [Discriminator loss: 0.267212, acc.: 93.75%] [Generator loss: 4.484190]\n",
      "4531 [Discriminator loss: 0.259918, acc.: 85.94%] [Generator loss: 3.549338]\n",
      "4532 [Discriminator loss: 0.130422, acc.: 93.75%] [Generator loss: 3.689828]\n",
      "4533 [Discriminator loss: 0.211962, acc.: 92.19%] [Generator loss: 4.950272]\n",
      "4534 [Discriminator loss: 0.178596, acc.: 89.06%] [Generator loss: 4.457927]\n",
      "4535 [Discriminator loss: 0.217947, acc.: 92.19%] [Generator loss: 4.403439]\n",
      "4536 [Discriminator loss: 0.328116, acc.: 85.94%] [Generator loss: 4.119838]\n",
      "4537 [Discriminator loss: 0.139392, acc.: 95.31%] [Generator loss: 4.547027]\n",
      "4538 [Discriminator loss: 0.594890, acc.: 75.00%] [Generator loss: 4.691929]\n",
      "4539 [Discriminator loss: 0.155379, acc.: 93.75%] [Generator loss: 5.715020]\n",
      "4540 [Discriminator loss: 0.299012, acc.: 85.94%] [Generator loss: 4.360169]\n",
      "4541 [Discriminator loss: 0.248593, acc.: 89.06%] [Generator loss: 4.659606]\n",
      "4542 [Discriminator loss: 0.142272, acc.: 93.75%] [Generator loss: 4.560740]\n",
      "4543 [Discriminator loss: 0.357231, acc.: 84.38%] [Generator loss: 4.703074]\n",
      "4544 [Discriminator loss: 0.202388, acc.: 89.06%] [Generator loss: 3.980077]\n",
      "4545 [Discriminator loss: 0.238788, acc.: 89.06%] [Generator loss: 3.703342]\n",
      "4546 [Discriminator loss: 0.223694, acc.: 90.62%] [Generator loss: 4.604528]\n",
      "4547 [Discriminator loss: 0.451686, acc.: 82.81%] [Generator loss: 5.574431]\n",
      "4548 [Discriminator loss: 0.219699, acc.: 87.50%] [Generator loss: 5.428120]\n",
      "4549 [Discriminator loss: 0.153596, acc.: 96.88%] [Generator loss: 5.024761]\n",
      "4550 [Discriminator loss: 0.278055, acc.: 82.81%] [Generator loss: 4.942689]\n",
      "4551 [Discriminator loss: 0.157327, acc.: 93.75%] [Generator loss: 4.386791]\n",
      "4552 [Discriminator loss: 0.510135, acc.: 76.56%] [Generator loss: 3.719162]\n",
      "4553 [Discriminator loss: 0.330830, acc.: 81.25%] [Generator loss: 5.682228]\n",
      "4554 [Discriminator loss: 0.289238, acc.: 85.94%] [Generator loss: 5.573821]\n",
      "4555 [Discriminator loss: 0.380659, acc.: 84.38%] [Generator loss: 3.564071]\n",
      "4556 [Discriminator loss: 0.248853, acc.: 92.19%] [Generator loss: 5.506355]\n",
      "4557 [Discriminator loss: 0.427220, acc.: 82.81%] [Generator loss: 6.080515]\n",
      "4558 [Discriminator loss: 0.176758, acc.: 95.31%] [Generator loss: 4.209834]\n",
      "4559 [Discriminator loss: 0.283769, acc.: 84.38%] [Generator loss: 3.656239]\n",
      "4560 [Discriminator loss: 0.280259, acc.: 89.06%] [Generator loss: 3.917156]\n",
      "4561 [Discriminator loss: 0.334842, acc.: 81.25%] [Generator loss: 3.762312]\n",
      "4562 [Discriminator loss: 0.251420, acc.: 87.50%] [Generator loss: 5.674173]\n",
      "4563 [Discriminator loss: 0.141093, acc.: 95.31%] [Generator loss: 4.384328]\n",
      "4564 [Discriminator loss: 0.372410, acc.: 81.25%] [Generator loss: 4.098517]\n",
      "4565 [Discriminator loss: 0.479436, acc.: 75.00%] [Generator loss: 6.048421]\n",
      "4566 [Discriminator loss: 0.276198, acc.: 85.94%] [Generator loss: 5.661086]\n",
      "4567 [Discriminator loss: 0.189269, acc.: 89.06%] [Generator loss: 4.385291]\n",
      "4568 [Discriminator loss: 0.284376, acc.: 90.62%] [Generator loss: 4.917123]\n",
      "4569 [Discriminator loss: 0.308744, acc.: 87.50%] [Generator loss: 5.375412]\n",
      "4570 [Discriminator loss: 0.488246, acc.: 79.69%] [Generator loss: 5.408549]\n",
      "4571 [Discriminator loss: 0.123611, acc.: 93.75%] [Generator loss: 4.371490]\n",
      "4572 [Discriminator loss: 0.327294, acc.: 87.50%] [Generator loss: 4.774061]\n",
      "4573 [Discriminator loss: 0.323699, acc.: 89.06%] [Generator loss: 4.141987]\n",
      "4574 [Discriminator loss: 0.286744, acc.: 89.06%] [Generator loss: 4.417435]\n",
      "4575 [Discriminator loss: 0.314034, acc.: 89.06%] [Generator loss: 4.703840]\n",
      "4576 [Discriminator loss: 0.171328, acc.: 95.31%] [Generator loss: 4.619517]\n",
      "4577 [Discriminator loss: 0.221016, acc.: 93.75%] [Generator loss: 3.584942]\n",
      "4578 [Discriminator loss: 0.087824, acc.: 96.88%] [Generator loss: 4.510835]\n",
      "4579 [Discriminator loss: 0.313338, acc.: 85.94%] [Generator loss: 3.783925]\n",
      "4580 [Discriminator loss: 0.179161, acc.: 92.19%] [Generator loss: 4.484380]\n",
      "4581 [Discriminator loss: 0.314106, acc.: 87.50%] [Generator loss: 4.917090]\n",
      "4582 [Discriminator loss: 0.154419, acc.: 93.75%] [Generator loss: 5.072180]\n",
      "4583 [Discriminator loss: 0.351962, acc.: 84.38%] [Generator loss: 3.546067]\n",
      "4584 [Discriminator loss: 0.294534, acc.: 87.50%] [Generator loss: 4.964768]\n",
      "4585 [Discriminator loss: 0.120145, acc.: 95.31%] [Generator loss: 5.448060]\n",
      "4586 [Discriminator loss: 0.256667, acc.: 89.06%] [Generator loss: 4.402165]\n",
      "4587 [Discriminator loss: 0.160586, acc.: 92.19%] [Generator loss: 4.068453]\n",
      "4588 [Discriminator loss: 0.316579, acc.: 87.50%] [Generator loss: 2.978067]\n",
      "4589 [Discriminator loss: 0.437184, acc.: 89.06%] [Generator loss: 4.508085]\n",
      "4590 [Discriminator loss: 0.209784, acc.: 92.19%] [Generator loss: 4.580947]\n",
      "4591 [Discriminator loss: 0.278448, acc.: 85.94%] [Generator loss: 5.198267]\n",
      "4592 [Discriminator loss: 0.294416, acc.: 82.81%] [Generator loss: 4.427242]\n",
      "4593 [Discriminator loss: 0.363463, acc.: 85.94%] [Generator loss: 4.525093]\n",
      "4594 [Discriminator loss: 0.113671, acc.: 96.88%] [Generator loss: 5.359625]\n",
      "4595 [Discriminator loss: 0.157541, acc.: 95.31%] [Generator loss: 4.314270]\n",
      "4596 [Discriminator loss: 0.345702, acc.: 79.69%] [Generator loss: 4.784895]\n",
      "4597 [Discriminator loss: 0.287185, acc.: 85.94%] [Generator loss: 4.392384]\n",
      "4598 [Discriminator loss: 0.290449, acc.: 95.31%] [Generator loss: 4.692667]\n",
      "4599 [Discriminator loss: 0.179987, acc.: 93.75%] [Generator loss: 3.876897]\n",
      "4600 [Discriminator loss: 0.272826, acc.: 85.94%] [Generator loss: 4.393146]\n",
      "4601 [Discriminator loss: 0.300547, acc.: 84.38%] [Generator loss: 4.351151]\n",
      "4602 [Discriminator loss: 0.280772, acc.: 87.50%] [Generator loss: 3.686367]\n",
      "4603 [Discriminator loss: 0.245131, acc.: 89.06%] [Generator loss: 4.155346]\n",
      "4604 [Discriminator loss: 0.227715, acc.: 92.19%] [Generator loss: 4.385393]\n",
      "4605 [Discriminator loss: 0.186198, acc.: 90.62%] [Generator loss: 4.833864]\n",
      "4606 [Discriminator loss: 0.321548, acc.: 84.38%] [Generator loss: 4.061904]\n",
      "4607 [Discriminator loss: 0.242655, acc.: 90.62%] [Generator loss: 4.211487]\n",
      "4608 [Discriminator loss: 0.177049, acc.: 96.88%] [Generator loss: 5.155431]\n",
      "4609 [Discriminator loss: 0.361271, acc.: 87.50%] [Generator loss: 3.773375]\n",
      "4610 [Discriminator loss: 0.196313, acc.: 89.06%] [Generator loss: 4.350607]\n",
      "4611 [Discriminator loss: 0.302583, acc.: 84.38%] [Generator loss: 5.927140]\n",
      "4612 [Discriminator loss: 0.402718, acc.: 76.56%] [Generator loss: 4.531960]\n",
      "4613 [Discriminator loss: 0.244863, acc.: 89.06%] [Generator loss: 4.118706]\n",
      "4614 [Discriminator loss: 0.291422, acc.: 84.38%] [Generator loss: 4.946398]\n",
      "4615 [Discriminator loss: 0.324539, acc.: 87.50%] [Generator loss: 5.013338]\n",
      "4616 [Discriminator loss: 0.339617, acc.: 82.81%] [Generator loss: 3.980208]\n",
      "4617 [Discriminator loss: 0.189653, acc.: 92.19%] [Generator loss: 4.684108]\n",
      "4618 [Discriminator loss: 0.397862, acc.: 79.69%] [Generator loss: 3.846042]\n",
      "4619 [Discriminator loss: 0.319495, acc.: 89.06%] [Generator loss: 5.417819]\n",
      "4620 [Discriminator loss: 0.396767, acc.: 85.94%] [Generator loss: 3.430304]\n",
      "4621 [Discriminator loss: 0.191078, acc.: 93.75%] [Generator loss: 4.594693]\n",
      "4622 [Discriminator loss: 0.276289, acc.: 89.06%] [Generator loss: 4.408630]\n",
      "4623 [Discriminator loss: 0.273958, acc.: 81.25%] [Generator loss: 5.558906]\n",
      "4624 [Discriminator loss: 0.139450, acc.: 92.19%] [Generator loss: 4.387502]\n",
      "4625 [Discriminator loss: 0.392767, acc.: 82.81%] [Generator loss: 4.897827]\n",
      "4626 [Discriminator loss: 0.202705, acc.: 89.06%] [Generator loss: 3.955538]\n",
      "4627 [Discriminator loss: 0.318931, acc.: 85.94%] [Generator loss: 4.521574]\n",
      "4628 [Discriminator loss: 0.171150, acc.: 93.75%] [Generator loss: 5.508892]\n",
      "4629 [Discriminator loss: 0.444848, acc.: 84.38%] [Generator loss: 3.871455]\n",
      "4630 [Discriminator loss: 0.168676, acc.: 93.75%] [Generator loss: 5.083127]\n",
      "4631 [Discriminator loss: 0.498997, acc.: 84.38%] [Generator loss: 3.660103]\n",
      "4632 [Discriminator loss: 0.269792, acc.: 84.38%] [Generator loss: 5.211712]\n",
      "4633 [Discriminator loss: 0.330788, acc.: 82.81%] [Generator loss: 5.205714]\n",
      "4634 [Discriminator loss: 0.368372, acc.: 82.81%] [Generator loss: 4.803286]\n",
      "4635 [Discriminator loss: 0.224398, acc.: 90.62%] [Generator loss: 4.388093]\n",
      "4636 [Discriminator loss: 0.106376, acc.: 98.44%] [Generator loss: 4.071029]\n",
      "4637 [Discriminator loss: 0.202519, acc.: 92.19%] [Generator loss: 4.286283]\n",
      "4638 [Discriminator loss: 0.211535, acc.: 90.62%] [Generator loss: 3.487373]\n",
      "4639 [Discriminator loss: 0.286781, acc.: 85.94%] [Generator loss: 4.403615]\n",
      "4640 [Discriminator loss: 0.227714, acc.: 89.06%] [Generator loss: 4.279016]\n",
      "4641 [Discriminator loss: 0.225504, acc.: 92.19%] [Generator loss: 5.131755]\n",
      "4642 [Discriminator loss: 0.340413, acc.: 84.38%] [Generator loss: 5.150695]\n",
      "4643 [Discriminator loss: 0.297747, acc.: 85.94%] [Generator loss: 4.648399]\n",
      "4644 [Discriminator loss: 0.321790, acc.: 87.50%] [Generator loss: 4.179762]\n",
      "4645 [Discriminator loss: 0.254935, acc.: 90.62%] [Generator loss: 4.746279]\n",
      "4646 [Discriminator loss: 0.358158, acc.: 84.38%] [Generator loss: 4.386000]\n",
      "4647 [Discriminator loss: 0.488688, acc.: 79.69%] [Generator loss: 4.456639]\n",
      "4648 [Discriminator loss: 0.130469, acc.: 96.88%] [Generator loss: 5.319322]\n",
      "4649 [Discriminator loss: 0.273750, acc.: 89.06%] [Generator loss: 3.646781]\n",
      "4650 [Discriminator loss: 0.355742, acc.: 81.25%] [Generator loss: 4.588768]\n",
      "4651 [Discriminator loss: 0.118997, acc.: 95.31%] [Generator loss: 4.683295]\n",
      "4652 [Discriminator loss: 0.278457, acc.: 90.62%] [Generator loss: 3.828095]\n",
      "4653 [Discriminator loss: 0.211298, acc.: 93.75%] [Generator loss: 4.075939]\n",
      "4654 [Discriminator loss: 0.175485, acc.: 93.75%] [Generator loss: 4.356280]\n",
      "4655 [Discriminator loss: 0.325931, acc.: 85.94%] [Generator loss: 4.535883]\n",
      "4656 [Discriminator loss: 0.150144, acc.: 93.75%] [Generator loss: 3.897027]\n",
      "4657 [Discriminator loss: 0.268795, acc.: 87.50%] [Generator loss: 4.332926]\n",
      "4658 [Discriminator loss: 0.078605, acc.: 100.00%] [Generator loss: 4.363640]\n",
      "4659 [Discriminator loss: 0.190998, acc.: 93.75%] [Generator loss: 3.202246]\n",
      "4660 [Discriminator loss: 0.275014, acc.: 84.38%] [Generator loss: 4.148965]\n",
      "4661 [Discriminator loss: 0.259929, acc.: 90.62%] [Generator loss: 3.885399]\n",
      "4662 [Discriminator loss: 0.339069, acc.: 87.50%] [Generator loss: 4.454288]\n",
      "4663 [Discriminator loss: 0.316506, acc.: 87.50%] [Generator loss: 5.264722]\n",
      "4664 [Discriminator loss: 0.197010, acc.: 93.75%] [Generator loss: 5.320811]\n",
      "4665 [Discriminator loss: 0.211262, acc.: 92.19%] [Generator loss: 4.942195]\n",
      "4666 [Discriminator loss: 0.275099, acc.: 84.38%] [Generator loss: 4.042703]\n",
      "4667 [Discriminator loss: 0.397304, acc.: 82.81%] [Generator loss: 5.818524]\n",
      "4668 [Discriminator loss: 0.217779, acc.: 89.06%] [Generator loss: 4.745761]\n",
      "4669 [Discriminator loss: 0.222891, acc.: 93.75%] [Generator loss: 4.278254]\n",
      "4670 [Discriminator loss: 0.295376, acc.: 84.38%] [Generator loss: 4.173829]\n",
      "4671 [Discriminator loss: 0.307900, acc.: 87.50%] [Generator loss: 4.795607]\n",
      "4672 [Discriminator loss: 0.328194, acc.: 89.06%] [Generator loss: 3.815687]\n",
      "4673 [Discriminator loss: 0.122377, acc.: 95.31%] [Generator loss: 4.209151]\n",
      "4674 [Discriminator loss: 0.265573, acc.: 87.50%] [Generator loss: 3.913527]\n",
      "4675 [Discriminator loss: 0.163715, acc.: 95.31%] [Generator loss: 4.053325]\n",
      "4676 [Discriminator loss: 0.208368, acc.: 92.19%] [Generator loss: 4.746463]\n",
      "4677 [Discriminator loss: 0.344761, acc.: 84.38%] [Generator loss: 4.815772]\n",
      "4678 [Discriminator loss: 0.342230, acc.: 84.38%] [Generator loss: 4.124916]\n",
      "4679 [Discriminator loss: 0.301009, acc.: 85.94%] [Generator loss: 4.927904]\n",
      "4680 [Discriminator loss: 0.174349, acc.: 92.19%] [Generator loss: 4.104790]\n",
      "4681 [Discriminator loss: 0.187519, acc.: 92.19%] [Generator loss: 4.538658]\n",
      "4682 [Discriminator loss: 0.182192, acc.: 95.31%] [Generator loss: 5.273616]\n",
      "4683 [Discriminator loss: 0.133746, acc.: 95.31%] [Generator loss: 5.432396]\n",
      "4684 [Discriminator loss: 0.196837, acc.: 92.19%] [Generator loss: 4.590352]\n",
      "4685 [Discriminator loss: 0.246284, acc.: 87.50%] [Generator loss: 3.856767]\n",
      "4686 [Discriminator loss: 0.247505, acc.: 87.50%] [Generator loss: 3.810023]\n",
      "4687 [Discriminator loss: 0.149035, acc.: 90.62%] [Generator loss: 4.337334]\n",
      "4688 [Discriminator loss: 0.270110, acc.: 92.19%] [Generator loss: 3.784343]\n",
      "4689 [Discriminator loss: 0.274077, acc.: 89.06%] [Generator loss: 4.022813]\n",
      "4690 [Discriminator loss: 0.352602, acc.: 85.94%] [Generator loss: 4.388150]\n",
      "4691 [Discriminator loss: 0.173086, acc.: 93.75%] [Generator loss: 5.180415]\n",
      "4692 [Discriminator loss: 0.247635, acc.: 87.50%] [Generator loss: 4.614641]\n",
      "4693 [Discriminator loss: 0.198391, acc.: 90.62%] [Generator loss: 4.148153]\n",
      "4694 [Discriminator loss: 0.384987, acc.: 79.69%] [Generator loss: 5.824654]\n",
      "4695 [Discriminator loss: 0.182934, acc.: 92.19%] [Generator loss: 3.985547]\n",
      "4696 [Discriminator loss: 0.098704, acc.: 98.44%] [Generator loss: 4.455372]\n",
      "4697 [Discriminator loss: 0.137773, acc.: 95.31%] [Generator loss: 3.566603]\n",
      "4698 [Discriminator loss: 0.146515, acc.: 95.31%] [Generator loss: 4.533784]\n",
      "4699 [Discriminator loss: 0.267930, acc.: 92.19%] [Generator loss: 4.662573]\n",
      "4700 [Discriminator loss: 0.298390, acc.: 87.50%] [Generator loss: 5.243174]\n",
      "4701 [Discriminator loss: 0.187376, acc.: 90.62%] [Generator loss: 4.459025]\n",
      "4702 [Discriminator loss: 0.218230, acc.: 93.75%] [Generator loss: 4.629475]\n",
      "4703 [Discriminator loss: 0.272959, acc.: 85.94%] [Generator loss: 5.499009]\n",
      "4704 [Discriminator loss: 0.399272, acc.: 81.25%] [Generator loss: 4.453811]\n",
      "4705 [Discriminator loss: 0.127889, acc.: 93.75%] [Generator loss: 4.720214]\n",
      "4706 [Discriminator loss: 0.213607, acc.: 90.62%] [Generator loss: 5.495025]\n",
      "4707 [Discriminator loss: 0.261186, acc.: 93.75%] [Generator loss: 4.010735]\n",
      "4708 [Discriminator loss: 0.297979, acc.: 81.25%] [Generator loss: 4.812573]\n",
      "4709 [Discriminator loss: 0.106318, acc.: 96.88%] [Generator loss: 4.705579]\n",
      "4710 [Discriminator loss: 0.507594, acc.: 78.12%] [Generator loss: 3.897244]\n",
      "4711 [Discriminator loss: 0.192301, acc.: 95.31%] [Generator loss: 4.658480]\n",
      "4712 [Discriminator loss: 0.152016, acc.: 93.75%] [Generator loss: 5.626597]\n",
      "4713 [Discriminator loss: 0.116164, acc.: 96.88%] [Generator loss: 4.394505]\n",
      "4714 [Discriminator loss: 0.422819, acc.: 78.12%] [Generator loss: 3.973370]\n",
      "4715 [Discriminator loss: 0.135728, acc.: 96.88%] [Generator loss: 5.028250]\n",
      "4716 [Discriminator loss: 0.299523, acc.: 92.19%] [Generator loss: 4.151503]\n",
      "4717 [Discriminator loss: 0.407436, acc.: 81.25%] [Generator loss: 3.591409]\n",
      "4718 [Discriminator loss: 0.275984, acc.: 89.06%] [Generator loss: 5.227024]\n",
      "4719 [Discriminator loss: 0.202725, acc.: 90.62%] [Generator loss: 4.975781]\n",
      "4720 [Discriminator loss: 0.125947, acc.: 96.88%] [Generator loss: 4.157261]\n",
      "4721 [Discriminator loss: 0.362187, acc.: 85.94%] [Generator loss: 4.509319]\n",
      "4722 [Discriminator loss: 0.155773, acc.: 95.31%] [Generator loss: 4.721784]\n",
      "4723 [Discriminator loss: 0.354548, acc.: 81.25%] [Generator loss: 4.543537]\n",
      "4724 [Discriminator loss: 0.169699, acc.: 93.75%] [Generator loss: 4.699994]\n",
      "4725 [Discriminator loss: 0.338796, acc.: 84.38%] [Generator loss: 4.400945]\n",
      "4726 [Discriminator loss: 0.412131, acc.: 87.50%] [Generator loss: 4.978133]\n",
      "4727 [Discriminator loss: 0.175360, acc.: 95.31%] [Generator loss: 4.368220]\n",
      "4728 [Discriminator loss: 0.226862, acc.: 95.31%] [Generator loss: 4.019978]\n",
      "4729 [Discriminator loss: 0.254560, acc.: 89.06%] [Generator loss: 4.717938]\n",
      "4730 [Discriminator loss: 0.189585, acc.: 95.31%] [Generator loss: 4.445516]\n",
      "4731 [Discriminator loss: 0.211122, acc.: 92.19%] [Generator loss: 3.884788]\n",
      "4732 [Discriminator loss: 0.185897, acc.: 90.62%] [Generator loss: 4.999182]\n",
      "4733 [Discriminator loss: 0.112296, acc.: 95.31%] [Generator loss: 4.589067]\n",
      "4734 [Discriminator loss: 0.250777, acc.: 89.06%] [Generator loss: 4.131908]\n",
      "4735 [Discriminator loss: 0.207858, acc.: 93.75%] [Generator loss: 4.306777]\n",
      "4736 [Discriminator loss: 0.219596, acc.: 89.06%] [Generator loss: 4.263184]\n",
      "4737 [Discriminator loss: 0.249200, acc.: 90.62%] [Generator loss: 4.167220]\n",
      "4738 [Discriminator loss: 0.324388, acc.: 82.81%] [Generator loss: 4.585791]\n",
      "4739 [Discriminator loss: 0.141187, acc.: 95.31%] [Generator loss: 4.521476]\n",
      "4740 [Discriminator loss: 0.225015, acc.: 92.19%] [Generator loss: 4.416222]\n",
      "4741 [Discriminator loss: 0.210462, acc.: 90.62%] [Generator loss: 4.273784]\n",
      "4742 [Discriminator loss: 0.159788, acc.: 95.31%] [Generator loss: 4.201251]\n",
      "4743 [Discriminator loss: 0.223526, acc.: 89.06%] [Generator loss: 3.407930]\n",
      "4744 [Discriminator loss: 0.187635, acc.: 92.19%] [Generator loss: 4.554927]\n",
      "4745 [Discriminator loss: 0.339751, acc.: 84.38%] [Generator loss: 3.835953]\n",
      "4746 [Discriminator loss: 0.505483, acc.: 79.69%] [Generator loss: 5.178071]\n",
      "4747 [Discriminator loss: 0.356110, acc.: 85.94%] [Generator loss: 3.849496]\n",
      "4748 [Discriminator loss: 0.149317, acc.: 95.31%] [Generator loss: 4.099563]\n",
      "4749 [Discriminator loss: 0.136036, acc.: 93.75%] [Generator loss: 4.680698]\n",
      "4750 [Discriminator loss: 0.213853, acc.: 92.19%] [Generator loss: 4.401006]\n",
      "4751 [Discriminator loss: 0.164426, acc.: 95.31%] [Generator loss: 5.220876]\n",
      "4752 [Discriminator loss: 0.319773, acc.: 89.06%] [Generator loss: 3.928911]\n",
      "4753 [Discriminator loss: 0.210433, acc.: 93.75%] [Generator loss: 3.816229]\n",
      "4754 [Discriminator loss: 0.437575, acc.: 82.81%] [Generator loss: 5.707347]\n",
      "4755 [Discriminator loss: 0.431948, acc.: 78.12%] [Generator loss: 5.183502]\n",
      "4756 [Discriminator loss: 0.416288, acc.: 79.69%] [Generator loss: 3.651066]\n",
      "4757 [Discriminator loss: 0.269259, acc.: 87.50%] [Generator loss: 4.824525]\n",
      "4758 [Discriminator loss: 0.265218, acc.: 89.06%] [Generator loss: 5.703963]\n",
      "4759 [Discriminator loss: 0.231864, acc.: 87.50%] [Generator loss: 3.837156]\n",
      "4760 [Discriminator loss: 0.304994, acc.: 84.38%] [Generator loss: 4.764903]\n",
      "4761 [Discriminator loss: 0.357160, acc.: 85.94%] [Generator loss: 3.598099]\n",
      "4762 [Discriminator loss: 0.140863, acc.: 93.75%] [Generator loss: 4.789355]\n",
      "4763 [Discriminator loss: 0.346708, acc.: 84.38%] [Generator loss: 4.601776]\n",
      "4764 [Discriminator loss: 0.342744, acc.: 87.50%] [Generator loss: 4.831894]\n",
      "4765 [Discriminator loss: 0.249931, acc.: 90.62%] [Generator loss: 4.564963]\n",
      "4766 [Discriminator loss: 0.207202, acc.: 92.19%] [Generator loss: 4.724893]\n",
      "4767 [Discriminator loss: 0.249834, acc.: 92.19%] [Generator loss: 4.281826]\n",
      "4768 [Discriminator loss: 0.567186, acc.: 76.56%] [Generator loss: 6.371075]\n",
      "4769 [Discriminator loss: 0.538846, acc.: 78.12%] [Generator loss: 3.721941]\n",
      "4770 [Discriminator loss: 0.181813, acc.: 92.19%] [Generator loss: 4.164742]\n",
      "4771 [Discriminator loss: 0.203353, acc.: 90.62%] [Generator loss: 4.296483]\n",
      "4772 [Discriminator loss: 0.117499, acc.: 95.31%] [Generator loss: 4.895499]\n",
      "4773 [Discriminator loss: 0.177354, acc.: 93.75%] [Generator loss: 4.223703]\n",
      "4774 [Discriminator loss: 0.453878, acc.: 82.81%] [Generator loss: 4.414536]\n",
      "4775 [Discriminator loss: 0.206764, acc.: 92.19%] [Generator loss: 3.776773]\n",
      "4776 [Discriminator loss: 0.378751, acc.: 84.38%] [Generator loss: 4.660569]\n",
      "4777 [Discriminator loss: 0.200626, acc.: 93.75%] [Generator loss: 5.436764]\n",
      "4778 [Discriminator loss: 0.266634, acc.: 87.50%] [Generator loss: 3.930353]\n",
      "4779 [Discriminator loss: 0.548649, acc.: 73.44%] [Generator loss: 6.168301]\n",
      "4780 [Discriminator loss: 0.262196, acc.: 89.06%] [Generator loss: 3.783746]\n",
      "4781 [Discriminator loss: 0.537120, acc.: 78.12%] [Generator loss: 5.229613]\n",
      "4782 [Discriminator loss: 0.154720, acc.: 90.62%] [Generator loss: 5.490137]\n",
      "4783 [Discriminator loss: 0.384352, acc.: 76.56%] [Generator loss: 5.530982]\n",
      "4784 [Discriminator loss: 0.178941, acc.: 93.75%] [Generator loss: 5.047747]\n",
      "4785 [Discriminator loss: 0.268914, acc.: 89.06%] [Generator loss: 4.780421]\n",
      "4786 [Discriminator loss: 0.237334, acc.: 89.06%] [Generator loss: 4.477423]\n",
      "4787 [Discriminator loss: 0.368771, acc.: 78.12%] [Generator loss: 4.168048]\n",
      "4788 [Discriminator loss: 0.134917, acc.: 95.31%] [Generator loss: 4.359251]\n",
      "4789 [Discriminator loss: 0.202308, acc.: 90.62%] [Generator loss: 3.954265]\n",
      "4790 [Discriminator loss: 0.376308, acc.: 82.81%] [Generator loss: 4.395455]\n",
      "4791 [Discriminator loss: 0.252971, acc.: 89.06%] [Generator loss: 4.958390]\n",
      "4792 [Discriminator loss: 0.251414, acc.: 90.62%] [Generator loss: 3.782670]\n",
      "4793 [Discriminator loss: 0.214492, acc.: 90.62%] [Generator loss: 4.772587]\n",
      "4794 [Discriminator loss: 0.252247, acc.: 92.19%] [Generator loss: 4.425895]\n",
      "4795 [Discriminator loss: 0.241815, acc.: 90.62%] [Generator loss: 5.033232]\n",
      "4796 [Discriminator loss: 0.315726, acc.: 87.50%] [Generator loss: 4.665388]\n",
      "4797 [Discriminator loss: 0.175096, acc.: 93.75%] [Generator loss: 4.253850]\n",
      "4798 [Discriminator loss: 0.309023, acc.: 90.62%] [Generator loss: 5.511370]\n",
      "4799 [Discriminator loss: 0.280216, acc.: 90.62%] [Generator loss: 4.078899]\n",
      "4800 [Discriminator loss: 0.266018, acc.: 87.50%] [Generator loss: 4.565680]\n",
      "4801 [Discriminator loss: 0.325659, acc.: 84.38%] [Generator loss: 4.834746]\n",
      "4802 [Discriminator loss: 0.228630, acc.: 89.06%] [Generator loss: 5.345400]\n",
      "4803 [Discriminator loss: 0.175167, acc.: 90.62%] [Generator loss: 5.616786]\n",
      "4804 [Discriminator loss: 0.206379, acc.: 95.31%] [Generator loss: 3.279898]\n",
      "4805 [Discriminator loss: 0.340524, acc.: 82.81%] [Generator loss: 5.030804]\n",
      "4806 [Discriminator loss: 0.216816, acc.: 92.19%] [Generator loss: 4.443295]\n",
      "4807 [Discriminator loss: 0.248114, acc.: 93.75%] [Generator loss: 5.332268]\n",
      "4808 [Discriminator loss: 0.197899, acc.: 85.94%] [Generator loss: 5.448678]\n",
      "4809 [Discriminator loss: 0.415141, acc.: 79.69%] [Generator loss: 5.068173]\n",
      "4810 [Discriminator loss: 0.101440, acc.: 95.31%] [Generator loss: 5.349064]\n",
      "4811 [Discriminator loss: 0.257814, acc.: 85.94%] [Generator loss: 4.995791]\n",
      "4812 [Discriminator loss: 0.392709, acc.: 81.25%] [Generator loss: 3.496562]\n",
      "4813 [Discriminator loss: 0.108789, acc.: 96.88%] [Generator loss: 5.710132]\n",
      "4814 [Discriminator loss: 0.290963, acc.: 90.62%] [Generator loss: 4.929945]\n",
      "4815 [Discriminator loss: 0.333178, acc.: 87.50%] [Generator loss: 3.852670]\n",
      "4816 [Discriminator loss: 0.207249, acc.: 95.31%] [Generator loss: 5.019863]\n",
      "4817 [Discriminator loss: 0.238079, acc.: 92.19%] [Generator loss: 5.448374]\n",
      "4818 [Discriminator loss: 0.208912, acc.: 90.62%] [Generator loss: 5.127144]\n",
      "4819 [Discriminator loss: 0.358849, acc.: 84.38%] [Generator loss: 5.175296]\n",
      "4820 [Discriminator loss: 0.419445, acc.: 85.94%] [Generator loss: 4.812711]\n",
      "4821 [Discriminator loss: 0.147131, acc.: 95.31%] [Generator loss: 4.785542]\n",
      "4822 [Discriminator loss: 0.147854, acc.: 95.31%] [Generator loss: 4.062973]\n",
      "4823 [Discriminator loss: 0.285061, acc.: 87.50%] [Generator loss: 5.508916]\n",
      "4824 [Discriminator loss: 0.268569, acc.: 92.19%] [Generator loss: 3.475213]\n",
      "4825 [Discriminator loss: 0.293316, acc.: 89.06%] [Generator loss: 5.367413]\n",
      "4826 [Discriminator loss: 0.168267, acc.: 95.31%] [Generator loss: 2.925566]\n",
      "4827 [Discriminator loss: 0.309703, acc.: 84.38%] [Generator loss: 5.033553]\n",
      "4828 [Discriminator loss: 0.249177, acc.: 87.50%] [Generator loss: 4.245193]\n",
      "4829 [Discriminator loss: 0.820021, acc.: 65.62%] [Generator loss: 5.775781]\n",
      "4830 [Discriminator loss: 0.239643, acc.: 89.06%] [Generator loss: 4.778302]\n",
      "4831 [Discriminator loss: 0.387316, acc.: 84.38%] [Generator loss: 4.309752]\n",
      "4832 [Discriminator loss: 0.178236, acc.: 93.75%] [Generator loss: 5.771585]\n",
      "4833 [Discriminator loss: 0.104762, acc.: 96.88%] [Generator loss: 4.993271]\n",
      "4834 [Discriminator loss: 0.438955, acc.: 78.12%] [Generator loss: 5.443227]\n",
      "4835 [Discriminator loss: 0.334053, acc.: 84.38%] [Generator loss: 4.887011]\n",
      "4836 [Discriminator loss: 0.130123, acc.: 95.31%] [Generator loss: 4.309881]\n",
      "4837 [Discriminator loss: 0.183478, acc.: 95.31%] [Generator loss: 4.281702]\n",
      "4838 [Discriminator loss: 0.220110, acc.: 92.19%] [Generator loss: 5.066190]\n",
      "4839 [Discriminator loss: 0.148724, acc.: 95.31%] [Generator loss: 4.900078]\n",
      "4840 [Discriminator loss: 0.257020, acc.: 89.06%] [Generator loss: 4.876684]\n",
      "4841 [Discriminator loss: 0.234917, acc.: 89.06%] [Generator loss: 5.348731]\n",
      "4842 [Discriminator loss: 0.247003, acc.: 93.75%] [Generator loss: 4.780874]\n",
      "4843 [Discriminator loss: 0.231434, acc.: 90.62%] [Generator loss: 4.606306]\n",
      "4844 [Discriminator loss: 0.293649, acc.: 93.75%] [Generator loss: 4.189038]\n",
      "4845 [Discriminator loss: 0.223164, acc.: 89.06%] [Generator loss: 4.271320]\n",
      "4846 [Discriminator loss: 0.314301, acc.: 84.38%] [Generator loss: 4.790581]\n",
      "4847 [Discriminator loss: 0.130778, acc.: 96.88%] [Generator loss: 5.112204]\n",
      "4848 [Discriminator loss: 0.155214, acc.: 93.75%] [Generator loss: 4.275272]\n",
      "4849 [Discriminator loss: 0.302600, acc.: 84.38%] [Generator loss: 5.748243]\n",
      "4850 [Discriminator loss: 0.335597, acc.: 85.94%] [Generator loss: 5.102237]\n",
      "4851 [Discriminator loss: 0.178955, acc.: 90.62%] [Generator loss: 4.054118]\n",
      "4852 [Discriminator loss: 0.208916, acc.: 90.62%] [Generator loss: 4.539618]\n",
      "4853 [Discriminator loss: 0.172963, acc.: 93.75%] [Generator loss: 5.250899]\n",
      "4854 [Discriminator loss: 0.168823, acc.: 95.31%] [Generator loss: 3.984765]\n",
      "4855 [Discriminator loss: 0.270340, acc.: 85.94%] [Generator loss: 4.815220]\n",
      "4856 [Discriminator loss: 0.215436, acc.: 93.75%] [Generator loss: 5.134405]\n",
      "4857 [Discriminator loss: 0.355728, acc.: 82.81%] [Generator loss: 4.310291]\n",
      "4858 [Discriminator loss: 0.375354, acc.: 84.38%] [Generator loss: 5.168636]\n",
      "4859 [Discriminator loss: 0.145969, acc.: 95.31%] [Generator loss: 4.435080]\n",
      "4860 [Discriminator loss: 0.376487, acc.: 82.81%] [Generator loss: 4.402907]\n",
      "4861 [Discriminator loss: 0.238783, acc.: 89.06%] [Generator loss: 5.004128]\n",
      "4862 [Discriminator loss: 0.208235, acc.: 96.88%] [Generator loss: 4.923754]\n",
      "4863 [Discriminator loss: 0.230687, acc.: 87.50%] [Generator loss: 5.073993]\n",
      "4864 [Discriminator loss: 0.203219, acc.: 92.19%] [Generator loss: 4.627017]\n",
      "4865 [Discriminator loss: 0.129980, acc.: 95.31%] [Generator loss: 5.297273]\n",
      "4866 [Discriminator loss: 0.152444, acc.: 93.75%] [Generator loss: 3.561212]\n",
      "4867 [Discriminator loss: 0.302637, acc.: 87.50%] [Generator loss: 6.218243]\n",
      "4868 [Discriminator loss: 0.241134, acc.: 85.94%] [Generator loss: 4.781938]\n",
      "4869 [Discriminator loss: 0.332674, acc.: 84.38%] [Generator loss: 5.113704]\n",
      "4870 [Discriminator loss: 0.367440, acc.: 84.38%] [Generator loss: 4.130725]\n",
      "4871 [Discriminator loss: 0.215272, acc.: 92.19%] [Generator loss: 5.148607]\n",
      "4872 [Discriminator loss: 0.316507, acc.: 89.06%] [Generator loss: 4.349713]\n",
      "4873 [Discriminator loss: 0.292294, acc.: 85.94%] [Generator loss: 5.695029]\n",
      "4874 [Discriminator loss: 0.384592, acc.: 85.94%] [Generator loss: 3.896230]\n",
      "4875 [Discriminator loss: 0.508218, acc.: 73.44%] [Generator loss: 5.006857]\n",
      "4876 [Discriminator loss: 0.217017, acc.: 90.62%] [Generator loss: 4.576628]\n",
      "4877 [Discriminator loss: 0.274987, acc.: 90.62%] [Generator loss: 4.589204]\n",
      "4878 [Discriminator loss: 0.201626, acc.: 90.62%] [Generator loss: 4.367752]\n",
      "4879 [Discriminator loss: 0.376967, acc.: 81.25%] [Generator loss: 5.920321]\n",
      "4880 [Discriminator loss: 0.087661, acc.: 96.88%] [Generator loss: 5.686052]\n",
      "4881 [Discriminator loss: 0.228425, acc.: 90.62%] [Generator loss: 3.536232]\n",
      "4882 [Discriminator loss: 0.177009, acc.: 95.31%] [Generator loss: 4.028903]\n",
      "4883 [Discriminator loss: 0.151192, acc.: 92.19%] [Generator loss: 4.270715]\n",
      "4884 [Discriminator loss: 0.162194, acc.: 92.19%] [Generator loss: 4.630524]\n",
      "4885 [Discriminator loss: 0.158027, acc.: 96.88%] [Generator loss: 4.314751]\n",
      "4886 [Discriminator loss: 0.219703, acc.: 89.06%] [Generator loss: 4.836599]\n",
      "4887 [Discriminator loss: 0.345143, acc.: 84.38%] [Generator loss: 4.166286]\n",
      "4888 [Discriminator loss: 0.210515, acc.: 89.06%] [Generator loss: 4.685735]\n",
      "4889 [Discriminator loss: 0.363831, acc.: 85.94%] [Generator loss: 4.626604]\n",
      "4890 [Discriminator loss: 0.096739, acc.: 98.44%] [Generator loss: 4.698847]\n",
      "4891 [Discriminator loss: 0.300377, acc.: 85.94%] [Generator loss: 5.007106]\n",
      "4892 [Discriminator loss: 0.188521, acc.: 93.75%] [Generator loss: 3.779423]\n",
      "4893 [Discriminator loss: 0.260361, acc.: 90.62%] [Generator loss: 4.423875]\n",
      "4894 [Discriminator loss: 0.265516, acc.: 87.50%] [Generator loss: 4.631629]\n",
      "4895 [Discriminator loss: 0.231536, acc.: 87.50%] [Generator loss: 4.718368]\n",
      "4896 [Discriminator loss: 0.363414, acc.: 85.94%] [Generator loss: 5.389688]\n",
      "4897 [Discriminator loss: 0.291183, acc.: 85.94%] [Generator loss: 5.363101]\n",
      "4898 [Discriminator loss: 0.252891, acc.: 90.62%] [Generator loss: 2.983220]\n",
      "4899 [Discriminator loss: 0.320237, acc.: 85.94%] [Generator loss: 4.607252]\n",
      "4900 [Discriminator loss: 0.172707, acc.: 93.75%] [Generator loss: 5.179769]\n",
      "4901 [Discriminator loss: 0.378163, acc.: 90.62%] [Generator loss: 4.950483]\n",
      "4902 [Discriminator loss: 0.220520, acc.: 92.19%] [Generator loss: 3.182347]\n",
      "4903 [Discriminator loss: 0.363947, acc.: 81.25%] [Generator loss: 5.597973]\n",
      "4904 [Discriminator loss: 0.347131, acc.: 85.94%] [Generator loss: 4.221651]\n",
      "4905 [Discriminator loss: 0.399980, acc.: 81.25%] [Generator loss: 6.186876]\n",
      "4906 [Discriminator loss: 0.255928, acc.: 90.62%] [Generator loss: 5.689418]\n",
      "4907 [Discriminator loss: 0.340290, acc.: 82.81%] [Generator loss: 5.656862]\n",
      "4908 [Discriminator loss: 0.089507, acc.: 98.44%] [Generator loss: 4.424291]\n",
      "4909 [Discriminator loss: 0.407537, acc.: 87.50%] [Generator loss: 4.746943]\n",
      "4910 [Discriminator loss: 0.275112, acc.: 81.25%] [Generator loss: 4.862059]\n",
      "4911 [Discriminator loss: 0.171788, acc.: 93.75%] [Generator loss: 4.403308]\n",
      "4912 [Discriminator loss: 0.355366, acc.: 89.06%] [Generator loss: 4.690643]\n",
      "4913 [Discriminator loss: 0.104460, acc.: 95.31%] [Generator loss: 4.967450]\n",
      "4914 [Discriminator loss: 0.222017, acc.: 92.19%] [Generator loss: 4.015460]\n",
      "4915 [Discriminator loss: 0.367561, acc.: 85.94%] [Generator loss: 3.975543]\n",
      "4916 [Discriminator loss: 0.146134, acc.: 96.88%] [Generator loss: 5.208886]\n",
      "4917 [Discriminator loss: 0.187636, acc.: 90.62%] [Generator loss: 3.767626]\n",
      "4918 [Discriminator loss: 0.391117, acc.: 90.62%] [Generator loss: 4.662791]\n",
      "4919 [Discriminator loss: 0.107466, acc.: 95.31%] [Generator loss: 5.815212]\n",
      "4920 [Discriminator loss: 0.218035, acc.: 90.62%] [Generator loss: 4.633088]\n",
      "4921 [Discriminator loss: 0.155207, acc.: 93.75%] [Generator loss: 4.316304]\n",
      "4922 [Discriminator loss: 0.490255, acc.: 79.69%] [Generator loss: 5.004080]\n",
      "4923 [Discriminator loss: 0.143718, acc.: 95.31%] [Generator loss: 4.466654]\n",
      "4924 [Discriminator loss: 0.339663, acc.: 82.81%] [Generator loss: 4.708272]\n",
      "4925 [Discriminator loss: 0.158636, acc.: 95.31%] [Generator loss: 3.812015]\n",
      "4926 [Discriminator loss: 0.390941, acc.: 84.38%] [Generator loss: 4.376502]\n",
      "4927 [Discriminator loss: 0.096222, acc.: 93.75%] [Generator loss: 5.046950]\n",
      "4928 [Discriminator loss: 0.440334, acc.: 79.69%] [Generator loss: 4.493288]\n",
      "4929 [Discriminator loss: 0.146055, acc.: 95.31%] [Generator loss: 4.246713]\n",
      "4930 [Discriminator loss: 0.521358, acc.: 81.25%] [Generator loss: 5.373707]\n",
      "4931 [Discriminator loss: 0.264480, acc.: 87.50%] [Generator loss: 4.086133]\n",
      "4932 [Discriminator loss: 0.312371, acc.: 87.50%] [Generator loss: 5.057607]\n",
      "4933 [Discriminator loss: 0.210740, acc.: 90.62%] [Generator loss: 4.766491]\n",
      "4934 [Discriminator loss: 0.150204, acc.: 95.31%] [Generator loss: 5.486536]\n",
      "4935 [Discriminator loss: 0.289990, acc.: 90.62%] [Generator loss: 5.073413]\n",
      "4936 [Discriminator loss: 0.417900, acc.: 78.12%] [Generator loss: 4.930228]\n",
      "4937 [Discriminator loss: 0.552107, acc.: 73.44%] [Generator loss: 5.969984]\n",
      "4938 [Discriminator loss: 0.218542, acc.: 92.19%] [Generator loss: 5.461314]\n",
      "4939 [Discriminator loss: 0.225502, acc.: 92.19%] [Generator loss: 4.639051]\n",
      "4940 [Discriminator loss: 0.147116, acc.: 96.88%] [Generator loss: 3.840090]\n",
      "4941 [Discriminator loss: 0.265939, acc.: 87.50%] [Generator loss: 5.077093]\n",
      "4942 [Discriminator loss: 0.207548, acc.: 92.19%] [Generator loss: 3.743995]\n",
      "4943 [Discriminator loss: 0.155028, acc.: 96.88%] [Generator loss: 3.706434]\n",
      "4944 [Discriminator loss: 0.380622, acc.: 85.94%] [Generator loss: 6.074268]\n",
      "4945 [Discriminator loss: 0.313240, acc.: 87.50%] [Generator loss: 4.386704]\n",
      "4946 [Discriminator loss: 0.429552, acc.: 79.69%] [Generator loss: 5.441084]\n",
      "4947 [Discriminator loss: 0.311738, acc.: 89.06%] [Generator loss: 4.782341]\n",
      "4948 [Discriminator loss: 0.172253, acc.: 90.62%] [Generator loss: 4.751949]\n",
      "4949 [Discriminator loss: 0.279000, acc.: 90.62%] [Generator loss: 4.565772]\n",
      "4950 [Discriminator loss: 0.140614, acc.: 96.88%] [Generator loss: 4.866164]\n",
      "4951 [Discriminator loss: 0.151122, acc.: 96.88%] [Generator loss: 3.567921]\n",
      "4952 [Discriminator loss: 0.307584, acc.: 87.50%] [Generator loss: 5.207897]\n",
      "4953 [Discriminator loss: 0.305340, acc.: 90.62%] [Generator loss: 3.228857]\n",
      "4954 [Discriminator loss: 0.148828, acc.: 93.75%] [Generator loss: 4.608257]\n",
      "4955 [Discriminator loss: 0.314849, acc.: 84.38%] [Generator loss: 4.623880]\n",
      "4956 [Discriminator loss: 0.399250, acc.: 81.25%] [Generator loss: 5.306133]\n",
      "4957 [Discriminator loss: 0.291285, acc.: 87.50%] [Generator loss: 4.626491]\n",
      "4958 [Discriminator loss: 0.195070, acc.: 90.62%] [Generator loss: 4.878613]\n",
      "4959 [Discriminator loss: 0.165242, acc.: 95.31%] [Generator loss: 4.918720]\n",
      "4960 [Discriminator loss: 0.459911, acc.: 76.56%] [Generator loss: 4.778781]\n",
      "4961 [Discriminator loss: 0.276636, acc.: 89.06%] [Generator loss: 6.700027]\n",
      "4962 [Discriminator loss: 0.438618, acc.: 79.69%] [Generator loss: 4.289499]\n",
      "4963 [Discriminator loss: 0.195318, acc.: 89.06%] [Generator loss: 4.811568]\n",
      "4964 [Discriminator loss: 0.175476, acc.: 90.62%] [Generator loss: 5.688983]\n",
      "4965 [Discriminator loss: 0.228605, acc.: 89.06%] [Generator loss: 3.820722]\n",
      "4966 [Discriminator loss: 0.281005, acc.: 92.19%] [Generator loss: 4.555132]\n",
      "4967 [Discriminator loss: 0.324861, acc.: 87.50%] [Generator loss: 5.915827]\n",
      "4968 [Discriminator loss: 0.226055, acc.: 92.19%] [Generator loss: 4.994566]\n",
      "4969 [Discriminator loss: 0.415622, acc.: 81.25%] [Generator loss: 4.001309]\n",
      "4970 [Discriminator loss: 0.153982, acc.: 90.62%] [Generator loss: 5.937606]\n",
      "4971 [Discriminator loss: 0.209618, acc.: 90.62%] [Generator loss: 4.583894]\n",
      "4972 [Discriminator loss: 0.283733, acc.: 87.50%] [Generator loss: 4.372288]\n",
      "4973 [Discriminator loss: 0.319242, acc.: 84.38%] [Generator loss: 4.836000]\n",
      "4974 [Discriminator loss: 0.180630, acc.: 95.31%] [Generator loss: 4.338099]\n",
      "4975 [Discriminator loss: 0.244107, acc.: 90.62%] [Generator loss: 4.552928]\n",
      "4976 [Discriminator loss: 0.287544, acc.: 90.62%] [Generator loss: 4.793599]\n",
      "4977 [Discriminator loss: 0.267940, acc.: 90.62%] [Generator loss: 4.700891]\n",
      "4978 [Discriminator loss: 0.340048, acc.: 84.38%] [Generator loss: 4.420707]\n",
      "4979 [Discriminator loss: 0.217660, acc.: 92.19%] [Generator loss: 4.142097]\n",
      "4980 [Discriminator loss: 0.458006, acc.: 78.12%] [Generator loss: 4.284579]\n",
      "4981 [Discriminator loss: 0.158599, acc.: 93.75%] [Generator loss: 4.988441]\n",
      "4982 [Discriminator loss: 0.185478, acc.: 93.75%] [Generator loss: 4.882679]\n",
      "4983 [Discriminator loss: 0.221131, acc.: 93.75%] [Generator loss: 4.312835]\n",
      "4984 [Discriminator loss: 0.179322, acc.: 92.19%] [Generator loss: 5.459453]\n",
      "4985 [Discriminator loss: 0.336967, acc.: 85.94%] [Generator loss: 4.071974]\n",
      "4986 [Discriminator loss: 0.182618, acc.: 89.06%] [Generator loss: 3.772134]\n",
      "4987 [Discriminator loss: 0.302583, acc.: 85.94%] [Generator loss: 4.351705]\n",
      "4988 [Discriminator loss: 0.314049, acc.: 89.06%] [Generator loss: 3.475139]\n",
      "4989 [Discriminator loss: 0.324555, acc.: 85.94%] [Generator loss: 4.977492]\n",
      "4990 [Discriminator loss: 0.194125, acc.: 87.50%] [Generator loss: 4.449109]\n",
      "4991 [Discriminator loss: 0.306373, acc.: 82.81%] [Generator loss: 4.871971]\n",
      "4992 [Discriminator loss: 0.292216, acc.: 87.50%] [Generator loss: 5.395674]\n",
      "4993 [Discriminator loss: 0.219228, acc.: 92.19%] [Generator loss: 3.994936]\n",
      "4994 [Discriminator loss: 0.334267, acc.: 82.81%] [Generator loss: 5.991718]\n",
      "4995 [Discriminator loss: 0.285737, acc.: 87.50%] [Generator loss: 6.354291]\n",
      "4996 [Discriminator loss: 0.302994, acc.: 87.50%] [Generator loss: 3.945311]\n",
      "4997 [Discriminator loss: 0.195008, acc.: 92.19%] [Generator loss: 4.360748]\n",
      "4998 [Discriminator loss: 0.248543, acc.: 89.06%] [Generator loss: 4.716805]\n",
      "4999 [Discriminator loss: 0.222157, acc.: 93.75%] [Generator loss: 5.472390]\n",
      "5000 [Discriminator loss: 0.393087, acc.: 82.81%] [Generator loss: 5.049560]\n",
      "5001 [Discriminator loss: 0.092291, acc.: 96.88%] [Generator loss: 5.111163]\n",
      "5002 [Discriminator loss: 0.221769, acc.: 90.62%] [Generator loss: 4.431306]\n",
      "5003 [Discriminator loss: 0.219645, acc.: 90.62%] [Generator loss: 5.028268]\n",
      "5004 [Discriminator loss: 0.222405, acc.: 90.62%] [Generator loss: 4.316943]\n",
      "5005 [Discriminator loss: 0.108868, acc.: 96.88%] [Generator loss: 5.715865]\n",
      "5006 [Discriminator loss: 0.165785, acc.: 90.62%] [Generator loss: 4.356798]\n",
      "5007 [Discriminator loss: 0.146341, acc.: 95.31%] [Generator loss: 4.083745]\n",
      "5008 [Discriminator loss: 0.337931, acc.: 84.38%] [Generator loss: 4.851650]\n",
      "5009 [Discriminator loss: 0.277420, acc.: 93.75%] [Generator loss: 4.761599]\n",
      "5010 [Discriminator loss: 0.329994, acc.: 81.25%] [Generator loss: 6.216309]\n",
      "5011 [Discriminator loss: 0.237223, acc.: 89.06%] [Generator loss: 4.080026]\n",
      "5012 [Discriminator loss: 0.279389, acc.: 92.19%] [Generator loss: 5.045793]\n",
      "5013 [Discriminator loss: 0.265631, acc.: 85.94%] [Generator loss: 4.595924]\n",
      "5014 [Discriminator loss: 0.152543, acc.: 96.88%] [Generator loss: 4.981209]\n",
      "5015 [Discriminator loss: 0.348512, acc.: 81.25%] [Generator loss: 4.040599]\n",
      "5016 [Discriminator loss: 0.081944, acc.: 98.44%] [Generator loss: 4.464514]\n",
      "5017 [Discriminator loss: 0.267590, acc.: 89.06%] [Generator loss: 4.619960]\n",
      "5018 [Discriminator loss: 0.093412, acc.: 96.88%] [Generator loss: 5.040090]\n",
      "5019 [Discriminator loss: 0.294517, acc.: 85.94%] [Generator loss: 3.418451]\n",
      "5020 [Discriminator loss: 0.357810, acc.: 84.38%] [Generator loss: 3.985792]\n",
      "5021 [Discriminator loss: 0.218143, acc.: 89.06%] [Generator loss: 5.247054]\n",
      "5022 [Discriminator loss: 0.188764, acc.: 90.62%] [Generator loss: 4.297874]\n",
      "5023 [Discriminator loss: 0.301943, acc.: 81.25%] [Generator loss: 5.088591]\n",
      "5024 [Discriminator loss: 0.336993, acc.: 84.38%] [Generator loss: 5.157964]\n",
      "5025 [Discriminator loss: 0.116170, acc.: 96.88%] [Generator loss: 5.779153]\n",
      "5026 [Discriminator loss: 0.180040, acc.: 93.75%] [Generator loss: 4.418870]\n",
      "5027 [Discriminator loss: 0.145524, acc.: 95.31%] [Generator loss: 4.309649]\n",
      "5028 [Discriminator loss: 0.408742, acc.: 79.69%] [Generator loss: 5.561448]\n",
      "5029 [Discriminator loss: 0.239538, acc.: 90.62%] [Generator loss: 4.646874]\n",
      "5030 [Discriminator loss: 0.198454, acc.: 95.31%] [Generator loss: 4.199280]\n",
      "5031 [Discriminator loss: 0.292194, acc.: 87.50%] [Generator loss: 4.949471]\n",
      "5032 [Discriminator loss: 0.207534, acc.: 89.06%] [Generator loss: 4.420228]\n",
      "5033 [Discriminator loss: 0.290346, acc.: 90.62%] [Generator loss: 4.199771]\n",
      "5034 [Discriminator loss: 0.161636, acc.: 95.31%] [Generator loss: 5.150730]\n",
      "5035 [Discriminator loss: 0.239698, acc.: 90.62%] [Generator loss: 4.100033]\n",
      "5036 [Discriminator loss: 0.402821, acc.: 82.81%] [Generator loss: 5.384479]\n",
      "5037 [Discriminator loss: 0.312953, acc.: 81.25%] [Generator loss: 4.474253]\n",
      "5038 [Discriminator loss: 0.317315, acc.: 87.50%] [Generator loss: 4.642234]\n",
      "5039 [Discriminator loss: 0.123086, acc.: 93.75%] [Generator loss: 5.276324]\n",
      "5040 [Discriminator loss: 0.175016, acc.: 93.75%] [Generator loss: 4.039569]\n",
      "5041 [Discriminator loss: 0.245889, acc.: 87.50%] [Generator loss: 5.759554]\n",
      "5042 [Discriminator loss: 0.114644, acc.: 95.31%] [Generator loss: 5.007050]\n",
      "5043 [Discriminator loss: 0.170857, acc.: 92.19%] [Generator loss: 3.831022]\n",
      "5044 [Discriminator loss: 0.083853, acc.: 96.88%] [Generator loss: 4.522168]\n",
      "5045 [Discriminator loss: 0.387133, acc.: 75.00%] [Generator loss: 5.408241]\n",
      "5046 [Discriminator loss: 0.345709, acc.: 85.94%] [Generator loss: 4.932339]\n",
      "5047 [Discriminator loss: 0.242814, acc.: 85.94%] [Generator loss: 3.819180]\n",
      "5048 [Discriminator loss: 0.229227, acc.: 90.62%] [Generator loss: 5.537006]\n",
      "5049 [Discriminator loss: 0.241838, acc.: 85.94%] [Generator loss: 5.749705]\n",
      "5050 [Discriminator loss: 0.165448, acc.: 93.75%] [Generator loss: 4.905625]\n",
      "5051 [Discriminator loss: 0.210422, acc.: 93.75%] [Generator loss: 4.122093]\n",
      "5052 [Discriminator loss: 0.187526, acc.: 93.75%] [Generator loss: 4.220312]\n",
      "5053 [Discriminator loss: 0.246316, acc.: 90.62%] [Generator loss: 5.104998]\n",
      "5054 [Discriminator loss: 0.197090, acc.: 90.62%] [Generator loss: 4.367957]\n",
      "5055 [Discriminator loss: 0.523962, acc.: 75.00%] [Generator loss: 5.708563]\n",
      "5056 [Discriminator loss: 0.147076, acc.: 96.88%] [Generator loss: 5.503078]\n",
      "5057 [Discriminator loss: 0.211738, acc.: 87.50%] [Generator loss: 3.927741]\n",
      "5058 [Discriminator loss: 0.184044, acc.: 93.75%] [Generator loss: 3.946353]\n",
      "5059 [Discriminator loss: 0.312964, acc.: 84.38%] [Generator loss: 4.418607]\n",
      "5060 [Discriminator loss: 0.097559, acc.: 98.44%] [Generator loss: 5.407840]\n",
      "5061 [Discriminator loss: 0.194286, acc.: 93.75%] [Generator loss: 4.470115]\n",
      "5062 [Discriminator loss: 0.245146, acc.: 89.06%] [Generator loss: 5.034380]\n",
      "5063 [Discriminator loss: 0.354713, acc.: 87.50%] [Generator loss: 4.600047]\n",
      "5064 [Discriminator loss: 0.236599, acc.: 90.62%] [Generator loss: 4.294431]\n",
      "5065 [Discriminator loss: 0.270701, acc.: 85.94%] [Generator loss: 6.637903]\n",
      "5066 [Discriminator loss: 0.258826, acc.: 89.06%] [Generator loss: 3.942948]\n",
      "5067 [Discriminator loss: 0.298470, acc.: 95.31%] [Generator loss: 4.353406]\n",
      "5068 [Discriminator loss: 0.123834, acc.: 95.31%] [Generator loss: 4.127935]\n",
      "5069 [Discriminator loss: 0.218208, acc.: 92.19%] [Generator loss: 4.412794]\n",
      "5070 [Discriminator loss: 0.183665, acc.: 92.19%] [Generator loss: 5.670393]\n",
      "5071 [Discriminator loss: 0.111323, acc.: 95.31%] [Generator loss: 5.108816]\n",
      "5072 [Discriminator loss: 0.428976, acc.: 81.25%] [Generator loss: 5.465629]\n",
      "5073 [Discriminator loss: 0.204620, acc.: 95.31%] [Generator loss: 7.026633]\n",
      "5074 [Discriminator loss: 0.158025, acc.: 93.75%] [Generator loss: 4.058505]\n",
      "5075 [Discriminator loss: 0.250395, acc.: 85.94%] [Generator loss: 3.774510]\n",
      "5076 [Discriminator loss: 0.177287, acc.: 89.06%] [Generator loss: 4.540436]\n",
      "5077 [Discriminator loss: 0.392186, acc.: 76.56%] [Generator loss: 5.923524]\n",
      "5078 [Discriminator loss: 0.146875, acc.: 96.88%] [Generator loss: 5.352959]\n",
      "5079 [Discriminator loss: 0.405223, acc.: 81.25%] [Generator loss: 4.649538]\n",
      "5080 [Discriminator loss: 0.074527, acc.: 100.00%] [Generator loss: 4.606859]\n",
      "5081 [Discriminator loss: 0.227405, acc.: 89.06%] [Generator loss: 4.770645]\n",
      "5082 [Discriminator loss: 0.221840, acc.: 87.50%] [Generator loss: 4.820338]\n",
      "5083 [Discriminator loss: 0.071710, acc.: 98.44%] [Generator loss: 5.257570]\n",
      "5084 [Discriminator loss: 0.270706, acc.: 87.50%] [Generator loss: 3.631249]\n",
      "5085 [Discriminator loss: 0.168737, acc.: 95.31%] [Generator loss: 5.543729]\n",
      "5086 [Discriminator loss: 0.237300, acc.: 90.62%] [Generator loss: 6.056488]\n",
      "5087 [Discriminator loss: 0.191062, acc.: 89.06%] [Generator loss: 5.020956]\n",
      "5088 [Discriminator loss: 0.218290, acc.: 89.06%] [Generator loss: 4.573009]\n",
      "5089 [Discriminator loss: 0.381391, acc.: 82.81%] [Generator loss: 5.831730]\n",
      "5090 [Discriminator loss: 0.291335, acc.: 84.38%] [Generator loss: 4.811394]\n",
      "5091 [Discriminator loss: 0.307411, acc.: 84.38%] [Generator loss: 4.797679]\n",
      "5092 [Discriminator loss: 0.118121, acc.: 95.31%] [Generator loss: 5.362442]\n",
      "5093 [Discriminator loss: 0.119440, acc.: 96.88%] [Generator loss: 5.359466]\n",
      "5094 [Discriminator loss: 0.351066, acc.: 90.62%] [Generator loss: 3.836570]\n",
      "5095 [Discriminator loss: 0.469680, acc.: 82.81%] [Generator loss: 6.666164]\n",
      "5096 [Discriminator loss: 0.262243, acc.: 93.75%] [Generator loss: 5.983039]\n",
      "5097 [Discriminator loss: 0.692964, acc.: 70.31%] [Generator loss: 4.521821]\n",
      "5098 [Discriminator loss: 0.177014, acc.: 92.19%] [Generator loss: 5.346298]\n",
      "5099 [Discriminator loss: 0.192800, acc.: 89.06%] [Generator loss: 5.970612]\n",
      "5100 [Discriminator loss: 0.115958, acc.: 96.88%] [Generator loss: 4.458312]\n",
      "5101 [Discriminator loss: 0.274912, acc.: 84.38%] [Generator loss: 3.743948]\n",
      "5102 [Discriminator loss: 0.241391, acc.: 93.75%] [Generator loss: 4.950885]\n",
      "5103 [Discriminator loss: 0.525260, acc.: 75.00%] [Generator loss: 6.334545]\n",
      "5104 [Discriminator loss: 0.201772, acc.: 92.19%] [Generator loss: 6.487349]\n",
      "5105 [Discriminator loss: 0.308024, acc.: 85.94%] [Generator loss: 4.834899]\n",
      "5106 [Discriminator loss: 0.226897, acc.: 90.62%] [Generator loss: 4.398879]\n",
      "5107 [Discriminator loss: 0.257390, acc.: 92.19%] [Generator loss: 4.640740]\n",
      "5108 [Discriminator loss: 0.463893, acc.: 76.56%] [Generator loss: 3.248809]\n",
      "5109 [Discriminator loss: 0.217292, acc.: 89.06%] [Generator loss: 4.490119]\n",
      "5110 [Discriminator loss: 0.353774, acc.: 82.81%] [Generator loss: 4.240154]\n",
      "5111 [Discriminator loss: 0.382313, acc.: 85.94%] [Generator loss: 4.532124]\n",
      "5112 [Discriminator loss: 0.380453, acc.: 78.12%] [Generator loss: 4.046674]\n",
      "5113 [Discriminator loss: 0.141756, acc.: 93.75%] [Generator loss: 4.701350]\n",
      "5114 [Discriminator loss: 0.295357, acc.: 92.19%] [Generator loss: 4.991255]\n",
      "5115 [Discriminator loss: 0.329879, acc.: 81.25%] [Generator loss: 4.385989]\n",
      "5116 [Discriminator loss: 0.155808, acc.: 93.75%] [Generator loss: 4.587644]\n",
      "5117 [Discriminator loss: 0.522587, acc.: 81.25%] [Generator loss: 4.202981]\n",
      "5118 [Discriminator loss: 0.328793, acc.: 89.06%] [Generator loss: 4.463262]\n",
      "5119 [Discriminator loss: 0.139441, acc.: 93.75%] [Generator loss: 4.753630]\n",
      "5120 [Discriminator loss: 0.354427, acc.: 85.94%] [Generator loss: 5.036345]\n",
      "5121 [Discriminator loss: 0.283695, acc.: 82.81%] [Generator loss: 6.222477]\n",
      "5122 [Discriminator loss: 0.416345, acc.: 81.25%] [Generator loss: 2.631497]\n",
      "5123 [Discriminator loss: 0.191947, acc.: 92.19%] [Generator loss: 5.287209]\n",
      "5124 [Discriminator loss: 0.327459, acc.: 87.50%] [Generator loss: 3.915295]\n",
      "5125 [Discriminator loss: 0.099116, acc.: 98.44%] [Generator loss: 5.052310]\n",
      "5126 [Discriminator loss: 0.225682, acc.: 93.75%] [Generator loss: 3.645487]\n",
      "5127 [Discriminator loss: 0.235953, acc.: 93.75%] [Generator loss: 4.357229]\n",
      "5128 [Discriminator loss: 0.174767, acc.: 95.31%] [Generator loss: 4.191850]\n",
      "5129 [Discriminator loss: 0.310134, acc.: 85.94%] [Generator loss: 4.578062]\n",
      "5130 [Discriminator loss: 0.157807, acc.: 90.62%] [Generator loss: 4.798393]\n",
      "5131 [Discriminator loss: 0.242204, acc.: 90.62%] [Generator loss: 4.824349]\n",
      "5132 [Discriminator loss: 0.239299, acc.: 90.62%] [Generator loss: 4.662722]\n",
      "5133 [Discriminator loss: 0.173119, acc.: 96.88%] [Generator loss: 4.937866]\n",
      "5134 [Discriminator loss: 0.433389, acc.: 81.25%] [Generator loss: 4.544473]\n",
      "5135 [Discriminator loss: 0.150532, acc.: 95.31%] [Generator loss: 5.697947]\n",
      "5136 [Discriminator loss: 0.154120, acc.: 95.31%] [Generator loss: 3.881300]\n",
      "5137 [Discriminator loss: 0.232397, acc.: 87.50%] [Generator loss: 3.857397]\n",
      "5138 [Discriminator loss: 0.305148, acc.: 87.50%] [Generator loss: 4.930289]\n",
      "5139 [Discriminator loss: 0.170414, acc.: 90.62%] [Generator loss: 5.055576]\n",
      "5140 [Discriminator loss: 0.257141, acc.: 90.62%] [Generator loss: 4.206045]\n",
      "5141 [Discriminator loss: 0.127275, acc.: 96.88%] [Generator loss: 4.952987]\n",
      "5142 [Discriminator loss: 0.146655, acc.: 95.31%] [Generator loss: 4.911253]\n",
      "5143 [Discriminator loss: 0.286613, acc.: 87.50%] [Generator loss: 5.093668]\n",
      "5144 [Discriminator loss: 0.142866, acc.: 93.75%] [Generator loss: 4.446862]\n",
      "5145 [Discriminator loss: 0.459722, acc.: 81.25%] [Generator loss: 3.332638]\n",
      "5146 [Discriminator loss: 0.189733, acc.: 90.62%] [Generator loss: 4.918453]\n",
      "5147 [Discriminator loss: 0.143019, acc.: 95.31%] [Generator loss: 3.916998]\n",
      "5148 [Discriminator loss: 0.188649, acc.: 92.19%] [Generator loss: 4.671556]\n",
      "5149 [Discriminator loss: 0.220323, acc.: 93.75%] [Generator loss: 4.452142]\n",
      "5150 [Discriminator loss: 0.230482, acc.: 92.19%] [Generator loss: 4.268064]\n",
      "5151 [Discriminator loss: 0.324453, acc.: 84.38%] [Generator loss: 4.952970]\n",
      "5152 [Discriminator loss: 0.188099, acc.: 90.62%] [Generator loss: 5.167443]\n",
      "5153 [Discriminator loss: 0.307688, acc.: 81.25%] [Generator loss: 5.031466]\n",
      "5154 [Discriminator loss: 0.301909, acc.: 89.06%] [Generator loss: 6.163690]\n",
      "5155 [Discriminator loss: 0.410698, acc.: 76.56%] [Generator loss: 4.643948]\n",
      "5156 [Discriminator loss: 0.320121, acc.: 85.94%] [Generator loss: 6.267967]\n",
      "5157 [Discriminator loss: 0.312920, acc.: 84.38%] [Generator loss: 5.426072]\n",
      "5158 [Discriminator loss: 0.282823, acc.: 87.50%] [Generator loss: 4.596684]\n",
      "5159 [Discriminator loss: 0.233756, acc.: 90.62%] [Generator loss: 6.604709]\n",
      "5160 [Discriminator loss: 0.146240, acc.: 92.19%] [Generator loss: 5.687454]\n",
      "5161 [Discriminator loss: 0.332830, acc.: 87.50%] [Generator loss: 3.736649]\n",
      "5162 [Discriminator loss: 0.129937, acc.: 93.75%] [Generator loss: 4.593904]\n",
      "5163 [Discriminator loss: 0.134272, acc.: 93.75%] [Generator loss: 4.443733]\n",
      "5164 [Discriminator loss: 0.262049, acc.: 90.62%] [Generator loss: 4.552328]\n",
      "5165 [Discriminator loss: 0.181043, acc.: 95.31%] [Generator loss: 3.801407]\n",
      "5166 [Discriminator loss: 0.228190, acc.: 92.19%] [Generator loss: 3.350677]\n",
      "5167 [Discriminator loss: 0.267710, acc.: 87.50%] [Generator loss: 4.438301]\n",
      "5168 [Discriminator loss: 0.185144, acc.: 95.31%] [Generator loss: 4.489031]\n",
      "5169 [Discriminator loss: 0.219461, acc.: 90.62%] [Generator loss: 4.922865]\n",
      "5170 [Discriminator loss: 0.321937, acc.: 85.94%] [Generator loss: 4.171714]\n",
      "5171 [Discriminator loss: 0.174982, acc.: 92.19%] [Generator loss: 5.010759]\n",
      "5172 [Discriminator loss: 0.106659, acc.: 95.31%] [Generator loss: 5.562396]\n",
      "5173 [Discriminator loss: 0.237508, acc.: 93.75%] [Generator loss: 5.027905]\n",
      "5174 [Discriminator loss: 0.128340, acc.: 95.31%] [Generator loss: 4.411353]\n",
      "5175 [Discriminator loss: 0.217308, acc.: 93.75%] [Generator loss: 5.134700]\n",
      "5176 [Discriminator loss: 0.104302, acc.: 98.44%] [Generator loss: 3.822219]\n",
      "5177 [Discriminator loss: 0.274846, acc.: 89.06%] [Generator loss: 4.719936]\n",
      "5178 [Discriminator loss: 0.154442, acc.: 96.88%] [Generator loss: 4.232002]\n",
      "5179 [Discriminator loss: 0.319799, acc.: 85.94%] [Generator loss: 4.749526]\n",
      "5180 [Discriminator loss: 0.230433, acc.: 92.19%] [Generator loss: 3.480457]\n",
      "5181 [Discriminator loss: 0.419839, acc.: 78.12%] [Generator loss: 6.271581]\n",
      "5182 [Discriminator loss: 0.210233, acc.: 90.62%] [Generator loss: 5.760487]\n",
      "5183 [Discriminator loss: 0.210554, acc.: 92.19%] [Generator loss: 5.590558]\n",
      "5184 [Discriminator loss: 0.198077, acc.: 93.75%] [Generator loss: 3.867052]\n",
      "5185 [Discriminator loss: 0.427574, acc.: 84.38%] [Generator loss: 4.695682]\n",
      "5186 [Discriminator loss: 0.145596, acc.: 93.75%] [Generator loss: 4.935280]\n",
      "5187 [Discriminator loss: 0.259565, acc.: 89.06%] [Generator loss: 4.582036]\n",
      "5188 [Discriminator loss: 0.192256, acc.: 93.75%] [Generator loss: 5.216665]\n",
      "5189 [Discriminator loss: 0.422116, acc.: 82.81%] [Generator loss: 6.125551]\n",
      "5190 [Discriminator loss: 0.194802, acc.: 90.62%] [Generator loss: 5.432423]\n",
      "5191 [Discriminator loss: 0.354532, acc.: 82.81%] [Generator loss: 4.365380]\n",
      "5192 [Discriminator loss: 0.232898, acc.: 89.06%] [Generator loss: 5.882776]\n",
      "5193 [Discriminator loss: 0.326646, acc.: 87.50%] [Generator loss: 5.832093]\n",
      "5194 [Discriminator loss: 0.208768, acc.: 90.62%] [Generator loss: 4.769041]\n",
      "5195 [Discriminator loss: 0.207695, acc.: 90.62%] [Generator loss: 4.960275]\n",
      "5196 [Discriminator loss: 0.103671, acc.: 95.31%] [Generator loss: 4.666375]\n",
      "5197 [Discriminator loss: 0.246114, acc.: 85.94%] [Generator loss: 3.200755]\n",
      "5198 [Discriminator loss: 0.291516, acc.: 85.94%] [Generator loss: 4.627079]\n",
      "5199 [Discriminator loss: 0.319309, acc.: 85.94%] [Generator loss: 4.555943]\n",
      "5200 [Discriminator loss: 0.076151, acc.: 98.44%] [Generator loss: 3.860083]\n",
      "5201 [Discriminator loss: 0.296848, acc.: 87.50%] [Generator loss: 5.546766]\n",
      "5202 [Discriminator loss: 0.244091, acc.: 89.06%] [Generator loss: 5.890998]\n",
      "5203 [Discriminator loss: 0.160800, acc.: 92.19%] [Generator loss: 4.416870]\n",
      "5204 [Discriminator loss: 0.275814, acc.: 87.50%] [Generator loss: 4.401923]\n",
      "5205 [Discriminator loss: 0.328177, acc.: 85.94%] [Generator loss: 6.349395]\n",
      "5206 [Discriminator loss: 0.236709, acc.: 95.31%] [Generator loss: 5.408328]\n",
      "5207 [Discriminator loss: 0.436215, acc.: 84.38%] [Generator loss: 3.665171]\n",
      "5208 [Discriminator loss: 0.308431, acc.: 82.81%] [Generator loss: 6.334532]\n",
      "5209 [Discriminator loss: 0.498739, acc.: 82.81%] [Generator loss: 3.955473]\n",
      "5210 [Discriminator loss: 0.283134, acc.: 85.94%] [Generator loss: 4.745650]\n",
      "5211 [Discriminator loss: 0.103517, acc.: 95.31%] [Generator loss: 4.842259]\n",
      "5212 [Discriminator loss: 0.394622, acc.: 82.81%] [Generator loss: 4.552463]\n",
      "5213 [Discriminator loss: 0.229134, acc.: 89.06%] [Generator loss: 3.666258]\n",
      "5214 [Discriminator loss: 0.325750, acc.: 87.50%] [Generator loss: 5.343010]\n",
      "5215 [Discriminator loss: 0.157834, acc.: 95.31%] [Generator loss: 4.718489]\n",
      "5216 [Discriminator loss: 0.146873, acc.: 92.19%] [Generator loss: 4.662778]\n",
      "5217 [Discriminator loss: 0.202245, acc.: 95.31%] [Generator loss: 5.073348]\n",
      "5218 [Discriminator loss: 0.478204, acc.: 76.56%] [Generator loss: 5.113501]\n",
      "5219 [Discriminator loss: 0.088296, acc.: 98.44%] [Generator loss: 5.652534]\n",
      "5220 [Discriminator loss: 0.304510, acc.: 87.50%] [Generator loss: 3.869268]\n",
      "5221 [Discriminator loss: 0.203780, acc.: 92.19%] [Generator loss: 4.891529]\n",
      "5222 [Discriminator loss: 0.185721, acc.: 90.62%] [Generator loss: 4.556550]\n",
      "5223 [Discriminator loss: 0.296460, acc.: 85.94%] [Generator loss: 3.177935]\n",
      "5224 [Discriminator loss: 0.139843, acc.: 95.31%] [Generator loss: 3.384521]\n",
      "5225 [Discriminator loss: 0.280488, acc.: 89.06%] [Generator loss: 5.112118]\n",
      "5226 [Discriminator loss: 0.360213, acc.: 87.50%] [Generator loss: 5.388028]\n",
      "5227 [Discriminator loss: 0.097248, acc.: 96.88%] [Generator loss: 5.387964]\n",
      "5228 [Discriminator loss: 0.261053, acc.: 89.06%] [Generator loss: 4.762399]\n",
      "5229 [Discriminator loss: 0.412983, acc.: 84.38%] [Generator loss: 3.914828]\n",
      "5230 [Discriminator loss: 0.177968, acc.: 93.75%] [Generator loss: 5.101262]\n",
      "5231 [Discriminator loss: 0.300585, acc.: 89.06%] [Generator loss: 5.818443]\n",
      "5232 [Discriminator loss: 0.309069, acc.: 87.50%] [Generator loss: 4.746655]\n",
      "5233 [Discriminator loss: 0.334830, acc.: 84.38%] [Generator loss: 4.777492]\n",
      "5234 [Discriminator loss: 0.300054, acc.: 85.94%] [Generator loss: 4.212545]\n",
      "5235 [Discriminator loss: 0.361038, acc.: 89.06%] [Generator loss: 5.491075]\n",
      "5236 [Discriminator loss: 0.344800, acc.: 87.50%] [Generator loss: 4.749555]\n",
      "5237 [Discriminator loss: 0.127020, acc.: 96.88%] [Generator loss: 4.745715]\n",
      "5238 [Discriminator loss: 0.269993, acc.: 89.06%] [Generator loss: 3.978427]\n",
      "5239 [Discriminator loss: 0.256988, acc.: 90.62%] [Generator loss: 3.748840]\n",
      "5240 [Discriminator loss: 0.301957, acc.: 82.81%] [Generator loss: 4.458658]\n",
      "5241 [Discriminator loss: 0.102874, acc.: 95.31%] [Generator loss: 4.827277]\n",
      "5242 [Discriminator loss: 0.089870, acc.: 98.44%] [Generator loss: 4.044329]\n",
      "5243 [Discriminator loss: 0.351636, acc.: 84.38%] [Generator loss: 4.776356]\n",
      "5244 [Discriminator loss: 0.191175, acc.: 92.19%] [Generator loss: 3.890682]\n",
      "5245 [Discriminator loss: 0.239720, acc.: 90.62%] [Generator loss: 5.058434]\n",
      "5246 [Discriminator loss: 0.406960, acc.: 76.56%] [Generator loss: 4.152427]\n",
      "5247 [Discriminator loss: 0.331332, acc.: 85.94%] [Generator loss: 5.070954]\n",
      "5248 [Discriminator loss: 0.176850, acc.: 92.19%] [Generator loss: 5.181764]\n",
      "5249 [Discriminator loss: 0.137938, acc.: 92.19%] [Generator loss: 5.119338]\n",
      "5250 [Discriminator loss: 0.224678, acc.: 92.19%] [Generator loss: 4.226236]\n",
      "5251 [Discriminator loss: 0.292590, acc.: 89.06%] [Generator loss: 4.451798]\n",
      "5252 [Discriminator loss: 0.233371, acc.: 93.75%] [Generator loss: 4.861792]\n",
      "5253 [Discriminator loss: 0.302695, acc.: 89.06%] [Generator loss: 3.794142]\n",
      "5254 [Discriminator loss: 0.129311, acc.: 96.88%] [Generator loss: 5.191544]\n",
      "5255 [Discriminator loss: 0.209905, acc.: 90.62%] [Generator loss: 5.537214]\n",
      "5256 [Discriminator loss: 0.306212, acc.: 85.94%] [Generator loss: 4.649784]\n",
      "5257 [Discriminator loss: 0.415132, acc.: 82.81%] [Generator loss: 4.751259]\n",
      "5258 [Discriminator loss: 0.222258, acc.: 87.50%] [Generator loss: 5.738722]\n",
      "5259 [Discriminator loss: 0.199235, acc.: 89.06%] [Generator loss: 5.370772]\n",
      "5260 [Discriminator loss: 0.249599, acc.: 90.62%] [Generator loss: 4.419662]\n",
      "5261 [Discriminator loss: 0.174344, acc.: 93.75%] [Generator loss: 5.313217]\n",
      "5262 [Discriminator loss: 0.158493, acc.: 90.62%] [Generator loss: 5.768052]\n",
      "5263 [Discriminator loss: 0.247438, acc.: 89.06%] [Generator loss: 4.267937]\n",
      "5264 [Discriminator loss: 0.243835, acc.: 92.19%] [Generator loss: 4.772892]\n",
      "5265 [Discriminator loss: 0.356515, acc.: 82.81%] [Generator loss: 5.242610]\n",
      "5266 [Discriminator loss: 0.108774, acc.: 96.88%] [Generator loss: 4.537778]\n",
      "5267 [Discriminator loss: 0.126180, acc.: 95.31%] [Generator loss: 5.428083]\n",
      "5268 [Discriminator loss: 0.232173, acc.: 92.19%] [Generator loss: 4.524378]\n",
      "5269 [Discriminator loss: 0.106698, acc.: 95.31%] [Generator loss: 4.612699]\n",
      "5270 [Discriminator loss: 0.266155, acc.: 89.06%] [Generator loss: 4.686177]\n",
      "5271 [Discriminator loss: 0.254761, acc.: 89.06%] [Generator loss: 5.951615]\n",
      "5272 [Discriminator loss: 0.526377, acc.: 73.44%] [Generator loss: 3.750913]\n",
      "5273 [Discriminator loss: 0.176796, acc.: 92.19%] [Generator loss: 5.869697]\n",
      "5274 [Discriminator loss: 0.159248, acc.: 92.19%] [Generator loss: 5.827692]\n",
      "5275 [Discriminator loss: 0.203788, acc.: 93.75%] [Generator loss: 4.325820]\n",
      "5276 [Discriminator loss: 0.225116, acc.: 89.06%] [Generator loss: 5.404890]\n",
      "5277 [Discriminator loss: 0.177046, acc.: 92.19%] [Generator loss: 5.692732]\n",
      "5278 [Discriminator loss: 0.172571, acc.: 95.31%] [Generator loss: 5.026876]\n",
      "5279 [Discriminator loss: 0.152819, acc.: 93.75%] [Generator loss: 6.558960]\n",
      "5280 [Discriminator loss: 0.190861, acc.: 92.19%] [Generator loss: 5.040648]\n",
      "5281 [Discriminator loss: 0.274367, acc.: 87.50%] [Generator loss: 5.802112]\n",
      "5282 [Discriminator loss: 0.361844, acc.: 82.81%] [Generator loss: 4.093348]\n",
      "5283 [Discriminator loss: 0.294273, acc.: 89.06%] [Generator loss: 5.811653]\n",
      "5284 [Discriminator loss: 0.182320, acc.: 92.19%] [Generator loss: 5.270214]\n",
      "5285 [Discriminator loss: 0.321695, acc.: 87.50%] [Generator loss: 4.171711]\n",
      "5286 [Discriminator loss: 0.276962, acc.: 84.38%] [Generator loss: 5.235760]\n",
      "5287 [Discriminator loss: 0.198081, acc.: 93.75%] [Generator loss: 5.637548]\n",
      "5288 [Discriminator loss: 0.180767, acc.: 92.19%] [Generator loss: 4.424296]\n",
      "5289 [Discriminator loss: 0.119925, acc.: 98.44%] [Generator loss: 4.506169]\n",
      "5290 [Discriminator loss: 0.310027, acc.: 87.50%] [Generator loss: 5.607171]\n",
      "5291 [Discriminator loss: 0.122986, acc.: 93.75%] [Generator loss: 5.930779]\n",
      "5292 [Discriminator loss: 0.296533, acc.: 89.06%] [Generator loss: 4.172351]\n",
      "5293 [Discriminator loss: 0.301991, acc.: 87.50%] [Generator loss: 4.268606]\n",
      "5294 [Discriminator loss: 0.208759, acc.: 87.50%] [Generator loss: 4.542379]\n",
      "5295 [Discriminator loss: 0.244157, acc.: 89.06%] [Generator loss: 5.361881]\n",
      "5296 [Discriminator loss: 0.219303, acc.: 90.62%] [Generator loss: 4.176030]\n",
      "5297 [Discriminator loss: 0.176933, acc.: 92.19%] [Generator loss: 4.690497]\n",
      "5298 [Discriminator loss: 0.244165, acc.: 90.62%] [Generator loss: 4.942400]\n",
      "5299 [Discriminator loss: 0.173912, acc.: 90.62%] [Generator loss: 4.818000]\n",
      "5300 [Discriminator loss: 0.357216, acc.: 82.81%] [Generator loss: 5.889540]\n",
      "5301 [Discriminator loss: 0.289760, acc.: 85.94%] [Generator loss: 6.617259]\n",
      "5302 [Discriminator loss: 0.223657, acc.: 90.62%] [Generator loss: 4.031641]\n",
      "5303 [Discriminator loss: 0.221989, acc.: 90.62%] [Generator loss: 5.248713]\n",
      "5304 [Discriminator loss: 0.295023, acc.: 87.50%] [Generator loss: 4.811113]\n",
      "5305 [Discriminator loss: 0.350215, acc.: 81.25%] [Generator loss: 4.970081]\n",
      "5306 [Discriminator loss: 0.178234, acc.: 93.75%] [Generator loss: 4.394885]\n",
      "5307 [Discriminator loss: 0.320043, acc.: 84.38%] [Generator loss: 5.920753]\n",
      "5308 [Discriminator loss: 0.223907, acc.: 87.50%] [Generator loss: 4.639334]\n",
      "5309 [Discriminator loss: 0.147150, acc.: 92.19%] [Generator loss: 4.761526]\n",
      "5310 [Discriminator loss: 0.289766, acc.: 90.62%] [Generator loss: 4.726740]\n",
      "5311 [Discriminator loss: 0.208145, acc.: 90.62%] [Generator loss: 4.148143]\n",
      "5312 [Discriminator loss: 0.179162, acc.: 95.31%] [Generator loss: 5.419503]\n",
      "5313 [Discriminator loss: 0.153199, acc.: 93.75%] [Generator loss: 4.819249]\n",
      "5314 [Discriminator loss: 0.335594, acc.: 82.81%] [Generator loss: 4.813416]\n",
      "5315 [Discriminator loss: 0.477599, acc.: 76.56%] [Generator loss: 5.874957]\n",
      "5316 [Discriminator loss: 0.146575, acc.: 92.19%] [Generator loss: 5.777034]\n",
      "5317 [Discriminator loss: 0.460463, acc.: 76.56%] [Generator loss: 2.960944]\n",
      "5318 [Discriminator loss: 0.381964, acc.: 82.81%] [Generator loss: 5.374906]\n",
      "5319 [Discriminator loss: 0.167357, acc.: 93.75%] [Generator loss: 5.530868]\n",
      "5320 [Discriminator loss: 0.567204, acc.: 81.25%] [Generator loss: 4.177928]\n",
      "5321 [Discriminator loss: 0.134978, acc.: 93.75%] [Generator loss: 5.398842]\n",
      "5322 [Discriminator loss: 0.191563, acc.: 95.31%] [Generator loss: 5.380997]\n",
      "5323 [Discriminator loss: 0.250580, acc.: 90.62%] [Generator loss: 5.377717]\n",
      "5324 [Discriminator loss: 0.091353, acc.: 96.88%] [Generator loss: 4.983115]\n",
      "5325 [Discriminator loss: 0.122442, acc.: 96.88%] [Generator loss: 3.883312]\n",
      "5326 [Discriminator loss: 0.142563, acc.: 96.88%] [Generator loss: 4.792269]\n",
      "5327 [Discriminator loss: 0.224046, acc.: 87.50%] [Generator loss: 4.760924]\n",
      "5328 [Discriminator loss: 0.186810, acc.: 93.75%] [Generator loss: 4.315159]\n",
      "5329 [Discriminator loss: 0.080745, acc.: 100.00%] [Generator loss: 4.773128]\n",
      "5330 [Discriminator loss: 0.280455, acc.: 87.50%] [Generator loss: 5.040117]\n",
      "5331 [Discriminator loss: 0.118969, acc.: 96.88%] [Generator loss: 4.878671]\n",
      "5332 [Discriminator loss: 0.794065, acc.: 64.06%] [Generator loss: 5.243156]\n",
      "5333 [Discriminator loss: 0.069244, acc.: 100.00%] [Generator loss: 5.271315]\n",
      "5334 [Discriminator loss: 0.335536, acc.: 90.62%] [Generator loss: 4.188568]\n",
      "5335 [Discriminator loss: 0.179555, acc.: 92.19%] [Generator loss: 4.776032]\n",
      "5336 [Discriminator loss: 0.176700, acc.: 90.62%] [Generator loss: 7.184440]\n",
      "5337 [Discriminator loss: 0.315892, acc.: 87.50%] [Generator loss: 4.928307]\n",
      "5338 [Discriminator loss: 0.193873, acc.: 92.19%] [Generator loss: 3.719223]\n",
      "5339 [Discriminator loss: 0.238470, acc.: 90.62%] [Generator loss: 5.696074]\n",
      "5340 [Discriminator loss: 0.099825, acc.: 95.31%] [Generator loss: 5.579814]\n",
      "5341 [Discriminator loss: 0.226968, acc.: 92.19%] [Generator loss: 3.127402]\n",
      "5342 [Discriminator loss: 0.263213, acc.: 85.94%] [Generator loss: 5.016420]\n",
      "5343 [Discriminator loss: 0.179637, acc.: 90.62%] [Generator loss: 4.928714]\n",
      "5344 [Discriminator loss: 0.240579, acc.: 90.62%] [Generator loss: 4.854371]\n",
      "5345 [Discriminator loss: 0.232464, acc.: 87.50%] [Generator loss: 3.958386]\n",
      "5346 [Discriminator loss: 0.256581, acc.: 90.62%] [Generator loss: 6.125113]\n",
      "5347 [Discriminator loss: 0.216606, acc.: 92.19%] [Generator loss: 4.396260]\n",
      "5348 [Discriminator loss: 0.083473, acc.: 100.00%] [Generator loss: 4.697869]\n",
      "5349 [Discriminator loss: 0.204663, acc.: 93.75%] [Generator loss: 3.800113]\n",
      "5350 [Discriminator loss: 0.305287, acc.: 89.06%] [Generator loss: 5.054331]\n",
      "5351 [Discriminator loss: 0.262913, acc.: 90.62%] [Generator loss: 4.470413]\n",
      "5352 [Discriminator loss: 0.203440, acc.: 95.31%] [Generator loss: 4.440734]\n",
      "5353 [Discriminator loss: 0.300469, acc.: 90.62%] [Generator loss: 4.914696]\n",
      "5354 [Discriminator loss: 0.109398, acc.: 96.88%] [Generator loss: 4.831529]\n",
      "5355 [Discriminator loss: 0.227537, acc.: 89.06%] [Generator loss: 3.482471]\n",
      "5356 [Discriminator loss: 0.450374, acc.: 73.44%] [Generator loss: 6.365721]\n",
      "5357 [Discriminator loss: 0.215605, acc.: 93.75%] [Generator loss: 5.972703]\n",
      "5358 [Discriminator loss: 0.303905, acc.: 82.81%] [Generator loss: 4.631436]\n",
      "5359 [Discriminator loss: 0.181580, acc.: 96.88%] [Generator loss: 4.679447]\n",
      "5360 [Discriminator loss: 0.123436, acc.: 93.75%] [Generator loss: 4.331245]\n",
      "5361 [Discriminator loss: 0.379947, acc.: 82.81%] [Generator loss: 4.167956]\n",
      "5362 [Discriminator loss: 0.258721, acc.: 92.19%] [Generator loss: 4.520561]\n",
      "5363 [Discriminator loss: 0.118638, acc.: 93.75%] [Generator loss: 5.654914]\n",
      "5364 [Discriminator loss: 0.326846, acc.: 84.38%] [Generator loss: 4.594464]\n",
      "5365 [Discriminator loss: 0.319226, acc.: 89.06%] [Generator loss: 4.917239]\n",
      "5366 [Discriminator loss: 0.228451, acc.: 90.62%] [Generator loss: 4.648011]\n",
      "5367 [Discriminator loss: 0.286184, acc.: 84.38%] [Generator loss: 4.983101]\n",
      "5368 [Discriminator loss: 0.118784, acc.: 98.44%] [Generator loss: 4.127555]\n",
      "5369 [Discriminator loss: 0.249590, acc.: 89.06%] [Generator loss: 4.794621]\n",
      "5370 [Discriminator loss: 0.225799, acc.: 87.50%] [Generator loss: 4.768196]\n",
      "5371 [Discriminator loss: 0.136360, acc.: 96.88%] [Generator loss: 5.089238]\n",
      "5372 [Discriminator loss: 0.101926, acc.: 100.00%] [Generator loss: 4.104167]\n",
      "5373 [Discriminator loss: 0.113325, acc.: 96.88%] [Generator loss: 4.527808]\n",
      "5374 [Discriminator loss: 0.171878, acc.: 92.19%] [Generator loss: 4.701043]\n",
      "5375 [Discriminator loss: 0.272436, acc.: 85.94%] [Generator loss: 3.839780]\n",
      "5376 [Discriminator loss: 0.233565, acc.: 90.62%] [Generator loss: 5.781041]\n",
      "5377 [Discriminator loss: 0.231311, acc.: 85.94%] [Generator loss: 5.259027]\n",
      "5378 [Discriminator loss: 0.247567, acc.: 85.94%] [Generator loss: 5.793704]\n",
      "5379 [Discriminator loss: 0.500160, acc.: 71.88%] [Generator loss: 4.428157]\n",
      "5380 [Discriminator loss: 0.156080, acc.: 93.75%] [Generator loss: 4.555301]\n",
      "5381 [Discriminator loss: 0.230323, acc.: 87.50%] [Generator loss: 5.343718]\n",
      "5382 [Discriminator loss: 0.252193, acc.: 90.62%] [Generator loss: 4.415909]\n",
      "5383 [Discriminator loss: 0.378341, acc.: 89.06%] [Generator loss: 4.202707]\n",
      "5384 [Discriminator loss: 0.137054, acc.: 96.88%] [Generator loss: 4.977026]\n",
      "5385 [Discriminator loss: 0.230112, acc.: 89.06%] [Generator loss: 4.697916]\n",
      "5386 [Discriminator loss: 0.448162, acc.: 82.81%] [Generator loss: 5.066798]\n",
      "5387 [Discriminator loss: 0.361835, acc.: 89.06%] [Generator loss: 5.580173]\n",
      "5388 [Discriminator loss: 0.199237, acc.: 95.31%] [Generator loss: 5.901986]\n",
      "5389 [Discriminator loss: 0.124764, acc.: 95.31%] [Generator loss: 5.253588]\n",
      "5390 [Discriminator loss: 0.401421, acc.: 79.69%] [Generator loss: 5.469498]\n",
      "5391 [Discriminator loss: 0.193503, acc.: 93.75%] [Generator loss: 5.721814]\n",
      "5392 [Discriminator loss: 0.161244, acc.: 95.31%] [Generator loss: 3.946957]\n",
      "5393 [Discriminator loss: 0.185688, acc.: 89.06%] [Generator loss: 4.172388]\n",
      "5394 [Discriminator loss: 0.154273, acc.: 93.75%] [Generator loss: 4.567904]\n",
      "5395 [Discriminator loss: 0.148853, acc.: 93.75%] [Generator loss: 5.901114]\n",
      "5396 [Discriminator loss: 0.148757, acc.: 95.31%] [Generator loss: 4.810262]\n",
      "5397 [Discriminator loss: 0.239083, acc.: 90.62%] [Generator loss: 4.626722]\n",
      "5398 [Discriminator loss: 0.233448, acc.: 92.19%] [Generator loss: 5.611607]\n",
      "5399 [Discriminator loss: 0.193326, acc.: 90.62%] [Generator loss: 5.388875]\n",
      "5400 [Discriminator loss: 0.248871, acc.: 92.19%] [Generator loss: 5.169703]\n",
      "5401 [Discriminator loss: 0.146683, acc.: 95.31%] [Generator loss: 3.991876]\n",
      "5402 [Discriminator loss: 0.331589, acc.: 85.94%] [Generator loss: 5.823488]\n",
      "5403 [Discriminator loss: 0.125866, acc.: 90.62%] [Generator loss: 5.077148]\n",
      "5404 [Discriminator loss: 0.311280, acc.: 89.06%] [Generator loss: 5.073429]\n",
      "5405 [Discriminator loss: 0.203293, acc.: 90.62%] [Generator loss: 5.007041]\n",
      "5406 [Discriminator loss: 0.171237, acc.: 93.75%] [Generator loss: 4.634370]\n",
      "5407 [Discriminator loss: 0.159300, acc.: 92.19%] [Generator loss: 5.769602]\n",
      "5408 [Discriminator loss: 0.212171, acc.: 92.19%] [Generator loss: 6.019191]\n",
      "5409 [Discriminator loss: 0.255353, acc.: 92.19%] [Generator loss: 3.933405]\n",
      "5410 [Discriminator loss: 0.124680, acc.: 95.31%] [Generator loss: 4.404847]\n",
      "5411 [Discriminator loss: 0.408208, acc.: 84.38%] [Generator loss: 5.866980]\n",
      "5412 [Discriminator loss: 0.224898, acc.: 87.50%] [Generator loss: 5.522143]\n",
      "5413 [Discriminator loss: 0.122422, acc.: 96.88%] [Generator loss: 5.273653]\n",
      "5414 [Discriminator loss: 0.271307, acc.: 87.50%] [Generator loss: 4.420193]\n",
      "5415 [Discriminator loss: 0.175101, acc.: 92.19%] [Generator loss: 5.821686]\n",
      "5416 [Discriminator loss: 0.375843, acc.: 84.38%] [Generator loss: 5.043397]\n",
      "5417 [Discriminator loss: 0.119089, acc.: 95.31%] [Generator loss: 4.469597]\n",
      "5418 [Discriminator loss: 0.229646, acc.: 89.06%] [Generator loss: 4.239917]\n",
      "5419 [Discriminator loss: 0.129279, acc.: 96.88%] [Generator loss: 5.649121]\n",
      "5420 [Discriminator loss: 0.184340, acc.: 95.31%] [Generator loss: 6.391532]\n",
      "5421 [Discriminator loss: 0.324017, acc.: 85.94%] [Generator loss: 3.668792]\n",
      "5422 [Discriminator loss: 0.181212, acc.: 93.75%] [Generator loss: 4.369471]\n",
      "5423 [Discriminator loss: 0.189604, acc.: 93.75%] [Generator loss: 4.464895]\n",
      "5424 [Discriminator loss: 0.178582, acc.: 90.62%] [Generator loss: 5.573462]\n",
      "5425 [Discriminator loss: 0.306041, acc.: 87.50%] [Generator loss: 3.637238]\n",
      "5426 [Discriminator loss: 0.494850, acc.: 84.38%] [Generator loss: 6.857825]\n",
      "5427 [Discriminator loss: 0.167464, acc.: 90.62%] [Generator loss: 5.200032]\n",
      "5428 [Discriminator loss: 0.300114, acc.: 87.50%] [Generator loss: 4.924059]\n",
      "5429 [Discriminator loss: 0.220883, acc.: 89.06%] [Generator loss: 5.557765]\n",
      "5430 [Discriminator loss: 0.183222, acc.: 95.31%] [Generator loss: 5.450356]\n",
      "5431 [Discriminator loss: 0.346139, acc.: 78.12%] [Generator loss: 6.290325]\n",
      "5432 [Discriminator loss: 0.150036, acc.: 95.31%] [Generator loss: 5.270626]\n",
      "5433 [Discriminator loss: 0.227420, acc.: 87.50%] [Generator loss: 3.643813]\n",
      "5434 [Discriminator loss: 0.241728, acc.: 89.06%] [Generator loss: 5.206666]\n",
      "5435 [Discriminator loss: 0.138851, acc.: 95.31%] [Generator loss: 6.812699]\n",
      "5436 [Discriminator loss: 0.494831, acc.: 84.38%] [Generator loss: 3.941801]\n",
      "5437 [Discriminator loss: 0.508089, acc.: 81.25%] [Generator loss: 5.625167]\n",
      "5438 [Discriminator loss: 0.232304, acc.: 90.62%] [Generator loss: 4.699716]\n",
      "5439 [Discriminator loss: 0.359263, acc.: 82.81%] [Generator loss: 3.951954]\n",
      "5440 [Discriminator loss: 0.251082, acc.: 87.50%] [Generator loss: 5.660128]\n",
      "5441 [Discriminator loss: 0.107455, acc.: 96.88%] [Generator loss: 5.784678]\n",
      "5442 [Discriminator loss: 0.165181, acc.: 95.31%] [Generator loss: 4.638405]\n",
      "5443 [Discriminator loss: 0.249145, acc.: 89.06%] [Generator loss: 4.921437]\n",
      "5444 [Discriminator loss: 0.290406, acc.: 90.62%] [Generator loss: 4.915128]\n",
      "5445 [Discriminator loss: 0.296150, acc.: 85.94%] [Generator loss: 4.756133]\n",
      "5446 [Discriminator loss: 0.141669, acc.: 93.75%] [Generator loss: 5.122485]\n",
      "5447 [Discriminator loss: 0.203069, acc.: 95.31%] [Generator loss: 4.314304]\n",
      "5448 [Discriminator loss: 0.215096, acc.: 90.62%] [Generator loss: 6.033006]\n",
      "5449 [Discriminator loss: 0.196109, acc.: 92.19%] [Generator loss: 5.810421]\n",
      "5450 [Discriminator loss: 0.372066, acc.: 81.25%] [Generator loss: 4.224617]\n",
      "5451 [Discriminator loss: 0.473719, acc.: 79.69%] [Generator loss: 6.036253]\n",
      "5452 [Discriminator loss: 0.176614, acc.: 92.19%] [Generator loss: 5.080264]\n",
      "5453 [Discriminator loss: 0.278451, acc.: 90.62%] [Generator loss: 4.332267]\n",
      "5454 [Discriminator loss: 0.180273, acc.: 95.31%] [Generator loss: 5.224689]\n",
      "5455 [Discriminator loss: 0.151546, acc.: 95.31%] [Generator loss: 5.467973]\n",
      "5456 [Discriminator loss: 0.177018, acc.: 93.75%] [Generator loss: 4.528671]\n",
      "5457 [Discriminator loss: 0.316944, acc.: 85.94%] [Generator loss: 4.740161]\n",
      "5458 [Discriminator loss: 0.123040, acc.: 96.88%] [Generator loss: 5.701996]\n",
      "5459 [Discriminator loss: 0.122091, acc.: 93.75%] [Generator loss: 4.978342]\n",
      "5460 [Discriminator loss: 0.206045, acc.: 95.31%] [Generator loss: 4.080498]\n",
      "5461 [Discriminator loss: 0.150771, acc.: 95.31%] [Generator loss: 4.153193]\n",
      "5462 [Discriminator loss: 0.204983, acc.: 90.62%] [Generator loss: 6.071097]\n",
      "5463 [Discriminator loss: 0.324832, acc.: 85.94%] [Generator loss: 3.828442]\n",
      "5464 [Discriminator loss: 0.218043, acc.: 92.19%] [Generator loss: 4.699183]\n",
      "5465 [Discriminator loss: 0.138054, acc.: 92.19%] [Generator loss: 5.239780]\n",
      "5466 [Discriminator loss: 0.129462, acc.: 95.31%] [Generator loss: 5.106441]\n",
      "5467 [Discriminator loss: 0.218721, acc.: 93.75%] [Generator loss: 4.763667]\n",
      "5468 [Discriminator loss: 0.382775, acc.: 84.38%] [Generator loss: 5.294077]\n",
      "5469 [Discriminator loss: 0.134997, acc.: 95.31%] [Generator loss: 4.874177]\n",
      "5470 [Discriminator loss: 0.232640, acc.: 89.06%] [Generator loss: 6.032793]\n",
      "5471 [Discriminator loss: 0.274003, acc.: 89.06%] [Generator loss: 4.746143]\n",
      "5472 [Discriminator loss: 0.131756, acc.: 93.75%] [Generator loss: 4.255990]\n",
      "5473 [Discriminator loss: 0.208897, acc.: 89.06%] [Generator loss: 3.787395]\n",
      "5474 [Discriminator loss: 0.149143, acc.: 96.88%] [Generator loss: 4.996505]\n",
      "5475 [Discriminator loss: 0.258315, acc.: 84.38%] [Generator loss: 5.826128]\n",
      "5476 [Discriminator loss: 0.133106, acc.: 96.88%] [Generator loss: 4.657486]\n",
      "5477 [Discriminator loss: 0.271677, acc.: 90.62%] [Generator loss: 6.185414]\n",
      "5478 [Discriminator loss: 0.159958, acc.: 93.75%] [Generator loss: 4.759366]\n",
      "5479 [Discriminator loss: 0.519714, acc.: 76.56%] [Generator loss: 5.108436]\n",
      "5480 [Discriminator loss: 0.246278, acc.: 87.50%] [Generator loss: 7.056499]\n",
      "5481 [Discriminator loss: 0.331392, acc.: 85.94%] [Generator loss: 4.915634]\n",
      "5482 [Discriminator loss: 0.398012, acc.: 84.38%] [Generator loss: 4.183561]\n",
      "5483 [Discriminator loss: 0.068864, acc.: 98.44%] [Generator loss: 6.077096]\n",
      "5484 [Discriminator loss: 0.123772, acc.: 95.31%] [Generator loss: 4.394023]\n",
      "5485 [Discriminator loss: 0.144038, acc.: 96.88%] [Generator loss: 4.263001]\n",
      "5486 [Discriminator loss: 0.207681, acc.: 90.62%] [Generator loss: 4.483970]\n",
      "5487 [Discriminator loss: 0.150747, acc.: 92.19%] [Generator loss: 5.225240]\n",
      "5488 [Discriminator loss: 0.190614, acc.: 89.06%] [Generator loss: 5.428115]\n",
      "5489 [Discriminator loss: 0.237093, acc.: 90.62%] [Generator loss: 5.035002]\n",
      "5490 [Discriminator loss: 0.140666, acc.: 96.88%] [Generator loss: 4.808205]\n",
      "5491 [Discriminator loss: 0.166096, acc.: 93.75%] [Generator loss: 5.163707]\n",
      "5492 [Discriminator loss: 0.337528, acc.: 84.38%] [Generator loss: 6.132435]\n",
      "5493 [Discriminator loss: 0.273903, acc.: 85.94%] [Generator loss: 4.092573]\n",
      "5494 [Discriminator loss: 0.161161, acc.: 90.62%] [Generator loss: 4.282486]\n",
      "5495 [Discriminator loss: 0.198810, acc.: 92.19%] [Generator loss: 4.647703]\n",
      "5496 [Discriminator loss: 0.110110, acc.: 95.31%] [Generator loss: 4.622679]\n",
      "5497 [Discriminator loss: 0.387628, acc.: 87.50%] [Generator loss: 4.434204]\n",
      "5498 [Discriminator loss: 0.394750, acc.: 78.12%] [Generator loss: 4.705583]\n",
      "5499 [Discriminator loss: 0.141146, acc.: 93.75%] [Generator loss: 3.986126]\n",
      "5500 [Discriminator loss: 0.253681, acc.: 87.50%] [Generator loss: 4.566933]\n",
      "5501 [Discriminator loss: 0.351438, acc.: 85.94%] [Generator loss: 4.827604]\n",
      "5502 [Discriminator loss: 0.091987, acc.: 96.88%] [Generator loss: 6.449541]\n",
      "5503 [Discriminator loss: 0.258811, acc.: 89.06%] [Generator loss: 4.995070]\n",
      "5504 [Discriminator loss: 0.364121, acc.: 82.81%] [Generator loss: 5.729805]\n",
      "5505 [Discriminator loss: 0.186776, acc.: 93.75%] [Generator loss: 4.746010]\n",
      "5506 [Discriminator loss: 0.298919, acc.: 89.06%] [Generator loss: 4.961996]\n",
      "5507 [Discriminator loss: 0.137356, acc.: 95.31%] [Generator loss: 6.042915]\n",
      "5508 [Discriminator loss: 0.166499, acc.: 93.75%] [Generator loss: 5.090481]\n",
      "5509 [Discriminator loss: 0.330862, acc.: 84.38%] [Generator loss: 5.292274]\n",
      "5510 [Discriminator loss: 0.161145, acc.: 92.19%] [Generator loss: 5.257868]\n",
      "5511 [Discriminator loss: 0.132487, acc.: 98.44%] [Generator loss: 5.785425]\n",
      "5512 [Discriminator loss: 0.129846, acc.: 93.75%] [Generator loss: 4.747018]\n",
      "5513 [Discriminator loss: 0.352646, acc.: 85.94%] [Generator loss: 3.966210]\n",
      "5514 [Discriminator loss: 0.200314, acc.: 90.62%] [Generator loss: 5.302718]\n",
      "5515 [Discriminator loss: 0.278433, acc.: 90.62%] [Generator loss: 4.164382]\n",
      "5516 [Discriminator loss: 0.316351, acc.: 81.25%] [Generator loss: 4.950564]\n",
      "5517 [Discriminator loss: 0.093963, acc.: 95.31%] [Generator loss: 5.198913]\n",
      "5518 [Discriminator loss: 0.341527, acc.: 82.81%] [Generator loss: 4.508556]\n",
      "5519 [Discriminator loss: 0.236911, acc.: 92.19%] [Generator loss: 4.505542]\n",
      "5520 [Discriminator loss: 0.158149, acc.: 93.75%] [Generator loss: 4.929121]\n",
      "5521 [Discriminator loss: 0.211701, acc.: 92.19%] [Generator loss: 4.474946]\n",
      "5522 [Discriminator loss: 0.210678, acc.: 90.62%] [Generator loss: 4.760292]\n",
      "5523 [Discriminator loss: 0.109284, acc.: 96.88%] [Generator loss: 5.037594]\n",
      "5524 [Discriminator loss: 0.183246, acc.: 95.31%] [Generator loss: 5.611548]\n",
      "5525 [Discriminator loss: 0.290744, acc.: 89.06%] [Generator loss: 4.279698]\n",
      "5526 [Discriminator loss: 0.313166, acc.: 84.38%] [Generator loss: 7.034135]\n",
      "5527 [Discriminator loss: 0.161372, acc.: 92.19%] [Generator loss: 5.406383]\n",
      "5528 [Discriminator loss: 0.359035, acc.: 85.94%] [Generator loss: 4.743650]\n",
      "5529 [Discriminator loss: 0.122281, acc.: 92.19%] [Generator loss: 6.869987]\n",
      "5530 [Discriminator loss: 0.439536, acc.: 81.25%] [Generator loss: 3.958117]\n",
      "5531 [Discriminator loss: 0.234311, acc.: 89.06%] [Generator loss: 5.442920]\n",
      "5532 [Discriminator loss: 0.212397, acc.: 93.75%] [Generator loss: 5.784634]\n",
      "5533 [Discriminator loss: 0.246592, acc.: 92.19%] [Generator loss: 4.986037]\n",
      "5534 [Discriminator loss: 0.099312, acc.: 96.88%] [Generator loss: 6.354180]\n",
      "5535 [Discriminator loss: 0.260752, acc.: 89.06%] [Generator loss: 5.434492]\n",
      "5536 [Discriminator loss: 0.267224, acc.: 89.06%] [Generator loss: 3.879379]\n",
      "5537 [Discriminator loss: 0.242058, acc.: 95.31%] [Generator loss: 4.431169]\n",
      "5538 [Discriminator loss: 0.261236, acc.: 87.50%] [Generator loss: 4.659602]\n",
      "5539 [Discriminator loss: 0.203479, acc.: 89.06%] [Generator loss: 4.226383]\n",
      "5540 [Discriminator loss: 0.116368, acc.: 95.31%] [Generator loss: 4.073992]\n",
      "5541 [Discriminator loss: 0.525082, acc.: 73.44%] [Generator loss: 6.031482]\n",
      "5542 [Discriminator loss: 0.395063, acc.: 78.12%] [Generator loss: 5.021867]\n",
      "5543 [Discriminator loss: 0.145398, acc.: 95.31%] [Generator loss: 4.881864]\n",
      "5544 [Discriminator loss: 0.343863, acc.: 84.38%] [Generator loss: 5.670498]\n",
      "5545 [Discriminator loss: 0.176022, acc.: 92.19%] [Generator loss: 4.664003]\n",
      "5546 [Discriminator loss: 0.152895, acc.: 92.19%] [Generator loss: 5.233816]\n",
      "5547 [Discriminator loss: 0.196935, acc.: 89.06%] [Generator loss: 3.759170]\n",
      "5548 [Discriminator loss: 0.248118, acc.: 87.50%] [Generator loss: 3.975028]\n",
      "5549 [Discriminator loss: 0.142083, acc.: 95.31%] [Generator loss: 4.275679]\n",
      "5550 [Discriminator loss: 0.275840, acc.: 87.50%] [Generator loss: 5.106661]\n",
      "5551 [Discriminator loss: 0.197829, acc.: 95.31%] [Generator loss: 5.685370]\n",
      "5552 [Discriminator loss: 0.356218, acc.: 81.25%] [Generator loss: 4.383280]\n",
      "5553 [Discriminator loss: 0.234698, acc.: 87.50%] [Generator loss: 5.008882]\n",
      "5554 [Discriminator loss: 0.201240, acc.: 89.06%] [Generator loss: 5.074265]\n",
      "5555 [Discriminator loss: 0.377315, acc.: 81.25%] [Generator loss: 3.564731]\n",
      "5556 [Discriminator loss: 0.118270, acc.: 96.88%] [Generator loss: 5.222557]\n",
      "5557 [Discriminator loss: 0.106583, acc.: 98.44%] [Generator loss: 4.646244]\n",
      "5558 [Discriminator loss: 0.088189, acc.: 96.88%] [Generator loss: 5.380966]\n",
      "5559 [Discriminator loss: 0.153061, acc.: 95.31%] [Generator loss: 6.110087]\n",
      "5560 [Discriminator loss: 0.309933, acc.: 85.94%] [Generator loss: 4.090931]\n",
      "5561 [Discriminator loss: 0.275715, acc.: 87.50%] [Generator loss: 4.011604]\n",
      "5562 [Discriminator loss: 0.208347, acc.: 93.75%] [Generator loss: 5.642824]\n",
      "5563 [Discriminator loss: 0.149712, acc.: 96.88%] [Generator loss: 4.283004]\n",
      "5564 [Discriminator loss: 0.222877, acc.: 92.19%] [Generator loss: 4.835530]\n",
      "5565 [Discriminator loss: 0.177882, acc.: 90.62%] [Generator loss: 5.098156]\n",
      "5566 [Discriminator loss: 0.362324, acc.: 85.94%] [Generator loss: 4.156567]\n",
      "5567 [Discriminator loss: 0.134552, acc.: 95.31%] [Generator loss: 5.590660]\n",
      "5568 [Discriminator loss: 0.194428, acc.: 90.62%] [Generator loss: 5.709188]\n",
      "5569 [Discriminator loss: 0.164549, acc.: 95.31%] [Generator loss: 3.681099]\n",
      "5570 [Discriminator loss: 0.324542, acc.: 89.06%] [Generator loss: 6.008507]\n",
      "5571 [Discriminator loss: 0.180885, acc.: 92.19%] [Generator loss: 5.431262]\n",
      "5572 [Discriminator loss: 0.181186, acc.: 90.62%] [Generator loss: 5.416657]\n",
      "5573 [Discriminator loss: 0.300567, acc.: 84.38%] [Generator loss: 4.461022]\n",
      "5574 [Discriminator loss: 0.127256, acc.: 95.31%] [Generator loss: 4.932712]\n",
      "5575 [Discriminator loss: 0.293484, acc.: 89.06%] [Generator loss: 3.640252]\n",
      "5576 [Discriminator loss: 0.168262, acc.: 90.62%] [Generator loss: 4.544178]\n",
      "5577 [Discriminator loss: 0.216790, acc.: 89.06%] [Generator loss: 4.726028]\n",
      "5578 [Discriminator loss: 0.142159, acc.: 92.19%] [Generator loss: 5.956488]\n",
      "5579 [Discriminator loss: 0.282396, acc.: 90.62%] [Generator loss: 4.537732]\n",
      "5580 [Discriminator loss: 0.503579, acc.: 76.56%] [Generator loss: 5.390772]\n",
      "5581 [Discriminator loss: 0.339893, acc.: 85.94%] [Generator loss: 5.314409]\n",
      "5582 [Discriminator loss: 0.150992, acc.: 95.31%] [Generator loss: 4.811313]\n",
      "5583 [Discriminator loss: 0.211884, acc.: 89.06%] [Generator loss: 4.889571]\n",
      "5584 [Discriminator loss: 0.189492, acc.: 96.88%] [Generator loss: 5.304777]\n",
      "5585 [Discriminator loss: 0.321047, acc.: 92.19%] [Generator loss: 5.204336]\n",
      "5586 [Discriminator loss: 0.119570, acc.: 96.88%] [Generator loss: 4.509934]\n",
      "5587 [Discriminator loss: 0.564917, acc.: 75.00%] [Generator loss: 5.126932]\n",
      "5588 [Discriminator loss: 0.119489, acc.: 93.75%] [Generator loss: 6.868182]\n",
      "5589 [Discriminator loss: 0.354841, acc.: 85.94%] [Generator loss: 4.288487]\n",
      "5590 [Discriminator loss: 0.266062, acc.: 87.50%] [Generator loss: 5.370459]\n",
      "5591 [Discriminator loss: 0.217340, acc.: 90.62%] [Generator loss: 5.799647]\n",
      "5592 [Discriminator loss: 0.271832, acc.: 85.94%] [Generator loss: 4.502876]\n",
      "5593 [Discriminator loss: 0.206675, acc.: 92.19%] [Generator loss: 4.589992]\n",
      "5594 [Discriminator loss: 0.133243, acc.: 93.75%] [Generator loss: 4.293313]\n",
      "5595 [Discriminator loss: 0.177951, acc.: 92.19%] [Generator loss: 3.905623]\n",
      "5596 [Discriminator loss: 0.120162, acc.: 95.31%] [Generator loss: 6.241477]\n",
      "5597 [Discriminator loss: 0.599744, acc.: 76.56%] [Generator loss: 5.213778]\n",
      "5598 [Discriminator loss: 0.167386, acc.: 92.19%] [Generator loss: 6.189151]\n",
      "5599 [Discriminator loss: 0.311019, acc.: 89.06%] [Generator loss: 5.279640]\n",
      "5600 [Discriminator loss: 0.121888, acc.: 96.88%] [Generator loss: 4.638028]\n",
      "5601 [Discriminator loss: 0.254110, acc.: 89.06%] [Generator loss: 4.980246]\n",
      "5602 [Discriminator loss: 0.222926, acc.: 89.06%] [Generator loss: 6.098250]\n",
      "5603 [Discriminator loss: 0.165409, acc.: 93.75%] [Generator loss: 4.423615]\n",
      "5604 [Discriminator loss: 0.221768, acc.: 90.62%] [Generator loss: 4.770627]\n",
      "5605 [Discriminator loss: 0.139531, acc.: 95.31%] [Generator loss: 5.001149]\n",
      "5606 [Discriminator loss: 0.226935, acc.: 92.19%] [Generator loss: 4.568446]\n",
      "5607 [Discriminator loss: 0.212140, acc.: 92.19%] [Generator loss: 4.864491]\n",
      "5608 [Discriminator loss: 0.250096, acc.: 89.06%] [Generator loss: 6.160461]\n",
      "5609 [Discriminator loss: 0.356374, acc.: 82.81%] [Generator loss: 5.760449]\n",
      "5610 [Discriminator loss: 0.267773, acc.: 87.50%] [Generator loss: 5.163953]\n",
      "5611 [Discriminator loss: 0.113674, acc.: 96.88%] [Generator loss: 5.632511]\n",
      "5612 [Discriminator loss: 0.199991, acc.: 92.19%] [Generator loss: 5.305173]\n",
      "5613 [Discriminator loss: 0.180360, acc.: 92.19%] [Generator loss: 4.790616]\n",
      "5614 [Discriminator loss: 0.246327, acc.: 85.94%] [Generator loss: 4.791955]\n",
      "5615 [Discriminator loss: 0.393451, acc.: 81.25%] [Generator loss: 4.462934]\n",
      "5616 [Discriminator loss: 0.183686, acc.: 93.75%] [Generator loss: 5.584875]\n",
      "5617 [Discriminator loss: 0.309742, acc.: 90.62%] [Generator loss: 4.646976]\n",
      "5618 [Discriminator loss: 0.157268, acc.: 93.75%] [Generator loss: 4.491260]\n",
      "5619 [Discriminator loss: 0.286453, acc.: 87.50%] [Generator loss: 4.789102]\n",
      "5620 [Discriminator loss: 0.196442, acc.: 93.75%] [Generator loss: 6.730458]\n",
      "5621 [Discriminator loss: 0.252944, acc.: 93.75%] [Generator loss: 5.770174]\n",
      "5622 [Discriminator loss: 0.440364, acc.: 84.38%] [Generator loss: 3.950954]\n",
      "5623 [Discriminator loss: 0.132135, acc.: 92.19%] [Generator loss: 5.322625]\n",
      "5624 [Discriminator loss: 0.191784, acc.: 92.19%] [Generator loss: 5.260198]\n",
      "5625 [Discriminator loss: 0.269545, acc.: 92.19%] [Generator loss: 6.015040]\n",
      "5626 [Discriminator loss: 0.191694, acc.: 89.06%] [Generator loss: 4.204763]\n",
      "5627 [Discriminator loss: 0.217206, acc.: 92.19%] [Generator loss: 6.203773]\n",
      "5628 [Discriminator loss: 0.118397, acc.: 95.31%] [Generator loss: 5.041042]\n",
      "5629 [Discriminator loss: 0.140768, acc.: 95.31%] [Generator loss: 4.098266]\n",
      "5630 [Discriminator loss: 0.229614, acc.: 92.19%] [Generator loss: 4.907807]\n",
      "5631 [Discriminator loss: 0.244138, acc.: 90.62%] [Generator loss: 5.034268]\n",
      "5632 [Discriminator loss: 0.127131, acc.: 96.88%] [Generator loss: 4.561831]\n",
      "5633 [Discriminator loss: 0.214922, acc.: 93.75%] [Generator loss: 5.590296]\n",
      "5634 [Discriminator loss: 0.241864, acc.: 90.62%] [Generator loss: 4.130185]\n",
      "5635 [Discriminator loss: 0.203621, acc.: 93.75%] [Generator loss: 4.882359]\n",
      "5636 [Discriminator loss: 0.299049, acc.: 89.06%] [Generator loss: 5.661787]\n",
      "5637 [Discriminator loss: 0.254889, acc.: 89.06%] [Generator loss: 4.755710]\n",
      "5638 [Discriminator loss: 0.227791, acc.: 92.19%] [Generator loss: 4.994810]\n",
      "5639 [Discriminator loss: 0.078315, acc.: 98.44%] [Generator loss: 5.241434]\n",
      "5640 [Discriminator loss: 0.092668, acc.: 96.88%] [Generator loss: 4.722796]\n",
      "5641 [Discriminator loss: 0.263409, acc.: 87.50%] [Generator loss: 4.842550]\n",
      "5642 [Discriminator loss: 0.256373, acc.: 84.38%] [Generator loss: 4.684987]\n",
      "5643 [Discriminator loss: 0.247291, acc.: 89.06%] [Generator loss: 4.248847]\n",
      "5644 [Discriminator loss: 0.213104, acc.: 92.19%] [Generator loss: 3.671951]\n",
      "5645 [Discriminator loss: 0.233518, acc.: 84.38%] [Generator loss: 4.272109]\n",
      "5646 [Discriminator loss: 0.126262, acc.: 95.31%] [Generator loss: 5.562711]\n",
      "5647 [Discriminator loss: 0.127806, acc.: 95.31%] [Generator loss: 5.566020]\n",
      "5648 [Discriminator loss: 0.206242, acc.: 89.06%] [Generator loss: 4.085910]\n",
      "5649 [Discriminator loss: 0.224683, acc.: 89.06%] [Generator loss: 4.945345]\n",
      "5650 [Discriminator loss: 0.170060, acc.: 93.75%] [Generator loss: 4.998934]\n",
      "5651 [Discriminator loss: 0.252836, acc.: 87.50%] [Generator loss: 5.623253]\n",
      "5652 [Discriminator loss: 0.199644, acc.: 90.62%] [Generator loss: 5.043164]\n",
      "5653 [Discriminator loss: 0.210292, acc.: 92.19%] [Generator loss: 6.599580]\n",
      "5654 [Discriminator loss: 0.307785, acc.: 87.50%] [Generator loss: 4.886340]\n",
      "5655 [Discriminator loss: 0.113915, acc.: 95.31%] [Generator loss: 5.666729]\n",
      "5656 [Discriminator loss: 0.453846, acc.: 82.81%] [Generator loss: 3.650659]\n",
      "5657 [Discriminator loss: 0.209671, acc.: 89.06%] [Generator loss: 5.712195]\n",
      "5658 [Discriminator loss: 0.177572, acc.: 93.75%] [Generator loss: 4.850379]\n",
      "5659 [Discriminator loss: 0.191907, acc.: 89.06%] [Generator loss: 4.009828]\n",
      "5660 [Discriminator loss: 0.324042, acc.: 92.19%] [Generator loss: 4.794024]\n",
      "5661 [Discriminator loss: 0.190935, acc.: 89.06%] [Generator loss: 4.311568]\n",
      "5662 [Discriminator loss: 0.221334, acc.: 90.62%] [Generator loss: 5.086497]\n",
      "5663 [Discriminator loss: 0.332131, acc.: 82.81%] [Generator loss: 4.687146]\n",
      "5664 [Discriminator loss: 0.255007, acc.: 89.06%] [Generator loss: 4.587346]\n",
      "5665 [Discriminator loss: 0.214601, acc.: 93.75%] [Generator loss: 4.087706]\n",
      "5666 [Discriminator loss: 0.445645, acc.: 82.81%] [Generator loss: 4.159800]\n",
      "5667 [Discriminator loss: 0.196530, acc.: 89.06%] [Generator loss: 5.765760]\n",
      "5668 [Discriminator loss: 0.137212, acc.: 98.44%] [Generator loss: 5.371264]\n",
      "5669 [Discriminator loss: 0.248076, acc.: 89.06%] [Generator loss: 4.428922]\n",
      "5670 [Discriminator loss: 0.149642, acc.: 93.75%] [Generator loss: 4.690660]\n",
      "5671 [Discriminator loss: 0.411328, acc.: 79.69%] [Generator loss: 5.570128]\n",
      "5672 [Discriminator loss: 0.066277, acc.: 98.44%] [Generator loss: 5.795284]\n",
      "5673 [Discriminator loss: 0.296227, acc.: 87.50%] [Generator loss: 4.512599]\n",
      "5674 [Discriminator loss: 0.259088, acc.: 90.62%] [Generator loss: 4.997832]\n",
      "5675 [Discriminator loss: 0.328391, acc.: 84.38%] [Generator loss: 5.684402]\n",
      "5676 [Discriminator loss: 0.226665, acc.: 89.06%] [Generator loss: 5.057951]\n",
      "5677 [Discriminator loss: 0.217725, acc.: 92.19%] [Generator loss: 5.178192]\n",
      "5678 [Discriminator loss: 0.313407, acc.: 90.62%] [Generator loss: 5.243360]\n",
      "5679 [Discriminator loss: 0.129987, acc.: 92.19%] [Generator loss: 4.886540]\n",
      "5680 [Discriminator loss: 0.191519, acc.: 90.62%] [Generator loss: 4.531689]\n",
      "5681 [Discriminator loss: 0.206299, acc.: 90.62%] [Generator loss: 4.425006]\n",
      "5682 [Discriminator loss: 0.166376, acc.: 90.62%] [Generator loss: 5.145921]\n",
      "5683 [Discriminator loss: 0.342668, acc.: 82.81%] [Generator loss: 4.436057]\n",
      "5684 [Discriminator loss: 0.543497, acc.: 78.12%] [Generator loss: 3.812295]\n",
      "5685 [Discriminator loss: 0.245650, acc.: 92.19%] [Generator loss: 5.979805]\n",
      "5686 [Discriminator loss: 0.239724, acc.: 90.62%] [Generator loss: 5.985145]\n",
      "5687 [Discriminator loss: 0.295753, acc.: 89.06%] [Generator loss: 4.318844]\n",
      "5688 [Discriminator loss: 0.334317, acc.: 87.50%] [Generator loss: 4.509099]\n",
      "5689 [Discriminator loss: 0.094840, acc.: 96.88%] [Generator loss: 4.822425]\n",
      "5690 [Discriminator loss: 0.258395, acc.: 90.62%] [Generator loss: 4.836058]\n",
      "5691 [Discriminator loss: 0.082760, acc.: 98.44%] [Generator loss: 5.818587]\n",
      "5692 [Discriminator loss: 0.141689, acc.: 93.75%] [Generator loss: 4.352980]\n",
      "5693 [Discriminator loss: 0.335593, acc.: 81.25%] [Generator loss: 5.852647]\n",
      "5694 [Discriminator loss: 0.113203, acc.: 98.44%] [Generator loss: 5.355574]\n",
      "5695 [Discriminator loss: 0.239877, acc.: 87.50%] [Generator loss: 4.503223]\n",
      "5696 [Discriminator loss: 0.188172, acc.: 93.75%] [Generator loss: 6.265336]\n",
      "5697 [Discriminator loss: 0.381741, acc.: 85.94%] [Generator loss: 5.070469]\n",
      "5698 [Discriminator loss: 0.486883, acc.: 82.81%] [Generator loss: 4.365345]\n",
      "5699 [Discriminator loss: 0.173827, acc.: 92.19%] [Generator loss: 5.333358]\n",
      "5700 [Discriminator loss: 0.372313, acc.: 85.94%] [Generator loss: 5.680407]\n",
      "5701 [Discriminator loss: 0.388376, acc.: 84.38%] [Generator loss: 4.746149]\n",
      "5702 [Discriminator loss: 0.153152, acc.: 95.31%] [Generator loss: 4.845954]\n",
      "5703 [Discriminator loss: 0.184245, acc.: 90.62%] [Generator loss: 4.090977]\n",
      "5704 [Discriminator loss: 0.097696, acc.: 95.31%] [Generator loss: 4.607265]\n",
      "5705 [Discriminator loss: 0.325860, acc.: 84.38%] [Generator loss: 5.139285]\n",
      "5706 [Discriminator loss: 0.404836, acc.: 82.81%] [Generator loss: 5.309208]\n",
      "5707 [Discriminator loss: 0.134969, acc.: 95.31%] [Generator loss: 5.311707]\n",
      "5708 [Discriminator loss: 0.190255, acc.: 92.19%] [Generator loss: 4.828522]\n",
      "5709 [Discriminator loss: 0.162574, acc.: 95.31%] [Generator loss: 5.273155]\n",
      "5710 [Discriminator loss: 0.103832, acc.: 98.44%] [Generator loss: 5.188042]\n",
      "5711 [Discriminator loss: 0.264340, acc.: 87.50%] [Generator loss: 5.235917]\n",
      "5712 [Discriminator loss: 0.117077, acc.: 95.31%] [Generator loss: 5.533437]\n",
      "5713 [Discriminator loss: 0.403760, acc.: 78.12%] [Generator loss: 4.692439]\n",
      "5714 [Discriminator loss: 0.240180, acc.: 85.94%] [Generator loss: 5.882200]\n",
      "5715 [Discriminator loss: 0.346219, acc.: 85.94%] [Generator loss: 5.209396]\n",
      "5716 [Discriminator loss: 0.087679, acc.: 96.88%] [Generator loss: 6.599589]\n",
      "5717 [Discriminator loss: 0.528826, acc.: 76.56%] [Generator loss: 4.771775]\n",
      "5718 [Discriminator loss: 0.110192, acc.: 95.31%] [Generator loss: 5.033181]\n",
      "5719 [Discriminator loss: 0.175128, acc.: 92.19%] [Generator loss: 5.402905]\n",
      "5720 [Discriminator loss: 0.099354, acc.: 96.88%] [Generator loss: 5.569622]\n",
      "5721 [Discriminator loss: 0.291390, acc.: 87.50%] [Generator loss: 4.550336]\n",
      "5722 [Discriminator loss: 0.159487, acc.: 92.19%] [Generator loss: 5.834657]\n",
      "5723 [Discriminator loss: 0.219417, acc.: 92.19%] [Generator loss: 5.005098]\n",
      "5724 [Discriminator loss: 0.309243, acc.: 85.94%] [Generator loss: 4.587857]\n",
      "5725 [Discriminator loss: 0.240375, acc.: 89.06%] [Generator loss: 5.086455]\n",
      "5726 [Discriminator loss: 0.355575, acc.: 87.50%] [Generator loss: 5.259336]\n",
      "5727 [Discriminator loss: 0.147208, acc.: 95.31%] [Generator loss: 4.966157]\n",
      "5728 [Discriminator loss: 0.466382, acc.: 82.81%] [Generator loss: 4.660410]\n",
      "5729 [Discriminator loss: 0.273116, acc.: 92.19%] [Generator loss: 5.738047]\n",
      "5730 [Discriminator loss: 0.110369, acc.: 95.31%] [Generator loss: 6.845411]\n",
      "5731 [Discriminator loss: 0.401608, acc.: 81.25%] [Generator loss: 4.537313]\n",
      "5732 [Discriminator loss: 0.231270, acc.: 92.19%] [Generator loss: 5.885522]\n",
      "5733 [Discriminator loss: 0.177335, acc.: 93.75%] [Generator loss: 4.432517]\n",
      "5734 [Discriminator loss: 0.454888, acc.: 79.69%] [Generator loss: 4.937453]\n",
      "5735 [Discriminator loss: 0.331421, acc.: 85.94%] [Generator loss: 5.016978]\n",
      "5736 [Discriminator loss: 0.280233, acc.: 85.94%] [Generator loss: 5.405498]\n",
      "5737 [Discriminator loss: 0.351426, acc.: 85.94%] [Generator loss: 4.256058]\n",
      "5738 [Discriminator loss: 0.279832, acc.: 84.38%] [Generator loss: 4.509165]\n",
      "5739 [Discriminator loss: 0.149846, acc.: 93.75%] [Generator loss: 5.144665]\n",
      "5740 [Discriminator loss: 0.213028, acc.: 85.94%] [Generator loss: 4.900766]\n",
      "5741 [Discriminator loss: 0.405293, acc.: 84.38%] [Generator loss: 4.889068]\n",
      "5742 [Discriminator loss: 0.289193, acc.: 84.38%] [Generator loss: 5.046538]\n",
      "5743 [Discriminator loss: 0.158841, acc.: 95.31%] [Generator loss: 4.515005]\n",
      "5744 [Discriminator loss: 0.219984, acc.: 90.62%] [Generator loss: 4.913040]\n",
      "5745 [Discriminator loss: 0.339068, acc.: 84.38%] [Generator loss: 4.363291]\n",
      "5746 [Discriminator loss: 0.201805, acc.: 92.19%] [Generator loss: 5.444082]\n",
      "5747 [Discriminator loss: 0.181853, acc.: 89.06%] [Generator loss: 5.406544]\n",
      "5748 [Discriminator loss: 0.151441, acc.: 92.19%] [Generator loss: 4.892070]\n",
      "5749 [Discriminator loss: 0.162191, acc.: 95.31%] [Generator loss: 4.733357]\n",
      "5750 [Discriminator loss: 0.173094, acc.: 92.19%] [Generator loss: 4.257227]\n",
      "5751 [Discriminator loss: 0.244753, acc.: 89.06%] [Generator loss: 4.758938]\n",
      "5752 [Discriminator loss: 0.247541, acc.: 87.50%] [Generator loss: 5.129426]\n",
      "5753 [Discriminator loss: 0.109103, acc.: 95.31%] [Generator loss: 3.980460]\n",
      "5754 [Discriminator loss: 0.358335, acc.: 79.69%] [Generator loss: 5.261482]\n",
      "5755 [Discriminator loss: 0.254456, acc.: 92.19%] [Generator loss: 6.117644]\n",
      "5756 [Discriminator loss: 0.259334, acc.: 90.62%] [Generator loss: 4.897730]\n",
      "5757 [Discriminator loss: 0.207963, acc.: 93.75%] [Generator loss: 5.045427]\n",
      "5758 [Discriminator loss: 0.147193, acc.: 93.75%] [Generator loss: 5.382886]\n",
      "5759 [Discriminator loss: 0.397612, acc.: 78.12%] [Generator loss: 3.221664]\n",
      "5760 [Discriminator loss: 0.359740, acc.: 82.81%] [Generator loss: 5.751372]\n",
      "5761 [Discriminator loss: 0.240229, acc.: 82.81%] [Generator loss: 4.307502]\n",
      "5762 [Discriminator loss: 0.105260, acc.: 98.44%] [Generator loss: 5.261545]\n",
      "5763 [Discriminator loss: 0.177570, acc.: 95.31%] [Generator loss: 4.146216]\n",
      "5764 [Discriminator loss: 0.127604, acc.: 96.88%] [Generator loss: 4.573810]\n",
      "5765 [Discriminator loss: 0.141477, acc.: 92.19%] [Generator loss: 4.378514]\n",
      "5766 [Discriminator loss: 0.497294, acc.: 79.69%] [Generator loss: 5.201944]\n",
      "5767 [Discriminator loss: 0.099944, acc.: 96.88%] [Generator loss: 4.496167]\n",
      "5768 [Discriminator loss: 0.308440, acc.: 85.94%] [Generator loss: 5.898712]\n",
      "5769 [Discriminator loss: 0.126645, acc.: 93.75%] [Generator loss: 5.809776]\n",
      "5770 [Discriminator loss: 0.165618, acc.: 92.19%] [Generator loss: 5.359730]\n",
      "5771 [Discriminator loss: 0.173525, acc.: 93.75%] [Generator loss: 5.206672]\n",
      "5772 [Discriminator loss: 0.198857, acc.: 87.50%] [Generator loss: 3.871023]\n",
      "5773 [Discriminator loss: 0.348430, acc.: 89.06%] [Generator loss: 4.679865]\n",
      "5774 [Discriminator loss: 0.077327, acc.: 98.44%] [Generator loss: 4.846166]\n",
      "5775 [Discriminator loss: 0.114145, acc.: 96.88%] [Generator loss: 5.141684]\n",
      "5776 [Discriminator loss: 0.297005, acc.: 87.50%] [Generator loss: 5.811457]\n",
      "5777 [Discriminator loss: 0.144027, acc.: 93.75%] [Generator loss: 5.086345]\n",
      "5778 [Discriminator loss: 0.097515, acc.: 98.44%] [Generator loss: 5.047352]\n",
      "5779 [Discriminator loss: 0.119931, acc.: 96.88%] [Generator loss: 5.144506]\n",
      "5780 [Discriminator loss: 0.258354, acc.: 85.94%] [Generator loss: 4.624863]\n",
      "5781 [Discriminator loss: 0.244242, acc.: 92.19%] [Generator loss: 5.783354]\n",
      "5782 [Discriminator loss: 0.079802, acc.: 100.00%] [Generator loss: 4.422776]\n",
      "5783 [Discriminator loss: 0.306263, acc.: 89.06%] [Generator loss: 3.899209]\n",
      "5784 [Discriminator loss: 0.279355, acc.: 87.50%] [Generator loss: 5.770736]\n",
      "5785 [Discriminator loss: 0.238549, acc.: 85.94%] [Generator loss: 4.929693]\n",
      "5786 [Discriminator loss: 0.264742, acc.: 85.94%] [Generator loss: 5.877607]\n",
      "5787 [Discriminator loss: 0.205039, acc.: 89.06%] [Generator loss: 5.199070]\n",
      "5788 [Discriminator loss: 0.323925, acc.: 82.81%] [Generator loss: 5.348045]\n",
      "5789 [Discriminator loss: 0.179411, acc.: 95.31%] [Generator loss: 5.843265]\n",
      "5790 [Discriminator loss: 0.210271, acc.: 89.06%] [Generator loss: 4.944253]\n",
      "5791 [Discriminator loss: 0.440193, acc.: 78.12%] [Generator loss: 3.679277]\n",
      "5792 [Discriminator loss: 0.197180, acc.: 92.19%] [Generator loss: 5.971781]\n",
      "5793 [Discriminator loss: 0.289395, acc.: 87.50%] [Generator loss: 4.886825]\n",
      "5794 [Discriminator loss: 0.272337, acc.: 85.94%] [Generator loss: 5.649607]\n",
      "5795 [Discriminator loss: 0.154603, acc.: 95.31%] [Generator loss: 4.796515]\n",
      "5796 [Discriminator loss: 0.074091, acc.: 96.88%] [Generator loss: 5.528886]\n",
      "5797 [Discriminator loss: 0.234367, acc.: 90.62%] [Generator loss: 4.978964]\n",
      "5798 [Discriminator loss: 0.165009, acc.: 93.75%] [Generator loss: 5.942915]\n",
      "5799 [Discriminator loss: 0.114008, acc.: 93.75%] [Generator loss: 5.236928]\n",
      "5800 [Discriminator loss: 0.144855, acc.: 95.31%] [Generator loss: 3.913356]\n",
      "5801 [Discriminator loss: 0.186648, acc.: 93.75%] [Generator loss: 4.316343]\n",
      "5802 [Discriminator loss: 0.357516, acc.: 85.94%] [Generator loss: 4.401731]\n",
      "5803 [Discriminator loss: 0.111812, acc.: 95.31%] [Generator loss: 5.119307]\n",
      "5804 [Discriminator loss: 0.173543, acc.: 93.75%] [Generator loss: 3.943510]\n",
      "5805 [Discriminator loss: 0.141793, acc.: 93.75%] [Generator loss: 4.746623]\n",
      "5806 [Discriminator loss: 0.324294, acc.: 85.94%] [Generator loss: 5.512650]\n",
      "5807 [Discriminator loss: 0.204521, acc.: 90.62%] [Generator loss: 5.059194]\n",
      "5808 [Discriminator loss: 0.339077, acc.: 82.81%] [Generator loss: 5.392630]\n",
      "5809 [Discriminator loss: 0.262391, acc.: 85.94%] [Generator loss: 4.593328]\n",
      "5810 [Discriminator loss: 0.198626, acc.: 93.75%] [Generator loss: 6.012991]\n",
      "5811 [Discriminator loss: 0.234696, acc.: 85.94%] [Generator loss: 5.353325]\n",
      "5812 [Discriminator loss: 0.228472, acc.: 90.62%] [Generator loss: 5.749893]\n",
      "5813 [Discriminator loss: 0.253313, acc.: 87.50%] [Generator loss: 5.638264]\n",
      "5814 [Discriminator loss: 0.158089, acc.: 93.75%] [Generator loss: 5.266355]\n",
      "5815 [Discriminator loss: 0.209711, acc.: 92.19%] [Generator loss: 5.289391]\n",
      "5816 [Discriminator loss: 0.214331, acc.: 87.50%] [Generator loss: 5.957036]\n",
      "5817 [Discriminator loss: 0.368335, acc.: 84.38%] [Generator loss: 4.502748]\n",
      "5818 [Discriminator loss: 0.181244, acc.: 90.62%] [Generator loss: 4.781631]\n",
      "5819 [Discriminator loss: 0.190370, acc.: 93.75%] [Generator loss: 5.030228]\n",
      "5820 [Discriminator loss: 0.157748, acc.: 95.31%] [Generator loss: 5.465792]\n",
      "5821 [Discriminator loss: 0.294347, acc.: 89.06%] [Generator loss: 4.354399]\n",
      "5822 [Discriminator loss: 0.244685, acc.: 87.50%] [Generator loss: 5.062803]\n",
      "5823 [Discriminator loss: 0.187142, acc.: 93.75%] [Generator loss: 5.023809]\n",
      "5824 [Discriminator loss: 0.195719, acc.: 89.06%] [Generator loss: 5.197550]\n",
      "5825 [Discriminator loss: 0.210095, acc.: 92.19%] [Generator loss: 5.769488]\n",
      "5826 [Discriminator loss: 0.165234, acc.: 93.75%] [Generator loss: 4.467779]\n",
      "5827 [Discriminator loss: 0.216075, acc.: 92.19%] [Generator loss: 4.442716]\n",
      "5828 [Discriminator loss: 0.182088, acc.: 92.19%] [Generator loss: 4.401527]\n",
      "5829 [Discriminator loss: 0.209593, acc.: 92.19%] [Generator loss: 4.314434]\n",
      "5830 [Discriminator loss: 0.172517, acc.: 92.19%] [Generator loss: 4.738637]\n",
      "5831 [Discriminator loss: 0.346493, acc.: 81.25%] [Generator loss: 6.022042]\n",
      "5832 [Discriminator loss: 0.425865, acc.: 90.62%] [Generator loss: 4.946607]\n",
      "5833 [Discriminator loss: 0.173050, acc.: 92.19%] [Generator loss: 3.980638]\n",
      "5834 [Discriminator loss: 0.120316, acc.: 96.88%] [Generator loss: 3.961484]\n",
      "5835 [Discriminator loss: 0.159792, acc.: 93.75%] [Generator loss: 4.917839]\n",
      "5836 [Discriminator loss: 0.176566, acc.: 93.75%] [Generator loss: 4.667129]\n",
      "5837 [Discriminator loss: 0.135030, acc.: 95.31%] [Generator loss: 4.244796]\n",
      "5838 [Discriminator loss: 0.263692, acc.: 89.06%] [Generator loss: 5.666050]\n",
      "5839 [Discriminator loss: 0.325166, acc.: 87.50%] [Generator loss: 4.420368]\n",
      "5840 [Discriminator loss: 0.210075, acc.: 93.75%] [Generator loss: 5.537938]\n",
      "5841 [Discriminator loss: 0.249456, acc.: 85.94%] [Generator loss: 4.459476]\n",
      "5842 [Discriminator loss: 0.116380, acc.: 96.88%] [Generator loss: 4.101696]\n",
      "5843 [Discriminator loss: 0.090374, acc.: 98.44%] [Generator loss: 4.112943]\n",
      "5844 [Discriminator loss: 0.215996, acc.: 93.75%] [Generator loss: 5.600239]\n",
      "5845 [Discriminator loss: 0.379542, acc.: 87.50%] [Generator loss: 3.868903]\n",
      "5846 [Discriminator loss: 0.262984, acc.: 84.38%] [Generator loss: 5.011176]\n",
      "5847 [Discriminator loss: 0.190276, acc.: 89.06%] [Generator loss: 6.006307]\n",
      "5848 [Discriminator loss: 0.053671, acc.: 98.44%] [Generator loss: 4.667376]\n",
      "5849 [Discriminator loss: 0.299646, acc.: 89.06%] [Generator loss: 3.864524]\n",
      "5850 [Discriminator loss: 0.074043, acc.: 96.88%] [Generator loss: 5.140471]\n",
      "5851 [Discriminator loss: 0.173054, acc.: 92.19%] [Generator loss: 5.960975]\n",
      "5852 [Discriminator loss: 0.218533, acc.: 89.06%] [Generator loss: 6.010855]\n",
      "5853 [Discriminator loss: 0.221202, acc.: 89.06%] [Generator loss: 4.187124]\n",
      "5854 [Discriminator loss: 0.338542, acc.: 85.94%] [Generator loss: 4.951959]\n",
      "5855 [Discriminator loss: 0.126773, acc.: 98.44%] [Generator loss: 5.155455]\n",
      "5856 [Discriminator loss: 0.288329, acc.: 89.06%] [Generator loss: 4.302453]\n",
      "5857 [Discriminator loss: 0.108177, acc.: 95.31%] [Generator loss: 4.582406]\n",
      "5858 [Discriminator loss: 0.109482, acc.: 93.75%] [Generator loss: 4.152132]\n",
      "5859 [Discriminator loss: 0.313641, acc.: 87.50%] [Generator loss: 4.758378]\n",
      "5860 [Discriminator loss: 0.160399, acc.: 95.31%] [Generator loss: 5.316023]\n",
      "5861 [Discriminator loss: 0.129651, acc.: 95.31%] [Generator loss: 4.577994]\n",
      "5862 [Discriminator loss: 0.235098, acc.: 90.62%] [Generator loss: 4.957958]\n",
      "5863 [Discriminator loss: 0.210559, acc.: 92.19%] [Generator loss: 5.507086]\n",
      "5864 [Discriminator loss: 0.280688, acc.: 89.06%] [Generator loss: 4.767237]\n",
      "5865 [Discriminator loss: 0.231083, acc.: 93.75%] [Generator loss: 5.505659]\n",
      "5866 [Discriminator loss: 0.273910, acc.: 85.94%] [Generator loss: 5.014522]\n",
      "5867 [Discriminator loss: 0.127867, acc.: 93.75%] [Generator loss: 3.953269]\n",
      "5868 [Discriminator loss: 0.183172, acc.: 92.19%] [Generator loss: 5.596197]\n",
      "5869 [Discriminator loss: 0.090858, acc.: 96.88%] [Generator loss: 5.445522]\n",
      "5870 [Discriminator loss: 0.321496, acc.: 87.50%] [Generator loss: 5.643958]\n",
      "5871 [Discriminator loss: 0.496751, acc.: 81.25%] [Generator loss: 5.363826]\n",
      "5872 [Discriminator loss: 0.171459, acc.: 90.62%] [Generator loss: 4.535321]\n",
      "5873 [Discriminator loss: 0.189797, acc.: 95.31%] [Generator loss: 5.211030]\n",
      "5874 [Discriminator loss: 0.125571, acc.: 95.31%] [Generator loss: 4.649479]\n",
      "5875 [Discriminator loss: 0.339793, acc.: 84.38%] [Generator loss: 4.333688]\n",
      "5876 [Discriminator loss: 0.139780, acc.: 93.75%] [Generator loss: 6.394226]\n",
      "5877 [Discriminator loss: 0.211388, acc.: 90.62%] [Generator loss: 5.444384]\n",
      "5878 [Discriminator loss: 0.114156, acc.: 96.88%] [Generator loss: 5.526336]\n",
      "5879 [Discriminator loss: 0.137045, acc.: 92.19%] [Generator loss: 5.247413]\n",
      "5880 [Discriminator loss: 0.256892, acc.: 89.06%] [Generator loss: 4.925697]\n",
      "5881 [Discriminator loss: 0.154097, acc.: 93.75%] [Generator loss: 4.842441]\n",
      "5882 [Discriminator loss: 0.173662, acc.: 95.31%] [Generator loss: 4.917047]\n",
      "5883 [Discriminator loss: 0.287391, acc.: 85.94%] [Generator loss: 4.646927]\n",
      "5884 [Discriminator loss: 0.214564, acc.: 93.75%] [Generator loss: 5.074691]\n",
      "5885 [Discriminator loss: 0.105849, acc.: 95.31%] [Generator loss: 5.855922]\n",
      "5886 [Discriminator loss: 0.227954, acc.: 90.62%] [Generator loss: 4.002295]\n",
      "5887 [Discriminator loss: 0.186818, acc.: 90.62%] [Generator loss: 5.256282]\n",
      "5888 [Discriminator loss: 0.396091, acc.: 78.12%] [Generator loss: 5.129414]\n",
      "5889 [Discriminator loss: 0.090123, acc.: 98.44%] [Generator loss: 5.385609]\n",
      "5890 [Discriminator loss: 0.247760, acc.: 92.19%] [Generator loss: 3.983057]\n",
      "5891 [Discriminator loss: 0.192680, acc.: 85.94%] [Generator loss: 5.332304]\n",
      "5892 [Discriminator loss: 0.201266, acc.: 92.19%] [Generator loss: 5.596880]\n",
      "5893 [Discriminator loss: 0.334732, acc.: 82.81%] [Generator loss: 5.303466]\n",
      "5894 [Discriminator loss: 0.142855, acc.: 93.75%] [Generator loss: 5.416218]\n",
      "5895 [Discriminator loss: 0.683880, acc.: 73.44%] [Generator loss: 5.569530]\n",
      "5896 [Discriminator loss: 0.090242, acc.: 98.44%] [Generator loss: 5.701614]\n",
      "5897 [Discriminator loss: 0.220014, acc.: 89.06%] [Generator loss: 5.687059]\n",
      "5898 [Discriminator loss: 0.173287, acc.: 93.75%] [Generator loss: 5.696849]\n",
      "5899 [Discriminator loss: 0.217746, acc.: 93.75%] [Generator loss: 4.814608]\n",
      "5900 [Discriminator loss: 0.320709, acc.: 81.25%] [Generator loss: 4.360359]\n",
      "5901 [Discriminator loss: 0.223244, acc.: 92.19%] [Generator loss: 4.548831]\n",
      "5902 [Discriminator loss: 0.159947, acc.: 92.19%] [Generator loss: 4.834069]\n",
      "5903 [Discriminator loss: 0.307143, acc.: 85.94%] [Generator loss: 4.052224]\n",
      "5904 [Discriminator loss: 0.195688, acc.: 90.62%] [Generator loss: 4.715681]\n",
      "5905 [Discriminator loss: 0.089359, acc.: 95.31%] [Generator loss: 6.077782]\n",
      "5906 [Discriminator loss: 0.140555, acc.: 89.06%] [Generator loss: 4.809800]\n",
      "5907 [Discriminator loss: 0.325890, acc.: 85.94%] [Generator loss: 5.398689]\n",
      "5908 [Discriminator loss: 0.191896, acc.: 95.31%] [Generator loss: 4.531391]\n",
      "5909 [Discriminator loss: 0.203677, acc.: 90.62%] [Generator loss: 3.949583]\n",
      "5910 [Discriminator loss: 0.343751, acc.: 84.38%] [Generator loss: 5.714587]\n",
      "5911 [Discriminator loss: 0.211571, acc.: 90.62%] [Generator loss: 6.273499]\n",
      "5912 [Discriminator loss: 0.259464, acc.: 87.50%] [Generator loss: 4.586549]\n",
      "5913 [Discriminator loss: 0.127799, acc.: 98.44%] [Generator loss: 5.359297]\n",
      "5914 [Discriminator loss: 0.150001, acc.: 93.75%] [Generator loss: 4.160666]\n",
      "5915 [Discriminator loss: 0.209681, acc.: 92.19%] [Generator loss: 4.064863]\n",
      "5916 [Discriminator loss: 0.309007, acc.: 85.94%] [Generator loss: 6.575250]\n",
      "5917 [Discriminator loss: 0.330939, acc.: 85.94%] [Generator loss: 4.482124]\n",
      "5918 [Discriminator loss: 0.304891, acc.: 87.50%] [Generator loss: 3.866343]\n",
      "5919 [Discriminator loss: 0.321240, acc.: 87.50%] [Generator loss: 5.767517]\n",
      "5920 [Discriminator loss: 0.097570, acc.: 95.31%] [Generator loss: 6.587881]\n",
      "5921 [Discriminator loss: 0.233476, acc.: 87.50%] [Generator loss: 4.918654]\n",
      "5922 [Discriminator loss: 0.260787, acc.: 84.38%] [Generator loss: 4.249139]\n",
      "5923 [Discriminator loss: 0.187759, acc.: 92.19%] [Generator loss: 5.551620]\n",
      "5924 [Discriminator loss: 0.287388, acc.: 82.81%] [Generator loss: 6.152544]\n",
      "5925 [Discriminator loss: 0.140175, acc.: 93.75%] [Generator loss: 5.314479]\n",
      "5926 [Discriminator loss: 0.230348, acc.: 90.62%] [Generator loss: 5.707475]\n",
      "5927 [Discriminator loss: 0.183150, acc.: 92.19%] [Generator loss: 4.768167]\n",
      "5928 [Discriminator loss: 0.371393, acc.: 84.38%] [Generator loss: 4.958535]\n",
      "5929 [Discriminator loss: 0.086024, acc.: 98.44%] [Generator loss: 6.108529]\n",
      "5930 [Discriminator loss: 0.160190, acc.: 93.75%] [Generator loss: 5.009172]\n",
      "5931 [Discriminator loss: 0.285583, acc.: 89.06%] [Generator loss: 5.011456]\n",
      "5932 [Discriminator loss: 0.169943, acc.: 95.31%] [Generator loss: 5.297170]\n",
      "5933 [Discriminator loss: 0.225875, acc.: 87.50%] [Generator loss: 4.744754]\n",
      "5934 [Discriminator loss: 0.279204, acc.: 87.50%] [Generator loss: 5.644861]\n",
      "5935 [Discriminator loss: 0.588640, acc.: 76.56%] [Generator loss: 5.468630]\n",
      "5936 [Discriminator loss: 0.196444, acc.: 87.50%] [Generator loss: 5.355183]\n",
      "5937 [Discriminator loss: 0.033794, acc.: 98.44%] [Generator loss: 5.840152]\n",
      "5938 [Discriminator loss: 0.305782, acc.: 87.50%] [Generator loss: 3.630298]\n",
      "5939 [Discriminator loss: 0.148293, acc.: 96.88%] [Generator loss: 5.777650]\n",
      "5940 [Discriminator loss: 0.248119, acc.: 87.50%] [Generator loss: 5.401999]\n",
      "5941 [Discriminator loss: 0.358429, acc.: 87.50%] [Generator loss: 4.921817]\n",
      "5942 [Discriminator loss: 0.178393, acc.: 93.75%] [Generator loss: 4.899371]\n",
      "5943 [Discriminator loss: 0.140205, acc.: 96.88%] [Generator loss: 5.684546]\n",
      "5944 [Discriminator loss: 0.241114, acc.: 92.19%] [Generator loss: 4.115090]\n",
      "5945 [Discriminator loss: 0.185621, acc.: 90.62%] [Generator loss: 3.655474]\n",
      "5946 [Discriminator loss: 0.273235, acc.: 87.50%] [Generator loss: 4.743731]\n",
      "5947 [Discriminator loss: 0.234486, acc.: 90.62%] [Generator loss: 5.763234]\n",
      "5948 [Discriminator loss: 0.311403, acc.: 84.38%] [Generator loss: 4.879311]\n",
      "5949 [Discriminator loss: 0.248748, acc.: 95.31%] [Generator loss: 4.740976]\n",
      "5950 [Discriminator loss: 0.466658, acc.: 81.25%] [Generator loss: 5.150018]\n",
      "5951 [Discriminator loss: 0.261943, acc.: 87.50%] [Generator loss: 5.063025]\n",
      "5952 [Discriminator loss: 0.373880, acc.: 79.69%] [Generator loss: 4.524520]\n",
      "5953 [Discriminator loss: 0.094742, acc.: 96.88%] [Generator loss: 5.213602]\n",
      "5954 [Discriminator loss: 0.121175, acc.: 96.88%] [Generator loss: 4.364306]\n",
      "5955 [Discriminator loss: 0.239570, acc.: 89.06%] [Generator loss: 4.534649]\n",
      "5956 [Discriminator loss: 0.184751, acc.: 93.75%] [Generator loss: 5.829160]\n",
      "5957 [Discriminator loss: 0.173446, acc.: 92.19%] [Generator loss: 5.038514]\n",
      "5958 [Discriminator loss: 0.165257, acc.: 93.75%] [Generator loss: 4.823565]\n",
      "5959 [Discriminator loss: 0.372558, acc.: 82.81%] [Generator loss: 5.081893]\n",
      "5960 [Discriminator loss: 0.162626, acc.: 93.75%] [Generator loss: 5.011364]\n",
      "5961 [Discriminator loss: 0.274790, acc.: 85.94%] [Generator loss: 4.817071]\n",
      "5962 [Discriminator loss: 0.216754, acc.: 90.62%] [Generator loss: 4.799871]\n",
      "5963 [Discriminator loss: 0.097255, acc.: 95.31%] [Generator loss: 5.591243]\n",
      "5964 [Discriminator loss: 0.243378, acc.: 89.06%] [Generator loss: 5.028925]\n",
      "5965 [Discriminator loss: 0.150159, acc.: 93.75%] [Generator loss: 4.468133]\n",
      "5966 [Discriminator loss: 0.129676, acc.: 95.31%] [Generator loss: 4.403408]\n",
      "5967 [Discriminator loss: 0.276792, acc.: 87.50%] [Generator loss: 5.074644]\n",
      "5968 [Discriminator loss: 0.217598, acc.: 89.06%] [Generator loss: 4.493069]\n",
      "5969 [Discriminator loss: 0.233479, acc.: 92.19%] [Generator loss: 4.948808]\n",
      "5970 [Discriminator loss: 0.097252, acc.: 98.44%] [Generator loss: 5.329916]\n",
      "5971 [Discriminator loss: 0.327810, acc.: 90.62%] [Generator loss: 5.374405]\n",
      "5972 [Discriminator loss: 0.386623, acc.: 84.38%] [Generator loss: 5.481736]\n",
      "5973 [Discriminator loss: 0.229137, acc.: 87.50%] [Generator loss: 5.111432]\n",
      "5974 [Discriminator loss: 0.171309, acc.: 92.19%] [Generator loss: 4.676856]\n",
      "5975 [Discriminator loss: 0.207344, acc.: 93.75%] [Generator loss: 6.455664]\n",
      "5976 [Discriminator loss: 0.188157, acc.: 92.19%] [Generator loss: 4.037000]\n",
      "5977 [Discriminator loss: 0.287335, acc.: 84.38%] [Generator loss: 5.244359]\n",
      "5978 [Discriminator loss: 0.250676, acc.: 89.06%] [Generator loss: 6.057702]\n",
      "5979 [Discriminator loss: 0.223361, acc.: 87.50%] [Generator loss: 5.229300]\n",
      "5980 [Discriminator loss: 0.111003, acc.: 95.31%] [Generator loss: 3.573895]\n",
      "5981 [Discriminator loss: 0.219568, acc.: 95.31%] [Generator loss: 4.976168]\n",
      "5982 [Discriminator loss: 0.080064, acc.: 96.88%] [Generator loss: 5.327304]\n",
      "5983 [Discriminator loss: 0.308317, acc.: 85.94%] [Generator loss: 5.437765]\n",
      "5984 [Discriminator loss: 0.122755, acc.: 95.31%] [Generator loss: 4.944020]\n",
      "5985 [Discriminator loss: 0.127898, acc.: 93.75%] [Generator loss: 5.171715]\n",
      "5986 [Discriminator loss: 0.157174, acc.: 92.19%] [Generator loss: 5.570953]\n",
      "5987 [Discriminator loss: 0.184792, acc.: 87.50%] [Generator loss: 5.600482]\n",
      "5988 [Discriminator loss: 0.247510, acc.: 89.06%] [Generator loss: 5.369378]\n",
      "5989 [Discriminator loss: 0.097926, acc.: 96.88%] [Generator loss: 5.870992]\n",
      "5990 [Discriminator loss: 0.270518, acc.: 93.75%] [Generator loss: 4.080262]\n",
      "5991 [Discriminator loss: 0.148039, acc.: 95.31%] [Generator loss: 5.301524]\n",
      "5992 [Discriminator loss: 0.154830, acc.: 95.31%] [Generator loss: 4.954982]\n",
      "5993 [Discriminator loss: 0.243772, acc.: 90.62%] [Generator loss: 5.060251]\n",
      "5994 [Discriminator loss: 0.211933, acc.: 89.06%] [Generator loss: 5.222573]\n",
      "5995 [Discriminator loss: 0.428563, acc.: 81.25%] [Generator loss: 6.680297]\n",
      "5996 [Discriminator loss: 0.405223, acc.: 82.81%] [Generator loss: 5.009108]\n",
      "5997 [Discriminator loss: 0.291187, acc.: 87.50%] [Generator loss: 5.303899]\n",
      "5998 [Discriminator loss: 0.113400, acc.: 96.88%] [Generator loss: 5.420000]\n",
      "5999 [Discriminator loss: 0.271730, acc.: 89.06%] [Generator loss: 5.467587]\n",
      "6000 [Discriminator loss: 0.065881, acc.: 100.00%] [Generator loss: 4.921664]\n",
      "6001 [Discriminator loss: 0.105398, acc.: 95.31%] [Generator loss: 5.269435]\n",
      "6002 [Discriminator loss: 0.107038, acc.: 95.31%] [Generator loss: 4.329507]\n",
      "6003 [Discriminator loss: 0.318110, acc.: 89.06%] [Generator loss: 5.138887]\n",
      "6004 [Discriminator loss: 0.232807, acc.: 93.75%] [Generator loss: 4.778911]\n",
      "6005 [Discriminator loss: 0.204616, acc.: 90.62%] [Generator loss: 3.971946]\n",
      "6006 [Discriminator loss: 0.241298, acc.: 89.06%] [Generator loss: 5.116775]\n",
      "6007 [Discriminator loss: 0.173755, acc.: 92.19%] [Generator loss: 5.954931]\n",
      "6008 [Discriminator loss: 0.185147, acc.: 90.62%] [Generator loss: 6.264381]\n",
      "6009 [Discriminator loss: 0.298359, acc.: 87.50%] [Generator loss: 3.771060]\n",
      "6010 [Discriminator loss: 0.087874, acc.: 96.88%] [Generator loss: 4.510155]\n",
      "6011 [Discriminator loss: 0.215380, acc.: 89.06%] [Generator loss: 5.075070]\n",
      "6012 [Discriminator loss: 0.098405, acc.: 95.31%] [Generator loss: 4.956785]\n",
      "6013 [Discriminator loss: 0.478289, acc.: 84.38%] [Generator loss: 5.188779]\n",
      "6014 [Discriminator loss: 0.303472, acc.: 89.06%] [Generator loss: 5.191125]\n",
      "6015 [Discriminator loss: 0.145499, acc.: 95.31%] [Generator loss: 6.307445]\n",
      "6016 [Discriminator loss: 0.079425, acc.: 96.88%] [Generator loss: 5.559219]\n",
      "6017 [Discriminator loss: 0.185389, acc.: 92.19%] [Generator loss: 4.272938]\n",
      "6018 [Discriminator loss: 0.184505, acc.: 90.62%] [Generator loss: 5.519143]\n",
      "6019 [Discriminator loss: 0.196080, acc.: 87.50%] [Generator loss: 5.843927]\n",
      "6020 [Discriminator loss: 0.179137, acc.: 89.06%] [Generator loss: 4.551287]\n",
      "6021 [Discriminator loss: 0.278876, acc.: 85.94%] [Generator loss: 5.632045]\n",
      "6022 [Discriminator loss: 0.117797, acc.: 93.75%] [Generator loss: 5.430268]\n",
      "6023 [Discriminator loss: 0.241650, acc.: 87.50%] [Generator loss: 5.441990]\n",
      "6024 [Discriminator loss: 0.246052, acc.: 89.06%] [Generator loss: 4.505298]\n",
      "6025 [Discriminator loss: 0.179923, acc.: 92.19%] [Generator loss: 4.687665]\n",
      "6026 [Discriminator loss: 0.175590, acc.: 93.75%] [Generator loss: 4.227018]\n",
      "6027 [Discriminator loss: 0.194215, acc.: 92.19%] [Generator loss: 4.160941]\n",
      "6028 [Discriminator loss: 0.203835, acc.: 92.19%] [Generator loss: 5.957909]\n",
      "6029 [Discriminator loss: 0.236231, acc.: 89.06%] [Generator loss: 4.723116]\n",
      "6030 [Discriminator loss: 0.211313, acc.: 92.19%] [Generator loss: 5.356410]\n",
      "6031 [Discriminator loss: 0.250731, acc.: 90.62%] [Generator loss: 4.459438]\n",
      "6032 [Discriminator loss: 0.309294, acc.: 89.06%] [Generator loss: 5.388908]\n",
      "6033 [Discriminator loss: 0.168254, acc.: 95.31%] [Generator loss: 4.123154]\n",
      "6034 [Discriminator loss: 0.106662, acc.: 96.88%] [Generator loss: 5.009656]\n",
      "6035 [Discriminator loss: 0.094788, acc.: 96.88%] [Generator loss: 5.058049]\n",
      "6036 [Discriminator loss: 0.300821, acc.: 89.06%] [Generator loss: 5.217948]\n",
      "6037 [Discriminator loss: 0.147825, acc.: 96.88%] [Generator loss: 6.039917]\n",
      "6038 [Discriminator loss: 0.207098, acc.: 90.62%] [Generator loss: 5.603487]\n",
      "6039 [Discriminator loss: 0.148079, acc.: 95.31%] [Generator loss: 5.593054]\n",
      "6040 [Discriminator loss: 0.149755, acc.: 96.88%] [Generator loss: 4.381083]\n",
      "6041 [Discriminator loss: 0.135716, acc.: 96.88%] [Generator loss: 6.434463]\n",
      "6042 [Discriminator loss: 0.106557, acc.: 96.88%] [Generator loss: 5.246753]\n",
      "6043 [Discriminator loss: 0.147345, acc.: 95.31%] [Generator loss: 4.310843]\n",
      "6044 [Discriminator loss: 0.258302, acc.: 90.62%] [Generator loss: 4.383035]\n",
      "6045 [Discriminator loss: 0.184760, acc.: 93.75%] [Generator loss: 5.085572]\n",
      "6046 [Discriminator loss: 0.380120, acc.: 81.25%] [Generator loss: 5.150968]\n",
      "6047 [Discriminator loss: 0.126196, acc.: 96.88%] [Generator loss: 5.467562]\n",
      "6048 [Discriminator loss: 0.249044, acc.: 87.50%] [Generator loss: 5.429436]\n",
      "6049 [Discriminator loss: 0.115516, acc.: 95.31%] [Generator loss: 5.911366]\n",
      "6050 [Discriminator loss: 0.266828, acc.: 89.06%] [Generator loss: 4.737221]\n",
      "6051 [Discriminator loss: 0.191112, acc.: 90.62%] [Generator loss: 4.592017]\n",
      "6052 [Discriminator loss: 0.337300, acc.: 87.50%] [Generator loss: 5.233480]\n",
      "6053 [Discriminator loss: 0.506348, acc.: 76.56%] [Generator loss: 4.674612]\n",
      "6054 [Discriminator loss: 0.114754, acc.: 93.75%] [Generator loss: 6.184340]\n",
      "6055 [Discriminator loss: 0.125740, acc.: 96.88%] [Generator loss: 4.445302]\n",
      "6056 [Discriminator loss: 0.166018, acc.: 93.75%] [Generator loss: 4.509226]\n",
      "6057 [Discriminator loss: 0.209572, acc.: 92.19%] [Generator loss: 4.756401]\n",
      "6058 [Discriminator loss: 0.176019, acc.: 90.62%] [Generator loss: 5.584092]\n",
      "6059 [Discriminator loss: 0.361369, acc.: 87.50%] [Generator loss: 5.673717]\n",
      "6060 [Discriminator loss: 0.216224, acc.: 92.19%] [Generator loss: 4.480928]\n",
      "6061 [Discriminator loss: 0.242261, acc.: 90.62%] [Generator loss: 4.621145]\n",
      "6062 [Discriminator loss: 0.138319, acc.: 93.75%] [Generator loss: 6.366096]\n",
      "6063 [Discriminator loss: 0.115913, acc.: 95.31%] [Generator loss: 6.192984]\n",
      "6064 [Discriminator loss: 0.333148, acc.: 78.12%] [Generator loss: 5.693314]\n",
      "6065 [Discriminator loss: 0.205285, acc.: 92.19%] [Generator loss: 4.452008]\n",
      "6066 [Discriminator loss: 0.300736, acc.: 84.38%] [Generator loss: 4.295885]\n",
      "6067 [Discriminator loss: 0.294660, acc.: 87.50%] [Generator loss: 4.959046]\n",
      "6068 [Discriminator loss: 0.181154, acc.: 93.75%] [Generator loss: 5.471944]\n",
      "6069 [Discriminator loss: 0.313295, acc.: 84.38%] [Generator loss: 6.442894]\n",
      "6070 [Discriminator loss: 0.386996, acc.: 84.38%] [Generator loss: 4.855341]\n",
      "6071 [Discriminator loss: 0.350817, acc.: 85.94%] [Generator loss: 6.119711]\n",
      "6072 [Discriminator loss: 0.152836, acc.: 93.75%] [Generator loss: 7.650433]\n",
      "6073 [Discriminator loss: 0.286435, acc.: 84.38%] [Generator loss: 4.286884]\n",
      "6074 [Discriminator loss: 0.310233, acc.: 90.62%] [Generator loss: 6.358236]\n",
      "6075 [Discriminator loss: 0.107993, acc.: 98.44%] [Generator loss: 6.374626]\n",
      "6076 [Discriminator loss: 0.168294, acc.: 92.19%] [Generator loss: 4.801729]\n",
      "6077 [Discriminator loss: 0.312484, acc.: 87.50%] [Generator loss: 5.522954]\n",
      "6078 [Discriminator loss: 0.178717, acc.: 90.62%] [Generator loss: 5.818792]\n",
      "6079 [Discriminator loss: 0.277025, acc.: 89.06%] [Generator loss: 5.800291]\n",
      "6080 [Discriminator loss: 0.225958, acc.: 96.88%] [Generator loss: 5.013931]\n",
      "6081 [Discriminator loss: 0.316640, acc.: 89.06%] [Generator loss: 5.798875]\n",
      "6082 [Discriminator loss: 0.234559, acc.: 85.94%] [Generator loss: 4.282984]\n",
      "6083 [Discriminator loss: 0.088345, acc.: 98.44%] [Generator loss: 4.463324]\n",
      "6084 [Discriminator loss: 0.323181, acc.: 85.94%] [Generator loss: 3.925660]\n",
      "6085 [Discriminator loss: 0.244186, acc.: 90.62%] [Generator loss: 5.681750]\n",
      "6086 [Discriminator loss: 0.146121, acc.: 92.19%] [Generator loss: 4.825371]\n",
      "6087 [Discriminator loss: 0.323267, acc.: 82.81%] [Generator loss: 4.423350]\n",
      "6088 [Discriminator loss: 0.166904, acc.: 90.62%] [Generator loss: 5.204708]\n",
      "6089 [Discriminator loss: 0.080399, acc.: 96.88%] [Generator loss: 5.058362]\n",
      "6090 [Discriminator loss: 0.258190, acc.: 85.94%] [Generator loss: 4.950272]\n",
      "6091 [Discriminator loss: 0.105199, acc.: 96.88%] [Generator loss: 4.533038]\n",
      "6092 [Discriminator loss: 0.213616, acc.: 89.06%] [Generator loss: 5.394487]\n",
      "6093 [Discriminator loss: 0.388500, acc.: 81.25%] [Generator loss: 3.532866]\n",
      "6094 [Discriminator loss: 0.347675, acc.: 84.38%] [Generator loss: 5.860016]\n",
      "6095 [Discriminator loss: 0.115505, acc.: 98.44%] [Generator loss: 5.423379]\n",
      "6096 [Discriminator loss: 0.272663, acc.: 90.62%] [Generator loss: 4.529260]\n",
      "6097 [Discriminator loss: 0.151438, acc.: 95.31%] [Generator loss: 5.983881]\n",
      "6098 [Discriminator loss: 0.066840, acc.: 98.44%] [Generator loss: 5.160216]\n",
      "6099 [Discriminator loss: 0.352510, acc.: 82.81%] [Generator loss: 5.003930]\n",
      "6100 [Discriminator loss: 0.141994, acc.: 90.62%] [Generator loss: 6.621154]\n",
      "6101 [Discriminator loss: 0.315923, acc.: 85.94%] [Generator loss: 5.096540]\n",
      "6102 [Discriminator loss: 0.188905, acc.: 92.19%] [Generator loss: 5.662669]\n",
      "6103 [Discriminator loss: 0.113112, acc.: 95.31%] [Generator loss: 4.569674]\n",
      "6104 [Discriminator loss: 0.190598, acc.: 90.62%] [Generator loss: 4.557209]\n",
      "6105 [Discriminator loss: 0.317274, acc.: 85.94%] [Generator loss: 5.176954]\n",
      "6106 [Discriminator loss: 0.096030, acc.: 93.75%] [Generator loss: 6.630353]\n",
      "6107 [Discriminator loss: 0.436686, acc.: 79.69%] [Generator loss: 4.583875]\n",
      "6108 [Discriminator loss: 0.125990, acc.: 93.75%] [Generator loss: 5.778469]\n",
      "6109 [Discriminator loss: 0.213237, acc.: 93.75%] [Generator loss: 4.045191]\n",
      "6110 [Discriminator loss: 0.156290, acc.: 90.62%] [Generator loss: 5.666735]\n",
      "6111 [Discriminator loss: 0.308154, acc.: 84.38%] [Generator loss: 4.415400]\n",
      "6112 [Discriminator loss: 0.309969, acc.: 89.06%] [Generator loss: 6.121950]\n",
      "6113 [Discriminator loss: 0.096768, acc.: 96.88%] [Generator loss: 5.764296]\n",
      "6114 [Discriminator loss: 0.302063, acc.: 85.94%] [Generator loss: 4.141003]\n",
      "6115 [Discriminator loss: 0.158070, acc.: 92.19%] [Generator loss: 5.045551]\n",
      "6116 [Discriminator loss: 0.070536, acc.: 98.44%] [Generator loss: 5.626187]\n",
      "6117 [Discriminator loss: 0.160420, acc.: 93.75%] [Generator loss: 5.439290]\n",
      "6118 [Discriminator loss: 0.160865, acc.: 93.75%] [Generator loss: 5.082826]\n",
      "6119 [Discriminator loss: 0.379320, acc.: 82.81%] [Generator loss: 3.760546]\n",
      "6120 [Discriminator loss: 0.174317, acc.: 95.31%] [Generator loss: 4.611097]\n",
      "6121 [Discriminator loss: 0.355833, acc.: 79.69%] [Generator loss: 7.953196]\n",
      "6122 [Discriminator loss: 0.555040, acc.: 75.00%] [Generator loss: 4.327748]\n",
      "6123 [Discriminator loss: 0.091249, acc.: 96.88%] [Generator loss: 4.613516]\n",
      "6124 [Discriminator loss: 0.190696, acc.: 92.19%] [Generator loss: 4.717391]\n",
      "6125 [Discriminator loss: 0.207557, acc.: 89.06%] [Generator loss: 4.841781]\n",
      "6126 [Discriminator loss: 0.292424, acc.: 90.62%] [Generator loss: 4.362055]\n",
      "6127 [Discriminator loss: 0.080540, acc.: 100.00%] [Generator loss: 4.937089]\n",
      "6128 [Discriminator loss: 0.244306, acc.: 87.50%] [Generator loss: 4.383277]\n",
      "6129 [Discriminator loss: 0.104930, acc.: 95.31%] [Generator loss: 4.523796]\n",
      "6130 [Discriminator loss: 0.205033, acc.: 95.31%] [Generator loss: 4.842910]\n",
      "6131 [Discriminator loss: 0.155908, acc.: 93.75%] [Generator loss: 3.959399]\n",
      "6132 [Discriminator loss: 0.330209, acc.: 87.50%] [Generator loss: 5.963768]\n",
      "6133 [Discriminator loss: 0.399503, acc.: 82.81%] [Generator loss: 3.953407]\n",
      "6134 [Discriminator loss: 0.220538, acc.: 87.50%] [Generator loss: 6.529134]\n",
      "6135 [Discriminator loss: 0.348723, acc.: 84.38%] [Generator loss: 3.853014]\n",
      "6136 [Discriminator loss: 0.102422, acc.: 96.88%] [Generator loss: 5.741935]\n",
      "6137 [Discriminator loss: 0.243426, acc.: 90.62%] [Generator loss: 5.491012]\n",
      "6138 [Discriminator loss: 0.165380, acc.: 93.75%] [Generator loss: 5.500516]\n",
      "6139 [Discriminator loss: 0.329002, acc.: 84.38%] [Generator loss: 6.357946]\n",
      "6140 [Discriminator loss: 0.398935, acc.: 85.94%] [Generator loss: 4.801996]\n",
      "6141 [Discriminator loss: 0.130588, acc.: 95.31%] [Generator loss: 4.319891]\n",
      "6142 [Discriminator loss: 0.177624, acc.: 93.75%] [Generator loss: 4.280034]\n",
      "6143 [Discriminator loss: 0.386138, acc.: 89.06%] [Generator loss: 6.682421]\n",
      "6144 [Discriminator loss: 0.224904, acc.: 92.19%] [Generator loss: 6.891182]\n",
      "6145 [Discriminator loss: 0.126277, acc.: 96.88%] [Generator loss: 5.236292]\n",
      "6146 [Discriminator loss: 0.192042, acc.: 93.75%] [Generator loss: 4.676570]\n",
      "6147 [Discriminator loss: 0.129156, acc.: 95.31%] [Generator loss: 4.863978]\n",
      "6148 [Discriminator loss: 0.143981, acc.: 95.31%] [Generator loss: 4.977297]\n",
      "6149 [Discriminator loss: 0.285669, acc.: 90.62%] [Generator loss: 5.127784]\n",
      "6150 [Discriminator loss: 0.153093, acc.: 93.75%] [Generator loss: 5.836662]\n",
      "6151 [Discriminator loss: 0.167452, acc.: 92.19%] [Generator loss: 4.744066]\n",
      "6152 [Discriminator loss: 0.210945, acc.: 90.62%] [Generator loss: 5.098571]\n",
      "6153 [Discriminator loss: 0.146313, acc.: 96.88%] [Generator loss: 4.510943]\n",
      "6154 [Discriminator loss: 0.070746, acc.: 100.00%] [Generator loss: 5.716634]\n",
      "6155 [Discriminator loss: 0.426038, acc.: 79.69%] [Generator loss: 5.355929]\n",
      "6156 [Discriminator loss: 0.187256, acc.: 93.75%] [Generator loss: 5.429389]\n",
      "6157 [Discriminator loss: 0.281275, acc.: 87.50%] [Generator loss: 4.662541]\n",
      "6158 [Discriminator loss: 0.217289, acc.: 90.62%] [Generator loss: 4.474699]\n",
      "6159 [Discriminator loss: 0.211827, acc.: 90.62%] [Generator loss: 5.487638]\n",
      "6160 [Discriminator loss: 0.230778, acc.: 90.62%] [Generator loss: 5.492248]\n",
      "6161 [Discriminator loss: 0.283489, acc.: 87.50%] [Generator loss: 6.204584]\n",
      "6162 [Discriminator loss: 0.368345, acc.: 84.38%] [Generator loss: 4.454998]\n",
      "6163 [Discriminator loss: 0.118229, acc.: 95.31%] [Generator loss: 4.727024]\n",
      "6164 [Discriminator loss: 0.152311, acc.: 93.75%] [Generator loss: 5.491168]\n",
      "6165 [Discriminator loss: 0.138170, acc.: 95.31%] [Generator loss: 4.421399]\n",
      "6166 [Discriminator loss: 0.149963, acc.: 95.31%] [Generator loss: 5.475938]\n",
      "6167 [Discriminator loss: 0.111842, acc.: 96.88%] [Generator loss: 5.805136]\n",
      "6168 [Discriminator loss: 0.266434, acc.: 90.62%] [Generator loss: 4.920930]\n",
      "6169 [Discriminator loss: 0.201663, acc.: 92.19%] [Generator loss: 5.345875]\n",
      "6170 [Discriminator loss: 0.173191, acc.: 92.19%] [Generator loss: 5.443001]\n",
      "6171 [Discriminator loss: 0.177884, acc.: 93.75%] [Generator loss: 4.903008]\n",
      "6172 [Discriminator loss: 0.128054, acc.: 93.75%] [Generator loss: 3.832647]\n",
      "6173 [Discriminator loss: 0.134343, acc.: 96.88%] [Generator loss: 4.563271]\n",
      "6174 [Discriminator loss: 0.222434, acc.: 90.62%] [Generator loss: 5.000552]\n",
      "6175 [Discriminator loss: 0.129173, acc.: 95.31%] [Generator loss: 5.066360]\n",
      "6176 [Discriminator loss: 0.197114, acc.: 92.19%] [Generator loss: 5.677249]\n",
      "6177 [Discriminator loss: 0.750801, acc.: 62.50%] [Generator loss: 4.569770]\n",
      "6178 [Discriminator loss: 0.089007, acc.: 96.88%] [Generator loss: 5.405921]\n",
      "6179 [Discriminator loss: 0.155736, acc.: 92.19%] [Generator loss: 6.823362]\n",
      "6180 [Discriminator loss: 0.193690, acc.: 92.19%] [Generator loss: 4.581332]\n",
      "6181 [Discriminator loss: 0.142686, acc.: 92.19%] [Generator loss: 5.511754]\n",
      "6182 [Discriminator loss: 0.191000, acc.: 92.19%] [Generator loss: 5.242549]\n",
      "6183 [Discriminator loss: 0.340891, acc.: 85.94%] [Generator loss: 4.922219]\n",
      "6184 [Discriminator loss: 0.251717, acc.: 85.94%] [Generator loss: 4.582338]\n",
      "6185 [Discriminator loss: 0.243797, acc.: 89.06%] [Generator loss: 4.889558]\n",
      "6186 [Discriminator loss: 0.222096, acc.: 90.62%] [Generator loss: 3.755062]\n",
      "6187 [Discriminator loss: 0.129291, acc.: 93.75%] [Generator loss: 4.974096]\n",
      "6188 [Discriminator loss: 0.083991, acc.: 96.88%] [Generator loss: 5.677104]\n",
      "6189 [Discriminator loss: 0.350805, acc.: 85.94%] [Generator loss: 4.056507]\n",
      "6190 [Discriminator loss: 0.163324, acc.: 93.75%] [Generator loss: 5.390300]\n",
      "6191 [Discriminator loss: 0.084219, acc.: 96.88%] [Generator loss: 4.608253]\n",
      "6192 [Discriminator loss: 0.388433, acc.: 87.50%] [Generator loss: 4.371035]\n",
      "6193 [Discriminator loss: 0.195438, acc.: 92.19%] [Generator loss: 4.897330]\n",
      "6194 [Discriminator loss: 0.111069, acc.: 96.88%] [Generator loss: 5.094522]\n",
      "6195 [Discriminator loss: 0.166571, acc.: 95.31%] [Generator loss: 5.256568]\n",
      "6196 [Discriminator loss: 0.067166, acc.: 98.44%] [Generator loss: 4.945233]\n",
      "6197 [Discriminator loss: 0.182795, acc.: 93.75%] [Generator loss: 4.444945]\n",
      "6198 [Discriminator loss: 0.232253, acc.: 92.19%] [Generator loss: 4.104360]\n",
      "6199 [Discriminator loss: 0.174047, acc.: 93.75%] [Generator loss: 5.943804]\n",
      "6200 [Discriminator loss: 0.130883, acc.: 96.88%] [Generator loss: 5.425009]\n",
      "6201 [Discriminator loss: 0.188908, acc.: 90.62%] [Generator loss: 4.944139]\n",
      "6202 [Discriminator loss: 0.208974, acc.: 92.19%] [Generator loss: 5.214684]\n",
      "6203 [Discriminator loss: 0.257343, acc.: 87.50%] [Generator loss: 5.680473]\n",
      "6204 [Discriminator loss: 0.209134, acc.: 90.62%] [Generator loss: 5.843375]\n",
      "6205 [Discriminator loss: 0.265630, acc.: 87.50%] [Generator loss: 5.317285]\n",
      "6206 [Discriminator loss: 0.281885, acc.: 87.50%] [Generator loss: 5.514607]\n",
      "6207 [Discriminator loss: 0.422253, acc.: 79.69%] [Generator loss: 4.833947]\n",
      "6208 [Discriminator loss: 0.099399, acc.: 93.75%] [Generator loss: 6.091444]\n",
      "6209 [Discriminator loss: 0.192245, acc.: 92.19%] [Generator loss: 5.430950]\n",
      "6210 [Discriminator loss: 0.353276, acc.: 84.38%] [Generator loss: 4.826824]\n",
      "6211 [Discriminator loss: 0.160494, acc.: 92.19%] [Generator loss: 5.373741]\n",
      "6212 [Discriminator loss: 0.153226, acc.: 93.75%] [Generator loss: 5.738461]\n",
      "6213 [Discriminator loss: 0.285546, acc.: 89.06%] [Generator loss: 4.851252]\n",
      "6214 [Discriminator loss: 0.215957, acc.: 90.62%] [Generator loss: 4.601015]\n",
      "6215 [Discriminator loss: 0.064483, acc.: 100.00%] [Generator loss: 4.794738]\n",
      "6216 [Discriminator loss: 0.128106, acc.: 95.31%] [Generator loss: 4.639670]\n",
      "6217 [Discriminator loss: 0.179939, acc.: 95.31%] [Generator loss: 4.040499]\n",
      "6218 [Discriminator loss: 0.357154, acc.: 82.81%] [Generator loss: 4.935658]\n",
      "6219 [Discriminator loss: 0.332947, acc.: 87.50%] [Generator loss: 5.713476]\n",
      "6220 [Discriminator loss: 0.201942, acc.: 92.19%] [Generator loss: 6.306598]\n",
      "6221 [Discriminator loss: 0.107567, acc.: 95.31%] [Generator loss: 5.773698]\n",
      "6222 [Discriminator loss: 0.171943, acc.: 93.75%] [Generator loss: 5.008334]\n",
      "6223 [Discriminator loss: 0.360399, acc.: 89.06%] [Generator loss: 6.279463]\n",
      "6224 [Discriminator loss: 0.157825, acc.: 90.62%] [Generator loss: 5.905685]\n",
      "6225 [Discriminator loss: 0.079793, acc.: 100.00%] [Generator loss: 4.966472]\n",
      "6226 [Discriminator loss: 0.193184, acc.: 93.75%] [Generator loss: 5.032094]\n",
      "6227 [Discriminator loss: 0.096057, acc.: 96.88%] [Generator loss: 4.981064]\n",
      "6228 [Discriminator loss: 0.184267, acc.: 95.31%] [Generator loss: 5.226653]\n",
      "6229 [Discriminator loss: 0.064343, acc.: 98.44%] [Generator loss: 5.413521]\n",
      "6230 [Discriminator loss: 0.103509, acc.: 95.31%] [Generator loss: 4.517464]\n",
      "6231 [Discriminator loss: 0.170079, acc.: 93.75%] [Generator loss: 5.506799]\n",
      "6232 [Discriminator loss: 0.230509, acc.: 90.62%] [Generator loss: 5.712800]\n",
      "6233 [Discriminator loss: 0.234146, acc.: 90.62%] [Generator loss: 4.710607]\n",
      "6234 [Discriminator loss: 0.250922, acc.: 92.19%] [Generator loss: 5.042325]\n",
      "6235 [Discriminator loss: 0.101223, acc.: 95.31%] [Generator loss: 6.346795]\n",
      "6236 [Discriminator loss: 0.083276, acc.: 98.44%] [Generator loss: 5.665324]\n",
      "6237 [Discriminator loss: 0.422919, acc.: 84.38%] [Generator loss: 4.595202]\n",
      "6238 [Discriminator loss: 0.219452, acc.: 93.75%] [Generator loss: 4.845851]\n",
      "6239 [Discriminator loss: 0.251253, acc.: 93.75%] [Generator loss: 4.545897]\n",
      "6240 [Discriminator loss: 0.201979, acc.: 92.19%] [Generator loss: 4.592772]\n",
      "6241 [Discriminator loss: 0.207552, acc.: 92.19%] [Generator loss: 6.486417]\n",
      "6242 [Discriminator loss: 0.228295, acc.: 92.19%] [Generator loss: 6.164225]\n",
      "6243 [Discriminator loss: 0.156893, acc.: 92.19%] [Generator loss: 5.841829]\n",
      "6244 [Discriminator loss: 0.320815, acc.: 89.06%] [Generator loss: 4.553044]\n",
      "6245 [Discriminator loss: 0.119515, acc.: 98.44%] [Generator loss: 5.968669]\n",
      "6246 [Discriminator loss: 0.217121, acc.: 90.62%] [Generator loss: 5.679174]\n",
      "6247 [Discriminator loss: 0.211905, acc.: 92.19%] [Generator loss: 5.984021]\n",
      "6248 [Discriminator loss: 0.141148, acc.: 93.75%] [Generator loss: 5.278665]\n",
      "6249 [Discriminator loss: 0.165911, acc.: 93.75%] [Generator loss: 4.542994]\n",
      "6250 [Discriminator loss: 0.091388, acc.: 95.31%] [Generator loss: 4.756157]\n",
      "6251 [Discriminator loss: 0.231084, acc.: 90.62%] [Generator loss: 4.919581]\n",
      "6252 [Discriminator loss: 0.173902, acc.: 90.62%] [Generator loss: 4.143118]\n",
      "6253 [Discriminator loss: 0.308089, acc.: 87.50%] [Generator loss: 4.892225]\n",
      "6254 [Discriminator loss: 0.234165, acc.: 90.62%] [Generator loss: 4.897297]\n",
      "6255 [Discriminator loss: 0.292731, acc.: 89.06%] [Generator loss: 6.264684]\n",
      "6256 [Discriminator loss: 0.115607, acc.: 96.88%] [Generator loss: 6.317322]\n",
      "6257 [Discriminator loss: 0.170873, acc.: 95.31%] [Generator loss: 5.030604]\n",
      "6258 [Discriminator loss: 0.178295, acc.: 93.75%] [Generator loss: 5.295489]\n",
      "6259 [Discriminator loss: 0.124482, acc.: 96.88%] [Generator loss: 5.355258]\n",
      "6260 [Discriminator loss: 0.551507, acc.: 82.81%] [Generator loss: 5.694577]\n",
      "6261 [Discriminator loss: 0.117285, acc.: 96.88%] [Generator loss: 6.649049]\n",
      "6262 [Discriminator loss: 0.191683, acc.: 92.19%] [Generator loss: 4.110440]\n",
      "6263 [Discriminator loss: 0.364327, acc.: 82.81%] [Generator loss: 6.653898]\n",
      "6264 [Discriminator loss: 0.156334, acc.: 95.31%] [Generator loss: 7.241565]\n",
      "6265 [Discriminator loss: 0.169529, acc.: 93.75%] [Generator loss: 5.065168]\n",
      "6266 [Discriminator loss: 0.264024, acc.: 87.50%] [Generator loss: 5.503392]\n",
      "6267 [Discriminator loss: 0.149683, acc.: 93.75%] [Generator loss: 5.034740]\n",
      "6268 [Discriminator loss: 0.264227, acc.: 85.94%] [Generator loss: 4.367826]\n",
      "6269 [Discriminator loss: 0.197448, acc.: 90.62%] [Generator loss: 4.993065]\n",
      "6270 [Discriminator loss: 0.443536, acc.: 81.25%] [Generator loss: 6.286895]\n",
      "6271 [Discriminator loss: 0.179668, acc.: 93.75%] [Generator loss: 6.780583]\n",
      "6272 [Discriminator loss: 0.473969, acc.: 71.88%] [Generator loss: 4.559237]\n",
      "6273 [Discriminator loss: 0.187673, acc.: 95.31%] [Generator loss: 4.983345]\n",
      "6274 [Discriminator loss: 0.188363, acc.: 89.06%] [Generator loss: 6.141330]\n",
      "6275 [Discriminator loss: 0.171649, acc.: 90.62%] [Generator loss: 5.296561]\n",
      "6276 [Discriminator loss: 0.244090, acc.: 87.50%] [Generator loss: 3.259280]\n",
      "6277 [Discriminator loss: 0.200441, acc.: 93.75%] [Generator loss: 5.556127]\n",
      "6278 [Discriminator loss: 0.154855, acc.: 95.31%] [Generator loss: 5.663125]\n",
      "6279 [Discriminator loss: 0.192344, acc.: 95.31%] [Generator loss: 5.337652]\n",
      "6280 [Discriminator loss: 0.280825, acc.: 85.94%] [Generator loss: 5.608089]\n",
      "6281 [Discriminator loss: 0.089658, acc.: 98.44%] [Generator loss: 6.680934]\n",
      "6282 [Discriminator loss: 0.190192, acc.: 92.19%] [Generator loss: 5.114784]\n",
      "6283 [Discriminator loss: 0.320424, acc.: 85.94%] [Generator loss: 5.186721]\n",
      "6284 [Discriminator loss: 0.150017, acc.: 93.75%] [Generator loss: 6.594529]\n",
      "6285 [Discriminator loss: 0.486832, acc.: 76.56%] [Generator loss: 4.203465]\n",
      "6286 [Discriminator loss: 0.119070, acc.: 98.44%] [Generator loss: 6.931146]\n",
      "6287 [Discriminator loss: 0.155602, acc.: 93.75%] [Generator loss: 5.207639]\n",
      "6288 [Discriminator loss: 0.188205, acc.: 89.06%] [Generator loss: 4.923249]\n",
      "6289 [Discriminator loss: 0.074479, acc.: 96.88%] [Generator loss: 5.896216]\n",
      "6290 [Discriminator loss: 0.172075, acc.: 93.75%] [Generator loss: 3.761397]\n",
      "6291 [Discriminator loss: 0.219500, acc.: 87.50%] [Generator loss: 4.641019]\n",
      "6292 [Discriminator loss: 0.117809, acc.: 96.88%] [Generator loss: 5.803604]\n",
      "6293 [Discriminator loss: 0.107514, acc.: 93.75%] [Generator loss: 5.381250]\n",
      "6294 [Discriminator loss: 0.246044, acc.: 89.06%] [Generator loss: 4.686841]\n",
      "6295 [Discriminator loss: 0.068851, acc.: 100.00%] [Generator loss: 4.504980]\n",
      "6296 [Discriminator loss: 0.203413, acc.: 93.75%] [Generator loss: 5.040884]\n",
      "6297 [Discriminator loss: 0.185309, acc.: 89.06%] [Generator loss: 5.779886]\n",
      "6298 [Discriminator loss: 0.130207, acc.: 92.19%] [Generator loss: 6.203859]\n",
      "6299 [Discriminator loss: 0.367274, acc.: 89.06%] [Generator loss: 5.107000]\n",
      "6300 [Discriminator loss: 0.065710, acc.: 98.44%] [Generator loss: 5.728316]\n",
      "6301 [Discriminator loss: 0.288766, acc.: 87.50%] [Generator loss: 5.464023]\n",
      "6302 [Discriminator loss: 0.101223, acc.: 95.31%] [Generator loss: 5.066137]\n",
      "6303 [Discriminator loss: 0.237472, acc.: 87.50%] [Generator loss: 6.317016]\n",
      "6304 [Discriminator loss: 0.318664, acc.: 85.94%] [Generator loss: 5.672654]\n",
      "6305 [Discriminator loss: 0.234503, acc.: 92.19%] [Generator loss: 4.826632]\n",
      "6306 [Discriminator loss: 0.209672, acc.: 90.62%] [Generator loss: 4.835511]\n",
      "6307 [Discriminator loss: 0.098738, acc.: 96.88%] [Generator loss: 5.910359]\n",
      "6308 [Discriminator loss: 0.335461, acc.: 82.81%] [Generator loss: 5.413079]\n",
      "6309 [Discriminator loss: 0.190146, acc.: 90.62%] [Generator loss: 6.148829]\n",
      "6310 [Discriminator loss: 0.109970, acc.: 93.75%] [Generator loss: 5.815389]\n",
      "6311 [Discriminator loss: 0.285082, acc.: 92.19%] [Generator loss: 5.682193]\n",
      "6312 [Discriminator loss: 0.236503, acc.: 90.62%] [Generator loss: 6.405853]\n",
      "6313 [Discriminator loss: 0.143464, acc.: 95.31%] [Generator loss: 5.534051]\n",
      "6314 [Discriminator loss: 0.159931, acc.: 90.62%] [Generator loss: 4.774729]\n",
      "6315 [Discriminator loss: 0.186628, acc.: 93.75%] [Generator loss: 6.323757]\n",
      "6316 [Discriminator loss: 0.098185, acc.: 96.88%] [Generator loss: 5.789777]\n",
      "6317 [Discriminator loss: 0.241720, acc.: 93.75%] [Generator loss: 5.146187]\n",
      "6318 [Discriminator loss: 0.093474, acc.: 95.31%] [Generator loss: 5.319534]\n",
      "6319 [Discriminator loss: 0.191000, acc.: 89.06%] [Generator loss: 6.455703]\n",
      "6320 [Discriminator loss: 0.199284, acc.: 90.62%] [Generator loss: 4.470303]\n",
      "6321 [Discriminator loss: 0.443084, acc.: 81.25%] [Generator loss: 5.635082]\n",
      "6322 [Discriminator loss: 0.252218, acc.: 89.06%] [Generator loss: 5.653007]\n",
      "6323 [Discriminator loss: 0.294419, acc.: 95.31%] [Generator loss: 6.521807]\n",
      "6324 [Discriminator loss: 0.103729, acc.: 95.31%] [Generator loss: 6.509470]\n",
      "6325 [Discriminator loss: 0.060641, acc.: 98.44%] [Generator loss: 4.820922]\n",
      "6326 [Discriminator loss: 0.201122, acc.: 89.06%] [Generator loss: 4.986103]\n",
      "6327 [Discriminator loss: 0.242864, acc.: 89.06%] [Generator loss: 5.922556]\n",
      "6328 [Discriminator loss: 0.406880, acc.: 85.94%] [Generator loss: 6.534860]\n",
      "6329 [Discriminator loss: 0.331181, acc.: 85.94%] [Generator loss: 4.603017]\n",
      "6330 [Discriminator loss: 0.185957, acc.: 93.75%] [Generator loss: 5.767209]\n",
      "6331 [Discriminator loss: 0.240042, acc.: 90.62%] [Generator loss: 5.850796]\n",
      "6332 [Discriminator loss: 0.283458, acc.: 92.19%] [Generator loss: 4.960084]\n",
      "6333 [Discriminator loss: 0.230446, acc.: 89.06%] [Generator loss: 4.770767]\n",
      "6334 [Discriminator loss: 0.203112, acc.: 89.06%] [Generator loss: 4.701743]\n",
      "6335 [Discriminator loss: 0.102814, acc.: 100.00%] [Generator loss: 4.251386]\n",
      "6336 [Discriminator loss: 0.260296, acc.: 87.50%] [Generator loss: 5.495194]\n",
      "6337 [Discriminator loss: 0.102789, acc.: 96.88%] [Generator loss: 5.344642]\n",
      "6338 [Discriminator loss: 0.111428, acc.: 96.88%] [Generator loss: 4.950674]\n",
      "6339 [Discriminator loss: 0.126760, acc.: 96.88%] [Generator loss: 4.743625]\n",
      "6340 [Discriminator loss: 0.135286, acc.: 96.88%] [Generator loss: 5.193981]\n",
      "6341 [Discriminator loss: 0.244143, acc.: 90.62%] [Generator loss: 4.876276]\n",
      "6342 [Discriminator loss: 0.156314, acc.: 95.31%] [Generator loss: 4.304711]\n",
      "6343 [Discriminator loss: 0.155816, acc.: 92.19%] [Generator loss: 5.059111]\n",
      "6344 [Discriminator loss: 0.266965, acc.: 89.06%] [Generator loss: 3.610264]\n",
      "6345 [Discriminator loss: 0.310649, acc.: 89.06%] [Generator loss: 6.646472]\n",
      "6346 [Discriminator loss: 0.213761, acc.: 92.19%] [Generator loss: 7.013721]\n",
      "6347 [Discriminator loss: 0.318841, acc.: 87.50%] [Generator loss: 5.145623]\n",
      "6348 [Discriminator loss: 0.126828, acc.: 96.88%] [Generator loss: 6.232229]\n",
      "6349 [Discriminator loss: 0.224236, acc.: 93.75%] [Generator loss: 4.757332]\n",
      "6350 [Discriminator loss: 0.119984, acc.: 95.31%] [Generator loss: 4.148605]\n",
      "6351 [Discriminator loss: 0.208565, acc.: 90.62%] [Generator loss: 7.164708]\n",
      "6352 [Discriminator loss: 0.237874, acc.: 93.75%] [Generator loss: 5.780871]\n",
      "6353 [Discriminator loss: 0.225928, acc.: 89.06%] [Generator loss: 4.273777]\n",
      "6354 [Discriminator loss: 0.195774, acc.: 92.19%] [Generator loss: 5.844606]\n",
      "6355 [Discriminator loss: 0.132711, acc.: 93.75%] [Generator loss: 5.300413]\n",
      "6356 [Discriminator loss: 0.437952, acc.: 78.12%] [Generator loss: 5.817317]\n",
      "6357 [Discriminator loss: 0.132249, acc.: 93.75%] [Generator loss: 4.801777]\n",
      "6358 [Discriminator loss: 0.159041, acc.: 93.75%] [Generator loss: 6.012239]\n",
      "6359 [Discriminator loss: 0.113883, acc.: 96.88%] [Generator loss: 4.655287]\n",
      "6360 [Discriminator loss: 0.192099, acc.: 93.75%] [Generator loss: 5.641514]\n",
      "6361 [Discriminator loss: 0.342145, acc.: 81.25%] [Generator loss: 5.026647]\n",
      "6362 [Discriminator loss: 0.209995, acc.: 92.19%] [Generator loss: 4.735815]\n",
      "6363 [Discriminator loss: 0.192086, acc.: 90.62%] [Generator loss: 4.398886]\n",
      "6364 [Discriminator loss: 0.422713, acc.: 81.25%] [Generator loss: 5.779808]\n",
      "6365 [Discriminator loss: 0.072944, acc.: 96.88%] [Generator loss: 5.479162]\n",
      "6366 [Discriminator loss: 0.400231, acc.: 79.69%] [Generator loss: 4.802032]\n",
      "6367 [Discriminator loss: 0.109808, acc.: 95.31%] [Generator loss: 5.820057]\n",
      "6368 [Discriminator loss: 0.226601, acc.: 89.06%] [Generator loss: 4.707773]\n",
      "6369 [Discriminator loss: 0.138277, acc.: 93.75%] [Generator loss: 4.987800]\n",
      "6370 [Discriminator loss: 0.206965, acc.: 89.06%] [Generator loss: 4.290717]\n",
      "6371 [Discriminator loss: 0.204559, acc.: 89.06%] [Generator loss: 5.453782]\n",
      "6372 [Discriminator loss: 0.078104, acc.: 98.44%] [Generator loss: 4.123932]\n",
      "6373 [Discriminator loss: 0.265590, acc.: 85.94%] [Generator loss: 3.995199]\n",
      "6374 [Discriminator loss: 0.164057, acc.: 92.19%] [Generator loss: 5.032191]\n",
      "6375 [Discriminator loss: 0.123441, acc.: 90.62%] [Generator loss: 6.948262]\n",
      "6376 [Discriminator loss: 0.347230, acc.: 82.81%] [Generator loss: 4.446142]\n",
      "6377 [Discriminator loss: 0.172576, acc.: 90.62%] [Generator loss: 5.906593]\n",
      "6378 [Discriminator loss: 0.122875, acc.: 93.75%] [Generator loss: 6.492333]\n",
      "6379 [Discriminator loss: 0.219356, acc.: 90.62%] [Generator loss: 5.199371]\n",
      "6380 [Discriminator loss: 0.208125, acc.: 92.19%] [Generator loss: 5.827332]\n",
      "6381 [Discriminator loss: 0.315021, acc.: 82.81%] [Generator loss: 4.491634]\n",
      "6382 [Discriminator loss: 0.170043, acc.: 93.75%] [Generator loss: 5.907511]\n",
      "6383 [Discriminator loss: 0.153933, acc.: 95.31%] [Generator loss: 4.973521]\n",
      "6384 [Discriminator loss: 0.203271, acc.: 92.19%] [Generator loss: 6.099694]\n",
      "6385 [Discriminator loss: 0.218144, acc.: 89.06%] [Generator loss: 6.228181]\n",
      "6386 [Discriminator loss: 0.325920, acc.: 87.50%] [Generator loss: 3.733557]\n",
      "6387 [Discriminator loss: 0.225352, acc.: 92.19%] [Generator loss: 5.986907]\n",
      "6388 [Discriminator loss: 0.103001, acc.: 96.88%] [Generator loss: 6.062126]\n",
      "6389 [Discriminator loss: 0.199314, acc.: 90.62%] [Generator loss: 5.536321]\n",
      "6390 [Discriminator loss: 0.168189, acc.: 90.62%] [Generator loss: 4.121834]\n",
      "6391 [Discriminator loss: 0.229237, acc.: 89.06%] [Generator loss: 5.131973]\n",
      "6392 [Discriminator loss: 0.233396, acc.: 84.38%] [Generator loss: 5.464574]\n",
      "6393 [Discriminator loss: 0.280501, acc.: 85.94%] [Generator loss: 5.321955]\n",
      "6394 [Discriminator loss: 0.046338, acc.: 98.44%] [Generator loss: 5.828100]\n",
      "6395 [Discriminator loss: 0.192473, acc.: 90.62%] [Generator loss: 6.543300]\n",
      "6396 [Discriminator loss: 0.097140, acc.: 98.44%] [Generator loss: 5.217451]\n",
      "6397 [Discriminator loss: 0.196077, acc.: 90.62%] [Generator loss: 5.843274]\n",
      "6398 [Discriminator loss: 0.400611, acc.: 85.94%] [Generator loss: 6.203695]\n",
      "6399 [Discriminator loss: 0.228306, acc.: 90.62%] [Generator loss: 6.380814]\n",
      "6400 [Discriminator loss: 0.290006, acc.: 92.19%] [Generator loss: 4.648952]\n",
      "6401 [Discriminator loss: 0.151100, acc.: 95.31%] [Generator loss: 3.759048]\n",
      "6402 [Discriminator loss: 0.211355, acc.: 85.94%] [Generator loss: 5.275047]\n",
      "6403 [Discriminator loss: 0.323833, acc.: 87.50%] [Generator loss: 5.171526]\n",
      "6404 [Discriminator loss: 0.144978, acc.: 95.31%] [Generator loss: 4.718740]\n",
      "6405 [Discriminator loss: 0.312644, acc.: 89.06%] [Generator loss: 4.101459]\n",
      "6406 [Discriminator loss: 0.418095, acc.: 78.12%] [Generator loss: 7.575865]\n",
      "6407 [Discriminator loss: 0.164701, acc.: 93.75%] [Generator loss: 6.488704]\n",
      "6408 [Discriminator loss: 0.355062, acc.: 82.81%] [Generator loss: 4.802541]\n",
      "6409 [Discriminator loss: 0.348029, acc.: 89.06%] [Generator loss: 5.345924]\n",
      "6410 [Discriminator loss: 0.182474, acc.: 93.75%] [Generator loss: 5.743084]\n",
      "6411 [Discriminator loss: 0.199578, acc.: 93.75%] [Generator loss: 4.229363]\n",
      "6412 [Discriminator loss: 0.098894, acc.: 98.44%] [Generator loss: 4.423994]\n",
      "6413 [Discriminator loss: 0.118394, acc.: 95.31%] [Generator loss: 5.519992]\n",
      "6414 [Discriminator loss: 0.122798, acc.: 92.19%] [Generator loss: 5.181332]\n",
      "6415 [Discriminator loss: 0.312507, acc.: 87.50%] [Generator loss: 7.218902]\n",
      "6416 [Discriminator loss: 0.255837, acc.: 85.94%] [Generator loss: 4.559081]\n",
      "6417 [Discriminator loss: 0.161353, acc.: 93.75%] [Generator loss: 5.435853]\n",
      "6418 [Discriminator loss: 0.200344, acc.: 92.19%] [Generator loss: 4.682300]\n",
      "6419 [Discriminator loss: 0.194128, acc.: 93.75%] [Generator loss: 5.506761]\n",
      "6420 [Discriminator loss: 0.171988, acc.: 89.06%] [Generator loss: 4.817711]\n",
      "6421 [Discriminator loss: 0.338656, acc.: 84.38%] [Generator loss: 5.343812]\n",
      "6422 [Discriminator loss: 0.261278, acc.: 84.38%] [Generator loss: 6.045085]\n",
      "6423 [Discriminator loss: 0.121727, acc.: 96.88%] [Generator loss: 4.957129]\n",
      "6424 [Discriminator loss: 0.285434, acc.: 87.50%] [Generator loss: 6.033308]\n",
      "6425 [Discriminator loss: 0.207646, acc.: 92.19%] [Generator loss: 3.822805]\n",
      "6426 [Discriminator loss: 0.311822, acc.: 87.50%] [Generator loss: 5.724608]\n",
      "6427 [Discriminator loss: 0.310190, acc.: 87.50%] [Generator loss: 5.185521]\n",
      "6428 [Discriminator loss: 0.199289, acc.: 93.75%] [Generator loss: 6.070556]\n",
      "6429 [Discriminator loss: 0.096279, acc.: 96.88%] [Generator loss: 6.227512]\n",
      "6430 [Discriminator loss: 0.119012, acc.: 95.31%] [Generator loss: 4.981655]\n",
      "6431 [Discriminator loss: 0.246271, acc.: 90.62%] [Generator loss: 4.187980]\n",
      "6432 [Discriminator loss: 0.058494, acc.: 100.00%] [Generator loss: 5.851820]\n",
      "6433 [Discriminator loss: 0.199950, acc.: 92.19%] [Generator loss: 5.318696]\n",
      "6434 [Discriminator loss: 0.137893, acc.: 93.75%] [Generator loss: 4.814335]\n",
      "6435 [Discriminator loss: 0.162321, acc.: 93.75%] [Generator loss: 4.616207]\n",
      "6436 [Discriminator loss: 0.416464, acc.: 82.81%] [Generator loss: 5.626842]\n",
      "6437 [Discriminator loss: 0.312724, acc.: 84.38%] [Generator loss: 6.403495]\n",
      "6438 [Discriminator loss: 0.191168, acc.: 89.06%] [Generator loss: 5.376547]\n",
      "6439 [Discriminator loss: 0.311240, acc.: 85.94%] [Generator loss: 5.048765]\n",
      "6440 [Discriminator loss: 0.107287, acc.: 98.44%] [Generator loss: 5.890926]\n",
      "6441 [Discriminator loss: 0.240016, acc.: 89.06%] [Generator loss: 4.820260]\n",
      "6442 [Discriminator loss: 0.355351, acc.: 84.38%] [Generator loss: 5.314889]\n",
      "6443 [Discriminator loss: 0.150792, acc.: 96.88%] [Generator loss: 5.549746]\n",
      "6444 [Discriminator loss: 0.203335, acc.: 92.19%] [Generator loss: 4.999575]\n",
      "6445 [Discriminator loss: 0.201967, acc.: 93.75%] [Generator loss: 5.776314]\n",
      "6446 [Discriminator loss: 0.261826, acc.: 89.06%] [Generator loss: 5.549647]\n",
      "6447 [Discriminator loss: 0.383602, acc.: 84.38%] [Generator loss: 4.903274]\n",
      "6448 [Discriminator loss: 0.161588, acc.: 96.88%] [Generator loss: 5.619643]\n",
      "6449 [Discriminator loss: 0.108907, acc.: 95.31%] [Generator loss: 6.094914]\n",
      "6450 [Discriminator loss: 0.152176, acc.: 95.31%] [Generator loss: 4.769984]\n",
      "6451 [Discriminator loss: 0.117993, acc.: 95.31%] [Generator loss: 5.171456]\n",
      "6452 [Discriminator loss: 0.317157, acc.: 87.50%] [Generator loss: 5.897803]\n",
      "6453 [Discriminator loss: 0.224421, acc.: 93.75%] [Generator loss: 4.941833]\n",
      "6454 [Discriminator loss: 0.101263, acc.: 93.75%] [Generator loss: 5.749601]\n",
      "6455 [Discriminator loss: 0.273286, acc.: 89.06%] [Generator loss: 5.774902]\n",
      "6456 [Discriminator loss: 0.175789, acc.: 90.62%] [Generator loss: 4.728532]\n",
      "6457 [Discriminator loss: 0.316253, acc.: 85.94%] [Generator loss: 4.691134]\n",
      "6458 [Discriminator loss: 0.091260, acc.: 98.44%] [Generator loss: 5.655019]\n",
      "6459 [Discriminator loss: 0.480850, acc.: 79.69%] [Generator loss: 4.244006]\n",
      "6460 [Discriminator loss: 0.094511, acc.: 96.88%] [Generator loss: 6.179834]\n",
      "6461 [Discriminator loss: 0.177025, acc.: 93.75%] [Generator loss: 5.412470]\n",
      "6462 [Discriminator loss: 0.147786, acc.: 93.75%] [Generator loss: 4.144896]\n",
      "6463 [Discriminator loss: 0.198542, acc.: 89.06%] [Generator loss: 5.111385]\n",
      "6464 [Discriminator loss: 0.128913, acc.: 95.31%] [Generator loss: 5.210324]\n",
      "6465 [Discriminator loss: 0.116290, acc.: 95.31%] [Generator loss: 4.401496]\n",
      "6466 [Discriminator loss: 0.148981, acc.: 95.31%] [Generator loss: 4.740893]\n",
      "6467 [Discriminator loss: 0.195241, acc.: 90.62%] [Generator loss: 5.630579]\n",
      "6468 [Discriminator loss: 0.246667, acc.: 87.50%] [Generator loss: 6.533459]\n",
      "6469 [Discriminator loss: 0.191340, acc.: 90.62%] [Generator loss: 3.961509]\n",
      "6470 [Discriminator loss: 0.162241, acc.: 95.31%] [Generator loss: 5.093171]\n",
      "6471 [Discriminator loss: 0.175078, acc.: 93.75%] [Generator loss: 5.654868]\n",
      "6472 [Discriminator loss: 0.316889, acc.: 87.50%] [Generator loss: 5.240057]\n",
      "6473 [Discriminator loss: 0.192083, acc.: 92.19%] [Generator loss: 4.355931]\n",
      "6474 [Discriminator loss: 0.239782, acc.: 90.62%] [Generator loss: 6.985543]\n",
      "6475 [Discriminator loss: 0.133789, acc.: 96.88%] [Generator loss: 6.382663]\n",
      "6476 [Discriminator loss: 0.274161, acc.: 92.19%] [Generator loss: 5.070385]\n",
      "6477 [Discriminator loss: 0.177625, acc.: 93.75%] [Generator loss: 5.460336]\n",
      "6478 [Discriminator loss: 0.131131, acc.: 93.75%] [Generator loss: 5.241940]\n",
      "6479 [Discriminator loss: 0.115163, acc.: 100.00%] [Generator loss: 5.418938]\n",
      "6480 [Discriminator loss: 0.194027, acc.: 93.75%] [Generator loss: 4.889097]\n",
      "6481 [Discriminator loss: 0.123643, acc.: 93.75%] [Generator loss: 4.232229]\n",
      "6482 [Discriminator loss: 0.274557, acc.: 85.94%] [Generator loss: 6.117546]\n",
      "6483 [Discriminator loss: 0.130803, acc.: 95.31%] [Generator loss: 5.364876]\n",
      "6484 [Discriminator loss: 0.196084, acc.: 92.19%] [Generator loss: 4.631048]\n",
      "6485 [Discriminator loss: 0.118863, acc.: 96.88%] [Generator loss: 5.242944]\n",
      "6486 [Discriminator loss: 0.269391, acc.: 92.19%] [Generator loss: 5.337887]\n",
      "6487 [Discriminator loss: 0.213536, acc.: 87.50%] [Generator loss: 4.418301]\n",
      "6488 [Discriminator loss: 0.146715, acc.: 95.31%] [Generator loss: 4.024716]\n",
      "6489 [Discriminator loss: 0.214921, acc.: 95.31%] [Generator loss: 5.029335]\n",
      "6490 [Discriminator loss: 0.047957, acc.: 100.00%] [Generator loss: 6.428617]\n",
      "6491 [Discriminator loss: 0.192564, acc.: 92.19%] [Generator loss: 5.259848]\n",
      "6492 [Discriminator loss: 0.152585, acc.: 93.75%] [Generator loss: 4.151182]\n",
      "6493 [Discriminator loss: 0.238486, acc.: 90.62%] [Generator loss: 4.181376]\n",
      "6494 [Discriminator loss: 0.131877, acc.: 96.88%] [Generator loss: 4.993468]\n",
      "6495 [Discriminator loss: 0.091734, acc.: 98.44%] [Generator loss: 4.813021]\n",
      "6496 [Discriminator loss: 0.088143, acc.: 96.88%] [Generator loss: 4.954278]\n",
      "6497 [Discriminator loss: 0.174993, acc.: 93.75%] [Generator loss: 5.510672]\n",
      "6498 [Discriminator loss: 0.242657, acc.: 87.50%] [Generator loss: 4.257960]\n",
      "6499 [Discriminator loss: 0.166391, acc.: 93.75%] [Generator loss: 5.850973]\n",
      "6500 [Discriminator loss: 0.284600, acc.: 87.50%] [Generator loss: 4.338719]\n",
      "6501 [Discriminator loss: 0.185834, acc.: 87.50%] [Generator loss: 5.789946]\n",
      "6502 [Discriminator loss: 0.070390, acc.: 100.00%] [Generator loss: 5.681792]\n",
      "6503 [Discriminator loss: 0.270859, acc.: 89.06%] [Generator loss: 4.746116]\n",
      "6504 [Discriminator loss: 0.341685, acc.: 87.50%] [Generator loss: 5.651277]\n",
      "6505 [Discriminator loss: 0.131826, acc.: 96.88%] [Generator loss: 3.692420]\n",
      "6506 [Discriminator loss: 0.127371, acc.: 95.31%] [Generator loss: 5.678372]\n",
      "6507 [Discriminator loss: 0.130208, acc.: 95.31%] [Generator loss: 5.131109]\n",
      "6508 [Discriminator loss: 0.212896, acc.: 93.75%] [Generator loss: 5.846527]\n",
      "6509 [Discriminator loss: 0.147234, acc.: 95.31%] [Generator loss: 5.659218]\n",
      "6510 [Discriminator loss: 0.277608, acc.: 89.06%] [Generator loss: 4.989900]\n",
      "6511 [Discriminator loss: 0.426478, acc.: 82.81%] [Generator loss: 5.538817]\n",
      "6512 [Discriminator loss: 0.141072, acc.: 95.31%] [Generator loss: 6.678441]\n",
      "6513 [Discriminator loss: 0.161472, acc.: 92.19%] [Generator loss: 5.441628]\n",
      "6514 [Discriminator loss: 0.206906, acc.: 92.19%] [Generator loss: 5.226380]\n",
      "6515 [Discriminator loss: 0.399664, acc.: 81.25%] [Generator loss: 5.258732]\n",
      "6516 [Discriminator loss: 0.225889, acc.: 89.06%] [Generator loss: 4.931291]\n",
      "6517 [Discriminator loss: 0.255892, acc.: 90.62%] [Generator loss: 3.972558]\n",
      "6518 [Discriminator loss: 0.052851, acc.: 100.00%] [Generator loss: 5.561234]\n",
      "6519 [Discriminator loss: 0.141022, acc.: 95.31%] [Generator loss: 4.937643]\n",
      "6520 [Discriminator loss: 0.193886, acc.: 95.31%] [Generator loss: 5.073695]\n",
      "6521 [Discriminator loss: 0.249764, acc.: 92.19%] [Generator loss: 5.444299]\n",
      "6522 [Discriminator loss: 0.330875, acc.: 84.38%] [Generator loss: 4.534918]\n",
      "6523 [Discriminator loss: 0.226289, acc.: 90.62%] [Generator loss: 6.499771]\n",
      "6524 [Discriminator loss: 0.396919, acc.: 85.94%] [Generator loss: 4.387523]\n",
      "6525 [Discriminator loss: 0.260529, acc.: 89.06%] [Generator loss: 5.544308]\n",
      "6526 [Discriminator loss: 0.206244, acc.: 89.06%] [Generator loss: 5.182445]\n",
      "6527 [Discriminator loss: 0.231451, acc.: 92.19%] [Generator loss: 4.747308]\n",
      "6528 [Discriminator loss: 0.138015, acc.: 95.31%] [Generator loss: 4.292865]\n",
      "6529 [Discriminator loss: 0.192599, acc.: 90.62%] [Generator loss: 5.678487]\n",
      "6530 [Discriminator loss: 0.460862, acc.: 81.25%] [Generator loss: 6.821506]\n",
      "6531 [Discriminator loss: 0.199794, acc.: 92.19%] [Generator loss: 4.581340]\n",
      "6532 [Discriminator loss: 0.273085, acc.: 87.50%] [Generator loss: 5.948629]\n",
      "6533 [Discriminator loss: 0.199435, acc.: 92.19%] [Generator loss: 6.714849]\n",
      "6534 [Discriminator loss: 0.281113, acc.: 84.38%] [Generator loss: 6.554928]\n",
      "6535 [Discriminator loss: 0.070332, acc.: 100.00%] [Generator loss: 6.317739]\n",
      "6536 [Discriminator loss: 0.411128, acc.: 76.56%] [Generator loss: 5.477925]\n",
      "6537 [Discriminator loss: 0.161522, acc.: 92.19%] [Generator loss: 5.418005]\n",
      "6538 [Discriminator loss: 0.204498, acc.: 90.62%] [Generator loss: 5.659437]\n",
      "6539 [Discriminator loss: 0.159162, acc.: 95.31%] [Generator loss: 5.316893]\n",
      "6540 [Discriminator loss: 0.306235, acc.: 84.38%] [Generator loss: 5.470771]\n",
      "6541 [Discriminator loss: 0.137800, acc.: 95.31%] [Generator loss: 5.924767]\n",
      "6542 [Discriminator loss: 0.292365, acc.: 90.62%] [Generator loss: 5.114383]\n",
      "6543 [Discriminator loss: 0.276876, acc.: 87.50%] [Generator loss: 4.184104]\n",
      "6544 [Discriminator loss: 0.147467, acc.: 95.31%] [Generator loss: 4.804303]\n",
      "6545 [Discriminator loss: 0.305865, acc.: 87.50%] [Generator loss: 7.343397]\n",
      "6546 [Discriminator loss: 0.143787, acc.: 95.31%] [Generator loss: 4.562389]\n",
      "6547 [Discriminator loss: 0.150185, acc.: 93.75%] [Generator loss: 5.295611]\n",
      "6548 [Discriminator loss: 0.149145, acc.: 93.75%] [Generator loss: 6.342177]\n",
      "6549 [Discriminator loss: 0.176191, acc.: 92.19%] [Generator loss: 6.044024]\n",
      "6550 [Discriminator loss: 0.197213, acc.: 93.75%] [Generator loss: 4.074689]\n",
      "6551 [Discriminator loss: 0.178036, acc.: 90.62%] [Generator loss: 6.561867]\n",
      "6552 [Discriminator loss: 0.183357, acc.: 95.31%] [Generator loss: 5.521996]\n",
      "6553 [Discriminator loss: 0.302595, acc.: 85.94%] [Generator loss: 5.038242]\n",
      "6554 [Discriminator loss: 0.295658, acc.: 87.50%] [Generator loss: 5.947018]\n",
      "6555 [Discriminator loss: 0.284882, acc.: 84.38%] [Generator loss: 4.897664]\n",
      "6556 [Discriminator loss: 0.195427, acc.: 93.75%] [Generator loss: 4.857052]\n",
      "6557 [Discriminator loss: 0.093529, acc.: 96.88%] [Generator loss: 4.817177]\n",
      "6558 [Discriminator loss: 0.128755, acc.: 95.31%] [Generator loss: 5.180149]\n",
      "6559 [Discriminator loss: 0.234650, acc.: 89.06%] [Generator loss: 6.000131]\n",
      "6560 [Discriminator loss: 0.355806, acc.: 90.62%] [Generator loss: 5.202071]\n",
      "6561 [Discriminator loss: 0.049750, acc.: 100.00%] [Generator loss: 5.190731]\n",
      "6562 [Discriminator loss: 0.170827, acc.: 92.19%] [Generator loss: 5.548221]\n",
      "6563 [Discriminator loss: 0.204000, acc.: 89.06%] [Generator loss: 5.117069]\n",
      "6564 [Discriminator loss: 0.131782, acc.: 93.75%] [Generator loss: 5.769078]\n",
      "6565 [Discriminator loss: 0.302127, acc.: 89.06%] [Generator loss: 5.769856]\n",
      "6566 [Discriminator loss: 0.170556, acc.: 93.75%] [Generator loss: 5.618154]\n",
      "6567 [Discriminator loss: 0.210865, acc.: 89.06%] [Generator loss: 5.759739]\n",
      "6568 [Discriminator loss: 0.335557, acc.: 87.50%] [Generator loss: 4.360069]\n",
      "6569 [Discriminator loss: 0.100040, acc.: 96.88%] [Generator loss: 6.137866]\n",
      "6570 [Discriminator loss: 0.138974, acc.: 95.31%] [Generator loss: 4.809727]\n",
      "6571 [Discriminator loss: 0.511219, acc.: 79.69%] [Generator loss: 5.273148]\n",
      "6572 [Discriminator loss: 0.110130, acc.: 96.88%] [Generator loss: 4.960359]\n",
      "6573 [Discriminator loss: 0.374479, acc.: 79.69%] [Generator loss: 6.956208]\n",
      "6574 [Discriminator loss: 0.176286, acc.: 95.31%] [Generator loss: 4.791577]\n",
      "6575 [Discriminator loss: 0.113545, acc.: 95.31%] [Generator loss: 5.784416]\n",
      "6576 [Discriminator loss: 0.171474, acc.: 96.88%] [Generator loss: 4.394242]\n",
      "6577 [Discriminator loss: 0.159010, acc.: 92.19%] [Generator loss: 4.577383]\n",
      "6578 [Discriminator loss: 0.289798, acc.: 87.50%] [Generator loss: 5.449941]\n",
      "6579 [Discriminator loss: 0.075095, acc.: 98.44%] [Generator loss: 6.707359]\n",
      "6580 [Discriminator loss: 0.353961, acc.: 87.50%] [Generator loss: 4.307167]\n",
      "6581 [Discriminator loss: 0.202460, acc.: 89.06%] [Generator loss: 4.854698]\n",
      "6582 [Discriminator loss: 0.269008, acc.: 89.06%] [Generator loss: 4.707106]\n",
      "6583 [Discriminator loss: 0.338973, acc.: 82.81%] [Generator loss: 5.233707]\n",
      "6584 [Discriminator loss: 0.177788, acc.: 96.88%] [Generator loss: 5.663275]\n",
      "6585 [Discriminator loss: 0.074529, acc.: 96.88%] [Generator loss: 6.311873]\n",
      "6586 [Discriminator loss: 0.177301, acc.: 92.19%] [Generator loss: 5.856295]\n",
      "6587 [Discriminator loss: 0.111707, acc.: 98.44%] [Generator loss: 6.144890]\n",
      "6588 [Discriminator loss: 0.149750, acc.: 93.75%] [Generator loss: 5.332580]\n",
      "6589 [Discriminator loss: 0.192373, acc.: 90.62%] [Generator loss: 6.126451]\n",
      "6590 [Discriminator loss: 0.049938, acc.: 100.00%] [Generator loss: 6.582663]\n",
      "6591 [Discriminator loss: 0.194553, acc.: 92.19%] [Generator loss: 5.096876]\n",
      "6592 [Discriminator loss: 0.283913, acc.: 85.94%] [Generator loss: 5.348160]\n",
      "6593 [Discriminator loss: 0.140504, acc.: 95.31%] [Generator loss: 6.487310]\n",
      "6594 [Discriminator loss: 0.212573, acc.: 93.75%] [Generator loss: 5.249633]\n",
      "6595 [Discriminator loss: 0.202897, acc.: 85.94%] [Generator loss: 5.323644]\n",
      "6596 [Discriminator loss: 0.192458, acc.: 92.19%] [Generator loss: 5.797940]\n",
      "6597 [Discriminator loss: 0.292454, acc.: 84.38%] [Generator loss: 5.379400]\n",
      "6598 [Discriminator loss: 0.128869, acc.: 95.31%] [Generator loss: 5.289039]\n",
      "6599 [Discriminator loss: 0.208285, acc.: 90.62%] [Generator loss: 5.632250]\n",
      "6600 [Discriminator loss: 0.126366, acc.: 96.88%] [Generator loss: 5.977923]\n",
      "6601 [Discriminator loss: 0.124244, acc.: 92.19%] [Generator loss: 4.525062]\n",
      "6602 [Discriminator loss: 0.305924, acc.: 89.06%] [Generator loss: 5.121936]\n",
      "6603 [Discriminator loss: 0.078403, acc.: 96.88%] [Generator loss: 5.308246]\n",
      "6604 [Discriminator loss: 0.192326, acc.: 92.19%] [Generator loss: 5.436145]\n",
      "6605 [Discriminator loss: 0.119644, acc.: 95.31%] [Generator loss: 4.685823]\n",
      "6606 [Discriminator loss: 0.230939, acc.: 87.50%] [Generator loss: 5.895415]\n",
      "6607 [Discriminator loss: 0.166334, acc.: 98.44%] [Generator loss: 5.402006]\n",
      "6608 [Discriminator loss: 0.276043, acc.: 87.50%] [Generator loss: 4.047160]\n",
      "6609 [Discriminator loss: 0.294895, acc.: 84.38%] [Generator loss: 6.689989]\n",
      "6610 [Discriminator loss: 0.258493, acc.: 85.94%] [Generator loss: 5.486033]\n",
      "6611 [Discriminator loss: 0.119698, acc.: 96.88%] [Generator loss: 5.566586]\n",
      "6612 [Discriminator loss: 0.204925, acc.: 90.62%] [Generator loss: 5.584551]\n",
      "6613 [Discriminator loss: 0.090907, acc.: 98.44%] [Generator loss: 5.366622]\n",
      "6614 [Discriminator loss: 0.106273, acc.: 98.44%] [Generator loss: 4.676946]\n",
      "6615 [Discriminator loss: 0.237590, acc.: 89.06%] [Generator loss: 6.185512]\n",
      "6616 [Discriminator loss: 0.182319, acc.: 92.19%] [Generator loss: 5.423172]\n",
      "6617 [Discriminator loss: 0.384209, acc.: 84.38%] [Generator loss: 6.374619]\n",
      "6618 [Discriminator loss: 0.058926, acc.: 98.44%] [Generator loss: 6.023674]\n",
      "6619 [Discriminator loss: 0.192595, acc.: 92.19%] [Generator loss: 5.199160]\n",
      "6620 [Discriminator loss: 0.294212, acc.: 92.19%] [Generator loss: 5.140647]\n",
      "6621 [Discriminator loss: 0.235522, acc.: 92.19%] [Generator loss: 6.197482]\n",
      "6622 [Discriminator loss: 0.201985, acc.: 89.06%] [Generator loss: 6.086106]\n",
      "6623 [Discriminator loss: 0.195567, acc.: 90.62%] [Generator loss: 5.418594]\n",
      "6624 [Discriminator loss: 0.477425, acc.: 78.12%] [Generator loss: 5.168020]\n",
      "6625 [Discriminator loss: 0.203242, acc.: 90.62%] [Generator loss: 6.134818]\n",
      "6626 [Discriminator loss: 0.184537, acc.: 93.75%] [Generator loss: 5.263162]\n",
      "6627 [Discriminator loss: 0.174455, acc.: 93.75%] [Generator loss: 5.133769]\n",
      "6628 [Discriminator loss: 0.077153, acc.: 96.88%] [Generator loss: 4.572478]\n",
      "6629 [Discriminator loss: 0.260488, acc.: 89.06%] [Generator loss: 4.913717]\n",
      "6630 [Discriminator loss: 0.198094, acc.: 89.06%] [Generator loss: 5.756227]\n",
      "6631 [Discriminator loss: 0.073460, acc.: 98.44%] [Generator loss: 5.010208]\n",
      "6632 [Discriminator loss: 0.265653, acc.: 90.62%] [Generator loss: 4.369740]\n",
      "6633 [Discriminator loss: 0.078764, acc.: 98.44%] [Generator loss: 4.822313]\n",
      "6634 [Discriminator loss: 0.290081, acc.: 89.06%] [Generator loss: 6.738686]\n",
      "6635 [Discriminator loss: 0.303284, acc.: 90.62%] [Generator loss: 4.549152]\n",
      "6636 [Discriminator loss: 0.189687, acc.: 90.62%] [Generator loss: 5.828389]\n",
      "6637 [Discriminator loss: 0.092504, acc.: 96.88%] [Generator loss: 5.812166]\n",
      "6638 [Discriminator loss: 0.473946, acc.: 82.81%] [Generator loss: 4.966990]\n",
      "6639 [Discriminator loss: 0.107555, acc.: 98.44%] [Generator loss: 6.673418]\n",
      "6640 [Discriminator loss: 0.099959, acc.: 95.31%] [Generator loss: 5.721239]\n",
      "6641 [Discriminator loss: 0.320904, acc.: 85.94%] [Generator loss: 5.288985]\n",
      "6642 [Discriminator loss: 0.135881, acc.: 95.31%] [Generator loss: 4.977996]\n",
      "6643 [Discriminator loss: 0.111964, acc.: 96.88%] [Generator loss: 4.997925]\n",
      "6644 [Discriminator loss: 0.198830, acc.: 92.19%] [Generator loss: 4.481639]\n",
      "6645 [Discriminator loss: 0.144290, acc.: 93.75%] [Generator loss: 5.129794]\n",
      "6646 [Discriminator loss: 0.154612, acc.: 95.31%] [Generator loss: 6.296083]\n",
      "6647 [Discriminator loss: 0.124712, acc.: 98.44%] [Generator loss: 5.325788]\n",
      "6648 [Discriminator loss: 0.153379, acc.: 90.62%] [Generator loss: 5.628549]\n",
      "6649 [Discriminator loss: 0.227139, acc.: 90.62%] [Generator loss: 4.444405]\n",
      "6650 [Discriminator loss: 0.144194, acc.: 90.62%] [Generator loss: 4.287322]\n",
      "6651 [Discriminator loss: 0.122071, acc.: 93.75%] [Generator loss: 5.582704]\n",
      "6652 [Discriminator loss: 0.208433, acc.: 89.06%] [Generator loss: 6.315732]\n",
      "6653 [Discriminator loss: 0.319560, acc.: 89.06%] [Generator loss: 5.516659]\n",
      "6654 [Discriminator loss: 0.236142, acc.: 90.62%] [Generator loss: 4.957913]\n",
      "6655 [Discriminator loss: 0.118655, acc.: 93.75%] [Generator loss: 6.322660]\n",
      "6656 [Discriminator loss: 0.302202, acc.: 84.38%] [Generator loss: 5.707984]\n",
      "6657 [Discriminator loss: 0.116294, acc.: 95.31%] [Generator loss: 5.489209]\n",
      "6658 [Discriminator loss: 0.164224, acc.: 96.88%] [Generator loss: 4.301583]\n",
      "6659 [Discriminator loss: 0.090828, acc.: 96.88%] [Generator loss: 5.648710]\n",
      "6660 [Discriminator loss: 0.129367, acc.: 96.88%] [Generator loss: 4.065797]\n",
      "6661 [Discriminator loss: 0.091097, acc.: 96.88%] [Generator loss: 6.019250]\n",
      "6662 [Discriminator loss: 0.056650, acc.: 100.00%] [Generator loss: 4.705420]\n",
      "6663 [Discriminator loss: 0.128095, acc.: 95.31%] [Generator loss: 5.481501]\n",
      "6664 [Discriminator loss: 0.342433, acc.: 85.94%] [Generator loss: 4.270420]\n",
      "6665 [Discriminator loss: 0.263449, acc.: 87.50%] [Generator loss: 6.356497]\n",
      "6666 [Discriminator loss: 0.083125, acc.: 96.88%] [Generator loss: 5.974106]\n",
      "6667 [Discriminator loss: 0.226108, acc.: 90.62%] [Generator loss: 4.274532]\n",
      "6668 [Discriminator loss: 0.087425, acc.: 98.44%] [Generator loss: 6.115343]\n",
      "6669 [Discriminator loss: 0.147757, acc.: 93.75%] [Generator loss: 5.109167]\n",
      "6670 [Discriminator loss: 0.343689, acc.: 82.81%] [Generator loss: 5.073384]\n",
      "6671 [Discriminator loss: 0.057826, acc.: 98.44%] [Generator loss: 5.074923]\n",
      "6672 [Discriminator loss: 0.357412, acc.: 84.38%] [Generator loss: 6.417907]\n",
      "6673 [Discriminator loss: 0.088507, acc.: 98.44%] [Generator loss: 6.414101]\n",
      "6674 [Discriminator loss: 0.244065, acc.: 90.62%] [Generator loss: 6.235256]\n",
      "6675 [Discriminator loss: 0.135057, acc.: 93.75%] [Generator loss: 6.896395]\n",
      "6676 [Discriminator loss: 0.174970, acc.: 93.75%] [Generator loss: 6.135211]\n",
      "6677 [Discriminator loss: 0.112178, acc.: 93.75%] [Generator loss: 3.561351]\n",
      "6678 [Discriminator loss: 0.406795, acc.: 81.25%] [Generator loss: 6.980112]\n",
      "6679 [Discriminator loss: 0.139983, acc.: 95.31%] [Generator loss: 7.425793]\n",
      "6680 [Discriminator loss: 0.300157, acc.: 89.06%] [Generator loss: 4.169518]\n",
      "6681 [Discriminator loss: 0.218843, acc.: 95.31%] [Generator loss: 4.422444]\n",
      "6682 [Discriminator loss: 0.272960, acc.: 87.50%] [Generator loss: 5.629413]\n",
      "6683 [Discriminator loss: 0.142526, acc.: 93.75%] [Generator loss: 5.419052]\n",
      "6684 [Discriminator loss: 0.281337, acc.: 87.50%] [Generator loss: 5.526494]\n",
      "6685 [Discriminator loss: 0.126789, acc.: 96.88%] [Generator loss: 5.802564]\n",
      "6686 [Discriminator loss: 0.219013, acc.: 90.62%] [Generator loss: 4.941676]\n",
      "6687 [Discriminator loss: 0.308179, acc.: 85.94%] [Generator loss: 4.469304]\n",
      "6688 [Discriminator loss: 0.208631, acc.: 90.62%] [Generator loss: 5.730019]\n",
      "6689 [Discriminator loss: 0.079444, acc.: 98.44%] [Generator loss: 4.962351]\n",
      "6690 [Discriminator loss: 0.177566, acc.: 95.31%] [Generator loss: 5.144380]\n",
      "6691 [Discriminator loss: 0.059810, acc.: 98.44%] [Generator loss: 5.252103]\n",
      "6692 [Discriminator loss: 0.110790, acc.: 98.44%] [Generator loss: 5.699350]\n",
      "6693 [Discriminator loss: 0.297206, acc.: 85.94%] [Generator loss: 4.320098]\n",
      "6694 [Discriminator loss: 0.167555, acc.: 93.75%] [Generator loss: 4.574538]\n",
      "6695 [Discriminator loss: 0.259782, acc.: 87.50%] [Generator loss: 5.501382]\n",
      "6696 [Discriminator loss: 0.182192, acc.: 93.75%] [Generator loss: 5.156673]\n",
      "6697 [Discriminator loss: 0.099941, acc.: 98.44%] [Generator loss: 5.308999]\n",
      "6698 [Discriminator loss: 0.337458, acc.: 82.81%] [Generator loss: 5.959332]\n",
      "6699 [Discriminator loss: 0.158074, acc.: 95.31%] [Generator loss: 5.098357]\n",
      "6700 [Discriminator loss: 0.114569, acc.: 93.75%] [Generator loss: 4.360481]\n",
      "6701 [Discriminator loss: 0.188625, acc.: 89.06%] [Generator loss: 6.618175]\n",
      "6702 [Discriminator loss: 0.242834, acc.: 93.75%] [Generator loss: 5.851027]\n",
      "6703 [Discriminator loss: 0.226560, acc.: 92.19%] [Generator loss: 5.448267]\n",
      "6704 [Discriminator loss: 0.214233, acc.: 89.06%] [Generator loss: 5.226017]\n",
      "6705 [Discriminator loss: 0.191751, acc.: 93.75%] [Generator loss: 5.376247]\n",
      "6706 [Discriminator loss: 0.210346, acc.: 93.75%] [Generator loss: 3.759092]\n",
      "6707 [Discriminator loss: 0.241546, acc.: 89.06%] [Generator loss: 5.720639]\n",
      "6708 [Discriminator loss: 0.118877, acc.: 92.19%] [Generator loss: 6.545216]\n",
      "6709 [Discriminator loss: 0.249437, acc.: 87.50%] [Generator loss: 4.405358]\n",
      "6710 [Discriminator loss: 0.332161, acc.: 82.81%] [Generator loss: 6.348711]\n",
      "6711 [Discriminator loss: 0.090595, acc.: 95.31%] [Generator loss: 5.008016]\n",
      "6712 [Discriminator loss: 0.525031, acc.: 75.00%] [Generator loss: 3.625394]\n",
      "6713 [Discriminator loss: 0.127366, acc.: 95.31%] [Generator loss: 6.759789]\n",
      "6714 [Discriminator loss: 0.157524, acc.: 92.19%] [Generator loss: 4.952657]\n",
      "6715 [Discriminator loss: 0.199066, acc.: 92.19%] [Generator loss: 5.346334]\n",
      "6716 [Discriminator loss: 0.230048, acc.: 87.50%] [Generator loss: 4.692746]\n",
      "6717 [Discriminator loss: 0.185382, acc.: 92.19%] [Generator loss: 6.367552]\n",
      "6718 [Discriminator loss: 0.261561, acc.: 89.06%] [Generator loss: 5.498648]\n",
      "6719 [Discriminator loss: 0.300393, acc.: 87.50%] [Generator loss: 3.713072]\n",
      "6720 [Discriminator loss: 0.148306, acc.: 95.31%] [Generator loss: 5.014044]\n",
      "6721 [Discriminator loss: 0.162903, acc.: 96.88%] [Generator loss: 4.950287]\n",
      "6722 [Discriminator loss: 0.250133, acc.: 89.06%] [Generator loss: 4.634034]\n",
      "6723 [Discriminator loss: 0.081997, acc.: 100.00%] [Generator loss: 4.774218]\n",
      "6724 [Discriminator loss: 0.340427, acc.: 89.06%] [Generator loss: 4.758794]\n",
      "6725 [Discriminator loss: 0.242325, acc.: 87.50%] [Generator loss: 5.636161]\n",
      "6726 [Discriminator loss: 0.106951, acc.: 96.88%] [Generator loss: 4.862921]\n",
      "6727 [Discriminator loss: 0.386925, acc.: 84.38%] [Generator loss: 4.328945]\n",
      "6728 [Discriminator loss: 0.072321, acc.: 96.88%] [Generator loss: 4.862465]\n",
      "6729 [Discriminator loss: 0.403755, acc.: 85.94%] [Generator loss: 5.671947]\n",
      "6730 [Discriminator loss: 0.129200, acc.: 96.88%] [Generator loss: 6.380895]\n",
      "6731 [Discriminator loss: 0.317238, acc.: 85.94%] [Generator loss: 4.229620]\n",
      "6732 [Discriminator loss: 0.110854, acc.: 95.31%] [Generator loss: 5.531387]\n",
      "6733 [Discriminator loss: 0.201100, acc.: 89.06%] [Generator loss: 5.331275]\n",
      "6734 [Discriminator loss: 0.241189, acc.: 87.50%] [Generator loss: 5.789269]\n",
      "6735 [Discriminator loss: 0.105607, acc.: 95.31%] [Generator loss: 5.712932]\n",
      "6736 [Discriminator loss: 0.241158, acc.: 92.19%] [Generator loss: 5.072223]\n",
      "6737 [Discriminator loss: 0.098791, acc.: 98.44%] [Generator loss: 4.900122]\n",
      "6738 [Discriminator loss: 0.393624, acc.: 84.38%] [Generator loss: 4.900714]\n",
      "6739 [Discriminator loss: 0.153570, acc.: 92.19%] [Generator loss: 4.685173]\n",
      "6740 [Discriminator loss: 0.078837, acc.: 96.88%] [Generator loss: 5.122234]\n",
      "6741 [Discriminator loss: 0.255075, acc.: 87.50%] [Generator loss: 5.409134]\n",
      "6742 [Discriminator loss: 0.238934, acc.: 92.19%] [Generator loss: 5.588761]\n",
      "6743 [Discriminator loss: 0.158266, acc.: 93.75%] [Generator loss: 6.110681]\n",
      "6744 [Discriminator loss: 0.179506, acc.: 93.75%] [Generator loss: 5.799150]\n",
      "6745 [Discriminator loss: 0.416683, acc.: 84.38%] [Generator loss: 4.600099]\n",
      "6746 [Discriminator loss: 0.120205, acc.: 95.31%] [Generator loss: 5.126429]\n",
      "6747 [Discriminator loss: 0.206617, acc.: 93.75%] [Generator loss: 5.696212]\n",
      "6748 [Discriminator loss: 0.267157, acc.: 87.50%] [Generator loss: 4.755556]\n",
      "6749 [Discriminator loss: 0.078551, acc.: 96.88%] [Generator loss: 6.330964]\n",
      "6750 [Discriminator loss: 0.169950, acc.: 92.19%] [Generator loss: 4.526711]\n",
      "6751 [Discriminator loss: 0.206291, acc.: 93.75%] [Generator loss: 4.351848]\n",
      "6752 [Discriminator loss: 0.160863, acc.: 93.75%] [Generator loss: 5.652963]\n",
      "6753 [Discriminator loss: 0.117744, acc.: 93.75%] [Generator loss: 4.291111]\n",
      "6754 [Discriminator loss: 0.263441, acc.: 90.62%] [Generator loss: 5.991020]\n",
      "6755 [Discriminator loss: 0.211214, acc.: 90.62%] [Generator loss: 4.998555]\n",
      "6756 [Discriminator loss: 0.073521, acc.: 98.44%] [Generator loss: 4.374134]\n",
      "6757 [Discriminator loss: 0.298502, acc.: 82.81%] [Generator loss: 4.189337]\n",
      "6758 [Discriminator loss: 0.236306, acc.: 92.19%] [Generator loss: 6.027187]\n",
      "6759 [Discriminator loss: 0.102540, acc.: 95.31%] [Generator loss: 5.809995]\n",
      "6760 [Discriminator loss: 0.258574, acc.: 89.06%] [Generator loss: 4.647666]\n",
      "6761 [Discriminator loss: 0.053584, acc.: 96.88%] [Generator loss: 5.856750]\n",
      "6762 [Discriminator loss: 0.072611, acc.: 98.44%] [Generator loss: 4.795075]\n",
      "6763 [Discriminator loss: 0.309905, acc.: 90.62%] [Generator loss: 5.743417]\n",
      "6764 [Discriminator loss: 0.125938, acc.: 98.44%] [Generator loss: 5.840746]\n",
      "6765 [Discriminator loss: 0.207715, acc.: 90.62%] [Generator loss: 4.445670]\n",
      "6766 [Discriminator loss: 0.360090, acc.: 81.25%] [Generator loss: 6.699468]\n",
      "6767 [Discriminator loss: 0.157125, acc.: 95.31%] [Generator loss: 5.667099]\n",
      "6768 [Discriminator loss: 0.169085, acc.: 93.75%] [Generator loss: 4.768260]\n",
      "6769 [Discriminator loss: 0.121241, acc.: 96.88%] [Generator loss: 5.386757]\n",
      "6770 [Discriminator loss: 0.140341, acc.: 95.31%] [Generator loss: 4.995740]\n",
      "6771 [Discriminator loss: 0.408066, acc.: 82.81%] [Generator loss: 6.953397]\n",
      "6772 [Discriminator loss: 0.216445, acc.: 85.94%] [Generator loss: 6.697140]\n",
      "6773 [Discriminator loss: 0.109429, acc.: 95.31%] [Generator loss: 5.960097]\n",
      "6774 [Discriminator loss: 0.194747, acc.: 90.62%] [Generator loss: 5.222393]\n",
      "6775 [Discriminator loss: 0.290784, acc.: 84.38%] [Generator loss: 5.155446]\n",
      "6776 [Discriminator loss: 0.096779, acc.: 98.44%] [Generator loss: 5.004091]\n",
      "6777 [Discriminator loss: 0.110521, acc.: 96.88%] [Generator loss: 6.577236]\n",
      "6778 [Discriminator loss: 0.098569, acc.: 92.19%] [Generator loss: 6.361895]\n",
      "6779 [Discriminator loss: 0.275887, acc.: 92.19%] [Generator loss: 6.165680]\n",
      "6780 [Discriminator loss: 0.275776, acc.: 84.38%] [Generator loss: 6.271477]\n",
      "6781 [Discriminator loss: 0.148851, acc.: 92.19%] [Generator loss: 4.646320]\n",
      "6782 [Discriminator loss: 0.368630, acc.: 89.06%] [Generator loss: 5.757387]\n",
      "6783 [Discriminator loss: 0.254236, acc.: 84.38%] [Generator loss: 5.796161]\n",
      "6784 [Discriminator loss: 0.365143, acc.: 85.94%] [Generator loss: 5.851509]\n",
      "6785 [Discriminator loss: 0.153734, acc.: 93.75%] [Generator loss: 4.621222]\n",
      "6786 [Discriminator loss: 0.107539, acc.: 95.31%] [Generator loss: 6.342542]\n",
      "6787 [Discriminator loss: 0.121150, acc.: 96.88%] [Generator loss: 5.635229]\n",
      "6788 [Discriminator loss: 0.169713, acc.: 90.62%] [Generator loss: 4.706020]\n",
      "6789 [Discriminator loss: 0.235991, acc.: 89.06%] [Generator loss: 5.969882]\n",
      "6790 [Discriminator loss: 0.297335, acc.: 89.06%] [Generator loss: 6.540900]\n",
      "6791 [Discriminator loss: 0.288302, acc.: 92.19%] [Generator loss: 4.898161]\n",
      "6792 [Discriminator loss: 0.271927, acc.: 82.81%] [Generator loss: 5.489850]\n",
      "6793 [Discriminator loss: 0.307122, acc.: 87.50%] [Generator loss: 5.719796]\n",
      "6794 [Discriminator loss: 0.160808, acc.: 93.75%] [Generator loss: 6.664801]\n",
      "6795 [Discriminator loss: 0.305760, acc.: 81.25%] [Generator loss: 6.664679]\n",
      "6796 [Discriminator loss: 0.151930, acc.: 92.19%] [Generator loss: 5.556943]\n",
      "6797 [Discriminator loss: 0.181660, acc.: 90.62%] [Generator loss: 4.385417]\n",
      "6798 [Discriminator loss: 0.351901, acc.: 85.94%] [Generator loss: 4.682041]\n",
      "6799 [Discriminator loss: 0.306463, acc.: 90.62%] [Generator loss: 6.275565]\n",
      "6800 [Discriminator loss: 0.109817, acc.: 95.31%] [Generator loss: 4.765727]\n",
      "6801 [Discriminator loss: 0.384961, acc.: 81.25%] [Generator loss: 6.218742]\n",
      "6802 [Discriminator loss: 0.249256, acc.: 93.75%] [Generator loss: 5.540557]\n",
      "6803 [Discriminator loss: 0.274439, acc.: 87.50%] [Generator loss: 5.911032]\n",
      "6804 [Discriminator loss: 0.121272, acc.: 95.31%] [Generator loss: 6.682357]\n",
      "6805 [Discriminator loss: 0.305801, acc.: 87.50%] [Generator loss: 6.380656]\n",
      "6806 [Discriminator loss: 0.372870, acc.: 89.06%] [Generator loss: 4.723276]\n",
      "6807 [Discriminator loss: 0.320669, acc.: 89.06%] [Generator loss: 4.679813]\n",
      "6808 [Discriminator loss: 0.177441, acc.: 93.75%] [Generator loss: 5.146657]\n",
      "6809 [Discriminator loss: 0.451566, acc.: 79.69%] [Generator loss: 5.489853]\n",
      "6810 [Discriminator loss: 0.134910, acc.: 96.88%] [Generator loss: 6.175711]\n",
      "6811 [Discriminator loss: 0.250449, acc.: 85.94%] [Generator loss: 4.503716]\n",
      "6812 [Discriminator loss: 0.397322, acc.: 81.25%] [Generator loss: 7.006340]\n",
      "6813 [Discriminator loss: 0.209195, acc.: 90.62%] [Generator loss: 5.523399]\n",
      "6814 [Discriminator loss: 0.167895, acc.: 93.75%] [Generator loss: 5.355044]\n",
      "6815 [Discriminator loss: 0.101055, acc.: 96.88%] [Generator loss: 4.120320]\n",
      "6816 [Discriminator loss: 0.247117, acc.: 85.94%] [Generator loss: 5.821516]\n",
      "6817 [Discriminator loss: 0.129681, acc.: 93.75%] [Generator loss: 5.675923]\n",
      "6818 [Discriminator loss: 0.410465, acc.: 82.81%] [Generator loss: 5.617496]\n",
      "6819 [Discriminator loss: 0.119008, acc.: 95.31%] [Generator loss: 5.314781]\n",
      "6820 [Discriminator loss: 0.116469, acc.: 93.75%] [Generator loss: 6.488314]\n",
      "6821 [Discriminator loss: 0.074176, acc.: 96.88%] [Generator loss: 4.531625]\n",
      "6822 [Discriminator loss: 0.242674, acc.: 87.50%] [Generator loss: 6.008179]\n",
      "6823 [Discriminator loss: 0.142610, acc.: 93.75%] [Generator loss: 6.394686]\n",
      "6824 [Discriminator loss: 0.833962, acc.: 71.88%] [Generator loss: 4.666166]\n",
      "6825 [Discriminator loss: 0.179932, acc.: 92.19%] [Generator loss: 5.624776]\n",
      "6826 [Discriminator loss: 0.125825, acc.: 95.31%] [Generator loss: 6.369809]\n",
      "6827 [Discriminator loss: 0.239167, acc.: 89.06%] [Generator loss: 3.999520]\n",
      "6828 [Discriminator loss: 0.199595, acc.: 89.06%] [Generator loss: 6.430297]\n",
      "6829 [Discriminator loss: 0.128352, acc.: 95.31%] [Generator loss: 5.187178]\n",
      "6830 [Discriminator loss: 0.369411, acc.: 82.81%] [Generator loss: 5.884102]\n",
      "6831 [Discriminator loss: 0.076443, acc.: 100.00%] [Generator loss: 5.111093]\n",
      "6832 [Discriminator loss: 0.387452, acc.: 84.38%] [Generator loss: 5.381231]\n",
      "6833 [Discriminator loss: 0.223601, acc.: 89.06%] [Generator loss: 5.372725]\n",
      "6834 [Discriminator loss: 0.135419, acc.: 93.75%] [Generator loss: 6.983459]\n",
      "6835 [Discriminator loss: 0.180856, acc.: 92.19%] [Generator loss: 6.030800]\n",
      "6836 [Discriminator loss: 0.182398, acc.: 92.19%] [Generator loss: 5.762117]\n",
      "6837 [Discriminator loss: 0.182795, acc.: 93.75%] [Generator loss: 4.758304]\n",
      "6838 [Discriminator loss: 0.278963, acc.: 89.06%] [Generator loss: 5.638923]\n",
      "6839 [Discriminator loss: 0.106801, acc.: 93.75%] [Generator loss: 5.320818]\n",
      "6840 [Discriminator loss: 0.233233, acc.: 90.62%] [Generator loss: 5.565718]\n",
      "6841 [Discriminator loss: 0.164181, acc.: 93.75%] [Generator loss: 4.718305]\n",
      "6842 [Discriminator loss: 0.353743, acc.: 85.94%] [Generator loss: 4.934881]\n",
      "6843 [Discriminator loss: 0.216479, acc.: 92.19%] [Generator loss: 6.158201]\n",
      "6844 [Discriminator loss: 0.175757, acc.: 92.19%] [Generator loss: 5.025107]\n",
      "6845 [Discriminator loss: 0.214753, acc.: 90.62%] [Generator loss: 4.904303]\n",
      "6846 [Discriminator loss: 0.200521, acc.: 93.75%] [Generator loss: 5.522208]\n",
      "6847 [Discriminator loss: 0.201623, acc.: 93.75%] [Generator loss: 6.555293]\n",
      "6848 [Discriminator loss: 0.128885, acc.: 93.75%] [Generator loss: 5.275317]\n",
      "6849 [Discriminator loss: 0.175700, acc.: 92.19%] [Generator loss: 4.564407]\n",
      "6850 [Discriminator loss: 0.141301, acc.: 95.31%] [Generator loss: 5.151370]\n",
      "6851 [Discriminator loss: 0.241307, acc.: 92.19%] [Generator loss: 4.712643]\n",
      "6852 [Discriminator loss: 0.271159, acc.: 87.50%] [Generator loss: 5.842184]\n",
      "6853 [Discriminator loss: 0.234266, acc.: 89.06%] [Generator loss: 4.651412]\n",
      "6854 [Discriminator loss: 0.246422, acc.: 89.06%] [Generator loss: 6.035079]\n",
      "6855 [Discriminator loss: 0.168901, acc.: 89.06%] [Generator loss: 5.201433]\n",
      "6856 [Discriminator loss: 0.123728, acc.: 93.75%] [Generator loss: 5.338387]\n",
      "6857 [Discriminator loss: 0.146579, acc.: 92.19%] [Generator loss: 4.966396]\n",
      "6858 [Discriminator loss: 0.267577, acc.: 87.50%] [Generator loss: 5.085092]\n",
      "6859 [Discriminator loss: 0.098209, acc.: 100.00%] [Generator loss: 4.510588]\n",
      "6860 [Discriminator loss: 0.213311, acc.: 89.06%] [Generator loss: 4.879435]\n",
      "6861 [Discriminator loss: 0.223533, acc.: 90.62%] [Generator loss: 4.981381]\n",
      "6862 [Discriminator loss: 0.172619, acc.: 92.19%] [Generator loss: 6.086637]\n",
      "6863 [Discriminator loss: 0.247317, acc.: 89.06%] [Generator loss: 3.437083]\n",
      "6864 [Discriminator loss: 0.212452, acc.: 89.06%] [Generator loss: 5.660720]\n",
      "6865 [Discriminator loss: 0.101074, acc.: 96.88%] [Generator loss: 6.611666]\n",
      "6866 [Discriminator loss: 0.318847, acc.: 87.50%] [Generator loss: 4.840566]\n",
      "6867 [Discriminator loss: 0.270940, acc.: 90.62%] [Generator loss: 6.281549]\n",
      "6868 [Discriminator loss: 0.086743, acc.: 98.44%] [Generator loss: 6.863200]\n",
      "6869 [Discriminator loss: 0.206889, acc.: 92.19%] [Generator loss: 4.422901]\n",
      "6870 [Discriminator loss: 0.120032, acc.: 93.75%] [Generator loss: 5.122362]\n",
      "6871 [Discriminator loss: 0.121714, acc.: 93.75%] [Generator loss: 5.135129]\n",
      "6872 [Discriminator loss: 0.199797, acc.: 89.06%] [Generator loss: 4.256501]\n",
      "6873 [Discriminator loss: 0.172295, acc.: 90.62%] [Generator loss: 6.368976]\n",
      "6874 [Discriminator loss: 0.140931, acc.: 90.62%] [Generator loss: 5.397337]\n",
      "6875 [Discriminator loss: 0.100187, acc.: 96.88%] [Generator loss: 5.597560]\n",
      "6876 [Discriminator loss: 0.135507, acc.: 93.75%] [Generator loss: 5.910868]\n",
      "6877 [Discriminator loss: 0.162977, acc.: 93.75%] [Generator loss: 5.746300]\n",
      "6878 [Discriminator loss: 0.125048, acc.: 96.88%] [Generator loss: 5.037440]\n",
      "6879 [Discriminator loss: 0.393398, acc.: 84.38%] [Generator loss: 5.683783]\n",
      "6880 [Discriminator loss: 0.134953, acc.: 95.31%] [Generator loss: 5.885662]\n",
      "6881 [Discriminator loss: 0.218837, acc.: 90.62%] [Generator loss: 3.509153]\n",
      "6882 [Discriminator loss: 0.145446, acc.: 93.75%] [Generator loss: 5.201352]\n",
      "6883 [Discriminator loss: 0.153210, acc.: 92.19%] [Generator loss: 6.077259]\n",
      "6884 [Discriminator loss: 0.240854, acc.: 92.19%] [Generator loss: 6.659081]\n",
      "6885 [Discriminator loss: 0.069973, acc.: 98.44%] [Generator loss: 5.560269]\n",
      "6886 [Discriminator loss: 0.073395, acc.: 96.88%] [Generator loss: 4.750260]\n",
      "6887 [Discriminator loss: 0.209600, acc.: 92.19%] [Generator loss: 5.904080]\n",
      "6888 [Discriminator loss: 0.228080, acc.: 92.19%] [Generator loss: 5.657005]\n",
      "6889 [Discriminator loss: 0.197677, acc.: 93.75%] [Generator loss: 6.093444]\n",
      "6890 [Discriminator loss: 0.250962, acc.: 85.94%] [Generator loss: 4.075520]\n",
      "6891 [Discriminator loss: 0.195490, acc.: 95.31%] [Generator loss: 5.381942]\n",
      "6892 [Discriminator loss: 0.098293, acc.: 98.44%] [Generator loss: 5.282143]\n",
      "6893 [Discriminator loss: 0.310616, acc.: 85.94%] [Generator loss: 5.461129]\n",
      "6894 [Discriminator loss: 0.120317, acc.: 93.75%] [Generator loss: 5.843663]\n",
      "6895 [Discriminator loss: 0.182990, acc.: 90.62%] [Generator loss: 5.274255]\n",
      "6896 [Discriminator loss: 0.410112, acc.: 79.69%] [Generator loss: 4.232144]\n",
      "6897 [Discriminator loss: 0.051348, acc.: 98.44%] [Generator loss: 4.256923]\n",
      "6898 [Discriminator loss: 0.358122, acc.: 89.06%] [Generator loss: 5.587917]\n",
      "6899 [Discriminator loss: 0.402715, acc.: 82.81%] [Generator loss: 6.879598]\n",
      "6900 [Discriminator loss: 0.507210, acc.: 82.81%] [Generator loss: 4.531826]\n",
      "6901 [Discriminator loss: 0.083385, acc.: 96.88%] [Generator loss: 4.943501]\n",
      "6902 [Discriminator loss: 0.227088, acc.: 90.62%] [Generator loss: 6.111438]\n",
      "6903 [Discriminator loss: 0.240367, acc.: 89.06%] [Generator loss: 6.069464]\n",
      "6904 [Discriminator loss: 0.200776, acc.: 92.19%] [Generator loss: 5.287930]\n",
      "6905 [Discriminator loss: 0.073932, acc.: 100.00%] [Generator loss: 6.158974]\n",
      "6906 [Discriminator loss: 0.243241, acc.: 92.19%] [Generator loss: 4.893481]\n",
      "6907 [Discriminator loss: 0.121600, acc.: 95.31%] [Generator loss: 4.164663]\n",
      "6908 [Discriminator loss: 0.208070, acc.: 92.19%] [Generator loss: 5.141066]\n",
      "6909 [Discriminator loss: 0.173317, acc.: 92.19%] [Generator loss: 6.161332]\n",
      "6910 [Discriminator loss: 0.280006, acc.: 84.38%] [Generator loss: 7.089866]\n",
      "6911 [Discriminator loss: 0.336867, acc.: 82.81%] [Generator loss: 5.863724]\n",
      "6912 [Discriminator loss: 0.242636, acc.: 92.19%] [Generator loss: 6.454388]\n",
      "6913 [Discriminator loss: 0.177790, acc.: 92.19%] [Generator loss: 4.717386]\n",
      "6914 [Discriminator loss: 0.103915, acc.: 93.75%] [Generator loss: 4.862571]\n",
      "6915 [Discriminator loss: 0.193893, acc.: 90.62%] [Generator loss: 6.643343]\n",
      "6916 [Discriminator loss: 0.406161, acc.: 82.81%] [Generator loss: 4.557094]\n",
      "6917 [Discriminator loss: 0.168276, acc.: 92.19%] [Generator loss: 5.389136]\n",
      "6918 [Discriminator loss: 0.124367, acc.: 93.75%] [Generator loss: 5.715729]\n",
      "6919 [Discriminator loss: 0.048716, acc.: 98.44%] [Generator loss: 4.204455]\n",
      "6920 [Discriminator loss: 0.213199, acc.: 90.62%] [Generator loss: 5.252425]\n",
      "6921 [Discriminator loss: 0.047446, acc.: 100.00%] [Generator loss: 6.649686]\n",
      "6922 [Discriminator loss: 0.287284, acc.: 85.94%] [Generator loss: 5.630615]\n",
      "6923 [Discriminator loss: 0.145987, acc.: 95.31%] [Generator loss: 5.865516]\n",
      "6924 [Discriminator loss: 0.220844, acc.: 90.62%] [Generator loss: 5.496752]\n",
      "6925 [Discriminator loss: 0.229090, acc.: 93.75%] [Generator loss: 4.734297]\n",
      "6926 [Discriminator loss: 0.161184, acc.: 93.75%] [Generator loss: 4.913441]\n",
      "6927 [Discriminator loss: 0.134893, acc.: 95.31%] [Generator loss: 5.304054]\n",
      "6928 [Discriminator loss: 0.117532, acc.: 93.75%] [Generator loss: 6.070482]\n",
      "6929 [Discriminator loss: 0.048424, acc.: 100.00%] [Generator loss: 6.398911]\n",
      "6930 [Discriminator loss: 0.246636, acc.: 84.38%] [Generator loss: 5.156290]\n",
      "6931 [Discriminator loss: 0.199827, acc.: 90.62%] [Generator loss: 6.227486]\n",
      "6932 [Discriminator loss: 0.142228, acc.: 92.19%] [Generator loss: 5.529706]\n",
      "6933 [Discriminator loss: 0.236806, acc.: 92.19%] [Generator loss: 6.705559]\n",
      "6934 [Discriminator loss: 0.147623, acc.: 93.75%] [Generator loss: 5.120332]\n",
      "6935 [Discriminator loss: 0.253267, acc.: 90.62%] [Generator loss: 6.210109]\n",
      "6936 [Discriminator loss: 0.144151, acc.: 92.19%] [Generator loss: 6.360603]\n",
      "6937 [Discriminator loss: 0.230055, acc.: 89.06%] [Generator loss: 4.502439]\n",
      "6938 [Discriminator loss: 0.085265, acc.: 95.31%] [Generator loss: 5.605023]\n",
      "6939 [Discriminator loss: 0.098275, acc.: 95.31%] [Generator loss: 5.732057]\n",
      "6940 [Discriminator loss: 0.218976, acc.: 93.75%] [Generator loss: 4.468945]\n",
      "6941 [Discriminator loss: 0.236224, acc.: 92.19%] [Generator loss: 5.343544]\n",
      "6942 [Discriminator loss: 0.123138, acc.: 93.75%] [Generator loss: 5.102406]\n",
      "6943 [Discriminator loss: 0.151615, acc.: 95.31%] [Generator loss: 5.086400]\n",
      "6944 [Discriminator loss: 0.086975, acc.: 98.44%] [Generator loss: 4.991482]\n",
      "6945 [Discriminator loss: 0.176156, acc.: 90.62%] [Generator loss: 5.373610]\n",
      "6946 [Discriminator loss: 0.159552, acc.: 92.19%] [Generator loss: 6.347629]\n",
      "6947 [Discriminator loss: 0.146083, acc.: 93.75%] [Generator loss: 5.407647]\n",
      "6948 [Discriminator loss: 0.128090, acc.: 95.31%] [Generator loss: 6.083663]\n",
      "6949 [Discriminator loss: 0.269329, acc.: 89.06%] [Generator loss: 4.457593]\n",
      "6950 [Discriminator loss: 0.182047, acc.: 92.19%] [Generator loss: 5.996718]\n",
      "6951 [Discriminator loss: 0.199449, acc.: 90.62%] [Generator loss: 5.495006]\n",
      "6952 [Discriminator loss: 0.190143, acc.: 93.75%] [Generator loss: 4.743576]\n",
      "6953 [Discriminator loss: 0.086388, acc.: 93.75%] [Generator loss: 6.098608]\n",
      "6954 [Discriminator loss: 0.167159, acc.: 90.62%] [Generator loss: 6.289572]\n",
      "6955 [Discriminator loss: 0.121762, acc.: 93.75%] [Generator loss: 5.373463]\n",
      "6956 [Discriminator loss: 0.328828, acc.: 84.38%] [Generator loss: 5.801587]\n",
      "6957 [Discriminator loss: 0.100070, acc.: 96.88%] [Generator loss: 6.504197]\n",
      "6958 [Discriminator loss: 0.294671, acc.: 89.06%] [Generator loss: 4.965380]\n",
      "6959 [Discriminator loss: 0.067163, acc.: 98.44%] [Generator loss: 5.414201]\n",
      "6960 [Discriminator loss: 0.154929, acc.: 95.31%] [Generator loss: 5.628940]\n",
      "6961 [Discriminator loss: 0.162528, acc.: 92.19%] [Generator loss: 4.852416]\n",
      "6962 [Discriminator loss: 0.116348, acc.: 93.75%] [Generator loss: 6.024445]\n",
      "6963 [Discriminator loss: 0.100756, acc.: 95.31%] [Generator loss: 5.290459]\n",
      "6964 [Discriminator loss: 0.323364, acc.: 89.06%] [Generator loss: 6.495298]\n",
      "6965 [Discriminator loss: 0.267458, acc.: 89.06%] [Generator loss: 5.388190]\n",
      "6966 [Discriminator loss: 0.310349, acc.: 84.38%] [Generator loss: 6.823045]\n",
      "6967 [Discriminator loss: 0.204318, acc.: 92.19%] [Generator loss: 5.453415]\n",
      "6968 [Discriminator loss: 0.099713, acc.: 96.88%] [Generator loss: 4.876309]\n",
      "6969 [Discriminator loss: 0.189207, acc.: 90.62%] [Generator loss: 5.472456]\n",
      "6970 [Discriminator loss: 0.287523, acc.: 87.50%] [Generator loss: 5.792813]\n",
      "6971 [Discriminator loss: 0.271110, acc.: 89.06%] [Generator loss: 5.875847]\n",
      "6972 [Discriminator loss: 0.092761, acc.: 93.75%] [Generator loss: 5.578511]\n",
      "6973 [Discriminator loss: 0.195937, acc.: 89.06%] [Generator loss: 6.945836]\n",
      "6974 [Discriminator loss: 0.158265, acc.: 92.19%] [Generator loss: 5.734098]\n",
      "6975 [Discriminator loss: 0.142549, acc.: 96.88%] [Generator loss: 4.456825]\n",
      "6976 [Discriminator loss: 0.085370, acc.: 95.31%] [Generator loss: 4.689128]\n",
      "6977 [Discriminator loss: 0.105306, acc.: 96.88%] [Generator loss: 5.351324]\n",
      "6978 [Discriminator loss: 0.234993, acc.: 87.50%] [Generator loss: 4.083712]\n",
      "6979 [Discriminator loss: 0.262595, acc.: 93.75%] [Generator loss: 4.814612]\n",
      "6980 [Discriminator loss: 0.149607, acc.: 95.31%] [Generator loss: 5.809456]\n",
      "6981 [Discriminator loss: 0.101539, acc.: 96.88%] [Generator loss: 5.409367]\n",
      "6982 [Discriminator loss: 0.173467, acc.: 92.19%] [Generator loss: 5.845254]\n",
      "6983 [Discriminator loss: 0.229160, acc.: 89.06%] [Generator loss: 4.916861]\n",
      "6984 [Discriminator loss: 0.124382, acc.: 96.88%] [Generator loss: 6.613580]\n",
      "6985 [Discriminator loss: 0.298962, acc.: 90.62%] [Generator loss: 3.709374]\n",
      "6986 [Discriminator loss: 0.303780, acc.: 87.50%] [Generator loss: 6.491678]\n",
      "6987 [Discriminator loss: 0.297053, acc.: 85.94%] [Generator loss: 5.530386]\n",
      "6988 [Discriminator loss: 0.113350, acc.: 93.75%] [Generator loss: 5.264676]\n",
      "6989 [Discriminator loss: 0.253155, acc.: 92.19%] [Generator loss: 6.109878]\n",
      "6990 [Discriminator loss: 0.287940, acc.: 89.06%] [Generator loss: 5.213510]\n",
      "6991 [Discriminator loss: 0.354095, acc.: 84.38%] [Generator loss: 6.596091]\n",
      "6992 [Discriminator loss: 0.086842, acc.: 98.44%] [Generator loss: 7.451207]\n",
      "6993 [Discriminator loss: 0.368057, acc.: 90.62%] [Generator loss: 5.165841]\n",
      "6994 [Discriminator loss: 0.200676, acc.: 90.62%] [Generator loss: 6.185700]\n",
      "6995 [Discriminator loss: 0.103042, acc.: 95.31%] [Generator loss: 5.833280]\n",
      "6996 [Discriminator loss: 0.134841, acc.: 92.19%] [Generator loss: 5.734567]\n",
      "6997 [Discriminator loss: 0.239308, acc.: 89.06%] [Generator loss: 5.081049]\n",
      "6998 [Discriminator loss: 0.163815, acc.: 92.19%] [Generator loss: 5.907525]\n",
      "6999 [Discriminator loss: 0.100875, acc.: 96.88%] [Generator loss: 5.143428]\n",
      "7000 [Discriminator loss: 0.240915, acc.: 87.50%] [Generator loss: 4.291819]\n",
      "7001 [Discriminator loss: 0.197649, acc.: 96.88%] [Generator loss: 5.518402]\n",
      "7002 [Discriminator loss: 0.246415, acc.: 90.62%] [Generator loss: 5.059264]\n",
      "7003 [Discriminator loss: 0.263275, acc.: 89.06%] [Generator loss: 6.380184]\n",
      "7004 [Discriminator loss: 0.114031, acc.: 95.31%] [Generator loss: 5.485850]\n",
      "7005 [Discriminator loss: 0.188444, acc.: 92.19%] [Generator loss: 4.874752]\n",
      "7006 [Discriminator loss: 0.139136, acc.: 95.31%] [Generator loss: 4.519798]\n",
      "7007 [Discriminator loss: 0.118569, acc.: 96.88%] [Generator loss: 4.749910]\n",
      "7008 [Discriminator loss: 0.205588, acc.: 93.75%] [Generator loss: 4.439447]\n",
      "7009 [Discriminator loss: 0.148274, acc.: 95.31%] [Generator loss: 4.926831]\n",
      "7010 [Discriminator loss: 0.222052, acc.: 89.06%] [Generator loss: 4.823621]\n",
      "7011 [Discriminator loss: 0.091682, acc.: 98.44%] [Generator loss: 5.790170]\n",
      "7012 [Discriminator loss: 0.250821, acc.: 87.50%] [Generator loss: 5.619534]\n",
      "7013 [Discriminator loss: 0.152404, acc.: 92.19%] [Generator loss: 5.567038]\n",
      "7014 [Discriminator loss: 0.179568, acc.: 92.19%] [Generator loss: 4.892853]\n",
      "7015 [Discriminator loss: 0.237623, acc.: 90.62%] [Generator loss: 5.166255]\n",
      "7016 [Discriminator loss: 0.102626, acc.: 96.88%] [Generator loss: 5.859723]\n",
      "7017 [Discriminator loss: 0.139586, acc.: 92.19%] [Generator loss: 5.516252]\n",
      "7018 [Discriminator loss: 0.174591, acc.: 93.75%] [Generator loss: 6.468951]\n",
      "7019 [Discriminator loss: 0.215282, acc.: 92.19%] [Generator loss: 5.961491]\n",
      "7020 [Discriminator loss: 0.155595, acc.: 93.75%] [Generator loss: 5.368453]\n",
      "7021 [Discriminator loss: 0.136542, acc.: 95.31%] [Generator loss: 5.356867]\n",
      "7022 [Discriminator loss: 0.186255, acc.: 95.31%] [Generator loss: 4.983119]\n",
      "7023 [Discriminator loss: 0.110489, acc.: 96.88%] [Generator loss: 4.727707]\n",
      "7024 [Discriminator loss: 0.140662, acc.: 93.75%] [Generator loss: 6.159331]\n",
      "7025 [Discriminator loss: 0.268515, acc.: 89.06%] [Generator loss: 4.825376]\n",
      "7026 [Discriminator loss: 0.229136, acc.: 89.06%] [Generator loss: 6.073726]\n",
      "7027 [Discriminator loss: 0.066288, acc.: 98.44%] [Generator loss: 7.161975]\n",
      "7028 [Discriminator loss: 0.155171, acc.: 93.75%] [Generator loss: 5.211082]\n",
      "7029 [Discriminator loss: 0.265561, acc.: 89.06%] [Generator loss: 5.735697]\n",
      "7030 [Discriminator loss: 0.050090, acc.: 98.44%] [Generator loss: 6.136769]\n",
      "7031 [Discriminator loss: 0.193193, acc.: 90.62%] [Generator loss: 5.501577]\n",
      "7032 [Discriminator loss: 0.148969, acc.: 95.31%] [Generator loss: 5.951327]\n",
      "7033 [Discriminator loss: 0.150004, acc.: 93.75%] [Generator loss: 4.656488]\n",
      "7034 [Discriminator loss: 0.137671, acc.: 93.75%] [Generator loss: 6.167197]\n",
      "7035 [Discriminator loss: 0.210474, acc.: 92.19%] [Generator loss: 4.720892]\n",
      "7036 [Discriminator loss: 0.217918, acc.: 93.75%] [Generator loss: 5.661085]\n",
      "7037 [Discriminator loss: 0.174412, acc.: 90.62%] [Generator loss: 6.318882]\n",
      "7038 [Discriminator loss: 0.097028, acc.: 96.88%] [Generator loss: 6.008232]\n",
      "7039 [Discriminator loss: 0.178203, acc.: 92.19%] [Generator loss: 4.680965]\n",
      "7040 [Discriminator loss: 0.260459, acc.: 87.50%] [Generator loss: 5.925299]\n",
      "7041 [Discriminator loss: 0.106617, acc.: 95.31%] [Generator loss: 5.555681]\n",
      "7042 [Discriminator loss: 0.200461, acc.: 85.94%] [Generator loss: 5.920372]\n",
      "7043 [Discriminator loss: 0.200106, acc.: 92.19%] [Generator loss: 3.961015]\n",
      "7044 [Discriminator loss: 0.258575, acc.: 84.38%] [Generator loss: 5.390653]\n",
      "7045 [Discriminator loss: 0.133283, acc.: 95.31%] [Generator loss: 4.578259]\n",
      "7046 [Discriminator loss: 0.176370, acc.: 90.62%] [Generator loss: 4.445716]\n",
      "7047 [Discriminator loss: 0.040618, acc.: 100.00%] [Generator loss: 5.059984]\n",
      "7048 [Discriminator loss: 0.240821, acc.: 87.50%] [Generator loss: 5.419960]\n",
      "7049 [Discriminator loss: 0.199292, acc.: 90.62%] [Generator loss: 4.951006]\n",
      "7050 [Discriminator loss: 0.210088, acc.: 93.75%] [Generator loss: 5.438529]\n",
      "7051 [Discriminator loss: 0.203658, acc.: 90.62%] [Generator loss: 5.530694]\n",
      "7052 [Discriminator loss: 0.829836, acc.: 67.19%] [Generator loss: 5.823533]\n",
      "7053 [Discriminator loss: 0.085979, acc.: 96.88%] [Generator loss: 6.408665]\n",
      "7054 [Discriminator loss: 0.166991, acc.: 93.75%] [Generator loss: 4.532598]\n",
      "7055 [Discriminator loss: 0.124209, acc.: 96.88%] [Generator loss: 4.584206]\n",
      "7056 [Discriminator loss: 0.107093, acc.: 96.88%] [Generator loss: 5.309859]\n",
      "7057 [Discriminator loss: 0.073817, acc.: 98.44%] [Generator loss: 5.130793]\n",
      "7058 [Discriminator loss: 0.137529, acc.: 95.31%] [Generator loss: 4.967615]\n",
      "7059 [Discriminator loss: 0.100108, acc.: 100.00%] [Generator loss: 4.561030]\n",
      "7060 [Discriminator loss: 0.151350, acc.: 93.75%] [Generator loss: 6.460861]\n",
      "7061 [Discriminator loss: 0.191909, acc.: 90.62%] [Generator loss: 6.377454]\n",
      "7062 [Discriminator loss: 0.274680, acc.: 89.06%] [Generator loss: 4.765997]\n",
      "7063 [Discriminator loss: 0.185036, acc.: 90.62%] [Generator loss: 7.347385]\n",
      "7064 [Discriminator loss: 0.170648, acc.: 93.75%] [Generator loss: 5.662786]\n",
      "7065 [Discriminator loss: 0.079606, acc.: 96.88%] [Generator loss: 6.931220]\n",
      "7066 [Discriminator loss: 0.293468, acc.: 89.06%] [Generator loss: 5.430464]\n",
      "7067 [Discriminator loss: 0.102805, acc.: 93.75%] [Generator loss: 4.687441]\n",
      "7068 [Discriminator loss: 0.333890, acc.: 81.25%] [Generator loss: 6.070402]\n",
      "7069 [Discriminator loss: 0.189597, acc.: 92.19%] [Generator loss: 5.914341]\n",
      "7070 [Discriminator loss: 0.219584, acc.: 90.62%] [Generator loss: 5.270925]\n",
      "7071 [Discriminator loss: 0.097128, acc.: 96.88%] [Generator loss: 5.156308]\n",
      "7072 [Discriminator loss: 0.091913, acc.: 96.88%] [Generator loss: 6.090052]\n",
      "7073 [Discriminator loss: 0.128325, acc.: 93.75%] [Generator loss: 5.101062]\n",
      "7074 [Discriminator loss: 0.351333, acc.: 85.94%] [Generator loss: 5.072974]\n",
      "7075 [Discriminator loss: 0.123282, acc.: 93.75%] [Generator loss: 5.512370]\n",
      "7076 [Discriminator loss: 0.254161, acc.: 89.06%] [Generator loss: 4.933429]\n",
      "7077 [Discriminator loss: 0.200128, acc.: 93.75%] [Generator loss: 7.772110]\n",
      "7078 [Discriminator loss: 0.388257, acc.: 82.81%] [Generator loss: 5.126684]\n",
      "7079 [Discriminator loss: 0.201877, acc.: 90.62%] [Generator loss: 4.462297]\n",
      "7080 [Discriminator loss: 0.110262, acc.: 92.19%] [Generator loss: 5.676697]\n",
      "7081 [Discriminator loss: 0.269855, acc.: 87.50%] [Generator loss: 5.542368]\n",
      "7082 [Discriminator loss: 0.184201, acc.: 90.62%] [Generator loss: 4.680087]\n",
      "7083 [Discriminator loss: 0.124191, acc.: 96.88%] [Generator loss: 4.742334]\n",
      "7084 [Discriminator loss: 0.155622, acc.: 93.75%] [Generator loss: 4.637872]\n",
      "7085 [Discriminator loss: 0.075086, acc.: 98.44%] [Generator loss: 5.306420]\n",
      "7086 [Discriminator loss: 0.134182, acc.: 95.31%] [Generator loss: 5.149338]\n",
      "7087 [Discriminator loss: 0.130698, acc.: 96.88%] [Generator loss: 4.381390]\n",
      "7088 [Discriminator loss: 0.123572, acc.: 95.31%] [Generator loss: 5.279323]\n",
      "7089 [Discriminator loss: 0.154614, acc.: 95.31%] [Generator loss: 5.466402]\n",
      "7090 [Discriminator loss: 0.140113, acc.: 96.88%] [Generator loss: 6.578910]\n",
      "7091 [Discriminator loss: 0.147823, acc.: 96.88%] [Generator loss: 6.258083]\n",
      "7092 [Discriminator loss: 0.359284, acc.: 87.50%] [Generator loss: 4.864588]\n",
      "7093 [Discriminator loss: 0.137000, acc.: 93.75%] [Generator loss: 4.504099]\n",
      "7094 [Discriminator loss: 0.092238, acc.: 96.88%] [Generator loss: 4.806613]\n",
      "7095 [Discriminator loss: 0.195417, acc.: 90.62%] [Generator loss: 5.318445]\n",
      "7096 [Discriminator loss: 0.092249, acc.: 95.31%] [Generator loss: 4.946392]\n",
      "7097 [Discriminator loss: 0.202468, acc.: 90.62%] [Generator loss: 6.157815]\n",
      "7098 [Discriminator loss: 0.090489, acc.: 96.88%] [Generator loss: 6.023989]\n",
      "7099 [Discriminator loss: 0.405863, acc.: 79.69%] [Generator loss: 3.487125]\n",
      "7100 [Discriminator loss: 0.226005, acc.: 93.75%] [Generator loss: 6.441548]\n",
      "7101 [Discriminator loss: 0.096338, acc.: 98.44%] [Generator loss: 6.829043]\n",
      "7102 [Discriminator loss: 0.210104, acc.: 89.06%] [Generator loss: 5.214004]\n",
      "7103 [Discriminator loss: 0.363628, acc.: 85.94%] [Generator loss: 5.853543]\n",
      "7104 [Discriminator loss: 0.091199, acc.: 95.31%] [Generator loss: 6.355986]\n",
      "7105 [Discriminator loss: 0.146646, acc.: 92.19%] [Generator loss: 5.069805]\n",
      "7106 [Discriminator loss: 0.182275, acc.: 93.75%] [Generator loss: 5.627304]\n",
      "7107 [Discriminator loss: 0.451666, acc.: 76.56%] [Generator loss: 6.325365]\n",
      "7108 [Discriminator loss: 0.159905, acc.: 95.31%] [Generator loss: 6.128576]\n",
      "7109 [Discriminator loss: 0.201864, acc.: 85.94%] [Generator loss: 6.287765]\n",
      "7110 [Discriminator loss: 0.173321, acc.: 95.31%] [Generator loss: 6.147699]\n",
      "7111 [Discriminator loss: 0.156775, acc.: 92.19%] [Generator loss: 5.575066]\n",
      "7112 [Discriminator loss: 0.345278, acc.: 82.81%] [Generator loss: 5.279636]\n",
      "7113 [Discriminator loss: 0.424461, acc.: 82.81%] [Generator loss: 6.734850]\n",
      "7114 [Discriminator loss: 0.254405, acc.: 87.50%] [Generator loss: 6.856759]\n",
      "7115 [Discriminator loss: 0.251278, acc.: 87.50%] [Generator loss: 5.986960]\n",
      "7116 [Discriminator loss: 0.064589, acc.: 98.44%] [Generator loss: 5.965820]\n",
      "7117 [Discriminator loss: 0.235608, acc.: 92.19%] [Generator loss: 6.225497]\n",
      "7118 [Discriminator loss: 0.120010, acc.: 93.75%] [Generator loss: 6.665718]\n",
      "7119 [Discriminator loss: 0.267730, acc.: 85.94%] [Generator loss: 4.319871]\n",
      "7120 [Discriminator loss: 0.154975, acc.: 90.62%] [Generator loss: 5.151610]\n",
      "7121 [Discriminator loss: 0.529186, acc.: 81.25%] [Generator loss: 5.306937]\n",
      "7122 [Discriminator loss: 0.054714, acc.: 98.44%] [Generator loss: 5.451508]\n",
      "7123 [Discriminator loss: 0.427533, acc.: 87.50%] [Generator loss: 7.102496]\n",
      "7124 [Discriminator loss: 0.353897, acc.: 84.38%] [Generator loss: 6.668041]\n",
      "7125 [Discriminator loss: 0.105893, acc.: 95.31%] [Generator loss: 5.289050]\n",
      "7126 [Discriminator loss: 0.284718, acc.: 89.06%] [Generator loss: 7.256434]\n",
      "7127 [Discriminator loss: 0.185347, acc.: 90.62%] [Generator loss: 6.494248]\n",
      "7128 [Discriminator loss: 0.259933, acc.: 89.06%] [Generator loss: 5.119278]\n",
      "7129 [Discriminator loss: 0.103154, acc.: 96.88%] [Generator loss: 3.775119]\n",
      "7130 [Discriminator loss: 0.231102, acc.: 92.19%] [Generator loss: 4.985705]\n",
      "7131 [Discriminator loss: 0.142424, acc.: 93.75%] [Generator loss: 5.908829]\n",
      "7132 [Discriminator loss: 0.173436, acc.: 93.75%] [Generator loss: 4.192947]\n",
      "7133 [Discriminator loss: 0.331296, acc.: 84.38%] [Generator loss: 4.833128]\n",
      "7134 [Discriminator loss: 0.146631, acc.: 93.75%] [Generator loss: 7.380841]\n",
      "7135 [Discriminator loss: 0.130748, acc.: 95.31%] [Generator loss: 5.914460]\n",
      "7136 [Discriminator loss: 0.270495, acc.: 90.62%] [Generator loss: 4.232609]\n",
      "7137 [Discriminator loss: 0.184180, acc.: 93.75%] [Generator loss: 5.966736]\n",
      "7138 [Discriminator loss: 0.172418, acc.: 92.19%] [Generator loss: 5.169329]\n",
      "7139 [Discriminator loss: 0.697955, acc.: 67.19%] [Generator loss: 7.199884]\n",
      "7140 [Discriminator loss: 0.154869, acc.: 92.19%] [Generator loss: 6.923828]\n",
      "7141 [Discriminator loss: 0.182457, acc.: 89.06%] [Generator loss: 4.460978]\n",
      "7142 [Discriminator loss: 0.161404, acc.: 92.19%] [Generator loss: 4.806436]\n",
      "7143 [Discriminator loss: 0.079911, acc.: 98.44%] [Generator loss: 4.815243]\n",
      "7144 [Discriminator loss: 0.178297, acc.: 92.19%] [Generator loss: 5.201324]\n",
      "7145 [Discriminator loss: 0.123052, acc.: 95.31%] [Generator loss: 5.433305]\n",
      "7146 [Discriminator loss: 0.224618, acc.: 89.06%] [Generator loss: 5.355494]\n",
      "7147 [Discriminator loss: 0.169907, acc.: 92.19%] [Generator loss: 5.916177]\n",
      "7148 [Discriminator loss: 0.292399, acc.: 92.19%] [Generator loss: 5.222987]\n",
      "7149 [Discriminator loss: 0.188975, acc.: 92.19%] [Generator loss: 5.591751]\n",
      "7150 [Discriminator loss: 0.100370, acc.: 95.31%] [Generator loss: 6.189401]\n",
      "7151 [Discriminator loss: 0.293699, acc.: 89.06%] [Generator loss: 5.836386]\n",
      "7152 [Discriminator loss: 0.143518, acc.: 92.19%] [Generator loss: 4.384130]\n",
      "7153 [Discriminator loss: 0.120785, acc.: 96.88%] [Generator loss: 6.230230]\n",
      "7154 [Discriminator loss: 0.085403, acc.: 96.88%] [Generator loss: 6.719551]\n",
      "7155 [Discriminator loss: 0.183232, acc.: 90.62%] [Generator loss: 5.377853]\n",
      "7156 [Discriminator loss: 0.330418, acc.: 87.50%] [Generator loss: 5.876420]\n",
      "7157 [Discriminator loss: 0.226965, acc.: 93.75%] [Generator loss: 5.221349]\n",
      "7158 [Discriminator loss: 0.118819, acc.: 93.75%] [Generator loss: 5.143159]\n",
      "7159 [Discriminator loss: 0.292251, acc.: 82.81%] [Generator loss: 7.291687]\n",
      "7160 [Discriminator loss: 0.085043, acc.: 98.44%] [Generator loss: 8.857308]\n",
      "7161 [Discriminator loss: 0.256587, acc.: 82.81%] [Generator loss: 4.030848]\n",
      "7162 [Discriminator loss: 0.355481, acc.: 84.38%] [Generator loss: 6.094625]\n",
      "7163 [Discriminator loss: 0.077718, acc.: 95.31%] [Generator loss: 6.107288]\n",
      "7164 [Discriminator loss: 0.419614, acc.: 85.94%] [Generator loss: 4.760936]\n",
      "7165 [Discriminator loss: 0.081883, acc.: 98.44%] [Generator loss: 5.799583]\n",
      "7166 [Discriminator loss: 0.194371, acc.: 90.62%] [Generator loss: 4.433649]\n",
      "7167 [Discriminator loss: 0.113151, acc.: 95.31%] [Generator loss: 5.264661]\n",
      "7168 [Discriminator loss: 0.078824, acc.: 96.88%] [Generator loss: 4.436002]\n",
      "7169 [Discriminator loss: 0.118534, acc.: 95.31%] [Generator loss: 5.552066]\n",
      "7170 [Discriminator loss: 0.319448, acc.: 87.50%] [Generator loss: 5.500679]\n",
      "7171 [Discriminator loss: 0.141361, acc.: 95.31%] [Generator loss: 5.857895]\n",
      "7172 [Discriminator loss: 0.129895, acc.: 95.31%] [Generator loss: 5.948776]\n",
      "7173 [Discriminator loss: 0.186982, acc.: 95.31%] [Generator loss: 5.427761]\n",
      "7174 [Discriminator loss: 0.210530, acc.: 92.19%] [Generator loss: 4.088544]\n",
      "7175 [Discriminator loss: 0.189124, acc.: 95.31%] [Generator loss: 5.032541]\n",
      "7176 [Discriminator loss: 0.237872, acc.: 89.06%] [Generator loss: 5.229976]\n",
      "7177 [Discriminator loss: 0.196996, acc.: 92.19%] [Generator loss: 6.493763]\n",
      "7178 [Discriminator loss: 0.138982, acc.: 95.31%] [Generator loss: 5.881366]\n",
      "7179 [Discriminator loss: 0.185541, acc.: 93.75%] [Generator loss: 6.322660]\n",
      "7180 [Discriminator loss: 0.150423, acc.: 93.75%] [Generator loss: 5.606266]\n",
      "7181 [Discriminator loss: 0.195505, acc.: 92.19%] [Generator loss: 5.742782]\n",
      "7182 [Discriminator loss: 0.229808, acc.: 92.19%] [Generator loss: 6.456186]\n",
      "7183 [Discriminator loss: 0.139016, acc.: 93.75%] [Generator loss: 5.702369]\n",
      "7184 [Discriminator loss: 0.178370, acc.: 92.19%] [Generator loss: 4.064033]\n",
      "7185 [Discriminator loss: 0.233421, acc.: 89.06%] [Generator loss: 5.822089]\n",
      "7186 [Discriminator loss: 0.131087, acc.: 92.19%] [Generator loss: 6.103773]\n",
      "7187 [Discriminator loss: 0.318192, acc.: 84.38%] [Generator loss: 4.882946]\n",
      "7188 [Discriminator loss: 0.146747, acc.: 96.88%] [Generator loss: 5.113433]\n",
      "7189 [Discriminator loss: 0.186415, acc.: 89.06%] [Generator loss: 4.261133]\n",
      "7190 [Discriminator loss: 0.215796, acc.: 89.06%] [Generator loss: 5.389667]\n",
      "7191 [Discriminator loss: 0.183681, acc.: 92.19%] [Generator loss: 5.330926]\n",
      "7192 [Discriminator loss: 0.170828, acc.: 93.75%] [Generator loss: 5.866843]\n",
      "7193 [Discriminator loss: 0.238565, acc.: 87.50%] [Generator loss: 5.871866]\n",
      "7194 [Discriminator loss: 0.084551, acc.: 95.31%] [Generator loss: 5.607894]\n",
      "7195 [Discriminator loss: 0.170796, acc.: 90.62%] [Generator loss: 5.683970]\n",
      "7196 [Discriminator loss: 0.081314, acc.: 95.31%] [Generator loss: 5.487667]\n",
      "7197 [Discriminator loss: 0.139692, acc.: 95.31%] [Generator loss: 6.322237]\n",
      "7198 [Discriminator loss: 0.136096, acc.: 92.19%] [Generator loss: 5.930779]\n",
      "7199 [Discriminator loss: 0.173550, acc.: 92.19%] [Generator loss: 4.782231]\n",
      "7200 [Discriminator loss: 0.166580, acc.: 92.19%] [Generator loss: 4.532937]\n",
      "7201 [Discriminator loss: 0.218184, acc.: 92.19%] [Generator loss: 5.729505]\n",
      "7202 [Discriminator loss: 0.257449, acc.: 92.19%] [Generator loss: 4.762272]\n",
      "7203 [Discriminator loss: 0.133117, acc.: 92.19%] [Generator loss: 6.292572]\n",
      "7204 [Discriminator loss: 0.110341, acc.: 96.88%] [Generator loss: 6.414745]\n",
      "7205 [Discriminator loss: 0.187625, acc.: 90.62%] [Generator loss: 3.500491]\n",
      "7206 [Discriminator loss: 0.161883, acc.: 90.62%] [Generator loss: 6.649825]\n",
      "7207 [Discriminator loss: 0.121444, acc.: 98.44%] [Generator loss: 5.858587]\n",
      "7208 [Discriminator loss: 0.163058, acc.: 89.06%] [Generator loss: 5.677753]\n",
      "7209 [Discriminator loss: 0.103090, acc.: 96.88%] [Generator loss: 5.556359]\n",
      "7210 [Discriminator loss: 0.120468, acc.: 95.31%] [Generator loss: 6.270318]\n",
      "7211 [Discriminator loss: 0.067370, acc.: 96.88%] [Generator loss: 5.331800]\n",
      "7212 [Discriminator loss: 0.208541, acc.: 92.19%] [Generator loss: 5.587055]\n",
      "7213 [Discriminator loss: 0.173154, acc.: 90.62%] [Generator loss: 4.760866]\n",
      "7214 [Discriminator loss: 0.043631, acc.: 100.00%] [Generator loss: 5.827965]\n",
      "7215 [Discriminator loss: 0.296534, acc.: 89.06%] [Generator loss: 4.861325]\n",
      "7216 [Discriminator loss: 0.102516, acc.: 93.75%] [Generator loss: 5.571987]\n",
      "7217 [Discriminator loss: 0.144958, acc.: 93.75%] [Generator loss: 5.547200]\n",
      "7218 [Discriminator loss: 0.070844, acc.: 98.44%] [Generator loss: 3.931663]\n",
      "7219 [Discriminator loss: 0.353046, acc.: 89.06%] [Generator loss: 5.628849]\n",
      "7220 [Discriminator loss: 0.165016, acc.: 95.31%] [Generator loss: 6.173048]\n",
      "7221 [Discriminator loss: 0.204054, acc.: 89.06%] [Generator loss: 6.003970]\n",
      "7222 [Discriminator loss: 0.229245, acc.: 90.62%] [Generator loss: 7.086449]\n",
      "7223 [Discriminator loss: 0.102016, acc.: 96.88%] [Generator loss: 6.197084]\n",
      "7224 [Discriminator loss: 0.479694, acc.: 79.69%] [Generator loss: 5.311779]\n",
      "7225 [Discriminator loss: 0.044980, acc.: 100.00%] [Generator loss: 6.212461]\n",
      "7226 [Discriminator loss: 0.158253, acc.: 93.75%] [Generator loss: 7.122764]\n",
      "7227 [Discriminator loss: 0.150258, acc.: 93.75%] [Generator loss: 6.134938]\n",
      "7228 [Discriminator loss: 0.176344, acc.: 93.75%] [Generator loss: 5.810289]\n",
      "7229 [Discriminator loss: 0.141813, acc.: 93.75%] [Generator loss: 5.269279]\n",
      "7230 [Discriminator loss: 0.318127, acc.: 89.06%] [Generator loss: 5.193404]\n",
      "7231 [Discriminator loss: 0.281717, acc.: 89.06%] [Generator loss: 6.712590]\n",
      "7232 [Discriminator loss: 0.136166, acc.: 95.31%] [Generator loss: 5.474977]\n",
      "7233 [Discriminator loss: 0.139496, acc.: 95.31%] [Generator loss: 5.862032]\n",
      "7234 [Discriminator loss: 0.178957, acc.: 92.19%] [Generator loss: 4.984282]\n",
      "7235 [Discriminator loss: 0.173895, acc.: 90.62%] [Generator loss: 4.407001]\n",
      "7236 [Discriminator loss: 0.072647, acc.: 98.44%] [Generator loss: 5.534898]\n",
      "7237 [Discriminator loss: 0.314189, acc.: 87.50%] [Generator loss: 5.457137]\n",
      "7238 [Discriminator loss: 0.182363, acc.: 93.75%] [Generator loss: 6.262907]\n",
      "7239 [Discriminator loss: 0.099119, acc.: 96.88%] [Generator loss: 6.188835]\n",
      "7240 [Discriminator loss: 0.204567, acc.: 93.75%] [Generator loss: 5.128376]\n",
      "7241 [Discriminator loss: 0.163305, acc.: 92.19%] [Generator loss: 6.204949]\n",
      "7242 [Discriminator loss: 0.413423, acc.: 84.38%] [Generator loss: 7.254769]\n",
      "7243 [Discriminator loss: 0.203959, acc.: 90.62%] [Generator loss: 6.254852]\n",
      "7244 [Discriminator loss: 0.201650, acc.: 90.62%] [Generator loss: 4.794508]\n",
      "7245 [Discriminator loss: 0.113964, acc.: 93.75%] [Generator loss: 5.763479]\n",
      "7246 [Discriminator loss: 0.046403, acc.: 100.00%] [Generator loss: 6.031275]\n",
      "7247 [Discriminator loss: 0.287593, acc.: 89.06%] [Generator loss: 5.341863]\n",
      "7248 [Discriminator loss: 0.137230, acc.: 96.88%] [Generator loss: 5.744956]\n",
      "7249 [Discriminator loss: 0.078583, acc.: 98.44%] [Generator loss: 4.689650]\n",
      "7250 [Discriminator loss: 0.114996, acc.: 95.31%] [Generator loss: 5.867099]\n",
      "7251 [Discriminator loss: 0.259314, acc.: 90.62%] [Generator loss: 6.132197]\n",
      "7252 [Discriminator loss: 0.215790, acc.: 92.19%] [Generator loss: 6.002491]\n",
      "7253 [Discriminator loss: 0.208398, acc.: 87.50%] [Generator loss: 5.732944]\n",
      "7254 [Discriminator loss: 0.106228, acc.: 98.44%] [Generator loss: 6.237995]\n",
      "7255 [Discriminator loss: 0.224071, acc.: 90.62%] [Generator loss: 5.531299]\n",
      "7256 [Discriminator loss: 0.131638, acc.: 96.88%] [Generator loss: 5.780671]\n",
      "7257 [Discriminator loss: 0.125982, acc.: 95.31%] [Generator loss: 5.767588]\n",
      "7258 [Discriminator loss: 0.092019, acc.: 96.88%] [Generator loss: 6.253954]\n",
      "7259 [Discriminator loss: 0.162310, acc.: 92.19%] [Generator loss: 6.273650]\n",
      "7260 [Discriminator loss: 0.228042, acc.: 87.50%] [Generator loss: 5.031733]\n",
      "7261 [Discriminator loss: 0.095695, acc.: 95.31%] [Generator loss: 5.101008]\n",
      "7262 [Discriminator loss: 0.156642, acc.: 92.19%] [Generator loss: 4.383614]\n",
      "7263 [Discriminator loss: 0.185066, acc.: 92.19%] [Generator loss: 5.475298]\n",
      "7264 [Discriminator loss: 0.085628, acc.: 96.88%] [Generator loss: 6.291022]\n",
      "7265 [Discriminator loss: 0.197568, acc.: 93.75%] [Generator loss: 5.139105]\n",
      "7266 [Discriminator loss: 0.212070, acc.: 92.19%] [Generator loss: 6.926708]\n",
      "7267 [Discriminator loss: 0.180016, acc.: 90.62%] [Generator loss: 5.668255]\n",
      "7268 [Discriminator loss: 0.270005, acc.: 89.06%] [Generator loss: 5.904428]\n",
      "7269 [Discriminator loss: 0.150197, acc.: 95.31%] [Generator loss: 5.393666]\n",
      "7270 [Discriminator loss: 0.148418, acc.: 93.75%] [Generator loss: 5.390142]\n",
      "7271 [Discriminator loss: 0.066853, acc.: 98.44%] [Generator loss: 5.719825]\n",
      "7272 [Discriminator loss: 0.106853, acc.: 95.31%] [Generator loss: 5.151350]\n",
      "7273 [Discriminator loss: 0.134165, acc.: 95.31%] [Generator loss: 5.914603]\n",
      "7274 [Discriminator loss: 0.231270, acc.: 90.62%] [Generator loss: 6.133365]\n",
      "7275 [Discriminator loss: 0.348915, acc.: 82.81%] [Generator loss: 4.986900]\n",
      "7276 [Discriminator loss: 0.068752, acc.: 98.44%] [Generator loss: 5.674081]\n",
      "7277 [Discriminator loss: 0.140675, acc.: 92.19%] [Generator loss: 5.535028]\n",
      "7278 [Discriminator loss: 0.135714, acc.: 95.31%] [Generator loss: 5.753890]\n",
      "7279 [Discriminator loss: 0.207637, acc.: 93.75%] [Generator loss: 5.865522]\n",
      "7280 [Discriminator loss: 0.081504, acc.: 96.88%] [Generator loss: 5.344104]\n",
      "7281 [Discriminator loss: 0.131956, acc.: 93.75%] [Generator loss: 5.822679]\n",
      "7282 [Discriminator loss: 0.107735, acc.: 96.88%] [Generator loss: 5.616295]\n",
      "7283 [Discriminator loss: 0.118834, acc.: 95.31%] [Generator loss: 4.405374]\n",
      "7284 [Discriminator loss: 0.180555, acc.: 95.31%] [Generator loss: 5.282044]\n",
      "7285 [Discriminator loss: 0.350069, acc.: 82.81%] [Generator loss: 7.197078]\n",
      "7286 [Discriminator loss: 0.240837, acc.: 87.50%] [Generator loss: 5.300544]\n",
      "7287 [Discriminator loss: 0.238009, acc.: 92.19%] [Generator loss: 5.408257]\n",
      "7288 [Discriminator loss: 0.178820, acc.: 95.31%] [Generator loss: 6.621598]\n",
      "7289 [Discriminator loss: 0.223105, acc.: 89.06%] [Generator loss: 6.224429]\n",
      "7290 [Discriminator loss: 0.258615, acc.: 87.50%] [Generator loss: 5.766446]\n",
      "7291 [Discriminator loss: 0.117355, acc.: 96.88%] [Generator loss: 4.628880]\n",
      "7292 [Discriminator loss: 0.114371, acc.: 95.31%] [Generator loss: 4.673656]\n",
      "7293 [Discriminator loss: 0.203128, acc.: 89.06%] [Generator loss: 5.581936]\n",
      "7294 [Discriminator loss: 0.090715, acc.: 95.31%] [Generator loss: 5.519452]\n",
      "7295 [Discriminator loss: 0.306441, acc.: 87.50%] [Generator loss: 4.405482]\n",
      "7296 [Discriminator loss: 0.342200, acc.: 87.50%] [Generator loss: 7.683060]\n",
      "7297 [Discriminator loss: 0.139358, acc.: 92.19%] [Generator loss: 6.305048]\n",
      "7298 [Discriminator loss: 0.120984, acc.: 96.88%] [Generator loss: 5.066725]\n",
      "7299 [Discriminator loss: 0.056791, acc.: 98.44%] [Generator loss: 5.230889]\n",
      "7300 [Discriminator loss: 0.204180, acc.: 90.62%] [Generator loss: 6.313247]\n",
      "7301 [Discriminator loss: 0.127274, acc.: 96.88%] [Generator loss: 5.187614]\n",
      "7302 [Discriminator loss: 0.130035, acc.: 95.31%] [Generator loss: 5.229499]\n",
      "7303 [Discriminator loss: 0.223518, acc.: 87.50%] [Generator loss: 3.788210]\n",
      "7304 [Discriminator loss: 0.145523, acc.: 95.31%] [Generator loss: 6.110254]\n",
      "7305 [Discriminator loss: 0.196285, acc.: 92.19%] [Generator loss: 5.087256]\n",
      "7306 [Discriminator loss: 0.108101, acc.: 95.31%] [Generator loss: 4.676308]\n",
      "7307 [Discriminator loss: 0.076991, acc.: 98.44%] [Generator loss: 5.662373]\n",
      "7308 [Discriminator loss: 0.225960, acc.: 92.19%] [Generator loss: 5.725515]\n",
      "7309 [Discriminator loss: 0.328019, acc.: 82.81%] [Generator loss: 6.007696]\n",
      "7310 [Discriminator loss: 0.396211, acc.: 81.25%] [Generator loss: 5.655015]\n",
      "7311 [Discriminator loss: 0.112525, acc.: 96.88%] [Generator loss: 6.722935]\n",
      "7312 [Discriminator loss: 0.132853, acc.: 93.75%] [Generator loss: 5.507806]\n",
      "7313 [Discriminator loss: 0.101522, acc.: 98.44%] [Generator loss: 5.199289]\n",
      "7314 [Discriminator loss: 0.067816, acc.: 98.44%] [Generator loss: 5.488933]\n",
      "7315 [Discriminator loss: 0.081035, acc.: 95.31%] [Generator loss: 4.843444]\n",
      "7316 [Discriminator loss: 0.194131, acc.: 89.06%] [Generator loss: 5.870168]\n",
      "7317 [Discriminator loss: 0.210044, acc.: 93.75%] [Generator loss: 5.841515]\n",
      "7318 [Discriminator loss: 0.183555, acc.: 90.62%] [Generator loss: 4.865085]\n",
      "7319 [Discriminator loss: 0.193281, acc.: 90.62%] [Generator loss: 6.147603]\n",
      "7320 [Discriminator loss: 0.195360, acc.: 92.19%] [Generator loss: 6.782917]\n",
      "7321 [Discriminator loss: 0.178728, acc.: 92.19%] [Generator loss: 6.912945]\n",
      "7322 [Discriminator loss: 0.158926, acc.: 95.31%] [Generator loss: 5.085161]\n",
      "7323 [Discriminator loss: 0.111078, acc.: 98.44%] [Generator loss: 5.874029]\n",
      "7324 [Discriminator loss: 0.234408, acc.: 93.75%] [Generator loss: 5.805800]\n",
      "7325 [Discriminator loss: 0.104724, acc.: 95.31%] [Generator loss: 6.384030]\n",
      "7326 [Discriminator loss: 0.177948, acc.: 90.62%] [Generator loss: 5.146327]\n",
      "7327 [Discriminator loss: 0.044006, acc.: 98.44%] [Generator loss: 5.472016]\n",
      "7328 [Discriminator loss: 0.180316, acc.: 92.19%] [Generator loss: 5.806421]\n",
      "7329 [Discriminator loss: 0.067552, acc.: 98.44%] [Generator loss: 6.069476]\n",
      "7330 [Discriminator loss: 0.198422, acc.: 92.19%] [Generator loss: 4.803833]\n",
      "7331 [Discriminator loss: 0.134329, acc.: 93.75%] [Generator loss: 5.816460]\n",
      "7332 [Discriminator loss: 0.298248, acc.: 82.81%] [Generator loss: 6.033461]\n",
      "7333 [Discriminator loss: 0.214171, acc.: 90.62%] [Generator loss: 5.186604]\n",
      "7334 [Discriminator loss: 0.272651, acc.: 90.62%] [Generator loss: 4.977400]\n",
      "7335 [Discriminator loss: 0.079730, acc.: 95.31%] [Generator loss: 6.152271]\n",
      "7336 [Discriminator loss: 0.093788, acc.: 96.88%] [Generator loss: 5.743046]\n",
      "7337 [Discriminator loss: 0.289804, acc.: 89.06%] [Generator loss: 5.003238]\n",
      "7338 [Discriminator loss: 0.151808, acc.: 95.31%] [Generator loss: 4.730695]\n",
      "7339 [Discriminator loss: 0.232315, acc.: 92.19%] [Generator loss: 4.845907]\n",
      "7340 [Discriminator loss: 0.150383, acc.: 96.88%] [Generator loss: 5.884554]\n",
      "7341 [Discriminator loss: 0.147187, acc.: 93.75%] [Generator loss: 5.318924]\n",
      "7342 [Discriminator loss: 0.142012, acc.: 95.31%] [Generator loss: 7.133311]\n",
      "7343 [Discriminator loss: 0.205054, acc.: 85.94%] [Generator loss: 7.056382]\n",
      "7344 [Discriminator loss: 0.174555, acc.: 92.19%] [Generator loss: 5.996350]\n",
      "7345 [Discriminator loss: 0.110505, acc.: 95.31%] [Generator loss: 6.290395]\n",
      "7346 [Discriminator loss: 0.278351, acc.: 89.06%] [Generator loss: 6.252791]\n",
      "7347 [Discriminator loss: 0.128588, acc.: 93.75%] [Generator loss: 4.432375]\n",
      "7348 [Discriminator loss: 0.300901, acc.: 87.50%] [Generator loss: 6.685379]\n",
      "7349 [Discriminator loss: 0.227667, acc.: 92.19%] [Generator loss: 5.400425]\n",
      "7350 [Discriminator loss: 0.298160, acc.: 87.50%] [Generator loss: 5.665499]\n",
      "7351 [Discriminator loss: 0.073096, acc.: 98.44%] [Generator loss: 6.061664]\n",
      "7352 [Discriminator loss: 0.124196, acc.: 95.31%] [Generator loss: 4.742626]\n",
      "7353 [Discriminator loss: 0.293561, acc.: 87.50%] [Generator loss: 5.553521]\n",
      "7354 [Discriminator loss: 0.076634, acc.: 96.88%] [Generator loss: 5.971754]\n",
      "7355 [Discriminator loss: 0.116687, acc.: 98.44%] [Generator loss: 6.129051]\n",
      "7356 [Discriminator loss: 0.242270, acc.: 82.81%] [Generator loss: 3.728118]\n",
      "7357 [Discriminator loss: 0.270971, acc.: 85.94%] [Generator loss: 7.224258]\n",
      "7358 [Discriminator loss: 0.025307, acc.: 100.00%] [Generator loss: 8.166554]\n",
      "7359 [Discriminator loss: 0.361089, acc.: 82.81%] [Generator loss: 5.013531]\n",
      "7360 [Discriminator loss: 0.326043, acc.: 84.38%] [Generator loss: 7.800393]\n",
      "7361 [Discriminator loss: 0.182907, acc.: 93.75%] [Generator loss: 7.114271]\n",
      "7362 [Discriminator loss: 0.208195, acc.: 92.19%] [Generator loss: 5.474830]\n",
      "7363 [Discriminator loss: 0.113057, acc.: 93.75%] [Generator loss: 5.975333]\n",
      "7364 [Discriminator loss: 0.169946, acc.: 89.06%] [Generator loss: 5.865289]\n",
      "7365 [Discriminator loss: 0.114264, acc.: 95.31%] [Generator loss: 6.292730]\n",
      "7366 [Discriminator loss: 0.229248, acc.: 93.75%] [Generator loss: 4.882925]\n",
      "7367 [Discriminator loss: 0.134192, acc.: 93.75%] [Generator loss: 5.587473]\n",
      "7368 [Discriminator loss: 0.167401, acc.: 93.75%] [Generator loss: 6.789583]\n",
      "7369 [Discriminator loss: 0.273725, acc.: 90.62%] [Generator loss: 4.278708]\n",
      "7370 [Discriminator loss: 0.227965, acc.: 92.19%] [Generator loss: 5.415788]\n",
      "7371 [Discriminator loss: 0.065474, acc.: 98.44%] [Generator loss: 5.841847]\n",
      "7372 [Discriminator loss: 0.173672, acc.: 93.75%] [Generator loss: 5.806312]\n",
      "7373 [Discriminator loss: 0.130047, acc.: 95.31%] [Generator loss: 5.152918]\n",
      "7374 [Discriminator loss: 0.143640, acc.: 95.31%] [Generator loss: 5.744386]\n",
      "7375 [Discriminator loss: 0.198860, acc.: 90.62%] [Generator loss: 4.500970]\n",
      "7376 [Discriminator loss: 0.186504, acc.: 95.31%] [Generator loss: 6.635094]\n",
      "7377 [Discriminator loss: 0.279284, acc.: 82.81%] [Generator loss: 6.190668]\n",
      "7378 [Discriminator loss: 0.349538, acc.: 84.38%] [Generator loss: 5.113237]\n",
      "7379 [Discriminator loss: 0.191403, acc.: 90.62%] [Generator loss: 4.826121]\n",
      "7380 [Discriminator loss: 0.140011, acc.: 93.75%] [Generator loss: 6.330637]\n",
      "7381 [Discriminator loss: 0.173058, acc.: 95.31%] [Generator loss: 6.390691]\n",
      "7382 [Discriminator loss: 0.189910, acc.: 92.19%] [Generator loss: 7.386979]\n",
      "7383 [Discriminator loss: 0.177585, acc.: 93.75%] [Generator loss: 6.295079]\n",
      "7384 [Discriminator loss: 0.437527, acc.: 84.38%] [Generator loss: 6.013279]\n",
      "7385 [Discriminator loss: 0.068491, acc.: 96.88%] [Generator loss: 5.639904]\n",
      "7386 [Discriminator loss: 0.242835, acc.: 87.50%] [Generator loss: 5.150301]\n",
      "7387 [Discriminator loss: 0.092887, acc.: 96.88%] [Generator loss: 6.081814]\n",
      "7388 [Discriminator loss: 0.139161, acc.: 92.19%] [Generator loss: 6.196644]\n",
      "7389 [Discriminator loss: 0.200007, acc.: 92.19%] [Generator loss: 5.279880]\n",
      "7390 [Discriminator loss: 0.106557, acc.: 95.31%] [Generator loss: 5.895240]\n",
      "7391 [Discriminator loss: 0.241377, acc.: 89.06%] [Generator loss: 5.303139]\n",
      "7392 [Discriminator loss: 0.131077, acc.: 93.75%] [Generator loss: 5.537813]\n",
      "7393 [Discriminator loss: 0.144885, acc.: 90.62%] [Generator loss: 6.744390]\n",
      "7394 [Discriminator loss: 0.098258, acc.: 96.88%] [Generator loss: 6.199624]\n",
      "7395 [Discriminator loss: 0.269922, acc.: 89.06%] [Generator loss: 5.769307]\n",
      "7396 [Discriminator loss: 0.189486, acc.: 95.31%] [Generator loss: 6.670860]\n",
      "7397 [Discriminator loss: 0.167369, acc.: 92.19%] [Generator loss: 5.893015]\n",
      "7398 [Discriminator loss: 0.097142, acc.: 95.31%] [Generator loss: 5.945143]\n",
      "7399 [Discriminator loss: 0.221988, acc.: 93.75%] [Generator loss: 4.593126]\n",
      "7400 [Discriminator loss: 0.203030, acc.: 96.88%] [Generator loss: 5.682261]\n",
      "7401 [Discriminator loss: 0.343993, acc.: 84.38%] [Generator loss: 5.847012]\n",
      "7402 [Discriminator loss: 0.277060, acc.: 87.50%] [Generator loss: 5.186253]\n",
      "7403 [Discriminator loss: 0.105905, acc.: 96.88%] [Generator loss: 6.390373]\n",
      "7404 [Discriminator loss: 0.119823, acc.: 93.75%] [Generator loss: 4.774637]\n",
      "7405 [Discriminator loss: 0.087822, acc.: 98.44%] [Generator loss: 4.732936]\n",
      "7406 [Discriminator loss: 0.107252, acc.: 95.31%] [Generator loss: 5.780405]\n",
      "7407 [Discriminator loss: 0.114833, acc.: 96.88%] [Generator loss: 5.260849]\n",
      "7408 [Discriminator loss: 0.253715, acc.: 87.50%] [Generator loss: 6.734615]\n",
      "7409 [Discriminator loss: 0.224138, acc.: 90.62%] [Generator loss: 4.954125]\n",
      "7410 [Discriminator loss: 0.148001, acc.: 90.62%] [Generator loss: 6.036088]\n",
      "7411 [Discriminator loss: 0.167329, acc.: 92.19%] [Generator loss: 6.070255]\n",
      "7412 [Discriminator loss: 0.293549, acc.: 92.19%] [Generator loss: 4.770413]\n",
      "7413 [Discriminator loss: 0.253512, acc.: 93.75%] [Generator loss: 5.187057]\n",
      "7414 [Discriminator loss: 0.192487, acc.: 92.19%] [Generator loss: 4.738059]\n",
      "7415 [Discriminator loss: 0.333444, acc.: 89.06%] [Generator loss: 6.212621]\n",
      "7416 [Discriminator loss: 0.241907, acc.: 89.06%] [Generator loss: 6.081769]\n",
      "7417 [Discriminator loss: 0.246858, acc.: 92.19%] [Generator loss: 6.939379]\n",
      "7418 [Discriminator loss: 0.333648, acc.: 87.50%] [Generator loss: 5.670150]\n",
      "7419 [Discriminator loss: 0.122864, acc.: 95.31%] [Generator loss: 3.915684]\n",
      "7420 [Discriminator loss: 0.332373, acc.: 87.50%] [Generator loss: 5.934032]\n",
      "7421 [Discriminator loss: 0.090272, acc.: 95.31%] [Generator loss: 7.887653]\n",
      "7422 [Discriminator loss: 0.338019, acc.: 82.81%] [Generator loss: 4.982068]\n",
      "7423 [Discriminator loss: 0.095909, acc.: 98.44%] [Generator loss: 5.022329]\n",
      "7424 [Discriminator loss: 0.247461, acc.: 89.06%] [Generator loss: 6.523315]\n",
      "7425 [Discriminator loss: 0.165349, acc.: 95.31%] [Generator loss: 5.695475]\n",
      "7426 [Discriminator loss: 0.248083, acc.: 85.94%] [Generator loss: 6.325225]\n",
      "7427 [Discriminator loss: 0.223586, acc.: 89.06%] [Generator loss: 5.506901]\n",
      "7428 [Discriminator loss: 0.090084, acc.: 95.31%] [Generator loss: 5.747708]\n",
      "7429 [Discriminator loss: 0.074658, acc.: 98.44%] [Generator loss: 5.613857]\n",
      "7430 [Discriminator loss: 0.315520, acc.: 87.50%] [Generator loss: 5.513098]\n",
      "7431 [Discriminator loss: 0.229913, acc.: 90.62%] [Generator loss: 6.278008]\n",
      "7432 [Discriminator loss: 0.091821, acc.: 98.44%] [Generator loss: 5.136488]\n",
      "7433 [Discriminator loss: 0.088159, acc.: 98.44%] [Generator loss: 5.057103]\n",
      "7434 [Discriminator loss: 0.280365, acc.: 87.50%] [Generator loss: 5.471996]\n",
      "7435 [Discriminator loss: 0.198698, acc.: 87.50%] [Generator loss: 4.504952]\n",
      "7436 [Discriminator loss: 0.160788, acc.: 96.88%] [Generator loss: 5.370500]\n",
      "7437 [Discriminator loss: 0.132717, acc.: 95.31%] [Generator loss: 7.041560]\n",
      "7438 [Discriminator loss: 0.123398, acc.: 96.88%] [Generator loss: 6.133412]\n",
      "7439 [Discriminator loss: 0.293000, acc.: 84.38%] [Generator loss: 4.712205]\n",
      "7440 [Discriminator loss: 0.214615, acc.: 89.06%] [Generator loss: 6.506850]\n",
      "7441 [Discriminator loss: 0.156705, acc.: 93.75%] [Generator loss: 7.077392]\n",
      "7442 [Discriminator loss: 0.194978, acc.: 89.06%] [Generator loss: 5.970506]\n",
      "7443 [Discriminator loss: 0.155358, acc.: 95.31%] [Generator loss: 5.756366]\n",
      "7444 [Discriminator loss: 0.231712, acc.: 90.62%] [Generator loss: 7.009800]\n",
      "7445 [Discriminator loss: 0.130740, acc.: 92.19%] [Generator loss: 6.650082]\n",
      "7446 [Discriminator loss: 0.193378, acc.: 92.19%] [Generator loss: 4.583808]\n",
      "7447 [Discriminator loss: 0.287482, acc.: 89.06%] [Generator loss: 7.182111]\n",
      "7448 [Discriminator loss: 0.077462, acc.: 95.31%] [Generator loss: 7.167231]\n",
      "7449 [Discriminator loss: 0.277529, acc.: 84.38%] [Generator loss: 6.013611]\n",
      "7450 [Discriminator loss: 0.073403, acc.: 98.44%] [Generator loss: 5.734678]\n",
      "7451 [Discriminator loss: 0.168577, acc.: 92.19%] [Generator loss: 6.112568]\n",
      "7452 [Discriminator loss: 0.106020, acc.: 95.31%] [Generator loss: 6.435355]\n",
      "7453 [Discriminator loss: 0.198065, acc.: 92.19%] [Generator loss: 4.593696]\n",
      "7454 [Discriminator loss: 0.102680, acc.: 95.31%] [Generator loss: 5.163717]\n",
      "7455 [Discriminator loss: 0.126726, acc.: 95.31%] [Generator loss: 5.001195]\n",
      "7456 [Discriminator loss: 0.245167, acc.: 92.19%] [Generator loss: 5.492305]\n",
      "7457 [Discriminator loss: 0.205533, acc.: 92.19%] [Generator loss: 5.216603]\n",
      "7458 [Discriminator loss: 0.283908, acc.: 84.38%] [Generator loss: 4.563896]\n",
      "7459 [Discriminator loss: 0.243934, acc.: 87.50%] [Generator loss: 6.345682]\n",
      "7460 [Discriminator loss: 0.390119, acc.: 85.94%] [Generator loss: 6.853906]\n",
      "7461 [Discriminator loss: 0.299971, acc.: 87.50%] [Generator loss: 6.754747]\n",
      "7462 [Discriminator loss: 0.100078, acc.: 96.88%] [Generator loss: 5.025021]\n",
      "7463 [Discriminator loss: 0.106649, acc.: 96.88%] [Generator loss: 5.444003]\n",
      "7464 [Discriminator loss: 0.205675, acc.: 90.62%] [Generator loss: 5.473201]\n",
      "7465 [Discriminator loss: 0.162369, acc.: 92.19%] [Generator loss: 5.167897]\n",
      "7466 [Discriminator loss: 0.091136, acc.: 96.88%] [Generator loss: 4.431975]\n",
      "7467 [Discriminator loss: 0.128007, acc.: 93.75%] [Generator loss: 5.119542]\n",
      "7468 [Discriminator loss: 0.118976, acc.: 96.88%] [Generator loss: 6.329546]\n",
      "7469 [Discriminator loss: 0.156649, acc.: 93.75%] [Generator loss: 6.254680]\n",
      "7470 [Discriminator loss: 0.187759, acc.: 90.62%] [Generator loss: 4.000175]\n",
      "7471 [Discriminator loss: 0.094458, acc.: 98.44%] [Generator loss: 5.523840]\n",
      "7472 [Discriminator loss: 0.124614, acc.: 95.31%] [Generator loss: 5.452946]\n",
      "7473 [Discriminator loss: 0.139879, acc.: 93.75%] [Generator loss: 4.624667]\n",
      "7474 [Discriminator loss: 0.280483, acc.: 87.50%] [Generator loss: 6.672148]\n",
      "7475 [Discriminator loss: 0.194432, acc.: 90.62%] [Generator loss: 6.559553]\n",
      "7476 [Discriminator loss: 0.191289, acc.: 93.75%] [Generator loss: 6.018044]\n",
      "7477 [Discriminator loss: 0.128095, acc.: 92.19%] [Generator loss: 6.572829]\n",
      "7478 [Discriminator loss: 0.159480, acc.: 95.31%] [Generator loss: 5.271714]\n",
      "7479 [Discriminator loss: 0.200204, acc.: 93.75%] [Generator loss: 5.392574]\n",
      "7480 [Discriminator loss: 0.083733, acc.: 98.44%] [Generator loss: 5.719660]\n",
      "7481 [Discriminator loss: 0.099208, acc.: 96.88%] [Generator loss: 5.254415]\n",
      "7482 [Discriminator loss: 0.195106, acc.: 90.62%] [Generator loss: 5.492759]\n",
      "7483 [Discriminator loss: 0.176315, acc.: 93.75%] [Generator loss: 4.487814]\n",
      "7484 [Discriminator loss: 0.458468, acc.: 82.81%] [Generator loss: 6.131157]\n",
      "7485 [Discriminator loss: 0.115743, acc.: 96.88%] [Generator loss: 7.319014]\n",
      "7486 [Discriminator loss: 0.222241, acc.: 87.50%] [Generator loss: 5.234124]\n",
      "7487 [Discriminator loss: 0.139208, acc.: 93.75%] [Generator loss: 5.553790]\n",
      "7488 [Discriminator loss: 0.104977, acc.: 93.75%] [Generator loss: 5.716835]\n",
      "7489 [Discriminator loss: 0.054475, acc.: 100.00%] [Generator loss: 4.585859]\n",
      "7490 [Discriminator loss: 0.133259, acc.: 95.31%] [Generator loss: 6.702141]\n",
      "7491 [Discriminator loss: 0.409069, acc.: 82.81%] [Generator loss: 5.962596]\n",
      "7492 [Discriminator loss: 0.247185, acc.: 90.62%] [Generator loss: 5.778658]\n",
      "7493 [Discriminator loss: 0.082805, acc.: 96.88%] [Generator loss: 6.339173]\n",
      "7494 [Discriminator loss: 0.548094, acc.: 75.00%] [Generator loss: 4.833696]\n",
      "7495 [Discriminator loss: 0.213708, acc.: 87.50%] [Generator loss: 6.054453]\n",
      "7496 [Discriminator loss: 0.204793, acc.: 90.62%] [Generator loss: 6.401064]\n",
      "7497 [Discriminator loss: 0.140696, acc.: 96.88%] [Generator loss: 5.355199]\n",
      "7498 [Discriminator loss: 0.083047, acc.: 95.31%] [Generator loss: 6.384950]\n",
      "7499 [Discriminator loss: 0.048224, acc.: 98.44%] [Generator loss: 6.830178]\n",
      "7500 [Discriminator loss: 0.063546, acc.: 98.44%] [Generator loss: 5.577812]\n",
      "7501 [Discriminator loss: 0.269022, acc.: 87.50%] [Generator loss: 5.337127]\n",
      "7502 [Discriminator loss: 0.274961, acc.: 92.19%] [Generator loss: 5.610858]\n",
      "7503 [Discriminator loss: 0.151287, acc.: 92.19%] [Generator loss: 5.695672]\n",
      "7504 [Discriminator loss: 0.115840, acc.: 96.88%] [Generator loss: 6.805406]\n",
      "7505 [Discriminator loss: 0.332666, acc.: 89.06%] [Generator loss: 5.796317]\n",
      "7506 [Discriminator loss: 0.178875, acc.: 92.19%] [Generator loss: 5.703691]\n",
      "7507 [Discriminator loss: 0.113365, acc.: 95.31%] [Generator loss: 5.294409]\n",
      "7508 [Discriminator loss: 0.138820, acc.: 95.31%] [Generator loss: 5.725495]\n",
      "7509 [Discriminator loss: 0.184695, acc.: 92.19%] [Generator loss: 5.016846]\n",
      "7510 [Discriminator loss: 0.089913, acc.: 95.31%] [Generator loss: 6.502615]\n",
      "7511 [Discriminator loss: 0.215828, acc.: 93.75%] [Generator loss: 5.150263]\n",
      "7512 [Discriminator loss: 0.051727, acc.: 98.44%] [Generator loss: 6.384802]\n",
      "7513 [Discriminator loss: 0.311668, acc.: 85.94%] [Generator loss: 5.589108]\n",
      "7514 [Discriminator loss: 0.258525, acc.: 85.94%] [Generator loss: 5.917938]\n",
      "7515 [Discriminator loss: 0.046167, acc.: 98.44%] [Generator loss: 5.712464]\n",
      "7516 [Discriminator loss: 0.201066, acc.: 92.19%] [Generator loss: 5.548245]\n",
      "7517 [Discriminator loss: 0.162959, acc.: 95.31%] [Generator loss: 6.570927]\n",
      "7518 [Discriminator loss: 0.133560, acc.: 93.75%] [Generator loss: 5.177858]\n",
      "7519 [Discriminator loss: 0.102095, acc.: 95.31%] [Generator loss: 5.205718]\n",
      "7520 [Discriminator loss: 0.108880, acc.: 96.88%] [Generator loss: 5.350541]\n",
      "7521 [Discriminator loss: 0.180994, acc.: 92.19%] [Generator loss: 5.202680]\n",
      "7522 [Discriminator loss: 0.128025, acc.: 95.31%] [Generator loss: 5.005785]\n",
      "7523 [Discriminator loss: 0.225384, acc.: 89.06%] [Generator loss: 6.039100]\n",
      "7524 [Discriminator loss: 0.058280, acc.: 96.88%] [Generator loss: 6.100138]\n",
      "7525 [Discriminator loss: 0.411201, acc.: 82.81%] [Generator loss: 4.328300]\n",
      "7526 [Discriminator loss: 0.220091, acc.: 89.06%] [Generator loss: 6.923453]\n",
      "7527 [Discriminator loss: 0.375813, acc.: 79.69%] [Generator loss: 4.923389]\n",
      "7528 [Discriminator loss: 0.106944, acc.: 93.75%] [Generator loss: 6.723471]\n",
      "7529 [Discriminator loss: 0.176852, acc.: 93.75%] [Generator loss: 5.999870]\n",
      "7530 [Discriminator loss: 0.104079, acc.: 96.88%] [Generator loss: 6.707202]\n",
      "7531 [Discriminator loss: 0.121410, acc.: 95.31%] [Generator loss: 6.492681]\n",
      "7532 [Discriminator loss: 0.120165, acc.: 96.88%] [Generator loss: 5.466088]\n",
      "7533 [Discriminator loss: 0.231543, acc.: 92.19%] [Generator loss: 5.905887]\n",
      "7534 [Discriminator loss: 0.086276, acc.: 95.31%] [Generator loss: 6.476036]\n",
      "7535 [Discriminator loss: 0.402348, acc.: 82.81%] [Generator loss: 5.543433]\n",
      "7536 [Discriminator loss: 0.090941, acc.: 96.88%] [Generator loss: 6.769987]\n",
      "7537 [Discriminator loss: 0.113769, acc.: 93.75%] [Generator loss: 6.175831]\n",
      "7538 [Discriminator loss: 0.240899, acc.: 89.06%] [Generator loss: 6.659996]\n",
      "7539 [Discriminator loss: 0.074976, acc.: 100.00%] [Generator loss: 6.778456]\n",
      "7540 [Discriminator loss: 0.139910, acc.: 93.75%] [Generator loss: 5.586243]\n",
      "7541 [Discriminator loss: 0.127693, acc.: 92.19%] [Generator loss: 4.649868]\n",
      "7542 [Discriminator loss: 0.153563, acc.: 92.19%] [Generator loss: 5.291781]\n",
      "7543 [Discriminator loss: 0.133745, acc.: 96.88%] [Generator loss: 4.743551]\n",
      "7544 [Discriminator loss: 0.176794, acc.: 92.19%] [Generator loss: 6.124947]\n",
      "7545 [Discriminator loss: 0.213479, acc.: 87.50%] [Generator loss: 4.796677]\n",
      "7546 [Discriminator loss: 0.158911, acc.: 92.19%] [Generator loss: 5.906038]\n",
      "7547 [Discriminator loss: 0.262124, acc.: 92.19%] [Generator loss: 5.087957]\n",
      "7548 [Discriminator loss: 0.116879, acc.: 96.88%] [Generator loss: 5.375148]\n",
      "7549 [Discriminator loss: 0.127816, acc.: 95.31%] [Generator loss: 6.245608]\n",
      "7550 [Discriminator loss: 0.121537, acc.: 93.75%] [Generator loss: 4.966011]\n",
      "7551 [Discriminator loss: 0.137430, acc.: 96.88%] [Generator loss: 6.008896]\n",
      "7552 [Discriminator loss: 0.098764, acc.: 96.88%] [Generator loss: 5.208100]\n",
      "7553 [Discriminator loss: 0.100080, acc.: 95.31%] [Generator loss: 5.199204]\n",
      "7554 [Discriminator loss: 0.514359, acc.: 76.56%] [Generator loss: 6.898286]\n",
      "7555 [Discriminator loss: 0.136136, acc.: 95.31%] [Generator loss: 5.991078]\n",
      "7556 [Discriminator loss: 0.184699, acc.: 92.19%] [Generator loss: 6.088328]\n",
      "7557 [Discriminator loss: 0.128795, acc.: 93.75%] [Generator loss: 5.392309]\n",
      "7558 [Discriminator loss: 0.065239, acc.: 98.44%] [Generator loss: 4.862910]\n",
      "7559 [Discriminator loss: 0.202464, acc.: 89.06%] [Generator loss: 5.357369]\n",
      "7560 [Discriminator loss: 0.228427, acc.: 87.50%] [Generator loss: 5.798779]\n",
      "7561 [Discriminator loss: 0.079112, acc.: 96.88%] [Generator loss: 6.797711]\n",
      "7562 [Discriminator loss: 0.179372, acc.: 90.62%] [Generator loss: 6.498423]\n",
      "7563 [Discriminator loss: 0.167426, acc.: 90.62%] [Generator loss: 6.613460]\n",
      "7564 [Discriminator loss: 0.315688, acc.: 84.38%] [Generator loss: 4.803883]\n",
      "7565 [Discriminator loss: 0.182849, acc.: 92.19%] [Generator loss: 6.323665]\n",
      "7566 [Discriminator loss: 0.076207, acc.: 96.88%] [Generator loss: 6.331555]\n",
      "7567 [Discriminator loss: 0.159676, acc.: 90.62%] [Generator loss: 5.912354]\n",
      "7568 [Discriminator loss: 0.145098, acc.: 96.88%] [Generator loss: 4.140713]\n",
      "7569 [Discriminator loss: 0.082430, acc.: 98.44%] [Generator loss: 5.990601]\n",
      "7570 [Discriminator loss: 0.119911, acc.: 96.88%] [Generator loss: 4.689693]\n",
      "7571 [Discriminator loss: 0.146269, acc.: 93.75%] [Generator loss: 4.891800]\n",
      "7572 [Discriminator loss: 0.215241, acc.: 92.19%] [Generator loss: 6.774851]\n",
      "7573 [Discriminator loss: 0.203529, acc.: 92.19%] [Generator loss: 5.700498]\n",
      "7574 [Discriminator loss: 0.204726, acc.: 95.31%] [Generator loss: 5.324430]\n",
      "7575 [Discriminator loss: 0.167088, acc.: 95.31%] [Generator loss: 6.246749]\n",
      "7576 [Discriminator loss: 0.247469, acc.: 85.94%] [Generator loss: 5.213704]\n",
      "7577 [Discriminator loss: 0.175285, acc.: 92.19%] [Generator loss: 5.052545]\n",
      "7578 [Discriminator loss: 0.140190, acc.: 95.31%] [Generator loss: 7.183629]\n",
      "7579 [Discriminator loss: 0.212841, acc.: 95.31%] [Generator loss: 5.568320]\n",
      "7580 [Discriminator loss: 0.207389, acc.: 90.62%] [Generator loss: 4.880523]\n",
      "7581 [Discriminator loss: 0.185209, acc.: 92.19%] [Generator loss: 5.889714]\n",
      "7582 [Discriminator loss: 0.283356, acc.: 85.94%] [Generator loss: 6.350958]\n",
      "7583 [Discriminator loss: 0.253347, acc.: 93.75%] [Generator loss: 4.608824]\n",
      "7584 [Discriminator loss: 0.165149, acc.: 93.75%] [Generator loss: 5.483573]\n",
      "7585 [Discriminator loss: 0.115065, acc.: 95.31%] [Generator loss: 5.991317]\n",
      "7586 [Discriminator loss: 0.100279, acc.: 98.44%] [Generator loss: 5.685954]\n",
      "7587 [Discriminator loss: 0.148886, acc.: 92.19%] [Generator loss: 5.848384]\n",
      "7588 [Discriminator loss: 0.312921, acc.: 85.94%] [Generator loss: 4.922463]\n",
      "7589 [Discriminator loss: 0.277277, acc.: 93.75%] [Generator loss: 6.005065]\n",
      "7590 [Discriminator loss: 0.081030, acc.: 96.88%] [Generator loss: 6.615636]\n",
      "7591 [Discriminator loss: 0.204092, acc.: 89.06%] [Generator loss: 4.876747]\n",
      "7592 [Discriminator loss: 0.115623, acc.: 95.31%] [Generator loss: 6.789547]\n",
      "7593 [Discriminator loss: 0.092834, acc.: 96.88%] [Generator loss: 6.514318]\n",
      "7594 [Discriminator loss: 0.376855, acc.: 76.56%] [Generator loss: 5.129322]\n",
      "7595 [Discriminator loss: 0.126138, acc.: 96.88%] [Generator loss: 4.666755]\n",
      "7596 [Discriminator loss: 0.098799, acc.: 95.31%] [Generator loss: 4.506126]\n",
      "7597 [Discriminator loss: 0.085047, acc.: 96.88%] [Generator loss: 5.665713]\n",
      "7598 [Discriminator loss: 0.223062, acc.: 89.06%] [Generator loss: 5.435971]\n",
      "7599 [Discriminator loss: 0.134296, acc.: 93.75%] [Generator loss: 5.443913]\n",
      "7600 [Discriminator loss: 0.284841, acc.: 85.94%] [Generator loss: 5.561461]\n",
      "7601 [Discriminator loss: 0.228143, acc.: 90.62%] [Generator loss: 6.142684]\n",
      "7602 [Discriminator loss: 0.195032, acc.: 95.31%] [Generator loss: 6.867409]\n",
      "7603 [Discriminator loss: 0.297035, acc.: 89.06%] [Generator loss: 5.006246]\n",
      "7604 [Discriminator loss: 0.119484, acc.: 95.31%] [Generator loss: 5.678030]\n",
      "7605 [Discriminator loss: 0.172676, acc.: 95.31%] [Generator loss: 5.415699]\n",
      "7606 [Discriminator loss: 0.143101, acc.: 93.75%] [Generator loss: 4.559721]\n",
      "7607 [Discriminator loss: 0.181443, acc.: 90.62%] [Generator loss: 5.575797]\n",
      "7608 [Discriminator loss: 0.177740, acc.: 93.75%] [Generator loss: 6.531954]\n",
      "7609 [Discriminator loss: 0.078249, acc.: 98.44%] [Generator loss: 4.456245]\n",
      "7610 [Discriminator loss: 0.056922, acc.: 96.88%] [Generator loss: 5.044010]\n",
      "7611 [Discriminator loss: 0.124087, acc.: 96.88%] [Generator loss: 5.693427]\n",
      "7612 [Discriminator loss: 0.274412, acc.: 85.94%] [Generator loss: 4.912915]\n",
      "7613 [Discriminator loss: 0.240256, acc.: 89.06%] [Generator loss: 6.433350]\n",
      "7614 [Discriminator loss: 0.149501, acc.: 92.19%] [Generator loss: 6.188308]\n",
      "7615 [Discriminator loss: 0.171012, acc.: 95.31%] [Generator loss: 5.789588]\n",
      "7616 [Discriminator loss: 0.138683, acc.: 95.31%] [Generator loss: 5.237087]\n",
      "7617 [Discriminator loss: 0.202063, acc.: 92.19%] [Generator loss: 6.236595]\n",
      "7618 [Discriminator loss: 0.148129, acc.: 92.19%] [Generator loss: 6.400920]\n",
      "7619 [Discriminator loss: 0.124149, acc.: 96.88%] [Generator loss: 4.960957]\n",
      "7620 [Discriminator loss: 0.310702, acc.: 82.81%] [Generator loss: 4.553463]\n",
      "7621 [Discriminator loss: 0.085835, acc.: 96.88%] [Generator loss: 4.792037]\n",
      "7622 [Discriminator loss: 0.125049, acc.: 95.31%] [Generator loss: 4.572402]\n",
      "7623 [Discriminator loss: 0.221756, acc.: 95.31%] [Generator loss: 5.228263]\n",
      "7624 [Discriminator loss: 0.133272, acc.: 96.88%] [Generator loss: 5.961167]\n",
      "7625 [Discriminator loss: 0.180930, acc.: 90.62%] [Generator loss: 6.252431]\n",
      "7626 [Discriminator loss: 0.331992, acc.: 82.81%] [Generator loss: 6.754244]\n",
      "7627 [Discriminator loss: 0.183547, acc.: 92.19%] [Generator loss: 6.465288]\n",
      "7628 [Discriminator loss: 0.145083, acc.: 95.31%] [Generator loss: 5.254752]\n",
      "7629 [Discriminator loss: 0.264562, acc.: 84.38%] [Generator loss: 7.090138]\n",
      "7630 [Discriminator loss: 0.187333, acc.: 92.19%] [Generator loss: 5.527642]\n",
      "7631 [Discriminator loss: 0.289483, acc.: 89.06%] [Generator loss: 4.613409]\n",
      "7632 [Discriminator loss: 0.262027, acc.: 85.94%] [Generator loss: 5.661672]\n",
      "7633 [Discriminator loss: 0.145675, acc.: 92.19%] [Generator loss: 7.359653]\n",
      "7634 [Discriminator loss: 0.219857, acc.: 93.75%] [Generator loss: 5.824725]\n",
      "7635 [Discriminator loss: 0.116775, acc.: 95.31%] [Generator loss: 5.073332]\n",
      "7636 [Discriminator loss: 0.252541, acc.: 89.06%] [Generator loss: 5.504350]\n",
      "7637 [Discriminator loss: 0.105804, acc.: 93.75%] [Generator loss: 6.041061]\n",
      "7638 [Discriminator loss: 0.091471, acc.: 95.31%] [Generator loss: 5.661386]\n",
      "7639 [Discriminator loss: 0.139460, acc.: 96.88%] [Generator loss: 5.857431]\n",
      "7640 [Discriminator loss: 0.120126, acc.: 93.75%] [Generator loss: 5.444383]\n",
      "7641 [Discriminator loss: 0.221001, acc.: 90.62%] [Generator loss: 6.104894]\n",
      "7642 [Discriminator loss: 0.118210, acc.: 96.88%] [Generator loss: 6.365793]\n",
      "7643 [Discriminator loss: 0.253841, acc.: 90.62%] [Generator loss: 4.596288]\n",
      "7644 [Discriminator loss: 0.245681, acc.: 89.06%] [Generator loss: 6.900864]\n",
      "7645 [Discriminator loss: 0.172285, acc.: 90.62%] [Generator loss: 5.185920]\n",
      "7646 [Discriminator loss: 0.200879, acc.: 89.06%] [Generator loss: 5.501540]\n",
      "7647 [Discriminator loss: 0.033624, acc.: 100.00%] [Generator loss: 6.051781]\n",
      "7648 [Discriminator loss: 0.251037, acc.: 92.19%] [Generator loss: 4.781613]\n",
      "7649 [Discriminator loss: 0.313439, acc.: 85.94%] [Generator loss: 6.586423]\n",
      "7650 [Discriminator loss: 0.096971, acc.: 96.88%] [Generator loss: 6.489734]\n",
      "7651 [Discriminator loss: 0.347965, acc.: 85.94%] [Generator loss: 5.801295]\n",
      "7652 [Discriminator loss: 0.201217, acc.: 95.31%] [Generator loss: 6.584185]\n",
      "7653 [Discriminator loss: 0.282227, acc.: 87.50%] [Generator loss: 5.309024]\n",
      "7654 [Discriminator loss: 0.195094, acc.: 90.62%] [Generator loss: 5.497914]\n",
      "7655 [Discriminator loss: 0.142996, acc.: 95.31%] [Generator loss: 5.700216]\n",
      "7656 [Discriminator loss: 0.171567, acc.: 90.62%] [Generator loss: 4.319828]\n",
      "7657 [Discriminator loss: 0.142031, acc.: 96.88%] [Generator loss: 4.305903]\n",
      "7658 [Discriminator loss: 0.398902, acc.: 85.94%] [Generator loss: 6.047793]\n",
      "7659 [Discriminator loss: 0.072352, acc.: 98.44%] [Generator loss: 7.203057]\n",
      "7660 [Discriminator loss: 0.074773, acc.: 98.44%] [Generator loss: 6.195303]\n",
      "7661 [Discriminator loss: 0.283510, acc.: 84.38%] [Generator loss: 4.127056]\n",
      "7662 [Discriminator loss: 0.202404, acc.: 95.31%] [Generator loss: 6.018488]\n",
      "7663 [Discriminator loss: 0.057909, acc.: 98.44%] [Generator loss: 5.074062]\n",
      "7664 [Discriminator loss: 0.192065, acc.: 92.19%] [Generator loss: 5.382955]\n",
      "7665 [Discriminator loss: 0.154372, acc.: 93.75%] [Generator loss: 5.383539]\n",
      "7666 [Discriminator loss: 0.313986, acc.: 89.06%] [Generator loss: 5.297592]\n",
      "7667 [Discriminator loss: 0.444689, acc.: 75.00%] [Generator loss: 6.318313]\n",
      "7668 [Discriminator loss: 0.083492, acc.: 98.44%] [Generator loss: 6.123407]\n",
      "7669 [Discriminator loss: 0.067638, acc.: 98.44%] [Generator loss: 5.782648]\n",
      "7670 [Discriminator loss: 0.202284, acc.: 93.75%] [Generator loss: 4.277349]\n",
      "7671 [Discriminator loss: 0.093999, acc.: 98.44%] [Generator loss: 5.085881]\n",
      "7672 [Discriminator loss: 0.228912, acc.: 89.06%] [Generator loss: 4.783629]\n",
      "7673 [Discriminator loss: 0.070683, acc.: 98.44%] [Generator loss: 5.833721]\n",
      "7674 [Discriminator loss: 0.090413, acc.: 96.88%] [Generator loss: 5.543211]\n",
      "7675 [Discriminator loss: 0.189705, acc.: 95.31%] [Generator loss: 5.607757]\n",
      "7676 [Discriminator loss: 0.096956, acc.: 98.44%] [Generator loss: 6.106149]\n",
      "7677 [Discriminator loss: 0.209636, acc.: 89.06%] [Generator loss: 4.942266]\n",
      "7678 [Discriminator loss: 0.083541, acc.: 96.88%] [Generator loss: 6.103637]\n",
      "7679 [Discriminator loss: 0.228966, acc.: 89.06%] [Generator loss: 5.486976]\n",
      "7680 [Discriminator loss: 0.182452, acc.: 95.31%] [Generator loss: 5.994888]\n",
      "7681 [Discriminator loss: 0.038032, acc.: 100.00%] [Generator loss: 4.592415]\n",
      "7682 [Discriminator loss: 0.088922, acc.: 96.88%] [Generator loss: 5.343117]\n",
      "7683 [Discriminator loss: 0.244619, acc.: 92.19%] [Generator loss: 6.000216]\n",
      "7684 [Discriminator loss: 0.156009, acc.: 92.19%] [Generator loss: 4.707492]\n",
      "7685 [Discriminator loss: 0.207282, acc.: 90.62%] [Generator loss: 5.236054]\n",
      "7686 [Discriminator loss: 0.353028, acc.: 87.50%] [Generator loss: 6.547544]\n",
      "7687 [Discriminator loss: 0.056286, acc.: 98.44%] [Generator loss: 5.679859]\n",
      "7688 [Discriminator loss: 0.081031, acc.: 95.31%] [Generator loss: 6.567093]\n",
      "7689 [Discriminator loss: 0.217769, acc.: 90.62%] [Generator loss: 4.926506]\n",
      "7690 [Discriminator loss: 0.267913, acc.: 89.06%] [Generator loss: 4.780225]\n",
      "7691 [Discriminator loss: 0.086859, acc.: 95.31%] [Generator loss: 6.284080]\n",
      "7692 [Discriminator loss: 0.084449, acc.: 96.88%] [Generator loss: 4.826929]\n",
      "7693 [Discriminator loss: 0.152562, acc.: 93.75%] [Generator loss: 6.966485]\n",
      "7694 [Discriminator loss: 0.200474, acc.: 90.62%] [Generator loss: 6.741666]\n",
      "7695 [Discriminator loss: 0.249429, acc.: 89.06%] [Generator loss: 5.135485]\n",
      "7696 [Discriminator loss: 0.228734, acc.: 87.50%] [Generator loss: 5.962683]\n",
      "7697 [Discriminator loss: 0.081382, acc.: 96.88%] [Generator loss: 6.782379]\n",
      "7698 [Discriminator loss: 0.324245, acc.: 87.50%] [Generator loss: 5.774896]\n",
      "7699 [Discriminator loss: 0.095344, acc.: 95.31%] [Generator loss: 7.394591]\n",
      "7700 [Discriminator loss: 0.117982, acc.: 98.44%] [Generator loss: 5.683111]\n",
      "7701 [Discriminator loss: 0.122655, acc.: 96.88%] [Generator loss: 4.686647]\n",
      "7702 [Discriminator loss: 0.192684, acc.: 93.75%] [Generator loss: 6.467097]\n",
      "7703 [Discriminator loss: 0.274874, acc.: 84.38%] [Generator loss: 5.030382]\n",
      "7704 [Discriminator loss: 0.132682, acc.: 95.31%] [Generator loss: 6.170641]\n",
      "7705 [Discriminator loss: 0.260003, acc.: 90.62%] [Generator loss: 5.483939]\n",
      "7706 [Discriminator loss: 0.084757, acc.: 96.88%] [Generator loss: 4.700406]\n",
      "7707 [Discriminator loss: 0.126820, acc.: 95.31%] [Generator loss: 4.681348]\n",
      "7708 [Discriminator loss: 0.065424, acc.: 98.44%] [Generator loss: 5.744388]\n",
      "7709 [Discriminator loss: 0.044295, acc.: 100.00%] [Generator loss: 5.856298]\n",
      "7710 [Discriminator loss: 0.055842, acc.: 98.44%] [Generator loss: 5.871273]\n",
      "7711 [Discriminator loss: 0.110201, acc.: 98.44%] [Generator loss: 5.906305]\n",
      "7712 [Discriminator loss: 0.153853, acc.: 90.62%] [Generator loss: 5.954410]\n",
      "7713 [Discriminator loss: 0.255734, acc.: 89.06%] [Generator loss: 6.419167]\n",
      "7714 [Discriminator loss: 0.232310, acc.: 85.94%] [Generator loss: 6.140782]\n",
      "7715 [Discriminator loss: 0.164550, acc.: 92.19%] [Generator loss: 6.605808]\n",
      "7716 [Discriminator loss: 0.069710, acc.: 96.88%] [Generator loss: 5.721598]\n",
      "7717 [Discriminator loss: 0.038077, acc.: 98.44%] [Generator loss: 5.173966]\n",
      "7718 [Discriminator loss: 0.071741, acc.: 96.88%] [Generator loss: 6.025930]\n",
      "7719 [Discriminator loss: 0.250440, acc.: 90.62%] [Generator loss: 6.111252]\n",
      "7720 [Discriminator loss: 0.078308, acc.: 96.88%] [Generator loss: 7.227743]\n",
      "7721 [Discriminator loss: 0.407605, acc.: 82.81%] [Generator loss: 5.350130]\n",
      "7722 [Discriminator loss: 0.107034, acc.: 95.31%] [Generator loss: 4.975373]\n",
      "7723 [Discriminator loss: 0.189707, acc.: 92.19%] [Generator loss: 4.324419]\n",
      "7724 [Discriminator loss: 0.197262, acc.: 90.62%] [Generator loss: 6.257415]\n",
      "7725 [Discriminator loss: 0.152900, acc.: 93.75%] [Generator loss: 5.848344]\n",
      "7726 [Discriminator loss: 0.191809, acc.: 93.75%] [Generator loss: 6.317515]\n",
      "7727 [Discriminator loss: 0.198017, acc.: 90.62%] [Generator loss: 5.417565]\n",
      "7728 [Discriminator loss: 0.213708, acc.: 87.50%] [Generator loss: 5.557089]\n",
      "7729 [Discriminator loss: 0.147474, acc.: 93.75%] [Generator loss: 5.124463]\n",
      "7730 [Discriminator loss: 0.320884, acc.: 84.38%] [Generator loss: 6.439510]\n",
      "7731 [Discriminator loss: 0.160896, acc.: 95.31%] [Generator loss: 6.212844]\n",
      "7732 [Discriminator loss: 0.103495, acc.: 93.75%] [Generator loss: 6.164949]\n",
      "7733 [Discriminator loss: 0.270719, acc.: 87.50%] [Generator loss: 5.653495]\n",
      "7734 [Discriminator loss: 0.048616, acc.: 100.00%] [Generator loss: 6.694321]\n",
      "7735 [Discriminator loss: 0.141718, acc.: 95.31%] [Generator loss: 5.674865]\n",
      "7736 [Discriminator loss: 0.174752, acc.: 92.19%] [Generator loss: 6.070039]\n",
      "7737 [Discriminator loss: 0.154932, acc.: 96.88%] [Generator loss: 5.970821]\n",
      "7738 [Discriminator loss: 0.180632, acc.: 89.06%] [Generator loss: 5.419612]\n",
      "7739 [Discriminator loss: 0.131924, acc.: 96.88%] [Generator loss: 5.756991]\n",
      "7740 [Discriminator loss: 0.190131, acc.: 90.62%] [Generator loss: 5.187816]\n",
      "7741 [Discriminator loss: 0.244084, acc.: 92.19%] [Generator loss: 4.518047]\n",
      "7742 [Discriminator loss: 0.118379, acc.: 96.88%] [Generator loss: 4.366596]\n",
      "7743 [Discriminator loss: 0.144143, acc.: 96.88%] [Generator loss: 5.242548]\n",
      "7744 [Discriminator loss: 0.227028, acc.: 92.19%] [Generator loss: 5.154726]\n",
      "7745 [Discriminator loss: 0.201100, acc.: 90.62%] [Generator loss: 5.783803]\n",
      "7746 [Discriminator loss: 0.121059, acc.: 95.31%] [Generator loss: 5.635902]\n",
      "7747 [Discriminator loss: 0.155284, acc.: 90.62%] [Generator loss: 5.413398]\n",
      "7748 [Discriminator loss: 0.160413, acc.: 90.62%] [Generator loss: 6.328015]\n",
      "7749 [Discriminator loss: 0.200110, acc.: 92.19%] [Generator loss: 6.338464]\n",
      "7750 [Discriminator loss: 0.170078, acc.: 92.19%] [Generator loss: 5.603138]\n",
      "7751 [Discriminator loss: 0.178717, acc.: 93.75%] [Generator loss: 4.269271]\n",
      "7752 [Discriminator loss: 0.092482, acc.: 95.31%] [Generator loss: 4.813512]\n",
      "7753 [Discriminator loss: 0.176441, acc.: 95.31%] [Generator loss: 5.048366]\n",
      "7754 [Discriminator loss: 0.076204, acc.: 100.00%] [Generator loss: 6.669596]\n",
      "7755 [Discriminator loss: 0.383582, acc.: 85.94%] [Generator loss: 5.081319]\n",
      "7756 [Discriminator loss: 0.282161, acc.: 89.06%] [Generator loss: 6.370955]\n",
      "7757 [Discriminator loss: 0.117449, acc.: 93.75%] [Generator loss: 6.550966]\n",
      "7758 [Discriminator loss: 0.126381, acc.: 96.88%] [Generator loss: 6.752942]\n",
      "7759 [Discriminator loss: 0.067870, acc.: 96.88%] [Generator loss: 6.460387]\n",
      "7760 [Discriminator loss: 0.286980, acc.: 89.06%] [Generator loss: 5.975401]\n",
      "7761 [Discriminator loss: 0.113080, acc.: 95.31%] [Generator loss: 5.724401]\n",
      "7762 [Discriminator loss: 0.213464, acc.: 92.19%] [Generator loss: 5.446355]\n",
      "7763 [Discriminator loss: 0.087078, acc.: 96.88%] [Generator loss: 5.565629]\n",
      "7764 [Discriminator loss: 0.275262, acc.: 89.06%] [Generator loss: 6.221937]\n",
      "7765 [Discriminator loss: 0.129897, acc.: 92.19%] [Generator loss: 6.926868]\n",
      "7766 [Discriminator loss: 0.142542, acc.: 93.75%] [Generator loss: 5.942452]\n",
      "7767 [Discriminator loss: 0.143460, acc.: 96.88%] [Generator loss: 6.500287]\n",
      "7768 [Discriminator loss: 0.212830, acc.: 92.19%] [Generator loss: 4.900880]\n",
      "7769 [Discriminator loss: 0.248456, acc.: 87.50%] [Generator loss: 5.252388]\n",
      "7770 [Discriminator loss: 0.106587, acc.: 98.44%] [Generator loss: 7.343754]\n",
      "7771 [Discriminator loss: 0.149722, acc.: 93.75%] [Generator loss: 5.337922]\n",
      "7772 [Discriminator loss: 0.260549, acc.: 90.62%] [Generator loss: 6.253930]\n",
      "7773 [Discriminator loss: 0.202420, acc.: 90.62%] [Generator loss: 6.607049]\n",
      "7774 [Discriminator loss: 0.160028, acc.: 90.62%] [Generator loss: 6.140681]\n",
      "7775 [Discriminator loss: 0.047770, acc.: 98.44%] [Generator loss: 5.349400]\n",
      "7776 [Discriminator loss: 0.100496, acc.: 98.44%] [Generator loss: 5.262760]\n",
      "7777 [Discriminator loss: 0.142451, acc.: 93.75%] [Generator loss: 5.279641]\n",
      "7778 [Discriminator loss: 0.220967, acc.: 90.62%] [Generator loss: 6.614606]\n",
      "7779 [Discriminator loss: 0.194379, acc.: 92.19%] [Generator loss: 6.159104]\n",
      "7780 [Discriminator loss: 0.204602, acc.: 90.62%] [Generator loss: 6.056520]\n",
      "7781 [Discriminator loss: 0.080218, acc.: 96.88%] [Generator loss: 6.686049]\n",
      "7782 [Discriminator loss: 0.218432, acc.: 92.19%] [Generator loss: 4.420915]\n",
      "7783 [Discriminator loss: 0.309610, acc.: 90.62%] [Generator loss: 6.416785]\n",
      "7784 [Discriminator loss: 0.065809, acc.: 98.44%] [Generator loss: 7.090970]\n",
      "7785 [Discriminator loss: 0.342616, acc.: 85.94%] [Generator loss: 5.781161]\n",
      "7786 [Discriminator loss: 0.082422, acc.: 96.88%] [Generator loss: 6.253351]\n",
      "7787 [Discriminator loss: 0.139758, acc.: 93.75%] [Generator loss: 6.563053]\n",
      "7788 [Discriminator loss: 0.214059, acc.: 90.62%] [Generator loss: 7.357700]\n",
      "7789 [Discriminator loss: 0.092729, acc.: 96.88%] [Generator loss: 4.967821]\n",
      "7790 [Discriminator loss: 0.094814, acc.: 98.44%] [Generator loss: 5.642103]\n",
      "7791 [Discriminator loss: 0.141693, acc.: 93.75%] [Generator loss: 5.056999]\n",
      "7792 [Discriminator loss: 0.501777, acc.: 76.56%] [Generator loss: 5.686342]\n",
      "7793 [Discriminator loss: 0.073513, acc.: 96.88%] [Generator loss: 7.112275]\n",
      "7794 [Discriminator loss: 0.154977, acc.: 95.31%] [Generator loss: 4.814418]\n",
      "7795 [Discriminator loss: 0.141661, acc.: 96.88%] [Generator loss: 5.592791]\n",
      "7796 [Discriminator loss: 0.274157, acc.: 87.50%] [Generator loss: 7.240466]\n",
      "7797 [Discriminator loss: 0.252983, acc.: 90.62%] [Generator loss: 6.513618]\n",
      "7798 [Discriminator loss: 0.324714, acc.: 87.50%] [Generator loss: 6.159886]\n",
      "7799 [Discriminator loss: 0.078430, acc.: 95.31%] [Generator loss: 6.237547]\n",
      "7800 [Discriminator loss: 0.184729, acc.: 90.62%] [Generator loss: 4.368510]\n",
      "7801 [Discriminator loss: 0.082692, acc.: 98.44%] [Generator loss: 6.474032]\n",
      "7802 [Discriminator loss: 0.053207, acc.: 100.00%] [Generator loss: 5.211915]\n",
      "7803 [Discriminator loss: 0.107888, acc.: 98.44%] [Generator loss: 4.936636]\n",
      "7804 [Discriminator loss: 0.153730, acc.: 93.75%] [Generator loss: 5.126487]\n",
      "7805 [Discriminator loss: 0.199458, acc.: 90.62%] [Generator loss: 6.457571]\n",
      "7806 [Discriminator loss: 0.110117, acc.: 95.31%] [Generator loss: 5.293407]\n",
      "7807 [Discriminator loss: 0.242876, acc.: 92.19%] [Generator loss: 4.763102]\n",
      "7808 [Discriminator loss: 0.211714, acc.: 90.62%] [Generator loss: 6.863991]\n",
      "7809 [Discriminator loss: 0.190079, acc.: 93.75%] [Generator loss: 5.923727]\n",
      "7810 [Discriminator loss: 0.081745, acc.: 96.88%] [Generator loss: 6.958407]\n",
      "7811 [Discriminator loss: 0.102185, acc.: 95.31%] [Generator loss: 4.215055]\n",
      "7812 [Discriminator loss: 0.098905, acc.: 95.31%] [Generator loss: 5.026298]\n",
      "7813 [Discriminator loss: 0.085883, acc.: 98.44%] [Generator loss: 5.041058]\n",
      "7814 [Discriminator loss: 0.115704, acc.: 96.88%] [Generator loss: 5.096786]\n",
      "7815 [Discriminator loss: 0.104666, acc.: 96.88%] [Generator loss: 4.654853]\n",
      "7816 [Discriminator loss: 0.144421, acc.: 92.19%] [Generator loss: 6.931382]\n",
      "7817 [Discriminator loss: 0.125974, acc.: 93.75%] [Generator loss: 5.740288]\n",
      "7818 [Discriminator loss: 0.105713, acc.: 95.31%] [Generator loss: 5.518826]\n",
      "7819 [Discriminator loss: 0.257621, acc.: 87.50%] [Generator loss: 5.662787]\n",
      "7820 [Discriminator loss: 0.175104, acc.: 92.19%] [Generator loss: 6.787415]\n",
      "7821 [Discriminator loss: 0.190005, acc.: 89.06%] [Generator loss: 6.450347]\n",
      "7822 [Discriminator loss: 0.093381, acc.: 96.88%] [Generator loss: 5.918456]\n",
      "7823 [Discriminator loss: 0.349249, acc.: 87.50%] [Generator loss: 7.885458]\n",
      "7824 [Discriminator loss: 0.135494, acc.: 92.19%] [Generator loss: 7.253492]\n",
      "7825 [Discriminator loss: 0.206298, acc.: 92.19%] [Generator loss: 5.022245]\n",
      "7826 [Discriminator loss: 0.137738, acc.: 96.88%] [Generator loss: 6.329397]\n",
      "7827 [Discriminator loss: 0.074590, acc.: 96.88%] [Generator loss: 6.464333]\n",
      "7828 [Discriminator loss: 0.072917, acc.: 98.44%] [Generator loss: 5.726804]\n",
      "7829 [Discriminator loss: 0.172508, acc.: 89.06%] [Generator loss: 6.143778]\n",
      "7830 [Discriminator loss: 0.229094, acc.: 89.06%] [Generator loss: 6.720969]\n",
      "7831 [Discriminator loss: 0.254401, acc.: 90.62%] [Generator loss: 6.096131]\n",
      "7832 [Discriminator loss: 0.052865, acc.: 98.44%] [Generator loss: 5.168665]\n",
      "7833 [Discriminator loss: 0.148554, acc.: 93.75%] [Generator loss: 4.972324]\n",
      "7834 [Discriminator loss: 0.415025, acc.: 84.38%] [Generator loss: 8.220081]\n",
      "7835 [Discriminator loss: 0.132348, acc.: 93.75%] [Generator loss: 6.999901]\n",
      "7836 [Discriminator loss: 0.284312, acc.: 85.94%] [Generator loss: 4.640604]\n",
      "7837 [Discriminator loss: 0.217326, acc.: 89.06%] [Generator loss: 4.784198]\n",
      "7838 [Discriminator loss: 0.057808, acc.: 98.44%] [Generator loss: 8.058420]\n",
      "7839 [Discriminator loss: 0.349071, acc.: 82.81%] [Generator loss: 4.744395]\n",
      "7840 [Discriminator loss: 0.122651, acc.: 93.75%] [Generator loss: 4.891206]\n",
      "7841 [Discriminator loss: 0.068636, acc.: 96.88%] [Generator loss: 6.171122]\n",
      "7842 [Discriminator loss: 0.269253, acc.: 89.06%] [Generator loss: 6.804902]\n",
      "7843 [Discriminator loss: 0.238136, acc.: 89.06%] [Generator loss: 6.022604]\n",
      "7844 [Discriminator loss: 0.246470, acc.: 84.38%] [Generator loss: 6.475268]\n",
      "7845 [Discriminator loss: 0.055356, acc.: 98.44%] [Generator loss: 6.807374]\n",
      "7846 [Discriminator loss: 0.105924, acc.: 93.75%] [Generator loss: 5.502808]\n",
      "7847 [Discriminator loss: 0.302146, acc.: 85.94%] [Generator loss: 5.009727]\n",
      "7848 [Discriminator loss: 0.108598, acc.: 93.75%] [Generator loss: 6.742574]\n",
      "7849 [Discriminator loss: 0.091146, acc.: 95.31%] [Generator loss: 4.981886]\n",
      "7850 [Discriminator loss: 0.128036, acc.: 96.88%] [Generator loss: 6.186949]\n",
      "7851 [Discriminator loss: 0.316209, acc.: 90.62%] [Generator loss: 5.213285]\n",
      "7852 [Discriminator loss: 0.254566, acc.: 92.19%] [Generator loss: 6.803538]\n",
      "7853 [Discriminator loss: 0.121520, acc.: 96.88%] [Generator loss: 6.461773]\n",
      "7854 [Discriminator loss: 0.325121, acc.: 82.81%] [Generator loss: 4.571717]\n",
      "7855 [Discriminator loss: 0.057594, acc.: 100.00%] [Generator loss: 4.760042]\n",
      "7856 [Discriminator loss: 0.093471, acc.: 95.31%] [Generator loss: 4.756157]\n",
      "7857 [Discriminator loss: 0.129847, acc.: 92.19%] [Generator loss: 4.923195]\n",
      "7858 [Discriminator loss: 0.221424, acc.: 92.19%] [Generator loss: 6.994054]\n",
      "7859 [Discriminator loss: 0.156479, acc.: 92.19%] [Generator loss: 5.120997]\n",
      "7860 [Discriminator loss: 0.176463, acc.: 95.31%] [Generator loss: 5.073504]\n",
      "7861 [Discriminator loss: 0.113838, acc.: 96.88%] [Generator loss: 6.027567]\n",
      "7862 [Discriminator loss: 0.075021, acc.: 96.88%] [Generator loss: 6.387419]\n",
      "7863 [Discriminator loss: 0.286478, acc.: 87.50%] [Generator loss: 6.146326]\n",
      "7864 [Discriminator loss: 0.308326, acc.: 90.62%] [Generator loss: 6.813816]\n",
      "7865 [Discriminator loss: 0.124391, acc.: 93.75%] [Generator loss: 6.154077]\n",
      "7866 [Discriminator loss: 0.131238, acc.: 95.31%] [Generator loss: 5.677358]\n",
      "7867 [Discriminator loss: 0.212623, acc.: 89.06%] [Generator loss: 5.947946]\n",
      "7868 [Discriminator loss: 0.183632, acc.: 92.19%] [Generator loss: 6.499881]\n",
      "7869 [Discriminator loss: 0.389950, acc.: 85.94%] [Generator loss: 5.961627]\n",
      "7870 [Discriminator loss: 0.103723, acc.: 96.88%] [Generator loss: 6.524811]\n",
      "7871 [Discriminator loss: 0.065588, acc.: 100.00%] [Generator loss: 5.883624]\n",
      "7872 [Discriminator loss: 0.193415, acc.: 93.75%] [Generator loss: 4.655548]\n",
      "7873 [Discriminator loss: 0.200459, acc.: 92.19%] [Generator loss: 7.177191]\n",
      "7874 [Discriminator loss: 0.198846, acc.: 92.19%] [Generator loss: 6.051633]\n",
      "7875 [Discriminator loss: 0.207497, acc.: 93.75%] [Generator loss: 5.715343]\n",
      "7876 [Discriminator loss: 0.104446, acc.: 95.31%] [Generator loss: 6.409878]\n",
      "7877 [Discriminator loss: 0.063713, acc.: 98.44%] [Generator loss: 6.607235]\n",
      "7878 [Discriminator loss: 0.181847, acc.: 93.75%] [Generator loss: 4.176481]\n",
      "7879 [Discriminator loss: 0.138451, acc.: 93.75%] [Generator loss: 5.761656]\n",
      "7880 [Discriminator loss: 0.142600, acc.: 98.44%] [Generator loss: 7.077281]\n",
      "7881 [Discriminator loss: 0.068342, acc.: 96.88%] [Generator loss: 6.439899]\n",
      "7882 [Discriminator loss: 0.089403, acc.: 98.44%] [Generator loss: 5.845189]\n",
      "7883 [Discriminator loss: 0.342523, acc.: 85.94%] [Generator loss: 7.253052]\n",
      "7884 [Discriminator loss: 0.190017, acc.: 93.75%] [Generator loss: 6.867913]\n",
      "7885 [Discriminator loss: 0.051295, acc.: 98.44%] [Generator loss: 7.684509]\n",
      "7886 [Discriminator loss: 0.223229, acc.: 92.19%] [Generator loss: 5.351171]\n",
      "7887 [Discriminator loss: 0.180054, acc.: 92.19%] [Generator loss: 5.418082]\n",
      "7888 [Discriminator loss: 0.146176, acc.: 93.75%] [Generator loss: 6.408220]\n",
      "7889 [Discriminator loss: 0.137515, acc.: 93.75%] [Generator loss: 6.144475]\n",
      "7890 [Discriminator loss: 0.066270, acc.: 98.44%] [Generator loss: 5.128959]\n",
      "7891 [Discriminator loss: 0.270299, acc.: 89.06%] [Generator loss: 5.279061]\n",
      "7892 [Discriminator loss: 0.189627, acc.: 95.31%] [Generator loss: 6.646026]\n",
      "7893 [Discriminator loss: 0.297530, acc.: 85.94%] [Generator loss: 4.891924]\n",
      "7894 [Discriminator loss: 0.242209, acc.: 85.94%] [Generator loss: 8.325167]\n",
      "7895 [Discriminator loss: 0.209163, acc.: 89.06%] [Generator loss: 5.655939]\n",
      "7896 [Discriminator loss: 0.377177, acc.: 89.06%] [Generator loss: 6.886972]\n",
      "7897 [Discriminator loss: 0.123705, acc.: 93.75%] [Generator loss: 6.774169]\n",
      "7898 [Discriminator loss: 0.163070, acc.: 92.19%] [Generator loss: 5.920445]\n",
      "7899 [Discriminator loss: 0.094097, acc.: 95.31%] [Generator loss: 5.272043]\n",
      "7900 [Discriminator loss: 0.124111, acc.: 95.31%] [Generator loss: 6.193542]\n",
      "7901 [Discriminator loss: 0.182308, acc.: 93.75%] [Generator loss: 6.493190]\n",
      "7902 [Discriminator loss: 0.347095, acc.: 87.50%] [Generator loss: 4.680180]\n",
      "7903 [Discriminator loss: 0.074302, acc.: 98.44%] [Generator loss: 6.393863]\n",
      "7904 [Discriminator loss: 0.214421, acc.: 89.06%] [Generator loss: 7.535764]\n",
      "7905 [Discriminator loss: 0.169438, acc.: 95.31%] [Generator loss: 5.772069]\n",
      "7906 [Discriminator loss: 0.240126, acc.: 89.06%] [Generator loss: 5.736649]\n",
      "7907 [Discriminator loss: 0.146850, acc.: 95.31%] [Generator loss: 6.528588]\n",
      "7908 [Discriminator loss: 0.055186, acc.: 98.44%] [Generator loss: 6.057940]\n",
      "7909 [Discriminator loss: 0.157335, acc.: 93.75%] [Generator loss: 5.912681]\n",
      "7910 [Discriminator loss: 0.132022, acc.: 96.88%] [Generator loss: 6.053591]\n",
      "7911 [Discriminator loss: 0.119785, acc.: 95.31%] [Generator loss: 5.586812]\n",
      "7912 [Discriminator loss: 0.188462, acc.: 90.62%] [Generator loss: 5.949767]\n",
      "7913 [Discriminator loss: 0.124481, acc.: 95.31%] [Generator loss: 5.558693]\n",
      "7914 [Discriminator loss: 0.163350, acc.: 95.31%] [Generator loss: 5.149542]\n",
      "7915 [Discriminator loss: 0.131006, acc.: 95.31%] [Generator loss: 5.988540]\n",
      "7916 [Discriminator loss: 0.130496, acc.: 95.31%] [Generator loss: 5.146452]\n",
      "7917 [Discriminator loss: 0.270593, acc.: 87.50%] [Generator loss: 5.347058]\n",
      "7918 [Discriminator loss: 0.082239, acc.: 96.88%] [Generator loss: 5.497917]\n",
      "7919 [Discriminator loss: 0.185532, acc.: 90.62%] [Generator loss: 5.933904]\n",
      "7920 [Discriminator loss: 0.318081, acc.: 87.50%] [Generator loss: 6.389608]\n",
      "7921 [Discriminator loss: 0.042006, acc.: 100.00%] [Generator loss: 5.795330]\n",
      "7922 [Discriminator loss: 0.242791, acc.: 89.06%] [Generator loss: 5.287792]\n",
      "7923 [Discriminator loss: 0.122965, acc.: 93.75%] [Generator loss: 6.101605]\n",
      "7924 [Discriminator loss: 0.232986, acc.: 89.06%] [Generator loss: 5.953573]\n",
      "7925 [Discriminator loss: 0.290780, acc.: 84.38%] [Generator loss: 4.584208]\n",
      "7926 [Discriminator loss: 0.174814, acc.: 95.31%] [Generator loss: 4.264534]\n",
      "7927 [Discriminator loss: 0.056539, acc.: 96.88%] [Generator loss: 5.839238]\n",
      "7928 [Discriminator loss: 0.098742, acc.: 98.44%] [Generator loss: 5.500319]\n",
      "7929 [Discriminator loss: 0.124652, acc.: 96.88%] [Generator loss: 6.141824]\n",
      "7930 [Discriminator loss: 0.130713, acc.: 95.31%] [Generator loss: 5.106009]\n",
      "7931 [Discriminator loss: 0.159342, acc.: 92.19%] [Generator loss: 5.600320]\n",
      "7932 [Discriminator loss: 0.135780, acc.: 93.75%] [Generator loss: 6.320240]\n",
      "7933 [Discriminator loss: 0.073358, acc.: 98.44%] [Generator loss: 6.047040]\n",
      "7934 [Discriminator loss: 0.196961, acc.: 92.19%] [Generator loss: 4.486377]\n",
      "7935 [Discriminator loss: 0.328693, acc.: 89.06%] [Generator loss: 5.644469]\n",
      "7936 [Discriminator loss: 0.091749, acc.: 96.88%] [Generator loss: 6.611683]\n",
      "7937 [Discriminator loss: 0.217758, acc.: 87.50%] [Generator loss: 5.782259]\n",
      "7938 [Discriminator loss: 0.075216, acc.: 96.88%] [Generator loss: 7.016634]\n",
      "7939 [Discriminator loss: 0.043574, acc.: 100.00%] [Generator loss: 5.625942]\n",
      "7940 [Discriminator loss: 0.245009, acc.: 89.06%] [Generator loss: 6.232900]\n",
      "7941 [Discriminator loss: 0.160664, acc.: 96.88%] [Generator loss: 6.198642]\n",
      "7942 [Discriminator loss: 0.221335, acc.: 93.75%] [Generator loss: 5.441034]\n",
      "7943 [Discriminator loss: 0.072099, acc.: 98.44%] [Generator loss: 6.029386]\n",
      "7944 [Discriminator loss: 0.138824, acc.: 92.19%] [Generator loss: 5.122343]\n",
      "7945 [Discriminator loss: 0.131519, acc.: 92.19%] [Generator loss: 5.801233]\n",
      "7946 [Discriminator loss: 0.087550, acc.: 98.44%] [Generator loss: 5.352504]\n",
      "7947 [Discriminator loss: 0.158910, acc.: 93.75%] [Generator loss: 4.885220]\n",
      "7948 [Discriminator loss: 0.116059, acc.: 96.88%] [Generator loss: 4.882648]\n",
      "7949 [Discriminator loss: 0.305283, acc.: 89.06%] [Generator loss: 5.112529]\n",
      "7950 [Discriminator loss: 0.224728, acc.: 93.75%] [Generator loss: 6.904538]\n",
      "7951 [Discriminator loss: 0.213586, acc.: 89.06%] [Generator loss: 5.066845]\n",
      "7952 [Discriminator loss: 0.091663, acc.: 96.88%] [Generator loss: 5.732698]\n",
      "7953 [Discriminator loss: 0.069695, acc.: 96.88%] [Generator loss: 5.637774]\n",
      "7954 [Discriminator loss: 0.049166, acc.: 98.44%] [Generator loss: 6.655142]\n",
      "7955 [Discriminator loss: 0.152662, acc.: 96.88%] [Generator loss: 5.524677]\n",
      "7956 [Discriminator loss: 0.206275, acc.: 95.31%] [Generator loss: 5.961019]\n",
      "7957 [Discriminator loss: 0.111442, acc.: 95.31%] [Generator loss: 5.298605]\n",
      "7958 [Discriminator loss: 0.293271, acc.: 85.94%] [Generator loss: 5.085427]\n",
      "7959 [Discriminator loss: 0.116446, acc.: 93.75%] [Generator loss: 6.547852]\n",
      "7960 [Discriminator loss: 0.092812, acc.: 96.88%] [Generator loss: 5.601383]\n",
      "7961 [Discriminator loss: 0.076524, acc.: 96.88%] [Generator loss: 6.505670]\n",
      "7962 [Discriminator loss: 0.063856, acc.: 98.44%] [Generator loss: 6.486717]\n",
      "7963 [Discriminator loss: 0.141548, acc.: 95.31%] [Generator loss: 5.167263]\n",
      "7964 [Discriminator loss: 0.154721, acc.: 95.31%] [Generator loss: 6.514200]\n",
      "7965 [Discriminator loss: 0.127646, acc.: 95.31%] [Generator loss: 5.702881]\n",
      "7966 [Discriminator loss: 0.089188, acc.: 96.88%] [Generator loss: 5.477417]\n",
      "7967 [Discriminator loss: 0.182723, acc.: 95.31%] [Generator loss: 5.232766]\n",
      "7968 [Discriminator loss: 0.144519, acc.: 92.19%] [Generator loss: 6.880476]\n",
      "7969 [Discriminator loss: 0.186185, acc.: 95.31%] [Generator loss: 5.490227]\n",
      "7970 [Discriminator loss: 0.321723, acc.: 89.06%] [Generator loss: 6.315631]\n",
      "7971 [Discriminator loss: 0.045573, acc.: 98.44%] [Generator loss: 9.113269]\n",
      "7972 [Discriminator loss: 0.320762, acc.: 84.38%] [Generator loss: 5.947929]\n",
      "7973 [Discriminator loss: 0.139060, acc.: 90.62%] [Generator loss: 6.799534]\n",
      "7974 [Discriminator loss: 0.175549, acc.: 93.75%] [Generator loss: 6.038347]\n",
      "7975 [Discriminator loss: 0.077415, acc.: 98.44%] [Generator loss: 5.289318]\n",
      "7976 [Discriminator loss: 0.110253, acc.: 95.31%] [Generator loss: 6.169643]\n",
      "7977 [Discriminator loss: 0.102934, acc.: 96.88%] [Generator loss: 6.068881]\n",
      "7978 [Discriminator loss: 0.199154, acc.: 93.75%] [Generator loss: 5.752727]\n",
      "7979 [Discriminator loss: 0.084482, acc.: 98.44%] [Generator loss: 5.562761]\n",
      "7980 [Discriminator loss: 0.076249, acc.: 96.88%] [Generator loss: 4.519926]\n",
      "7981 [Discriminator loss: 0.348577, acc.: 84.38%] [Generator loss: 7.241987]\n",
      "7982 [Discriminator loss: 0.146750, acc.: 96.88%] [Generator loss: 7.960321]\n",
      "7983 [Discriminator loss: 0.498388, acc.: 81.25%] [Generator loss: 5.113368]\n",
      "7984 [Discriminator loss: 0.084499, acc.: 95.31%] [Generator loss: 5.333672]\n",
      "7985 [Discriminator loss: 0.160577, acc.: 93.75%] [Generator loss: 6.072562]\n",
      "7986 [Discriminator loss: 0.233883, acc.: 89.06%] [Generator loss: 7.141314]\n",
      "7987 [Discriminator loss: 0.249441, acc.: 89.06%] [Generator loss: 4.357687]\n",
      "7988 [Discriminator loss: 0.294091, acc.: 85.94%] [Generator loss: 6.360198]\n",
      "7989 [Discriminator loss: 0.161929, acc.: 92.19%] [Generator loss: 6.447648]\n",
      "7990 [Discriminator loss: 0.284160, acc.: 85.94%] [Generator loss: 5.554624]\n",
      "7991 [Discriminator loss: 0.066383, acc.: 98.44%] [Generator loss: 5.679420]\n",
      "7992 [Discriminator loss: 0.148300, acc.: 95.31%] [Generator loss: 4.896746]\n",
      "7993 [Discriminator loss: 0.076323, acc.: 96.88%] [Generator loss: 6.590897]\n",
      "7994 [Discriminator loss: 0.315278, acc.: 82.81%] [Generator loss: 4.508347]\n",
      "7995 [Discriminator loss: 0.137991, acc.: 95.31%] [Generator loss: 5.944823]\n",
      "7996 [Discriminator loss: 0.107942, acc.: 96.88%] [Generator loss: 5.179609]\n",
      "7997 [Discriminator loss: 0.293723, acc.: 87.50%] [Generator loss: 6.174253]\n",
      "7998 [Discriminator loss: 0.060132, acc.: 98.44%] [Generator loss: 6.287013]\n",
      "7999 [Discriminator loss: 0.134624, acc.: 92.19%] [Generator loss: 5.344804]\n",
      "8000 [Discriminator loss: 0.097273, acc.: 96.88%] [Generator loss: 5.668766]\n",
      "8001 [Discriminator loss: 0.118981, acc.: 95.31%] [Generator loss: 5.691994]\n",
      "8002 [Discriminator loss: 0.184839, acc.: 92.19%] [Generator loss: 7.184311]\n",
      "8003 [Discriminator loss: 0.119324, acc.: 93.75%] [Generator loss: 7.287695]\n",
      "8004 [Discriminator loss: 0.095172, acc.: 96.88%] [Generator loss: 5.392157]\n",
      "8005 [Discriminator loss: 0.226200, acc.: 90.62%] [Generator loss: 5.540298]\n",
      "8006 [Discriminator loss: 0.175869, acc.: 89.06%] [Generator loss: 5.856778]\n",
      "8007 [Discriminator loss: 0.197738, acc.: 96.88%] [Generator loss: 5.734458]\n",
      "8008 [Discriminator loss: 0.200808, acc.: 92.19%] [Generator loss: 5.940583]\n",
      "8009 [Discriminator loss: 0.087773, acc.: 98.44%] [Generator loss: 6.604899]\n",
      "8010 [Discriminator loss: 0.304877, acc.: 87.50%] [Generator loss: 4.601278]\n",
      "8011 [Discriminator loss: 0.187170, acc.: 92.19%] [Generator loss: 7.214015]\n",
      "8012 [Discriminator loss: 0.289822, acc.: 85.94%] [Generator loss: 5.204110]\n",
      "8013 [Discriminator loss: 0.287691, acc.: 85.94%] [Generator loss: 6.528653]\n",
      "8014 [Discriminator loss: 0.221144, acc.: 92.19%] [Generator loss: 6.128681]\n",
      "8015 [Discriminator loss: 0.194627, acc.: 92.19%] [Generator loss: 5.963890]\n",
      "8016 [Discriminator loss: 0.155664, acc.: 96.88%] [Generator loss: 6.391457]\n",
      "8017 [Discriminator loss: 0.307138, acc.: 82.81%] [Generator loss: 4.453793]\n",
      "8018 [Discriminator loss: 0.074420, acc.: 98.44%] [Generator loss: 5.287907]\n",
      "8019 [Discriminator loss: 0.114573, acc.: 96.88%] [Generator loss: 5.955173]\n",
      "8020 [Discriminator loss: 0.123838, acc.: 96.88%] [Generator loss: 7.352090]\n",
      "8021 [Discriminator loss: 0.186535, acc.: 92.19%] [Generator loss: 6.354039]\n",
      "8022 [Discriminator loss: 0.212898, acc.: 92.19%] [Generator loss: 6.258329]\n",
      "8023 [Discriminator loss: 0.489180, acc.: 85.94%] [Generator loss: 6.491210]\n",
      "8024 [Discriminator loss: 0.088102, acc.: 96.88%] [Generator loss: 6.393861]\n",
      "8025 [Discriminator loss: 0.191967, acc.: 93.75%] [Generator loss: 5.440916]\n",
      "8026 [Discriminator loss: 0.120818, acc.: 96.88%] [Generator loss: 5.109527]\n",
      "8027 [Discriminator loss: 0.135834, acc.: 93.75%] [Generator loss: 6.011366]\n",
      "8028 [Discriminator loss: 0.223453, acc.: 92.19%] [Generator loss: 4.943104]\n",
      "8029 [Discriminator loss: 0.450789, acc.: 81.25%] [Generator loss: 7.190794]\n",
      "8030 [Discriminator loss: 0.182857, acc.: 93.75%] [Generator loss: 7.821716]\n",
      "8031 [Discriminator loss: 0.328801, acc.: 87.50%] [Generator loss: 4.411532]\n",
      "8032 [Discriminator loss: 0.142902, acc.: 92.19%] [Generator loss: 5.523118]\n",
      "8033 [Discriminator loss: 0.069070, acc.: 98.44%] [Generator loss: 6.069742]\n",
      "8034 [Discriminator loss: 0.119870, acc.: 95.31%] [Generator loss: 6.563451]\n",
      "8035 [Discriminator loss: 0.157243, acc.: 95.31%] [Generator loss: 5.272198]\n",
      "8036 [Discriminator loss: 0.309954, acc.: 85.94%] [Generator loss: 5.783370]\n",
      "8037 [Discriminator loss: 0.091409, acc.: 96.88%] [Generator loss: 5.786047]\n",
      "8038 [Discriminator loss: 0.480785, acc.: 81.25%] [Generator loss: 4.754678]\n",
      "8039 [Discriminator loss: 0.201663, acc.: 92.19%] [Generator loss: 5.795058]\n",
      "8040 [Discriminator loss: 0.359821, acc.: 87.50%] [Generator loss: 5.697547]\n",
      "8041 [Discriminator loss: 0.204968, acc.: 90.62%] [Generator loss: 7.115944]\n",
      "8042 [Discriminator loss: 0.155799, acc.: 93.75%] [Generator loss: 5.907756]\n",
      "8043 [Discriminator loss: 0.132083, acc.: 95.31%] [Generator loss: 5.629538]\n",
      "8044 [Discriminator loss: 0.213722, acc.: 89.06%] [Generator loss: 5.402921]\n",
      "8045 [Discriminator loss: 0.097535, acc.: 98.44%] [Generator loss: 6.039304]\n",
      "8046 [Discriminator loss: 0.287277, acc.: 85.94%] [Generator loss: 5.993093]\n",
      "8047 [Discriminator loss: 0.169765, acc.: 96.88%] [Generator loss: 6.280484]\n",
      "8048 [Discriminator loss: 0.150682, acc.: 93.75%] [Generator loss: 5.605084]\n",
      "8049 [Discriminator loss: 0.136271, acc.: 95.31%] [Generator loss: 5.097030]\n",
      "8050 [Discriminator loss: 0.121951, acc.: 96.88%] [Generator loss: 5.764212]\n",
      "8051 [Discriminator loss: 0.191956, acc.: 92.19%] [Generator loss: 6.292919]\n",
      "8052 [Discriminator loss: 0.105231, acc.: 93.75%] [Generator loss: 6.709205]\n",
      "8053 [Discriminator loss: 0.110500, acc.: 96.88%] [Generator loss: 4.738436]\n",
      "8054 [Discriminator loss: 0.023628, acc.: 100.00%] [Generator loss: 5.075635]\n",
      "8055 [Discriminator loss: 0.183537, acc.: 92.19%] [Generator loss: 6.059255]\n",
      "8056 [Discriminator loss: 0.062939, acc.: 98.44%] [Generator loss: 6.973598]\n",
      "8057 [Discriminator loss: 0.256125, acc.: 92.19%] [Generator loss: 5.843412]\n",
      "8058 [Discriminator loss: 0.140886, acc.: 96.88%] [Generator loss: 6.016109]\n",
      "8059 [Discriminator loss: 0.076824, acc.: 98.44%] [Generator loss: 6.396558]\n",
      "8060 [Discriminator loss: 0.098613, acc.: 96.88%] [Generator loss: 4.839704]\n",
      "8061 [Discriminator loss: 0.045643, acc.: 100.00%] [Generator loss: 6.233178]\n",
      "8062 [Discriminator loss: 0.252871, acc.: 93.75%] [Generator loss: 5.305880]\n",
      "8063 [Discriminator loss: 0.045659, acc.: 100.00%] [Generator loss: 6.003956]\n",
      "8064 [Discriminator loss: 0.175057, acc.: 90.62%] [Generator loss: 4.307355]\n",
      "8065 [Discriminator loss: 0.153599, acc.: 92.19%] [Generator loss: 5.762897]\n",
      "8066 [Discriminator loss: 0.132096, acc.: 95.31%] [Generator loss: 6.976503]\n",
      "8067 [Discriminator loss: 0.142597, acc.: 93.75%] [Generator loss: 6.077067]\n",
      "8068 [Discriminator loss: 0.179777, acc.: 90.62%] [Generator loss: 5.353919]\n",
      "8069 [Discriminator loss: 0.091399, acc.: 98.44%] [Generator loss: 4.652987]\n",
      "8070 [Discriminator loss: 0.166470, acc.: 95.31%] [Generator loss: 5.847580]\n",
      "8071 [Discriminator loss: 0.243203, acc.: 92.19%] [Generator loss: 5.472677]\n",
      "8072 [Discriminator loss: 0.053685, acc.: 98.44%] [Generator loss: 6.534347]\n",
      "8073 [Discriminator loss: 0.157568, acc.: 93.75%] [Generator loss: 6.814755]\n",
      "8074 [Discriminator loss: 0.200505, acc.: 90.62%] [Generator loss: 5.797350]\n",
      "8075 [Discriminator loss: 0.341132, acc.: 89.06%] [Generator loss: 6.314984]\n",
      "8076 [Discriminator loss: 0.069119, acc.: 96.88%] [Generator loss: 6.039787]\n",
      "8077 [Discriminator loss: 0.271583, acc.: 90.62%] [Generator loss: 5.423431]\n",
      "8078 [Discriminator loss: 0.090708, acc.: 95.31%] [Generator loss: 5.931960]\n",
      "8079 [Discriminator loss: 0.306752, acc.: 90.62%] [Generator loss: 6.060505]\n",
      "8080 [Discriminator loss: 0.178911, acc.: 93.75%] [Generator loss: 5.556142]\n",
      "8081 [Discriminator loss: 0.215983, acc.: 92.19%] [Generator loss: 6.821439]\n",
      "8082 [Discriminator loss: 0.068164, acc.: 96.88%] [Generator loss: 5.875698]\n",
      "8083 [Discriminator loss: 0.313621, acc.: 89.06%] [Generator loss: 5.547007]\n",
      "8084 [Discriminator loss: 0.161659, acc.: 89.06%] [Generator loss: 6.426455]\n",
      "8085 [Discriminator loss: 0.371069, acc.: 81.25%] [Generator loss: 4.802627]\n",
      "8086 [Discriminator loss: 0.079338, acc.: 96.88%] [Generator loss: 7.040657]\n",
      "8087 [Discriminator loss: 0.096153, acc.: 96.88%] [Generator loss: 5.367037]\n",
      "8088 [Discriminator loss: 0.330085, acc.: 90.62%] [Generator loss: 5.745779]\n",
      "8089 [Discriminator loss: 0.209774, acc.: 90.62%] [Generator loss: 7.133145]\n",
      "8090 [Discriminator loss: 0.385826, acc.: 85.94%] [Generator loss: 5.495395]\n",
      "8091 [Discriminator loss: 0.081533, acc.: 95.31%] [Generator loss: 6.172740]\n",
      "8092 [Discriminator loss: 0.180914, acc.: 92.19%] [Generator loss: 6.577582]\n",
      "8093 [Discriminator loss: 0.043773, acc.: 98.44%] [Generator loss: 6.323438]\n",
      "8094 [Discriminator loss: 0.126315, acc.: 96.88%] [Generator loss: 5.744592]\n",
      "8095 [Discriminator loss: 0.217315, acc.: 89.06%] [Generator loss: 5.670248]\n",
      "8096 [Discriminator loss: 0.072870, acc.: 98.44%] [Generator loss: 6.252421]\n",
      "8097 [Discriminator loss: 0.154113, acc.: 89.06%] [Generator loss: 5.735690]\n",
      "8098 [Discriminator loss: 0.088261, acc.: 96.88%] [Generator loss: 5.084574]\n",
      "8099 [Discriminator loss: 0.238583, acc.: 87.50%] [Generator loss: 7.455830]\n",
      "8100 [Discriminator loss: 0.067565, acc.: 95.31%] [Generator loss: 7.663736]\n",
      "8101 [Discriminator loss: 0.374356, acc.: 84.38%] [Generator loss: 5.560643]\n",
      "8102 [Discriminator loss: 0.172468, acc.: 92.19%] [Generator loss: 5.103240]\n",
      "8103 [Discriminator loss: 0.171982, acc.: 92.19%] [Generator loss: 6.310391]\n",
      "8104 [Discriminator loss: 0.200287, acc.: 93.75%] [Generator loss: 5.653304]\n",
      "8105 [Discriminator loss: 0.129890, acc.: 93.75%] [Generator loss: 6.388463]\n",
      "8106 [Discriminator loss: 0.391290, acc.: 84.38%] [Generator loss: 6.071444]\n",
      "8107 [Discriminator loss: 0.514638, acc.: 79.69%] [Generator loss: 6.204995]\n",
      "8108 [Discriminator loss: 0.082948, acc.: 98.44%] [Generator loss: 7.853952]\n",
      "8109 [Discriminator loss: 0.104372, acc.: 95.31%] [Generator loss: 6.392112]\n",
      "8110 [Discriminator loss: 0.271422, acc.: 84.38%] [Generator loss: 4.896249]\n",
      "8111 [Discriminator loss: 0.112214, acc.: 93.75%] [Generator loss: 4.787448]\n",
      "8112 [Discriminator loss: 0.167417, acc.: 93.75%] [Generator loss: 6.948323]\n",
      "8113 [Discriminator loss: 0.233782, acc.: 87.50%] [Generator loss: 5.944437]\n",
      "8114 [Discriminator loss: 0.188141, acc.: 92.19%] [Generator loss: 5.074170]\n",
      "8115 [Discriminator loss: 0.045172, acc.: 100.00%] [Generator loss: 5.221438]\n",
      "8116 [Discriminator loss: 0.158726, acc.: 95.31%] [Generator loss: 5.455697]\n",
      "8117 [Discriminator loss: 0.179862, acc.: 92.19%] [Generator loss: 6.018462]\n",
      "8118 [Discriminator loss: 0.079115, acc.: 96.88%] [Generator loss: 6.894362]\n",
      "8119 [Discriminator loss: 0.357344, acc.: 85.94%] [Generator loss: 4.118479]\n",
      "8120 [Discriminator loss: 0.255564, acc.: 90.62%] [Generator loss: 7.784084]\n",
      "8121 [Discriminator loss: 0.034560, acc.: 100.00%] [Generator loss: 8.384949]\n",
      "8122 [Discriminator loss: 0.263014, acc.: 87.50%] [Generator loss: 4.995346]\n",
      "8123 [Discriminator loss: 0.289507, acc.: 87.50%] [Generator loss: 5.681858]\n",
      "8124 [Discriminator loss: 0.087683, acc.: 95.31%] [Generator loss: 6.364257]\n",
      "8125 [Discriminator loss: 0.092906, acc.: 95.31%] [Generator loss: 5.963444]\n",
      "8126 [Discriminator loss: 0.098312, acc.: 96.88%] [Generator loss: 5.409061]\n",
      "8127 [Discriminator loss: 0.186094, acc.: 93.75%] [Generator loss: 6.736406]\n",
      "8128 [Discriminator loss: 0.456096, acc.: 78.12%] [Generator loss: 6.308318]\n",
      "8129 [Discriminator loss: 0.082146, acc.: 98.44%] [Generator loss: 6.053312]\n",
      "8130 [Discriminator loss: 0.176943, acc.: 95.31%] [Generator loss: 5.243979]\n",
      "8131 [Discriminator loss: 0.189108, acc.: 93.75%] [Generator loss: 7.034617]\n",
      "8132 [Discriminator loss: 0.142442, acc.: 93.75%] [Generator loss: 5.889234]\n",
      "8133 [Discriminator loss: 0.170275, acc.: 92.19%] [Generator loss: 4.388018]\n",
      "8134 [Discriminator loss: 0.211120, acc.: 90.62%] [Generator loss: 6.311181]\n",
      "8135 [Discriminator loss: 0.096708, acc.: 93.75%] [Generator loss: 7.262309]\n",
      "8136 [Discriminator loss: 0.160927, acc.: 93.75%] [Generator loss: 6.462094]\n",
      "8137 [Discriminator loss: 0.143052, acc.: 95.31%] [Generator loss: 5.080122]\n",
      "8138 [Discriminator loss: 0.063272, acc.: 96.88%] [Generator loss: 5.563133]\n",
      "8139 [Discriminator loss: 0.137065, acc.: 95.31%] [Generator loss: 5.790112]\n",
      "8140 [Discriminator loss: 0.072530, acc.: 98.44%] [Generator loss: 5.432574]\n",
      "8141 [Discriminator loss: 0.215208, acc.: 87.50%] [Generator loss: 5.715701]\n",
      "8142 [Discriminator loss: 0.086035, acc.: 96.88%] [Generator loss: 4.258356]\n",
      "8143 [Discriminator loss: 0.257389, acc.: 92.19%] [Generator loss: 7.063718]\n",
      "8144 [Discriminator loss: 0.327837, acc.: 84.38%] [Generator loss: 5.786481]\n",
      "8145 [Discriminator loss: 0.213170, acc.: 89.06%] [Generator loss: 6.555889]\n",
      "8146 [Discriminator loss: 0.166010, acc.: 92.19%] [Generator loss: 6.240992]\n",
      "8147 [Discriminator loss: 0.110587, acc.: 93.75%] [Generator loss: 5.337632]\n",
      "8148 [Discriminator loss: 0.274196, acc.: 87.50%] [Generator loss: 5.555616]\n",
      "8149 [Discriminator loss: 0.100522, acc.: 95.31%] [Generator loss: 7.481087]\n",
      "8150 [Discriminator loss: 0.132838, acc.: 96.88%] [Generator loss: 5.558217]\n",
      "8151 [Discriminator loss: 0.041982, acc.: 100.00%] [Generator loss: 5.384422]\n",
      "8152 [Discriminator loss: 0.193776, acc.: 89.06%] [Generator loss: 5.896144]\n",
      "8153 [Discriminator loss: 0.207766, acc.: 87.50%] [Generator loss: 7.721317]\n",
      "8154 [Discriminator loss: 0.250641, acc.: 89.06%] [Generator loss: 6.612878]\n",
      "8155 [Discriminator loss: 0.236768, acc.: 90.62%] [Generator loss: 5.173180]\n",
      "8156 [Discriminator loss: 0.163750, acc.: 93.75%] [Generator loss: 5.527101]\n",
      "8157 [Discriminator loss: 0.176275, acc.: 93.75%] [Generator loss: 6.563029]\n",
      "8158 [Discriminator loss: 0.094060, acc.: 95.31%] [Generator loss: 6.334401]\n",
      "8159 [Discriminator loss: 0.281592, acc.: 89.06%] [Generator loss: 5.979935]\n",
      "8160 [Discriminator loss: 0.083701, acc.: 98.44%] [Generator loss: 6.163824]\n",
      "8161 [Discriminator loss: 0.258415, acc.: 90.62%] [Generator loss: 6.374363]\n",
      "8162 [Discriminator loss: 0.424109, acc.: 82.81%] [Generator loss: 6.053433]\n",
      "8163 [Discriminator loss: 0.164265, acc.: 93.75%] [Generator loss: 5.354342]\n",
      "8164 [Discriminator loss: 0.186179, acc.: 93.75%] [Generator loss: 5.738812]\n",
      "8165 [Discriminator loss: 0.167883, acc.: 93.75%] [Generator loss: 6.655770]\n",
      "8166 [Discriminator loss: 0.243402, acc.: 89.06%] [Generator loss: 7.209701]\n",
      "8167 [Discriminator loss: 0.178397, acc.: 92.19%] [Generator loss: 5.629734]\n",
      "8168 [Discriminator loss: 0.174728, acc.: 92.19%] [Generator loss: 4.770011]\n",
      "8169 [Discriminator loss: 0.238771, acc.: 89.06%] [Generator loss: 5.313286]\n",
      "8170 [Discriminator loss: 0.293756, acc.: 85.94%] [Generator loss: 6.604639]\n",
      "8171 [Discriminator loss: 0.284457, acc.: 89.06%] [Generator loss: 5.585932]\n",
      "8172 [Discriminator loss: 0.128660, acc.: 95.31%] [Generator loss: 6.870140]\n",
      "8173 [Discriminator loss: 0.269469, acc.: 89.06%] [Generator loss: 5.139320]\n",
      "8174 [Discriminator loss: 0.224060, acc.: 93.75%] [Generator loss: 7.198316]\n",
      "8175 [Discriminator loss: 0.132510, acc.: 96.88%] [Generator loss: 8.157685]\n",
      "8176 [Discriminator loss: 0.204438, acc.: 90.62%] [Generator loss: 6.859776]\n",
      "8177 [Discriminator loss: 0.053739, acc.: 100.00%] [Generator loss: 6.425380]\n",
      "8178 [Discriminator loss: 0.078040, acc.: 98.44%] [Generator loss: 5.675406]\n",
      "8179 [Discriminator loss: 0.081455, acc.: 100.00%] [Generator loss: 5.496989]\n",
      "8180 [Discriminator loss: 0.056847, acc.: 100.00%] [Generator loss: 5.689443]\n",
      "8181 [Discriminator loss: 0.254757, acc.: 89.06%] [Generator loss: 5.630007]\n",
      "8182 [Discriminator loss: 0.053521, acc.: 98.44%] [Generator loss: 6.471290]\n",
      "8183 [Discriminator loss: 0.369710, acc.: 87.50%] [Generator loss: 3.730145]\n",
      "8184 [Discriminator loss: 0.212113, acc.: 92.19%] [Generator loss: 7.132085]\n",
      "8185 [Discriminator loss: 0.084828, acc.: 96.88%] [Generator loss: 8.522326]\n",
      "8186 [Discriminator loss: 0.403190, acc.: 82.81%] [Generator loss: 3.992318]\n",
      "8187 [Discriminator loss: 0.402616, acc.: 82.81%] [Generator loss: 6.901324]\n",
      "8188 [Discriminator loss: 0.282274, acc.: 85.94%] [Generator loss: 7.357670]\n",
      "8189 [Discriminator loss: 0.231971, acc.: 92.19%] [Generator loss: 4.291898]\n",
      "8190 [Discriminator loss: 0.274394, acc.: 87.50%] [Generator loss: 8.825607]\n",
      "8191 [Discriminator loss: 0.213876, acc.: 92.19%] [Generator loss: 7.763758]\n",
      "8192 [Discriminator loss: 0.235259, acc.: 89.06%] [Generator loss: 6.019194]\n",
      "8193 [Discriminator loss: 0.090128, acc.: 96.88%] [Generator loss: 4.594318]\n",
      "8194 [Discriminator loss: 0.298156, acc.: 90.62%] [Generator loss: 6.448088]\n",
      "8195 [Discriminator loss: 0.271372, acc.: 87.50%] [Generator loss: 5.670111]\n",
      "8196 [Discriminator loss: 0.101216, acc.: 96.88%] [Generator loss: 5.420500]\n",
      "8197 [Discriminator loss: 0.127051, acc.: 93.75%] [Generator loss: 4.652356]\n",
      "8198 [Discriminator loss: 0.261563, acc.: 89.06%] [Generator loss: 4.303360]\n",
      "8199 [Discriminator loss: 0.141036, acc.: 93.75%] [Generator loss: 6.528839]\n",
      "8200 [Discriminator loss: 0.086206, acc.: 96.88%] [Generator loss: 7.676748]\n",
      "8201 [Discriminator loss: 0.243819, acc.: 90.62%] [Generator loss: 5.559978]\n",
      "8202 [Discriminator loss: 0.182953, acc.: 93.75%] [Generator loss: 7.078134]\n",
      "8203 [Discriminator loss: 0.181333, acc.: 92.19%] [Generator loss: 5.140642]\n",
      "8204 [Discriminator loss: 0.203340, acc.: 92.19%] [Generator loss: 6.419469]\n",
      "8205 [Discriminator loss: 0.084044, acc.: 98.44%] [Generator loss: 5.271517]\n",
      "8206 [Discriminator loss: 0.055430, acc.: 98.44%] [Generator loss: 5.243311]\n",
      "8207 [Discriminator loss: 0.211697, acc.: 93.75%] [Generator loss: 5.279911]\n",
      "8208 [Discriminator loss: 0.053958, acc.: 98.44%] [Generator loss: 5.215757]\n",
      "8209 [Discriminator loss: 0.402330, acc.: 82.81%] [Generator loss: 5.624185]\n",
      "8210 [Discriminator loss: 0.067089, acc.: 98.44%] [Generator loss: 7.655910]\n",
      "8211 [Discriminator loss: 0.219331, acc.: 90.62%] [Generator loss: 4.726218]\n",
      "8212 [Discriminator loss: 0.253946, acc.: 93.75%] [Generator loss: 6.987070]\n",
      "8213 [Discriminator loss: 0.194813, acc.: 93.75%] [Generator loss: 4.764130]\n",
      "8214 [Discriminator loss: 0.195279, acc.: 93.75%] [Generator loss: 5.946716]\n",
      "8215 [Discriminator loss: 0.095993, acc.: 96.88%] [Generator loss: 5.568927]\n",
      "8216 [Discriminator loss: 0.097971, acc.: 95.31%] [Generator loss: 5.821268]\n",
      "8217 [Discriminator loss: 0.286541, acc.: 85.94%] [Generator loss: 5.209451]\n",
      "8218 [Discriminator loss: 0.037570, acc.: 100.00%] [Generator loss: 5.757992]\n",
      "8219 [Discriminator loss: 0.031960, acc.: 100.00%] [Generator loss: 4.283533]\n",
      "8220 [Discriminator loss: 0.140535, acc.: 93.75%] [Generator loss: 5.427800]\n",
      "8221 [Discriminator loss: 0.209715, acc.: 90.62%] [Generator loss: 7.574142]\n",
      "8222 [Discriminator loss: 0.202223, acc.: 89.06%] [Generator loss: 5.945255]\n",
      "8223 [Discriminator loss: 0.348109, acc.: 85.94%] [Generator loss: 4.594760]\n",
      "8224 [Discriminator loss: 0.065831, acc.: 96.88%] [Generator loss: 4.975654]\n",
      "8225 [Discriminator loss: 0.130815, acc.: 95.31%] [Generator loss: 5.363670]\n",
      "8226 [Discriminator loss: 0.141032, acc.: 96.88%] [Generator loss: 5.732599]\n",
      "8227 [Discriminator loss: 0.220744, acc.: 89.06%] [Generator loss: 4.936024]\n",
      "8228 [Discriminator loss: 0.260004, acc.: 87.50%] [Generator loss: 6.706077]\n",
      "8229 [Discriminator loss: 0.162313, acc.: 93.75%] [Generator loss: 5.455820]\n",
      "8230 [Discriminator loss: 0.137019, acc.: 93.75%] [Generator loss: 4.997788]\n",
      "8231 [Discriminator loss: 0.183321, acc.: 92.19%] [Generator loss: 6.067935]\n",
      "8232 [Discriminator loss: 0.216473, acc.: 90.62%] [Generator loss: 5.069683]\n",
      "8233 [Discriminator loss: 0.158560, acc.: 92.19%] [Generator loss: 5.735019]\n",
      "8234 [Discriminator loss: 0.182944, acc.: 92.19%] [Generator loss: 6.282223]\n",
      "8235 [Discriminator loss: 0.102984, acc.: 95.31%] [Generator loss: 5.996367]\n",
      "8236 [Discriminator loss: 0.077442, acc.: 98.44%] [Generator loss: 6.387361]\n",
      "8237 [Discriminator loss: 0.205916, acc.: 92.19%] [Generator loss: 6.552691]\n",
      "8238 [Discriminator loss: 0.056127, acc.: 98.44%] [Generator loss: 6.747971]\n",
      "8239 [Discriminator loss: 0.094789, acc.: 95.31%] [Generator loss: 5.444304]\n",
      "8240 [Discriminator loss: 0.320226, acc.: 89.06%] [Generator loss: 6.619412]\n",
      "8241 [Discriminator loss: 0.182387, acc.: 93.75%] [Generator loss: 7.065449]\n",
      "8242 [Discriminator loss: 0.062423, acc.: 100.00%] [Generator loss: 6.318417]\n",
      "8243 [Discriminator loss: 0.177565, acc.: 89.06%] [Generator loss: 4.356445]\n",
      "8244 [Discriminator loss: 0.154040, acc.: 93.75%] [Generator loss: 6.444738]\n",
      "8245 [Discriminator loss: 0.087865, acc.: 96.88%] [Generator loss: 5.683271]\n",
      "8246 [Discriminator loss: 0.166604, acc.: 90.62%] [Generator loss: 7.952723]\n",
      "8247 [Discriminator loss: 0.090723, acc.: 96.88%] [Generator loss: 6.699113]\n",
      "8248 [Discriminator loss: 0.138600, acc.: 92.19%] [Generator loss: 5.252439]\n",
      "8249 [Discriminator loss: 0.072218, acc.: 98.44%] [Generator loss: 5.397054]\n",
      "8250 [Discriminator loss: 0.109549, acc.: 95.31%] [Generator loss: 5.637703]\n",
      "8251 [Discriminator loss: 0.208389, acc.: 87.50%] [Generator loss: 5.549971]\n",
      "8252 [Discriminator loss: 0.065806, acc.: 98.44%] [Generator loss: 7.034729]\n",
      "8253 [Discriminator loss: 0.084235, acc.: 96.88%] [Generator loss: 5.516839]\n",
      "8254 [Discriminator loss: 0.283642, acc.: 87.50%] [Generator loss: 7.008153]\n",
      "8255 [Discriminator loss: 0.207809, acc.: 92.19%] [Generator loss: 6.130268]\n",
      "8256 [Discriminator loss: 0.144997, acc.: 95.31%] [Generator loss: 6.074034]\n",
      "8257 [Discriminator loss: 0.222257, acc.: 92.19%] [Generator loss: 7.461122]\n",
      "8258 [Discriminator loss: 0.128720, acc.: 95.31%] [Generator loss: 8.011055]\n",
      "8259 [Discriminator loss: 0.121327, acc.: 92.19%] [Generator loss: 5.888933]\n",
      "8260 [Discriminator loss: 0.180184, acc.: 92.19%] [Generator loss: 5.524302]\n",
      "8261 [Discriminator loss: 0.085481, acc.: 100.00%] [Generator loss: 5.975119]\n",
      "8262 [Discriminator loss: 0.071116, acc.: 96.88%] [Generator loss: 5.688517]\n",
      "8263 [Discriminator loss: 0.147066, acc.: 95.31%] [Generator loss: 6.681110]\n",
      "8264 [Discriminator loss: 0.117560, acc.: 95.31%] [Generator loss: 6.940134]\n",
      "8265 [Discriminator loss: 0.225577, acc.: 89.06%] [Generator loss: 5.151643]\n",
      "8266 [Discriminator loss: 0.093735, acc.: 96.88%] [Generator loss: 6.303742]\n",
      "8267 [Discriminator loss: 0.072160, acc.: 100.00%] [Generator loss: 5.879766]\n",
      "8268 [Discriminator loss: 0.152284, acc.: 95.31%] [Generator loss: 4.854248]\n",
      "8269 [Discriminator loss: 0.230013, acc.: 84.38%] [Generator loss: 6.233047]\n",
      "8270 [Discriminator loss: 0.156631, acc.: 90.62%] [Generator loss: 5.589626]\n",
      "8271 [Discriminator loss: 0.188262, acc.: 96.88%] [Generator loss: 5.597251]\n",
      "8272 [Discriminator loss: 0.123335, acc.: 95.31%] [Generator loss: 6.496132]\n",
      "8273 [Discriminator loss: 0.072379, acc.: 96.88%] [Generator loss: 7.036734]\n",
      "8274 [Discriminator loss: 0.113956, acc.: 93.75%] [Generator loss: 5.502301]\n",
      "8275 [Discriminator loss: 0.331420, acc.: 89.06%] [Generator loss: 5.575531]\n",
      "8276 [Discriminator loss: 0.046377, acc.: 98.44%] [Generator loss: 6.192661]\n",
      "8277 [Discriminator loss: 0.228715, acc.: 93.75%] [Generator loss: 5.774360]\n",
      "8278 [Discriminator loss: 0.049287, acc.: 100.00%] [Generator loss: 5.568250]\n",
      "8279 [Discriminator loss: 0.064844, acc.: 98.44%] [Generator loss: 6.020571]\n",
      "8280 [Discriminator loss: 0.202136, acc.: 90.62%] [Generator loss: 5.387419]\n",
      "8281 [Discriminator loss: 0.159192, acc.: 90.62%] [Generator loss: 5.507207]\n",
      "8282 [Discriminator loss: 0.123029, acc.: 95.31%] [Generator loss: 4.236318]\n",
      "8283 [Discriminator loss: 0.129537, acc.: 93.75%] [Generator loss: 4.710774]\n",
      "8284 [Discriminator loss: 0.167183, acc.: 93.75%] [Generator loss: 6.569001]\n",
      "8285 [Discriminator loss: 0.091168, acc.: 95.31%] [Generator loss: 6.511110]\n",
      "8286 [Discriminator loss: 0.176111, acc.: 93.75%] [Generator loss: 6.927039]\n",
      "8287 [Discriminator loss: 0.244082, acc.: 85.94%] [Generator loss: 5.274552]\n",
      "8288 [Discriminator loss: 0.141452, acc.: 93.75%] [Generator loss: 5.436953]\n",
      "8289 [Discriminator loss: 0.109663, acc.: 93.75%] [Generator loss: 6.384333]\n",
      "8290 [Discriminator loss: 0.253247, acc.: 87.50%] [Generator loss: 5.742332]\n",
      "8291 [Discriminator loss: 0.354373, acc.: 89.06%] [Generator loss: 7.648657]\n",
      "8292 [Discriminator loss: 0.069192, acc.: 96.88%] [Generator loss: 6.793653]\n",
      "8293 [Discriminator loss: 0.090778, acc.: 96.88%] [Generator loss: 6.129130]\n",
      "8294 [Discriminator loss: 0.136919, acc.: 95.31%] [Generator loss: 4.037224]\n",
      "8295 [Discriminator loss: 0.190146, acc.: 92.19%] [Generator loss: 4.876929]\n",
      "8296 [Discriminator loss: 0.179815, acc.: 92.19%] [Generator loss: 6.207553]\n",
      "8297 [Discriminator loss: 0.067647, acc.: 98.44%] [Generator loss: 6.714332]\n",
      "8298 [Discriminator loss: 0.223034, acc.: 92.19%] [Generator loss: 5.548728]\n",
      "8299 [Discriminator loss: 0.129483, acc.: 95.31%] [Generator loss: 6.067355]\n",
      "8300 [Discriminator loss: 0.227259, acc.: 92.19%] [Generator loss: 6.410038]\n",
      "8301 [Discriminator loss: 0.143267, acc.: 96.88%] [Generator loss: 5.847029]\n",
      "8302 [Discriminator loss: 0.102055, acc.: 98.44%] [Generator loss: 6.768503]\n",
      "8303 [Discriminator loss: 0.079564, acc.: 98.44%] [Generator loss: 5.788960]\n",
      "8304 [Discriminator loss: 0.197372, acc.: 90.62%] [Generator loss: 6.281694]\n",
      "8305 [Discriminator loss: 0.188919, acc.: 92.19%] [Generator loss: 5.758427]\n",
      "8306 [Discriminator loss: 0.092393, acc.: 96.88%] [Generator loss: 7.178378]\n",
      "8307 [Discriminator loss: 0.264621, acc.: 85.94%] [Generator loss: 6.814628]\n",
      "8308 [Discriminator loss: 0.197849, acc.: 90.62%] [Generator loss: 6.610452]\n",
      "8309 [Discriminator loss: 0.242597, acc.: 89.06%] [Generator loss: 5.498589]\n",
      "8310 [Discriminator loss: 0.087090, acc.: 95.31%] [Generator loss: 5.933763]\n",
      "8311 [Discriminator loss: 0.112926, acc.: 95.31%] [Generator loss: 5.632696]\n",
      "8312 [Discriminator loss: 0.200280, acc.: 93.75%] [Generator loss: 6.089615]\n",
      "8313 [Discriminator loss: 0.039661, acc.: 100.00%] [Generator loss: 5.423352]\n",
      "8314 [Discriminator loss: 0.286515, acc.: 89.06%] [Generator loss: 4.952961]\n",
      "8315 [Discriminator loss: 0.103382, acc.: 95.31%] [Generator loss: 5.170889]\n",
      "8316 [Discriminator loss: 0.151653, acc.: 92.19%] [Generator loss: 6.483990]\n",
      "8317 [Discriminator loss: 0.240913, acc.: 89.06%] [Generator loss: 6.013639]\n",
      "8318 [Discriminator loss: 0.042520, acc.: 100.00%] [Generator loss: 6.926503]\n",
      "8319 [Discriminator loss: 0.186138, acc.: 92.19%] [Generator loss: 5.036731]\n",
      "8320 [Discriminator loss: 0.056455, acc.: 98.44%] [Generator loss: 4.978951]\n",
      "8321 [Discriminator loss: 0.070767, acc.: 98.44%] [Generator loss: 4.488606]\n",
      "8322 [Discriminator loss: 0.281807, acc.: 89.06%] [Generator loss: 6.265531]\n",
      "8323 [Discriminator loss: 0.052531, acc.: 98.44%] [Generator loss: 6.567875]\n",
      "8324 [Discriminator loss: 0.139145, acc.: 93.75%] [Generator loss: 6.688661]\n",
      "8325 [Discriminator loss: 0.106753, acc.: 95.31%] [Generator loss: 5.404593]\n",
      "8326 [Discriminator loss: 0.190155, acc.: 95.31%] [Generator loss: 6.329072]\n",
      "8327 [Discriminator loss: 0.058400, acc.: 100.00%] [Generator loss: 6.205609]\n",
      "8328 [Discriminator loss: 0.140260, acc.: 95.31%] [Generator loss: 4.430771]\n",
      "8329 [Discriminator loss: 0.208982, acc.: 92.19%] [Generator loss: 5.990405]\n",
      "8330 [Discriminator loss: 0.256418, acc.: 93.75%] [Generator loss: 6.749808]\n",
      "8331 [Discriminator loss: 0.184143, acc.: 92.19%] [Generator loss: 5.332745]\n",
      "8332 [Discriminator loss: 0.154496, acc.: 95.31%] [Generator loss: 5.855781]\n",
      "8333 [Discriminator loss: 0.132782, acc.: 95.31%] [Generator loss: 4.587270]\n",
      "8334 [Discriminator loss: 0.069494, acc.: 96.88%] [Generator loss: 6.178821]\n",
      "8335 [Discriminator loss: 0.200074, acc.: 93.75%] [Generator loss: 7.311319]\n",
      "8336 [Discriminator loss: 0.088431, acc.: 98.44%] [Generator loss: 6.404812]\n",
      "8337 [Discriminator loss: 0.065458, acc.: 100.00%] [Generator loss: 5.894394]\n",
      "8338 [Discriminator loss: 0.132925, acc.: 93.75%] [Generator loss: 5.920969]\n",
      "8339 [Discriminator loss: 0.067447, acc.: 96.88%] [Generator loss: 5.821602]\n",
      "8340 [Discriminator loss: 0.249558, acc.: 90.62%] [Generator loss: 5.964863]\n",
      "8341 [Discriminator loss: 0.143032, acc.: 93.75%] [Generator loss: 5.357982]\n",
      "8342 [Discriminator loss: 0.136113, acc.: 93.75%] [Generator loss: 5.754227]\n",
      "8343 [Discriminator loss: 0.257545, acc.: 87.50%] [Generator loss: 7.313024]\n",
      "8344 [Discriminator loss: 0.032431, acc.: 100.00%] [Generator loss: 8.116228]\n",
      "8345 [Discriminator loss: 0.324953, acc.: 89.06%] [Generator loss: 4.295546]\n",
      "8346 [Discriminator loss: 0.126355, acc.: 96.88%] [Generator loss: 5.535602]\n",
      "8347 [Discriminator loss: 0.057836, acc.: 98.44%] [Generator loss: 6.909085]\n",
      "8348 [Discriminator loss: 0.095524, acc.: 95.31%] [Generator loss: 5.831346]\n",
      "8349 [Discriminator loss: 0.126595, acc.: 95.31%] [Generator loss: 5.732873]\n",
      "8350 [Discriminator loss: 0.314157, acc.: 87.50%] [Generator loss: 6.002097]\n",
      "8351 [Discriminator loss: 0.096439, acc.: 96.88%] [Generator loss: 6.768183]\n",
      "8352 [Discriminator loss: 0.204631, acc.: 90.62%] [Generator loss: 5.712505]\n",
      "8353 [Discriminator loss: 0.268079, acc.: 92.19%] [Generator loss: 5.325924]\n",
      "8354 [Discriminator loss: 0.201304, acc.: 92.19%] [Generator loss: 6.140459]\n",
      "8355 [Discriminator loss: 0.085563, acc.: 96.88%] [Generator loss: 6.351197]\n",
      "8356 [Discriminator loss: 0.077290, acc.: 96.88%] [Generator loss: 5.635600]\n",
      "8357 [Discriminator loss: 0.175329, acc.: 93.75%] [Generator loss: 4.619301]\n",
      "8358 [Discriminator loss: 0.228345, acc.: 93.75%] [Generator loss: 5.295998]\n",
      "8359 [Discriminator loss: 0.112140, acc.: 95.31%] [Generator loss: 6.305651]\n",
      "8360 [Discriminator loss: 0.405817, acc.: 84.38%] [Generator loss: 5.046566]\n",
      "8361 [Discriminator loss: 0.145540, acc.: 93.75%] [Generator loss: 6.179341]\n",
      "8362 [Discriminator loss: 0.107580, acc.: 95.31%] [Generator loss: 6.194553]\n",
      "8363 [Discriminator loss: 0.185321, acc.: 93.75%] [Generator loss: 5.150640]\n",
      "8364 [Discriminator loss: 0.311893, acc.: 87.50%] [Generator loss: 5.232079]\n",
      "8365 [Discriminator loss: 0.028521, acc.: 100.00%] [Generator loss: 5.548132]\n",
      "8366 [Discriminator loss: 0.165976, acc.: 90.62%] [Generator loss: 6.504824]\n",
      "8367 [Discriminator loss: 0.233183, acc.: 89.06%] [Generator loss: 5.325662]\n",
      "8368 [Discriminator loss: 0.060724, acc.: 98.44%] [Generator loss: 5.165415]\n",
      "8369 [Discriminator loss: 0.105592, acc.: 93.75%] [Generator loss: 6.540833]\n",
      "8370 [Discriminator loss: 0.122339, acc.: 95.31%] [Generator loss: 5.172589]\n",
      "8371 [Discriminator loss: 0.136918, acc.: 93.75%] [Generator loss: 5.240900]\n",
      "8372 [Discriminator loss: 0.189569, acc.: 90.62%] [Generator loss: 4.762427]\n",
      "8373 [Discriminator loss: 0.216231, acc.: 96.88%] [Generator loss: 6.184071]\n",
      "8374 [Discriminator loss: 0.131609, acc.: 95.31%] [Generator loss: 5.640280]\n",
      "8375 [Discriminator loss: 0.162021, acc.: 92.19%] [Generator loss: 6.376790]\n",
      "8376 [Discriminator loss: 0.183507, acc.: 90.62%] [Generator loss: 5.789649]\n",
      "8377 [Discriminator loss: 0.176452, acc.: 92.19%] [Generator loss: 6.808266]\n",
      "8378 [Discriminator loss: 0.198846, acc.: 87.50%] [Generator loss: 7.158695]\n",
      "8379 [Discriminator loss: 0.275384, acc.: 85.94%] [Generator loss: 5.945511]\n",
      "8380 [Discriminator loss: 0.065615, acc.: 98.44%] [Generator loss: 6.231039]\n",
      "8381 [Discriminator loss: 0.141540, acc.: 93.75%] [Generator loss: 5.380615]\n",
      "8382 [Discriminator loss: 0.300115, acc.: 89.06%] [Generator loss: 6.473522]\n",
      "8383 [Discriminator loss: 0.143794, acc.: 95.31%] [Generator loss: 6.196793]\n",
      "8384 [Discriminator loss: 0.087434, acc.: 95.31%] [Generator loss: 4.608246]\n",
      "8385 [Discriminator loss: 0.265974, acc.: 92.19%] [Generator loss: 4.676436]\n",
      "8386 [Discriminator loss: 0.146150, acc.: 93.75%] [Generator loss: 5.648291]\n",
      "8387 [Discriminator loss: 0.203056, acc.: 93.75%] [Generator loss: 5.933023]\n",
      "8388 [Discriminator loss: 0.076326, acc.: 96.88%] [Generator loss: 6.652917]\n",
      "8389 [Discriminator loss: 0.164464, acc.: 96.88%] [Generator loss: 5.436059]\n",
      "8390 [Discriminator loss: 0.153602, acc.: 93.75%] [Generator loss: 5.741634]\n",
      "8391 [Discriminator loss: 0.078908, acc.: 96.88%] [Generator loss: 6.185101]\n",
      "8392 [Discriminator loss: 0.116633, acc.: 95.31%] [Generator loss: 6.114361]\n",
      "8393 [Discriminator loss: 0.187574, acc.: 92.19%] [Generator loss: 6.443906]\n",
      "8394 [Discriminator loss: 0.063312, acc.: 98.44%] [Generator loss: 6.693412]\n",
      "8395 [Discriminator loss: 0.274090, acc.: 90.62%] [Generator loss: 5.398066]\n",
      "8396 [Discriminator loss: 0.091751, acc.: 95.31%] [Generator loss: 5.692881]\n",
      "8397 [Discriminator loss: 0.106968, acc.: 96.88%] [Generator loss: 5.688856]\n",
      "8398 [Discriminator loss: 0.284230, acc.: 90.62%] [Generator loss: 7.730831]\n",
      "8399 [Discriminator loss: 0.206043, acc.: 89.06%] [Generator loss: 5.740764]\n",
      "8400 [Discriminator loss: 0.174831, acc.: 93.75%] [Generator loss: 5.531790]\n",
      "8401 [Discriminator loss: 0.195695, acc.: 90.62%] [Generator loss: 6.071038]\n",
      "8402 [Discriminator loss: 0.116123, acc.: 95.31%] [Generator loss: 4.988688]\n",
      "8403 [Discriminator loss: 0.063746, acc.: 98.44%] [Generator loss: 6.261067]\n",
      "8404 [Discriminator loss: 0.096837, acc.: 95.31%] [Generator loss: 5.975128]\n",
      "8405 [Discriminator loss: 0.106469, acc.: 96.88%] [Generator loss: 7.045515]\n",
      "8406 [Discriminator loss: 0.137623, acc.: 96.88%] [Generator loss: 5.554344]\n",
      "8407 [Discriminator loss: 0.096301, acc.: 93.75%] [Generator loss: 6.824753]\n",
      "8408 [Discriminator loss: 0.038413, acc.: 98.44%] [Generator loss: 7.040340]\n",
      "8409 [Discriminator loss: 0.073565, acc.: 96.88%] [Generator loss: 6.369607]\n",
      "8410 [Discriminator loss: 0.166051, acc.: 92.19%] [Generator loss: 6.378328]\n",
      "8411 [Discriminator loss: 0.224365, acc.: 89.06%] [Generator loss: 5.997976]\n",
      "8412 [Discriminator loss: 0.033484, acc.: 100.00%] [Generator loss: 6.399293]\n",
      "8413 [Discriminator loss: 0.208251, acc.: 93.75%] [Generator loss: 5.927605]\n",
      "8414 [Discriminator loss: 0.176868, acc.: 92.19%] [Generator loss: 6.078545]\n",
      "8415 [Discriminator loss: 0.062376, acc.: 96.88%] [Generator loss: 6.021109]\n",
      "8416 [Discriminator loss: 0.166489, acc.: 90.62%] [Generator loss: 6.195406]\n",
      "8417 [Discriminator loss: 0.125877, acc.: 93.75%] [Generator loss: 6.278266]\n",
      "8418 [Discriminator loss: 0.082928, acc.: 98.44%] [Generator loss: 6.981810]\n",
      "8419 [Discriminator loss: 0.122581, acc.: 96.88%] [Generator loss: 6.668533]\n",
      "8420 [Discriminator loss: 0.192053, acc.: 95.31%] [Generator loss: 5.958090]\n",
      "8421 [Discriminator loss: 0.102371, acc.: 96.88%] [Generator loss: 6.772364]\n",
      "8422 [Discriminator loss: 0.098772, acc.: 95.31%] [Generator loss: 6.518803]\n",
      "8423 [Discriminator loss: 0.149337, acc.: 92.19%] [Generator loss: 5.290508]\n",
      "8424 [Discriminator loss: 0.068023, acc.: 96.88%] [Generator loss: 4.489019]\n",
      "8425 [Discriminator loss: 0.164692, acc.: 95.31%] [Generator loss: 6.126668]\n",
      "8426 [Discriminator loss: 0.195704, acc.: 90.62%] [Generator loss: 5.627417]\n",
      "8427 [Discriminator loss: 0.124007, acc.: 96.88%] [Generator loss: 7.197788]\n",
      "8428 [Discriminator loss: 0.102811, acc.: 96.88%] [Generator loss: 6.199306]\n",
      "8429 [Discriminator loss: 0.145311, acc.: 95.31%] [Generator loss: 6.146907]\n",
      "8430 [Discriminator loss: 0.093102, acc.: 96.88%] [Generator loss: 5.843025]\n",
      "8431 [Discriminator loss: 0.307475, acc.: 84.38%] [Generator loss: 6.941846]\n",
      "8432 [Discriminator loss: 0.057686, acc.: 98.44%] [Generator loss: 7.176153]\n",
      "8433 [Discriminator loss: 0.111618, acc.: 95.31%] [Generator loss: 6.557929]\n",
      "8434 [Discriminator loss: 0.149070, acc.: 96.88%] [Generator loss: 5.503076]\n",
      "8435 [Discriminator loss: 0.224578, acc.: 90.62%] [Generator loss: 6.052354]\n",
      "8436 [Discriminator loss: 0.062579, acc.: 98.44%] [Generator loss: 7.355089]\n",
      "8437 [Discriminator loss: 0.227978, acc.: 90.62%] [Generator loss: 5.257871]\n",
      "8438 [Discriminator loss: 0.256417, acc.: 87.50%] [Generator loss: 5.730101]\n",
      "8439 [Discriminator loss: 0.139239, acc.: 96.88%] [Generator loss: 6.758695]\n",
      "8440 [Discriminator loss: 0.297759, acc.: 87.50%] [Generator loss: 5.527497]\n",
      "8441 [Discriminator loss: 0.097728, acc.: 96.88%] [Generator loss: 6.431135]\n",
      "8442 [Discriminator loss: 0.187270, acc.: 90.62%] [Generator loss: 6.095865]\n",
      "8443 [Discriminator loss: 0.136563, acc.: 93.75%] [Generator loss: 5.946078]\n",
      "8444 [Discriminator loss: 0.212591, acc.: 93.75%] [Generator loss: 7.280380]\n",
      "8445 [Discriminator loss: 0.201146, acc.: 90.62%] [Generator loss: 4.928325]\n",
      "8446 [Discriminator loss: 0.131370, acc.: 92.19%] [Generator loss: 4.448362]\n",
      "8447 [Discriminator loss: 0.121810, acc.: 95.31%] [Generator loss: 5.439624]\n",
      "8448 [Discriminator loss: 0.081675, acc.: 96.88%] [Generator loss: 6.382141]\n",
      "8449 [Discriminator loss: 0.095594, acc.: 96.88%] [Generator loss: 6.322735]\n",
      "8450 [Discriminator loss: 0.103798, acc.: 95.31%] [Generator loss: 6.576666]\n",
      "8451 [Discriminator loss: 0.181579, acc.: 90.62%] [Generator loss: 5.760935]\n",
      "8452 [Discriminator loss: 0.331306, acc.: 89.06%] [Generator loss: 7.407211]\n",
      "8453 [Discriminator loss: 0.160459, acc.: 95.31%] [Generator loss: 6.439336]\n",
      "8454 [Discriminator loss: 0.116936, acc.: 93.75%] [Generator loss: 5.617808]\n",
      "8455 [Discriminator loss: 0.080158, acc.: 96.88%] [Generator loss: 6.909155]\n",
      "8456 [Discriminator loss: 0.097953, acc.: 95.31%] [Generator loss: 5.057670]\n",
      "8457 [Discriminator loss: 0.086524, acc.: 96.88%] [Generator loss: 5.342672]\n",
      "8458 [Discriminator loss: 0.249639, acc.: 90.62%] [Generator loss: 6.421303]\n",
      "8459 [Discriminator loss: 0.232443, acc.: 89.06%] [Generator loss: 5.958636]\n",
      "8460 [Discriminator loss: 0.118363, acc.: 95.31%] [Generator loss: 5.493774]\n",
      "8461 [Discriminator loss: 0.158225, acc.: 95.31%] [Generator loss: 6.869198]\n",
      "8462 [Discriminator loss: 0.250658, acc.: 90.62%] [Generator loss: 6.276045]\n",
      "8463 [Discriminator loss: 0.107307, acc.: 95.31%] [Generator loss: 5.916423]\n",
      "8464 [Discriminator loss: 0.117127, acc.: 96.88%] [Generator loss: 5.605309]\n",
      "8465 [Discriminator loss: 0.145646, acc.: 93.75%] [Generator loss: 6.545547]\n",
      "8466 [Discriminator loss: 0.153875, acc.: 92.19%] [Generator loss: 6.208270]\n",
      "8467 [Discriminator loss: 0.150980, acc.: 95.31%] [Generator loss: 5.001884]\n",
      "8468 [Discriminator loss: 0.210891, acc.: 93.75%] [Generator loss: 7.828201]\n",
      "8469 [Discriminator loss: 0.092394, acc.: 96.88%] [Generator loss: 7.956580]\n",
      "8470 [Discriminator loss: 0.210998, acc.: 90.62%] [Generator loss: 7.413936]\n",
      "8471 [Discriminator loss: 0.174676, acc.: 95.31%] [Generator loss: 7.624630]\n",
      "8472 [Discriminator loss: 0.062871, acc.: 96.88%] [Generator loss: 6.523862]\n",
      "8473 [Discriminator loss: 0.149220, acc.: 93.75%] [Generator loss: 4.852320]\n",
      "8474 [Discriminator loss: 0.131659, acc.: 95.31%] [Generator loss: 6.514562]\n",
      "8475 [Discriminator loss: 0.114302, acc.: 96.88%] [Generator loss: 7.072102]\n",
      "8476 [Discriminator loss: 0.137247, acc.: 93.75%] [Generator loss: 6.353469]\n",
      "8477 [Discriminator loss: 0.260534, acc.: 90.62%] [Generator loss: 5.107225]\n",
      "8478 [Discriminator loss: 0.192427, acc.: 90.62%] [Generator loss: 7.473709]\n",
      "8479 [Discriminator loss: 0.167244, acc.: 89.06%] [Generator loss: 5.584334]\n",
      "8480 [Discriminator loss: 0.192496, acc.: 92.19%] [Generator loss: 6.333727]\n",
      "8481 [Discriminator loss: 0.090827, acc.: 98.44%] [Generator loss: 6.548618]\n",
      "8482 [Discriminator loss: 0.082979, acc.: 96.88%] [Generator loss: 6.309293]\n",
      "8483 [Discriminator loss: 0.073275, acc.: 98.44%] [Generator loss: 5.340096]\n",
      "8484 [Discriminator loss: 0.164515, acc.: 93.75%] [Generator loss: 5.888586]\n",
      "8485 [Discriminator loss: 0.190400, acc.: 92.19%] [Generator loss: 5.775763]\n",
      "8486 [Discriminator loss: 0.264529, acc.: 90.62%] [Generator loss: 5.287715]\n",
      "8487 [Discriminator loss: 0.150689, acc.: 93.75%] [Generator loss: 6.712868]\n",
      "8488 [Discriminator loss: 0.203905, acc.: 93.75%] [Generator loss: 5.761996]\n",
      "8489 [Discriminator loss: 0.065338, acc.: 96.88%] [Generator loss: 6.420084]\n",
      "8490 [Discriminator loss: 0.153407, acc.: 95.31%] [Generator loss: 6.251482]\n",
      "8491 [Discriminator loss: 0.116000, acc.: 98.44%] [Generator loss: 5.412357]\n",
      "8492 [Discriminator loss: 0.164414, acc.: 95.31%] [Generator loss: 5.785704]\n",
      "8493 [Discriminator loss: 0.130542, acc.: 93.75%] [Generator loss: 5.951811]\n",
      "8494 [Discriminator loss: 0.117851, acc.: 96.88%] [Generator loss: 6.423259]\n",
      "8495 [Discriminator loss: 0.318776, acc.: 93.75%] [Generator loss: 5.834610]\n",
      "8496 [Discriminator loss: 0.066978, acc.: 96.88%] [Generator loss: 6.489256]\n",
      "8497 [Discriminator loss: 0.186009, acc.: 92.19%] [Generator loss: 6.926725]\n",
      "8498 [Discriminator loss: 0.168171, acc.: 92.19%] [Generator loss: 6.474561]\n",
      "8499 [Discriminator loss: 0.177162, acc.: 93.75%] [Generator loss: 5.308415]\n",
      "8500 [Discriminator loss: 0.147228, acc.: 96.88%] [Generator loss: 5.744220]\n",
      "8501 [Discriminator loss: 0.325173, acc.: 89.06%] [Generator loss: 6.440023]\n",
      "8502 [Discriminator loss: 0.189397, acc.: 93.75%] [Generator loss: 6.152590]\n",
      "8503 [Discriminator loss: 0.137602, acc.: 96.88%] [Generator loss: 6.568791]\n",
      "8504 [Discriminator loss: 0.130342, acc.: 95.31%] [Generator loss: 4.908217]\n",
      "8505 [Discriminator loss: 0.151255, acc.: 95.31%] [Generator loss: 5.720157]\n",
      "8506 [Discriminator loss: 0.068050, acc.: 95.31%] [Generator loss: 5.665600]\n",
      "8507 [Discriminator loss: 0.107691, acc.: 96.88%] [Generator loss: 5.565513]\n",
      "8508 [Discriminator loss: 0.090670, acc.: 96.88%] [Generator loss: 5.916046]\n",
      "8509 [Discriminator loss: 0.058376, acc.: 98.44%] [Generator loss: 6.492805]\n",
      "8510 [Discriminator loss: 0.048404, acc.: 100.00%] [Generator loss: 5.090939]\n",
      "8511 [Discriminator loss: 0.241630, acc.: 85.94%] [Generator loss: 6.686494]\n",
      "8512 [Discriminator loss: 0.072262, acc.: 96.88%] [Generator loss: 5.622833]\n",
      "8513 [Discriminator loss: 0.166967, acc.: 95.31%] [Generator loss: 5.201072]\n",
      "8514 [Discriminator loss: 0.078342, acc.: 98.44%] [Generator loss: 5.662178]\n",
      "8515 [Discriminator loss: 0.140307, acc.: 95.31%] [Generator loss: 5.084465]\n",
      "8516 [Discriminator loss: 0.156092, acc.: 96.88%] [Generator loss: 4.579998]\n",
      "8517 [Discriminator loss: 0.259709, acc.: 87.50%] [Generator loss: 4.434477]\n",
      "8518 [Discriminator loss: 0.075868, acc.: 96.88%] [Generator loss: 6.382431]\n",
      "8519 [Discriminator loss: 0.082880, acc.: 96.88%] [Generator loss: 6.221539]\n",
      "8520 [Discriminator loss: 0.209492, acc.: 89.06%] [Generator loss: 7.451845]\n",
      "8521 [Discriminator loss: 0.252952, acc.: 89.06%] [Generator loss: 6.795687]\n",
      "8522 [Discriminator loss: 0.251452, acc.: 89.06%] [Generator loss: 6.737395]\n",
      "8523 [Discriminator loss: 0.121046, acc.: 98.44%] [Generator loss: 6.184401]\n",
      "8524 [Discriminator loss: 0.087506, acc.: 96.88%] [Generator loss: 6.705351]\n",
      "8525 [Discriminator loss: 0.154048, acc.: 95.31%] [Generator loss: 6.722523]\n",
      "8526 [Discriminator loss: 0.095844, acc.: 96.88%] [Generator loss: 6.646667]\n",
      "8527 [Discriminator loss: 0.067015, acc.: 98.44%] [Generator loss: 5.475524]\n",
      "8528 [Discriminator loss: 0.077916, acc.: 100.00%] [Generator loss: 6.190973]\n",
      "8529 [Discriminator loss: 0.199561, acc.: 95.31%] [Generator loss: 4.463850]\n",
      "8530 [Discriminator loss: 0.115108, acc.: 93.75%] [Generator loss: 6.036208]\n",
      "8531 [Discriminator loss: 0.076114, acc.: 95.31%] [Generator loss: 7.009731]\n",
      "8532 [Discriminator loss: 0.224767, acc.: 92.19%] [Generator loss: 6.402380]\n",
      "8533 [Discriminator loss: 0.382758, acc.: 85.94%] [Generator loss: 5.580067]\n",
      "8534 [Discriminator loss: 0.197642, acc.: 92.19%] [Generator loss: 4.972452]\n",
      "8535 [Discriminator loss: 0.052529, acc.: 98.44%] [Generator loss: 6.513924]\n",
      "8536 [Discriminator loss: 0.169491, acc.: 93.75%] [Generator loss: 7.852175]\n",
      "8537 [Discriminator loss: 0.152552, acc.: 92.19%] [Generator loss: 7.348871]\n",
      "8538 [Discriminator loss: 0.269358, acc.: 85.94%] [Generator loss: 4.540469]\n",
      "8539 [Discriminator loss: 0.117338, acc.: 93.75%] [Generator loss: 7.508396]\n",
      "8540 [Discriminator loss: 0.336337, acc.: 87.50%] [Generator loss: 6.119869]\n",
      "8541 [Discriminator loss: 0.166840, acc.: 93.75%] [Generator loss: 5.873322]\n",
      "8542 [Discriminator loss: 0.086129, acc.: 95.31%] [Generator loss: 6.141781]\n",
      "8543 [Discriminator loss: 0.098552, acc.: 96.88%] [Generator loss: 5.088443]\n",
      "8544 [Discriminator loss: 0.350067, acc.: 82.81%] [Generator loss: 6.273395]\n",
      "8545 [Discriminator loss: 0.225481, acc.: 87.50%] [Generator loss: 5.151809]\n",
      "8546 [Discriminator loss: 0.236125, acc.: 92.19%] [Generator loss: 6.981909]\n",
      "8547 [Discriminator loss: 0.322895, acc.: 89.06%] [Generator loss: 5.383697]\n",
      "8548 [Discriminator loss: 0.136254, acc.: 93.75%] [Generator loss: 5.856835]\n",
      "8549 [Discriminator loss: 0.143140, acc.: 93.75%] [Generator loss: 5.155659]\n",
      "8550 [Discriminator loss: 0.071049, acc.: 98.44%] [Generator loss: 6.290831]\n",
      "8551 [Discriminator loss: 0.199509, acc.: 89.06%] [Generator loss: 5.897666]\n",
      "8552 [Discriminator loss: 0.113300, acc.: 96.88%] [Generator loss: 7.366558]\n",
      "8553 [Discriminator loss: 0.195770, acc.: 93.75%] [Generator loss: 6.250890]\n",
      "8554 [Discriminator loss: 0.135631, acc.: 93.75%] [Generator loss: 6.385563]\n",
      "8555 [Discriminator loss: 0.062915, acc.: 98.44%] [Generator loss: 5.518905]\n",
      "8556 [Discriminator loss: 0.334813, acc.: 87.50%] [Generator loss: 7.161232]\n",
      "8557 [Discriminator loss: 0.065517, acc.: 96.88%] [Generator loss: 6.565618]\n",
      "8558 [Discriminator loss: 0.293875, acc.: 84.38%] [Generator loss: 4.440300]\n",
      "8559 [Discriminator loss: 0.225529, acc.: 85.94%] [Generator loss: 6.776100]\n",
      "8560 [Discriminator loss: 0.090517, acc.: 98.44%] [Generator loss: 7.278330]\n",
      "8561 [Discriminator loss: 0.335434, acc.: 87.50%] [Generator loss: 5.785873]\n",
      "8562 [Discriminator loss: 0.203102, acc.: 92.19%] [Generator loss: 7.863444]\n",
      "8563 [Discriminator loss: 0.092065, acc.: 95.31%] [Generator loss: 5.919787]\n",
      "8564 [Discriminator loss: 0.336199, acc.: 84.38%] [Generator loss: 7.247494]\n",
      "8565 [Discriminator loss: 0.055278, acc.: 98.44%] [Generator loss: 6.441951]\n",
      "8566 [Discriminator loss: 0.074267, acc.: 98.44%] [Generator loss: 5.945218]\n",
      "8567 [Discriminator loss: 0.121795, acc.: 96.88%] [Generator loss: 5.622203]\n",
      "8568 [Discriminator loss: 0.237314, acc.: 89.06%] [Generator loss: 5.759214]\n",
      "8569 [Discriminator loss: 0.072263, acc.: 98.44%] [Generator loss: 6.705370]\n",
      "8570 [Discriminator loss: 0.134276, acc.: 95.31%] [Generator loss: 7.071456]\n",
      "8571 [Discriminator loss: 0.281775, acc.: 82.81%] [Generator loss: 5.965473]\n",
      "8572 [Discriminator loss: 0.162279, acc.: 95.31%] [Generator loss: 5.201262]\n",
      "8573 [Discriminator loss: 0.106862, acc.: 98.44%] [Generator loss: 4.958346]\n",
      "8574 [Discriminator loss: 0.263872, acc.: 87.50%] [Generator loss: 7.834167]\n",
      "8575 [Discriminator loss: 0.260463, acc.: 89.06%] [Generator loss: 6.036633]\n",
      "8576 [Discriminator loss: 0.244058, acc.: 92.19%] [Generator loss: 6.033117]\n",
      "8577 [Discriminator loss: 0.073713, acc.: 98.44%] [Generator loss: 5.978681]\n",
      "8578 [Discriminator loss: 0.194569, acc.: 95.31%] [Generator loss: 5.686450]\n",
      "8579 [Discriminator loss: 0.069435, acc.: 96.88%] [Generator loss: 5.794082]\n",
      "8580 [Discriminator loss: 0.189563, acc.: 89.06%] [Generator loss: 5.659367]\n",
      "8581 [Discriminator loss: 0.126458, acc.: 95.31%] [Generator loss: 5.892215]\n",
      "8582 [Discriminator loss: 0.205032, acc.: 93.75%] [Generator loss: 5.958499]\n",
      "8583 [Discriminator loss: 0.135271, acc.: 93.75%] [Generator loss: 6.431594]\n",
      "8584 [Discriminator loss: 0.242681, acc.: 89.06%] [Generator loss: 7.313442]\n",
      "8585 [Discriminator loss: 0.061342, acc.: 100.00%] [Generator loss: 7.364142]\n",
      "8586 [Discriminator loss: 0.336254, acc.: 84.38%] [Generator loss: 6.837572]\n",
      "8587 [Discriminator loss: 0.144416, acc.: 95.31%] [Generator loss: 5.994802]\n",
      "8588 [Discriminator loss: 0.048965, acc.: 100.00%] [Generator loss: 5.994633]\n",
      "8589 [Discriminator loss: 0.170092, acc.: 95.31%] [Generator loss: 5.106890]\n",
      "8590 [Discriminator loss: 0.098710, acc.: 95.31%] [Generator loss: 5.998364]\n",
      "8591 [Discriminator loss: 0.055625, acc.: 100.00%] [Generator loss: 5.325251]\n",
      "8592 [Discriminator loss: 0.079517, acc.: 98.44%] [Generator loss: 5.081946]\n",
      "8593 [Discriminator loss: 0.188944, acc.: 95.31%] [Generator loss: 5.325430]\n",
      "8594 [Discriminator loss: 0.261438, acc.: 90.62%] [Generator loss: 6.020444]\n",
      "8595 [Discriminator loss: 0.056993, acc.: 98.44%] [Generator loss: 6.112778]\n",
      "8596 [Discriminator loss: 0.163162, acc.: 93.75%] [Generator loss: 5.621335]\n",
      "8597 [Discriminator loss: 0.045755, acc.: 100.00%] [Generator loss: 6.431825]\n",
      "8598 [Discriminator loss: 0.115515, acc.: 96.88%] [Generator loss: 5.420694]\n",
      "8599 [Discriminator loss: 0.177531, acc.: 92.19%] [Generator loss: 4.716451]\n",
      "8600 [Discriminator loss: 0.098668, acc.: 96.88%] [Generator loss: 6.082733]\n",
      "8601 [Discriminator loss: 0.271153, acc.: 85.94%] [Generator loss: 5.700346]\n",
      "8602 [Discriminator loss: 0.215980, acc.: 93.75%] [Generator loss: 5.348346]\n",
      "8603 [Discriminator loss: 0.092704, acc.: 96.88%] [Generator loss: 5.674573]\n",
      "8604 [Discriminator loss: 0.075217, acc.: 98.44%] [Generator loss: 5.701402]\n",
      "8605 [Discriminator loss: 0.144858, acc.: 96.88%] [Generator loss: 6.424022]\n",
      "8606 [Discriminator loss: 0.235277, acc.: 89.06%] [Generator loss: 5.303767]\n",
      "8607 [Discriminator loss: 0.162199, acc.: 90.62%] [Generator loss: 6.672534]\n",
      "8608 [Discriminator loss: 0.191779, acc.: 90.62%] [Generator loss: 5.172226]\n",
      "8609 [Discriminator loss: 0.193494, acc.: 89.06%] [Generator loss: 5.676694]\n",
      "8610 [Discriminator loss: 0.147107, acc.: 93.75%] [Generator loss: 7.508883]\n",
      "8611 [Discriminator loss: 0.176876, acc.: 93.75%] [Generator loss: 7.174547]\n",
      "8612 [Discriminator loss: 0.091758, acc.: 98.44%] [Generator loss: 7.653139]\n",
      "8613 [Discriminator loss: 0.157164, acc.: 90.62%] [Generator loss: 6.796601]\n",
      "8614 [Discriminator loss: 0.055283, acc.: 98.44%] [Generator loss: 4.993316]\n",
      "8615 [Discriminator loss: 0.276495, acc.: 89.06%] [Generator loss: 5.448555]\n",
      "8616 [Discriminator loss: 0.060693, acc.: 98.44%] [Generator loss: 5.515151]\n",
      "8617 [Discriminator loss: 0.242575, acc.: 92.19%] [Generator loss: 6.155065]\n",
      "8618 [Discriminator loss: 0.074039, acc.: 98.44%] [Generator loss: 6.550126]\n",
      "8619 [Discriminator loss: 0.146370, acc.: 93.75%] [Generator loss: 4.758902]\n",
      "8620 [Discriminator loss: 0.161346, acc.: 92.19%] [Generator loss: 6.427496]\n",
      "8621 [Discriminator loss: 0.207337, acc.: 92.19%] [Generator loss: 6.270195]\n",
      "8622 [Discriminator loss: 0.267631, acc.: 82.81%] [Generator loss: 5.818283]\n",
      "8623 [Discriminator loss: 0.144147, acc.: 96.88%] [Generator loss: 6.822486]\n",
      "8624 [Discriminator loss: 0.153236, acc.: 93.75%] [Generator loss: 6.096631]\n",
      "8625 [Discriminator loss: 0.223624, acc.: 93.75%] [Generator loss: 5.291709]\n",
      "8626 [Discriminator loss: 0.124025, acc.: 95.31%] [Generator loss: 4.874411]\n",
      "8627 [Discriminator loss: 0.112542, acc.: 95.31%] [Generator loss: 5.499326]\n",
      "8628 [Discriminator loss: 0.113615, acc.: 95.31%] [Generator loss: 6.331993]\n",
      "8629 [Discriminator loss: 0.304223, acc.: 87.50%] [Generator loss: 5.798966]\n",
      "8630 [Discriminator loss: 0.118458, acc.: 95.31%] [Generator loss: 6.279113]\n",
      "8631 [Discriminator loss: 0.091010, acc.: 98.44%] [Generator loss: 7.332573]\n",
      "8632 [Discriminator loss: 0.267341, acc.: 87.50%] [Generator loss: 4.592896]\n",
      "8633 [Discriminator loss: 0.091790, acc.: 98.44%] [Generator loss: 5.581002]\n",
      "8634 [Discriminator loss: 0.143064, acc.: 95.31%] [Generator loss: 5.003471]\n",
      "8635 [Discriminator loss: 0.216331, acc.: 92.19%] [Generator loss: 6.512269]\n",
      "8636 [Discriminator loss: 0.121055, acc.: 93.75%] [Generator loss: 6.493774]\n",
      "8637 [Discriminator loss: 0.133984, acc.: 95.31%] [Generator loss: 5.881887]\n",
      "8638 [Discriminator loss: 0.099940, acc.: 98.44%] [Generator loss: 5.617990]\n",
      "8639 [Discriminator loss: 0.160978, acc.: 92.19%] [Generator loss: 5.594191]\n",
      "8640 [Discriminator loss: 0.184173, acc.: 92.19%] [Generator loss: 6.591707]\n",
      "8641 [Discriminator loss: 0.215251, acc.: 90.62%] [Generator loss: 6.101959]\n",
      "8642 [Discriminator loss: 0.255957, acc.: 90.62%] [Generator loss: 5.040151]\n",
      "8643 [Discriminator loss: 0.092629, acc.: 98.44%] [Generator loss: 6.610053]\n",
      "8644 [Discriminator loss: 0.141450, acc.: 95.31%] [Generator loss: 6.417051]\n",
      "8645 [Discriminator loss: 0.054717, acc.: 98.44%] [Generator loss: 5.847061]\n",
      "8646 [Discriminator loss: 0.262554, acc.: 89.06%] [Generator loss: 4.822437]\n",
      "8647 [Discriminator loss: 0.219570, acc.: 92.19%] [Generator loss: 6.736495]\n",
      "8648 [Discriminator loss: 0.095519, acc.: 96.88%] [Generator loss: 6.556984]\n",
      "8649 [Discriminator loss: 0.279789, acc.: 85.94%] [Generator loss: 5.314349]\n",
      "8650 [Discriminator loss: 0.179664, acc.: 92.19%] [Generator loss: 6.881194]\n",
      "8651 [Discriminator loss: 0.131029, acc.: 96.88%] [Generator loss: 5.850448]\n",
      "8652 [Discriminator loss: 0.132886, acc.: 93.75%] [Generator loss: 7.072929]\n",
      "8653 [Discriminator loss: 0.140348, acc.: 95.31%] [Generator loss: 4.571498]\n",
      "8654 [Discriminator loss: 0.132761, acc.: 95.31%] [Generator loss: 6.433266]\n",
      "8655 [Discriminator loss: 0.280163, acc.: 85.94%] [Generator loss: 5.626560]\n",
      "8656 [Discriminator loss: 0.113671, acc.: 93.75%] [Generator loss: 6.907773]\n",
      "8657 [Discriminator loss: 0.193933, acc.: 85.94%] [Generator loss: 7.346732]\n",
      "8658 [Discriminator loss: 0.179263, acc.: 93.75%] [Generator loss: 7.530816]\n",
      "8659 [Discriminator loss: 0.078817, acc.: 96.88%] [Generator loss: 6.556690]\n",
      "8660 [Discriminator loss: 0.142132, acc.: 98.44%] [Generator loss: 5.899700]\n",
      "8661 [Discriminator loss: 0.116635, acc.: 95.31%] [Generator loss: 5.482315]\n",
      "8662 [Discriminator loss: 0.153335, acc.: 93.75%] [Generator loss: 5.511557]\n",
      "8663 [Discriminator loss: 0.200490, acc.: 92.19%] [Generator loss: 6.306658]\n",
      "8664 [Discriminator loss: 0.201434, acc.: 90.62%] [Generator loss: 4.930748]\n",
      "8665 [Discriminator loss: 0.190156, acc.: 90.62%] [Generator loss: 5.824739]\n",
      "8666 [Discriminator loss: 0.093709, acc.: 96.88%] [Generator loss: 6.213698]\n",
      "8667 [Discriminator loss: 0.097514, acc.: 93.75%] [Generator loss: 6.617034]\n",
      "8668 [Discriminator loss: 0.264875, acc.: 89.06%] [Generator loss: 5.986276]\n",
      "8669 [Discriminator loss: 0.030063, acc.: 100.00%] [Generator loss: 6.326580]\n",
      "8670 [Discriminator loss: 0.112662, acc.: 98.44%] [Generator loss: 5.785925]\n",
      "8671 [Discriminator loss: 0.060409, acc.: 98.44%] [Generator loss: 5.127971]\n",
      "8672 [Discriminator loss: 0.140699, acc.: 89.06%] [Generator loss: 7.577428]\n",
      "8673 [Discriminator loss: 0.128508, acc.: 92.19%] [Generator loss: 7.049131]\n",
      "8674 [Discriminator loss: 0.203223, acc.: 90.62%] [Generator loss: 4.825710]\n",
      "8675 [Discriminator loss: 0.121028, acc.: 93.75%] [Generator loss: 5.918777]\n",
      "8676 [Discriminator loss: 0.449537, acc.: 82.81%] [Generator loss: 6.552932]\n",
      "8677 [Discriminator loss: 0.210334, acc.: 92.19%] [Generator loss: 5.820035]\n",
      "8678 [Discriminator loss: 0.086396, acc.: 98.44%] [Generator loss: 6.974068]\n",
      "8679 [Discriminator loss: 0.222817, acc.: 87.50%] [Generator loss: 5.792556]\n",
      "8680 [Discriminator loss: 0.117705, acc.: 95.31%] [Generator loss: 6.304292]\n",
      "8681 [Discriminator loss: 0.040636, acc.: 100.00%] [Generator loss: 7.522743]\n",
      "8682 [Discriminator loss: 0.144883, acc.: 93.75%] [Generator loss: 6.866687]\n",
      "8683 [Discriminator loss: 0.057027, acc.: 96.88%] [Generator loss: 6.241596]\n",
      "8684 [Discriminator loss: 0.274905, acc.: 87.50%] [Generator loss: 5.207779]\n",
      "8685 [Discriminator loss: 0.084200, acc.: 98.44%] [Generator loss: 6.895910]\n",
      "8686 [Discriminator loss: 0.298295, acc.: 89.06%] [Generator loss: 6.208809]\n",
      "8687 [Discriminator loss: 0.119720, acc.: 93.75%] [Generator loss: 5.753376]\n",
      "8688 [Discriminator loss: 0.093915, acc.: 95.31%] [Generator loss: 5.870094]\n",
      "8689 [Discriminator loss: 0.286138, acc.: 90.62%] [Generator loss: 6.801925]\n",
      "8690 [Discriminator loss: 0.208072, acc.: 92.19%] [Generator loss: 7.411607]\n",
      "8691 [Discriminator loss: 0.348594, acc.: 85.94%] [Generator loss: 4.114191]\n",
      "8692 [Discriminator loss: 0.316608, acc.: 87.50%] [Generator loss: 6.774301]\n",
      "8693 [Discriminator loss: 0.123491, acc.: 92.19%] [Generator loss: 7.846728]\n",
      "8694 [Discriminator loss: 0.269976, acc.: 87.50%] [Generator loss: 6.926277]\n",
      "8695 [Discriminator loss: 0.189053, acc.: 90.62%] [Generator loss: 6.551753]\n",
      "8696 [Discriminator loss: 0.116380, acc.: 93.75%] [Generator loss: 6.572057]\n",
      "8697 [Discriminator loss: 0.175332, acc.: 93.75%] [Generator loss: 5.949441]\n",
      "8698 [Discriminator loss: 0.087043, acc.: 98.44%] [Generator loss: 6.003530]\n",
      "8699 [Discriminator loss: 0.234031, acc.: 93.75%] [Generator loss: 5.091270]\n",
      "8700 [Discriminator loss: 0.105867, acc.: 95.31%] [Generator loss: 6.337947]\n",
      "8701 [Discriminator loss: 0.166035, acc.: 93.75%] [Generator loss: 5.681394]\n",
      "8702 [Discriminator loss: 0.204824, acc.: 92.19%] [Generator loss: 4.887492]\n",
      "8703 [Discriminator loss: 0.279885, acc.: 87.50%] [Generator loss: 6.633407]\n",
      "8704 [Discriminator loss: 0.024629, acc.: 100.00%] [Generator loss: 6.300310]\n",
      "8705 [Discriminator loss: 0.249273, acc.: 89.06%] [Generator loss: 5.150325]\n",
      "8706 [Discriminator loss: 0.248584, acc.: 90.62%] [Generator loss: 7.047167]\n",
      "8707 [Discriminator loss: 0.034428, acc.: 100.00%] [Generator loss: 6.259848]\n",
      "8708 [Discriminator loss: 0.170793, acc.: 96.88%] [Generator loss: 6.706388]\n",
      "8709 [Discriminator loss: 0.062919, acc.: 98.44%] [Generator loss: 6.184983]\n",
      "8710 [Discriminator loss: 0.284495, acc.: 87.50%] [Generator loss: 6.392582]\n",
      "8711 [Discriminator loss: 0.143009, acc.: 92.19%] [Generator loss: 5.330343]\n",
      "8712 [Discriminator loss: 0.285278, acc.: 87.50%] [Generator loss: 5.083204]\n",
      "8713 [Discriminator loss: 0.192044, acc.: 93.75%] [Generator loss: 5.751698]\n",
      "8714 [Discriminator loss: 0.113791, acc.: 93.75%] [Generator loss: 6.493346]\n",
      "8715 [Discriminator loss: 0.100592, acc.: 98.44%] [Generator loss: 6.166998]\n",
      "8716 [Discriminator loss: 0.323368, acc.: 85.94%] [Generator loss: 6.334243]\n",
      "8717 [Discriminator loss: 0.248806, acc.: 92.19%] [Generator loss: 5.023491]\n",
      "8718 [Discriminator loss: 0.129689, acc.: 95.31%] [Generator loss: 7.712014]\n",
      "8719 [Discriminator loss: 0.181415, acc.: 92.19%] [Generator loss: 5.184186]\n",
      "8720 [Discriminator loss: 0.125143, acc.: 92.19%] [Generator loss: 6.380641]\n",
      "8721 [Discriminator loss: 0.234370, acc.: 89.06%] [Generator loss: 6.596706]\n",
      "8722 [Discriminator loss: 0.179940, acc.: 90.62%] [Generator loss: 5.662661]\n",
      "8723 [Discriminator loss: 0.155994, acc.: 96.88%] [Generator loss: 6.696473]\n",
      "8724 [Discriminator loss: 0.140559, acc.: 92.19%] [Generator loss: 6.019509]\n",
      "8725 [Discriminator loss: 0.107077, acc.: 95.31%] [Generator loss: 4.639971]\n",
      "8726 [Discriminator loss: 0.174750, acc.: 92.19%] [Generator loss: 5.075596]\n",
      "8727 [Discriminator loss: 0.196423, acc.: 90.62%] [Generator loss: 8.810226]\n",
      "8728 [Discriminator loss: 0.052656, acc.: 98.44%] [Generator loss: 8.156415]\n",
      "8729 [Discriminator loss: 0.555106, acc.: 79.69%] [Generator loss: 3.416536]\n",
      "8730 [Discriminator loss: 0.314399, acc.: 84.38%] [Generator loss: 7.798970]\n",
      "8731 [Discriminator loss: 0.114936, acc.: 93.75%] [Generator loss: 7.786386]\n",
      "8732 [Discriminator loss: 0.144165, acc.: 93.75%] [Generator loss: 5.798407]\n",
      "8733 [Discriminator loss: 0.099737, acc.: 96.88%] [Generator loss: 5.540895]\n",
      "8734 [Discriminator loss: 0.064804, acc.: 100.00%] [Generator loss: 5.656919]\n",
      "8735 [Discriminator loss: 0.095206, acc.: 96.88%] [Generator loss: 5.439232]\n",
      "8736 [Discriminator loss: 0.094529, acc.: 98.44%] [Generator loss: 7.018412]\n",
      "8737 [Discriminator loss: 0.141997, acc.: 95.31%] [Generator loss: 6.840768]\n",
      "8738 [Discriminator loss: 0.447605, acc.: 84.38%] [Generator loss: 6.841818]\n",
      "8739 [Discriminator loss: 0.162877, acc.: 98.44%] [Generator loss: 7.502738]\n",
      "8740 [Discriminator loss: 0.222399, acc.: 92.19%] [Generator loss: 6.802405]\n",
      "8741 [Discriminator loss: 0.091883, acc.: 95.31%] [Generator loss: 4.726143]\n",
      "8742 [Discriminator loss: 0.128086, acc.: 95.31%] [Generator loss: 5.501019]\n",
      "8743 [Discriminator loss: 0.080436, acc.: 95.31%] [Generator loss: 6.119238]\n",
      "8744 [Discriminator loss: 0.178371, acc.: 93.75%] [Generator loss: 4.538557]\n",
      "8745 [Discriminator loss: 0.172627, acc.: 93.75%] [Generator loss: 4.620750]\n",
      "8746 [Discriminator loss: 0.112027, acc.: 95.31%] [Generator loss: 6.604323]\n",
      "8747 [Discriminator loss: 0.307640, acc.: 92.19%] [Generator loss: 5.671421]\n",
      "8748 [Discriminator loss: 0.216941, acc.: 87.50%] [Generator loss: 7.025143]\n",
      "8749 [Discriminator loss: 0.174716, acc.: 93.75%] [Generator loss: 5.813082]\n",
      "8750 [Discriminator loss: 0.378649, acc.: 87.50%] [Generator loss: 6.782364]\n",
      "8751 [Discriminator loss: 0.090799, acc.: 95.31%] [Generator loss: 5.912914]\n",
      "8752 [Discriminator loss: 0.192839, acc.: 93.75%] [Generator loss: 5.011629]\n",
      "8753 [Discriminator loss: 0.349366, acc.: 90.62%] [Generator loss: 8.300994]\n",
      "8754 [Discriminator loss: 0.237218, acc.: 89.06%] [Generator loss: 6.243813]\n",
      "8755 [Discriminator loss: 0.281111, acc.: 87.50%] [Generator loss: 4.855468]\n",
      "8756 [Discriminator loss: 0.138016, acc.: 93.75%] [Generator loss: 5.188784]\n",
      "8757 [Discriminator loss: 0.055022, acc.: 98.44%] [Generator loss: 5.993414]\n",
      "8758 [Discriminator loss: 0.187776, acc.: 95.31%] [Generator loss: 6.313178]\n",
      "8759 [Discriminator loss: 0.101298, acc.: 96.88%] [Generator loss: 7.126126]\n",
      "8760 [Discriminator loss: 0.189940, acc.: 93.75%] [Generator loss: 5.443063]\n",
      "8761 [Discriminator loss: 0.338402, acc.: 84.38%] [Generator loss: 4.603939]\n",
      "8762 [Discriminator loss: 0.080801, acc.: 96.88%] [Generator loss: 7.029249]\n",
      "8763 [Discriminator loss: 0.058498, acc.: 100.00%] [Generator loss: 6.490115]\n",
      "8764 [Discriminator loss: 0.126357, acc.: 96.88%] [Generator loss: 6.389505]\n",
      "8765 [Discriminator loss: 0.033984, acc.: 100.00%] [Generator loss: 6.895983]\n",
      "8766 [Discriminator loss: 0.428944, acc.: 82.81%] [Generator loss: 6.029335]\n",
      "8767 [Discriminator loss: 0.061397, acc.: 96.88%] [Generator loss: 6.463478]\n",
      "8768 [Discriminator loss: 0.162624, acc.: 95.31%] [Generator loss: 5.201102]\n",
      "8769 [Discriminator loss: 0.143314, acc.: 92.19%] [Generator loss: 6.783840]\n",
      "8770 [Discriminator loss: 0.119989, acc.: 96.88%] [Generator loss: 5.438046]\n",
      "8771 [Discriminator loss: 0.174247, acc.: 92.19%] [Generator loss: 6.200234]\n",
      "8772 [Discriminator loss: 0.101039, acc.: 98.44%] [Generator loss: 7.063103]\n",
      "8773 [Discriminator loss: 0.081079, acc.: 96.88%] [Generator loss: 5.612920]\n",
      "8774 [Discriminator loss: 0.124861, acc.: 96.88%] [Generator loss: 4.922488]\n",
      "8775 [Discriminator loss: 0.128891, acc.: 92.19%] [Generator loss: 6.125636]\n",
      "8776 [Discriminator loss: 0.127414, acc.: 95.31%] [Generator loss: 7.685011]\n",
      "8777 [Discriminator loss: 0.158184, acc.: 93.75%] [Generator loss: 4.231978]\n",
      "8778 [Discriminator loss: 0.143803, acc.: 92.19%] [Generator loss: 6.129803]\n",
      "8779 [Discriminator loss: 0.116360, acc.: 95.31%] [Generator loss: 5.984476]\n",
      "8780 [Discriminator loss: 0.335687, acc.: 84.38%] [Generator loss: 5.654591]\n",
      "8781 [Discriminator loss: 0.168480, acc.: 95.31%] [Generator loss: 6.364942]\n",
      "8782 [Discriminator loss: 0.048717, acc.: 100.00%] [Generator loss: 5.937249]\n",
      "8783 [Discriminator loss: 0.086368, acc.: 96.88%] [Generator loss: 5.674517]\n",
      "8784 [Discriminator loss: 0.199146, acc.: 92.19%] [Generator loss: 6.212968]\n",
      "8785 [Discriminator loss: 0.133675, acc.: 93.75%] [Generator loss: 5.719358]\n",
      "8786 [Discriminator loss: 0.064397, acc.: 96.88%] [Generator loss: 6.372283]\n",
      "8787 [Discriminator loss: 0.189042, acc.: 90.62%] [Generator loss: 6.738580]\n",
      "8788 [Discriminator loss: 0.229929, acc.: 92.19%] [Generator loss: 6.937225]\n",
      "8789 [Discriminator loss: 0.096742, acc.: 95.31%] [Generator loss: 5.400249]\n",
      "8790 [Discriminator loss: 0.405034, acc.: 84.38%] [Generator loss: 6.298360]\n",
      "8791 [Discriminator loss: 0.048431, acc.: 98.44%] [Generator loss: 8.153848]\n",
      "8792 [Discriminator loss: 0.163233, acc.: 93.75%] [Generator loss: 5.194067]\n",
      "8793 [Discriminator loss: 0.316567, acc.: 87.50%] [Generator loss: 6.897659]\n",
      "8794 [Discriminator loss: 0.242625, acc.: 90.62%] [Generator loss: 6.079211]\n",
      "8795 [Discriminator loss: 0.176232, acc.: 92.19%] [Generator loss: 5.347684]\n",
      "8796 [Discriminator loss: 0.268669, acc.: 90.62%] [Generator loss: 6.076920]\n",
      "8797 [Discriminator loss: 0.057840, acc.: 96.88%] [Generator loss: 7.352219]\n",
      "8798 [Discriminator loss: 0.060075, acc.: 96.88%] [Generator loss: 7.443141]\n",
      "8799 [Discriminator loss: 0.162187, acc.: 96.88%] [Generator loss: 5.697826]\n",
      "8800 [Discriminator loss: 0.187201, acc.: 92.19%] [Generator loss: 6.590462]\n",
      "8801 [Discriminator loss: 0.121041, acc.: 96.88%] [Generator loss: 5.386195]\n",
      "8802 [Discriminator loss: 0.119340, acc.: 96.88%] [Generator loss: 5.797771]\n",
      "8803 [Discriminator loss: 0.241887, acc.: 90.62%] [Generator loss: 5.920679]\n",
      "8804 [Discriminator loss: 0.120264, acc.: 93.75%] [Generator loss: 6.233111]\n",
      "8805 [Discriminator loss: 0.069544, acc.: 98.44%] [Generator loss: 6.745194]\n",
      "8806 [Discriminator loss: 0.293371, acc.: 92.19%] [Generator loss: 5.919664]\n",
      "8807 [Discriminator loss: 0.051089, acc.: 98.44%] [Generator loss: 6.771676]\n",
      "8808 [Discriminator loss: 0.221328, acc.: 90.62%] [Generator loss: 7.214364]\n",
      "8809 [Discriminator loss: 0.073945, acc.: 96.88%] [Generator loss: 6.190649]\n",
      "8810 [Discriminator loss: 0.054193, acc.: 100.00%] [Generator loss: 6.038325]\n",
      "8811 [Discriminator loss: 0.172790, acc.: 89.06%] [Generator loss: 6.786646]\n",
      "8812 [Discriminator loss: 0.211425, acc.: 93.75%] [Generator loss: 6.594220]\n",
      "8813 [Discriminator loss: 0.153906, acc.: 95.31%] [Generator loss: 6.388982]\n",
      "8814 [Discriminator loss: 0.168002, acc.: 93.75%] [Generator loss: 4.505325]\n",
      "8815 [Discriminator loss: 0.045740, acc.: 100.00%] [Generator loss: 5.543310]\n",
      "8816 [Discriminator loss: 0.188495, acc.: 89.06%] [Generator loss: 7.046599]\n",
      "8817 [Discriminator loss: 0.122210, acc.: 96.88%] [Generator loss: 7.059187]\n",
      "8818 [Discriminator loss: 0.160993, acc.: 93.75%] [Generator loss: 5.908235]\n",
      "8819 [Discriminator loss: 0.067580, acc.: 98.44%] [Generator loss: 6.568064]\n",
      "8820 [Discriminator loss: 0.065757, acc.: 98.44%] [Generator loss: 5.374006]\n",
      "8821 [Discriminator loss: 0.150355, acc.: 95.31%] [Generator loss: 5.646006]\n",
      "8822 [Discriminator loss: 0.055929, acc.: 100.00%] [Generator loss: 6.015150]\n",
      "8823 [Discriminator loss: 0.137848, acc.: 95.31%] [Generator loss: 6.348006]\n",
      "8824 [Discriminator loss: 0.087630, acc.: 96.88%] [Generator loss: 4.521116]\n",
      "8825 [Discriminator loss: 0.194598, acc.: 93.75%] [Generator loss: 6.748812]\n",
      "8826 [Discriminator loss: 0.101856, acc.: 96.88%] [Generator loss: 4.938337]\n",
      "8827 [Discriminator loss: 0.331893, acc.: 87.50%] [Generator loss: 6.516889]\n",
      "8828 [Discriminator loss: 0.113852, acc.: 95.31%] [Generator loss: 7.156301]\n",
      "8829 [Discriminator loss: 0.260643, acc.: 92.19%] [Generator loss: 5.450770]\n",
      "8830 [Discriminator loss: 0.087780, acc.: 96.88%] [Generator loss: 5.184742]\n",
      "8831 [Discriminator loss: 0.123738, acc.: 93.75%] [Generator loss: 4.980835]\n",
      "8832 [Discriminator loss: 0.190297, acc.: 92.19%] [Generator loss: 6.442212]\n",
      "8833 [Discriminator loss: 0.167222, acc.: 95.31%] [Generator loss: 5.798451]\n",
      "8834 [Discriminator loss: 0.145289, acc.: 96.88%] [Generator loss: 5.664903]\n",
      "8835 [Discriminator loss: 0.121237, acc.: 95.31%] [Generator loss: 3.149497]\n",
      "8836 [Discriminator loss: 0.157311, acc.: 95.31%] [Generator loss: 6.035650]\n",
      "8837 [Discriminator loss: 0.246644, acc.: 87.50%] [Generator loss: 5.255558]\n",
      "8838 [Discriminator loss: 0.059863, acc.: 98.44%] [Generator loss: 5.092476]\n",
      "8839 [Discriminator loss: 0.080105, acc.: 96.88%] [Generator loss: 5.341143]\n",
      "8840 [Discriminator loss: 0.088572, acc.: 96.88%] [Generator loss: 5.641606]\n",
      "8841 [Discriminator loss: 0.209929, acc.: 90.62%] [Generator loss: 5.114854]\n",
      "8842 [Discriminator loss: 0.049256, acc.: 98.44%] [Generator loss: 7.144432]\n",
      "8843 [Discriminator loss: 0.241315, acc.: 85.94%] [Generator loss: 5.810617]\n",
      "8844 [Discriminator loss: 0.119646, acc.: 95.31%] [Generator loss: 6.275819]\n",
      "8845 [Discriminator loss: 0.157524, acc.: 90.62%] [Generator loss: 4.453002]\n",
      "8846 [Discriminator loss: 0.131204, acc.: 93.75%] [Generator loss: 5.352565]\n",
      "8847 [Discriminator loss: 0.109810, acc.: 93.75%] [Generator loss: 8.337947]\n",
      "8848 [Discriminator loss: 0.202293, acc.: 85.94%] [Generator loss: 7.019966]\n",
      "8849 [Discriminator loss: 0.291716, acc.: 89.06%] [Generator loss: 5.708627]\n",
      "8850 [Discriminator loss: 0.216799, acc.: 89.06%] [Generator loss: 6.827093]\n",
      "8851 [Discriminator loss: 0.086186, acc.: 95.31%] [Generator loss: 6.476361]\n",
      "8852 [Discriminator loss: 0.142736, acc.: 92.19%] [Generator loss: 5.498065]\n",
      "8853 [Discriminator loss: 0.086582, acc.: 96.88%] [Generator loss: 5.886840]\n",
      "8854 [Discriminator loss: 0.049842, acc.: 98.44%] [Generator loss: 6.381273]\n",
      "8855 [Discriminator loss: 0.080078, acc.: 98.44%] [Generator loss: 6.020682]\n",
      "8856 [Discriminator loss: 0.225976, acc.: 90.62%] [Generator loss: 5.851048]\n",
      "8857 [Discriminator loss: 0.119771, acc.: 96.88%] [Generator loss: 5.188475]\n",
      "8858 [Discriminator loss: 0.124341, acc.: 93.75%] [Generator loss: 5.757348]\n",
      "8859 [Discriminator loss: 0.064923, acc.: 100.00%] [Generator loss: 5.634757]\n",
      "8860 [Discriminator loss: 0.381504, acc.: 85.94%] [Generator loss: 5.977026]\n",
      "8861 [Discriminator loss: 0.050078, acc.: 100.00%] [Generator loss: 5.896351]\n",
      "8862 [Discriminator loss: 0.089934, acc.: 96.88%] [Generator loss: 5.753993]\n",
      "8863 [Discriminator loss: 0.185218, acc.: 89.06%] [Generator loss: 4.860135]\n",
      "8864 [Discriminator loss: 0.096473, acc.: 96.88%] [Generator loss: 6.261961]\n",
      "8865 [Discriminator loss: 0.168801, acc.: 93.75%] [Generator loss: 5.651357]\n",
      "8866 [Discriminator loss: 0.120895, acc.: 96.88%] [Generator loss: 6.496265]\n",
      "8867 [Discriminator loss: 0.072681, acc.: 98.44%] [Generator loss: 6.894405]\n",
      "8868 [Discriminator loss: 0.177893, acc.: 93.75%] [Generator loss: 6.607216]\n",
      "8869 [Discriminator loss: 0.160753, acc.: 92.19%] [Generator loss: 7.151184]\n",
      "8870 [Discriminator loss: 0.047217, acc.: 98.44%] [Generator loss: 7.721201]\n",
      "8871 [Discriminator loss: 0.154180, acc.: 93.75%] [Generator loss: 5.801790]\n",
      "8872 [Discriminator loss: 0.244757, acc.: 90.62%] [Generator loss: 6.058065]\n",
      "8873 [Discriminator loss: 0.132618, acc.: 96.88%] [Generator loss: 6.663921]\n",
      "8874 [Discriminator loss: 0.119728, acc.: 96.88%] [Generator loss: 6.535663]\n",
      "8875 [Discriminator loss: 0.106343, acc.: 95.31%] [Generator loss: 6.458022]\n",
      "8876 [Discriminator loss: 0.114547, acc.: 95.31%] [Generator loss: 6.651275]\n",
      "8877 [Discriminator loss: 0.061420, acc.: 98.44%] [Generator loss: 6.851532]\n",
      "8878 [Discriminator loss: 0.260892, acc.: 90.62%] [Generator loss: 5.050187]\n",
      "8879 [Discriminator loss: 0.143258, acc.: 95.31%] [Generator loss: 6.718099]\n",
      "8880 [Discriminator loss: 0.180182, acc.: 93.75%] [Generator loss: 7.313581]\n",
      "8881 [Discriminator loss: 0.195050, acc.: 93.75%] [Generator loss: 6.565420]\n",
      "8882 [Discriminator loss: 0.069984, acc.: 96.88%] [Generator loss: 7.285434]\n",
      "8883 [Discriminator loss: 0.118100, acc.: 96.88%] [Generator loss: 5.744830]\n",
      "8884 [Discriminator loss: 0.118217, acc.: 95.31%] [Generator loss: 5.529315]\n",
      "8885 [Discriminator loss: 0.109867, acc.: 93.75%] [Generator loss: 4.970607]\n",
      "8886 [Discriminator loss: 0.055251, acc.: 98.44%] [Generator loss: 6.493605]\n",
      "8887 [Discriminator loss: 0.253530, acc.: 90.62%] [Generator loss: 5.958653]\n",
      "8888 [Discriminator loss: 0.116015, acc.: 95.31%] [Generator loss: 6.486823]\n",
      "8889 [Discriminator loss: 0.052097, acc.: 98.44%] [Generator loss: 5.801909]\n",
      "8890 [Discriminator loss: 0.069752, acc.: 100.00%] [Generator loss: 6.113989]\n",
      "8891 [Discriminator loss: 0.246615, acc.: 90.62%] [Generator loss: 5.923424]\n",
      "8892 [Discriminator loss: 0.088410, acc.: 95.31%] [Generator loss: 6.406252]\n",
      "8893 [Discriminator loss: 0.156048, acc.: 93.75%] [Generator loss: 5.780255]\n",
      "8894 [Discriminator loss: 0.203728, acc.: 89.06%] [Generator loss: 7.045207]\n",
      "8895 [Discriminator loss: 0.093084, acc.: 96.88%] [Generator loss: 6.241942]\n",
      "8896 [Discriminator loss: 0.146020, acc.: 92.19%] [Generator loss: 6.402588]\n",
      "8897 [Discriminator loss: 0.139721, acc.: 95.31%] [Generator loss: 5.891929]\n",
      "8898 [Discriminator loss: 0.173533, acc.: 92.19%] [Generator loss: 6.128549]\n",
      "8899 [Discriminator loss: 0.118624, acc.: 95.31%] [Generator loss: 5.861225]\n",
      "8900 [Discriminator loss: 0.162071, acc.: 95.31%] [Generator loss: 5.745051]\n",
      "8901 [Discriminator loss: 0.116519, acc.: 95.31%] [Generator loss: 6.486677]\n",
      "8902 [Discriminator loss: 0.098749, acc.: 95.31%] [Generator loss: 6.581460]\n",
      "8903 [Discriminator loss: 0.350145, acc.: 84.38%] [Generator loss: 4.917428]\n",
      "8904 [Discriminator loss: 0.170900, acc.: 93.75%] [Generator loss: 6.824027]\n",
      "8905 [Discriminator loss: 0.069411, acc.: 98.44%] [Generator loss: 5.414197]\n",
      "8906 [Discriminator loss: 0.092187, acc.: 96.88%] [Generator loss: 5.754248]\n",
      "8907 [Discriminator loss: 0.233294, acc.: 87.50%] [Generator loss: 6.912188]\n",
      "8908 [Discriminator loss: 0.472141, acc.: 84.38%] [Generator loss: 6.327611]\n",
      "8909 [Discriminator loss: 0.078876, acc.: 96.88%] [Generator loss: 5.993905]\n",
      "8910 [Discriminator loss: 0.191448, acc.: 92.19%] [Generator loss: 5.731473]\n",
      "8911 [Discriminator loss: 0.150617, acc.: 92.19%] [Generator loss: 6.227803]\n",
      "8912 [Discriminator loss: 0.082832, acc.: 96.88%] [Generator loss: 6.711615]\n",
      "8913 [Discriminator loss: 0.065449, acc.: 98.44%] [Generator loss: 6.227642]\n",
      "8914 [Discriminator loss: 0.186086, acc.: 95.31%] [Generator loss: 6.662306]\n",
      "8915 [Discriminator loss: 0.145165, acc.: 93.75%] [Generator loss: 6.765336]\n",
      "8916 [Discriminator loss: 0.085053, acc.: 96.88%] [Generator loss: 6.458127]\n",
      "8917 [Discriminator loss: 0.128746, acc.: 96.88%] [Generator loss: 6.130674]\n",
      "8918 [Discriminator loss: 0.247923, acc.: 89.06%] [Generator loss: 6.971910]\n",
      "8919 [Discriminator loss: 0.103836, acc.: 95.31%] [Generator loss: 7.348842]\n",
      "8920 [Discriminator loss: 0.094043, acc.: 98.44%] [Generator loss: 5.493677]\n",
      "8921 [Discriminator loss: 0.062579, acc.: 98.44%] [Generator loss: 5.703600]\n",
      "8922 [Discriminator loss: 0.206713, acc.: 90.62%] [Generator loss: 7.492154]\n",
      "8923 [Discriminator loss: 0.031263, acc.: 100.00%] [Generator loss: 8.391544]\n",
      "8924 [Discriminator loss: 0.287320, acc.: 89.06%] [Generator loss: 5.375653]\n",
      "8925 [Discriminator loss: 0.069281, acc.: 98.44%] [Generator loss: 4.768465]\n",
      "8926 [Discriminator loss: 0.097801, acc.: 96.88%] [Generator loss: 6.170486]\n",
      "8927 [Discriminator loss: 0.148100, acc.: 95.31%] [Generator loss: 5.573098]\n",
      "8928 [Discriminator loss: 0.218044, acc.: 90.62%] [Generator loss: 6.291277]\n",
      "8929 [Discriminator loss: 0.320417, acc.: 85.94%] [Generator loss: 4.744107]\n",
      "8930 [Discriminator loss: 0.070235, acc.: 96.88%] [Generator loss: 6.696776]\n",
      "8931 [Discriminator loss: 0.109747, acc.: 95.31%] [Generator loss: 6.964625]\n",
      "8932 [Discriminator loss: 0.175288, acc.: 95.31%] [Generator loss: 5.901844]\n",
      "8933 [Discriminator loss: 0.210493, acc.: 89.06%] [Generator loss: 5.984639]\n",
      "8934 [Discriminator loss: 0.179220, acc.: 90.62%] [Generator loss: 6.652379]\n",
      "8935 [Discriminator loss: 0.224235, acc.: 92.19%] [Generator loss: 7.686292]\n",
      "8936 [Discriminator loss: 0.230741, acc.: 92.19%] [Generator loss: 5.339485]\n",
      "8937 [Discriminator loss: 0.378712, acc.: 79.69%] [Generator loss: 6.776766]\n",
      "8938 [Discriminator loss: 0.091297, acc.: 96.88%] [Generator loss: 7.919959]\n",
      "8939 [Discriminator loss: 0.232434, acc.: 89.06%] [Generator loss: 6.136974]\n",
      "8940 [Discriminator loss: 0.093506, acc.: 96.88%] [Generator loss: 6.092974]\n",
      "8941 [Discriminator loss: 0.117199, acc.: 95.31%] [Generator loss: 6.649586]\n",
      "8942 [Discriminator loss: 0.080696, acc.: 98.44%] [Generator loss: 6.773822]\n",
      "8943 [Discriminator loss: 0.284595, acc.: 84.38%] [Generator loss: 6.217957]\n",
      "8944 [Discriminator loss: 0.207891, acc.: 92.19%] [Generator loss: 7.299489]\n",
      "8945 [Discriminator loss: 0.058016, acc.: 98.44%] [Generator loss: 6.846459]\n",
      "8946 [Discriminator loss: 0.070687, acc.: 100.00%] [Generator loss: 7.038854]\n",
      "8947 [Discriminator loss: 0.105721, acc.: 96.88%] [Generator loss: 5.901277]\n",
      "8948 [Discriminator loss: 0.132087, acc.: 93.75%] [Generator loss: 6.053990]\n",
      "8949 [Discriminator loss: 0.067585, acc.: 96.88%] [Generator loss: 5.649390]\n",
      "8950 [Discriminator loss: 0.409135, acc.: 81.25%] [Generator loss: 7.908787]\n",
      "8951 [Discriminator loss: 0.232969, acc.: 90.62%] [Generator loss: 7.251500]\n",
      "8952 [Discriminator loss: 0.152334, acc.: 93.75%] [Generator loss: 6.063741]\n",
      "8953 [Discriminator loss: 0.168748, acc.: 95.31%] [Generator loss: 7.357760]\n",
      "8954 [Discriminator loss: 0.070035, acc.: 96.88%] [Generator loss: 5.555215]\n",
      "8955 [Discriminator loss: 0.157631, acc.: 93.75%] [Generator loss: 5.070267]\n",
      "8956 [Discriminator loss: 0.085319, acc.: 96.88%] [Generator loss: 5.614412]\n",
      "8957 [Discriminator loss: 0.126036, acc.: 95.31%] [Generator loss: 7.444564]\n",
      "8958 [Discriminator loss: 0.105432, acc.: 96.88%] [Generator loss: 7.014215]\n",
      "8959 [Discriminator loss: 0.378851, acc.: 84.38%] [Generator loss: 5.903122]\n",
      "8960 [Discriminator loss: 0.221397, acc.: 92.19%] [Generator loss: 5.393591]\n",
      "8961 [Discriminator loss: 0.125202, acc.: 95.31%] [Generator loss: 5.288781]\n",
      "8962 [Discriminator loss: 0.120625, acc.: 93.75%] [Generator loss: 5.613352]\n",
      "8963 [Discriminator loss: 0.335660, acc.: 89.06%] [Generator loss: 6.302679]\n",
      "8964 [Discriminator loss: 0.122612, acc.: 93.75%] [Generator loss: 7.174046]\n",
      "8965 [Discriminator loss: 0.101633, acc.: 95.31%] [Generator loss: 5.622485]\n",
      "8966 [Discriminator loss: 0.137904, acc.: 95.31%] [Generator loss: 4.987041]\n",
      "8967 [Discriminator loss: 0.086385, acc.: 96.88%] [Generator loss: 6.269075]\n",
      "8968 [Discriminator loss: 0.075280, acc.: 98.44%] [Generator loss: 5.345290]\n",
      "8969 [Discriminator loss: 0.156937, acc.: 93.75%] [Generator loss: 4.615074]\n",
      "8970 [Discriminator loss: 0.241640, acc.: 89.06%] [Generator loss: 5.872040]\n",
      "8971 [Discriminator loss: 0.170054, acc.: 93.75%] [Generator loss: 6.117034]\n",
      "8972 [Discriminator loss: 0.221245, acc.: 92.19%] [Generator loss: 6.005784]\n",
      "8973 [Discriminator loss: 0.106625, acc.: 93.75%] [Generator loss: 7.075292]\n",
      "8974 [Discriminator loss: 0.116335, acc.: 96.88%] [Generator loss: 6.009777]\n",
      "8975 [Discriminator loss: 0.117415, acc.: 96.88%] [Generator loss: 5.903954]\n",
      "8976 [Discriminator loss: 0.196070, acc.: 93.75%] [Generator loss: 6.356837]\n",
      "8977 [Discriminator loss: 0.149422, acc.: 93.75%] [Generator loss: 7.259931]\n",
      "8978 [Discriminator loss: 0.150377, acc.: 93.75%] [Generator loss: 6.975617]\n",
      "8979 [Discriminator loss: 0.133262, acc.: 96.88%] [Generator loss: 6.415378]\n",
      "8980 [Discriminator loss: 0.225968, acc.: 82.81%] [Generator loss: 6.518733]\n",
      "8981 [Discriminator loss: 0.181239, acc.: 90.62%] [Generator loss: 6.924219]\n",
      "8982 [Discriminator loss: 0.191985, acc.: 92.19%] [Generator loss: 6.687301]\n",
      "8983 [Discriminator loss: 0.223601, acc.: 93.75%] [Generator loss: 5.921965]\n",
      "8984 [Discriminator loss: 0.109637, acc.: 96.88%] [Generator loss: 6.361713]\n",
      "8985 [Discriminator loss: 0.200416, acc.: 89.06%] [Generator loss: 6.472486]\n",
      "8986 [Discriminator loss: 0.070969, acc.: 96.88%] [Generator loss: 5.970434]\n",
      "8987 [Discriminator loss: 0.171441, acc.: 93.75%] [Generator loss: 5.444880]\n",
      "8988 [Discriminator loss: 0.204548, acc.: 92.19%] [Generator loss: 6.748892]\n",
      "8989 [Discriminator loss: 0.352442, acc.: 85.94%] [Generator loss: 7.286913]\n",
      "8990 [Discriminator loss: 0.179864, acc.: 92.19%] [Generator loss: 6.976646]\n",
      "8991 [Discriminator loss: 0.454869, acc.: 84.38%] [Generator loss: 4.030947]\n",
      "8992 [Discriminator loss: 0.328007, acc.: 84.38%] [Generator loss: 7.428914]\n",
      "8993 [Discriminator loss: 0.048900, acc.: 100.00%] [Generator loss: 8.231264]\n",
      "8994 [Discriminator loss: 0.117739, acc.: 96.88%] [Generator loss: 6.434276]\n",
      "8995 [Discriminator loss: 0.177675, acc.: 93.75%] [Generator loss: 3.739145]\n",
      "8996 [Discriminator loss: 0.187875, acc.: 90.62%] [Generator loss: 5.302801]\n",
      "8997 [Discriminator loss: 0.059443, acc.: 96.88%] [Generator loss: 7.217646]\n",
      "8998 [Discriminator loss: 0.064827, acc.: 96.88%] [Generator loss: 6.291493]\n",
      "8999 [Discriminator loss: 0.223490, acc.: 89.06%] [Generator loss: 5.348905]\n",
      "9000 [Discriminator loss: 0.193508, acc.: 92.19%] [Generator loss: 3.694634]\n",
      "9001 [Discriminator loss: 0.215942, acc.: 92.19%] [Generator loss: 7.861125]\n",
      "9002 [Discriminator loss: 0.122577, acc.: 95.31%] [Generator loss: 7.216921]\n",
      "9003 [Discriminator loss: 0.196492, acc.: 95.31%] [Generator loss: 5.188252]\n",
      "9004 [Discriminator loss: 0.332882, acc.: 87.50%] [Generator loss: 7.630109]\n",
      "9005 [Discriminator loss: 0.103336, acc.: 95.31%] [Generator loss: 7.478703]\n",
      "9006 [Discriminator loss: 0.121378, acc.: 93.75%] [Generator loss: 6.542013]\n",
      "9007 [Discriminator loss: 0.178698, acc.: 93.75%] [Generator loss: 5.690602]\n",
      "9008 [Discriminator loss: 0.066475, acc.: 100.00%] [Generator loss: 5.639631]\n",
      "9009 [Discriminator loss: 0.149556, acc.: 95.31%] [Generator loss: 7.241973]\n",
      "9010 [Discriminator loss: 0.132015, acc.: 96.88%] [Generator loss: 4.819215]\n",
      "9011 [Discriminator loss: 0.252206, acc.: 89.06%] [Generator loss: 5.712478]\n",
      "9012 [Discriminator loss: 0.351849, acc.: 85.94%] [Generator loss: 6.300478]\n",
      "9013 [Discriminator loss: 0.049327, acc.: 98.44%] [Generator loss: 7.382781]\n",
      "9014 [Discriminator loss: 0.120004, acc.: 93.75%] [Generator loss: 5.904805]\n",
      "9015 [Discriminator loss: 0.135978, acc.: 96.88%] [Generator loss: 5.156135]\n",
      "9016 [Discriminator loss: 0.166129, acc.: 92.19%] [Generator loss: 5.019273]\n",
      "9017 [Discriminator loss: 0.074426, acc.: 98.44%] [Generator loss: 5.821497]\n",
      "9018 [Discriminator loss: 0.089154, acc.: 96.88%] [Generator loss: 7.092020]\n",
      "9019 [Discriminator loss: 0.076275, acc.: 96.88%] [Generator loss: 6.530732]\n",
      "9020 [Discriminator loss: 0.264909, acc.: 89.06%] [Generator loss: 5.544206]\n",
      "9021 [Discriminator loss: 0.206443, acc.: 89.06%] [Generator loss: 5.787877]\n",
      "9022 [Discriminator loss: 0.107591, acc.: 95.31%] [Generator loss: 6.690559]\n",
      "9023 [Discriminator loss: 0.053515, acc.: 100.00%] [Generator loss: 5.870810]\n",
      "9024 [Discriminator loss: 0.156900, acc.: 90.62%] [Generator loss: 5.741568]\n",
      "9025 [Discriminator loss: 0.089324, acc.: 95.31%] [Generator loss: 5.469188]\n",
      "9026 [Discriminator loss: 0.193746, acc.: 92.19%] [Generator loss: 6.397999]\n",
      "9027 [Discriminator loss: 0.066180, acc.: 100.00%] [Generator loss: 6.128760]\n",
      "9028 [Discriminator loss: 0.090278, acc.: 98.44%] [Generator loss: 4.255624]\n",
      "9029 [Discriminator loss: 0.124803, acc.: 96.88%] [Generator loss: 6.590954]\n",
      "9030 [Discriminator loss: 0.082584, acc.: 96.88%] [Generator loss: 6.997283]\n",
      "9031 [Discriminator loss: 0.305693, acc.: 82.81%] [Generator loss: 5.436048]\n",
      "9032 [Discriminator loss: 0.101198, acc.: 93.75%] [Generator loss: 5.581718]\n",
      "9033 [Discriminator loss: 0.083386, acc.: 96.88%] [Generator loss: 6.866255]\n",
      "9034 [Discriminator loss: 0.132889, acc.: 93.75%] [Generator loss: 5.522303]\n",
      "9035 [Discriminator loss: 0.334741, acc.: 82.81%] [Generator loss: 6.048097]\n",
      "9036 [Discriminator loss: 0.084093, acc.: 96.88%] [Generator loss: 6.117435]\n",
      "9037 [Discriminator loss: 0.149948, acc.: 95.31%] [Generator loss: 6.333784]\n",
      "9038 [Discriminator loss: 0.154786, acc.: 92.19%] [Generator loss: 5.844476]\n",
      "9039 [Discriminator loss: 0.233019, acc.: 89.06%] [Generator loss: 6.176087]\n",
      "9040 [Discriminator loss: 0.104101, acc.: 93.75%] [Generator loss: 5.399632]\n",
      "9041 [Discriminator loss: 0.248875, acc.: 93.75%] [Generator loss: 5.739635]\n",
      "9042 [Discriminator loss: 0.240155, acc.: 87.50%] [Generator loss: 5.282742]\n",
      "9043 [Discriminator loss: 0.044440, acc.: 98.44%] [Generator loss: 6.157465]\n",
      "9044 [Discriminator loss: 0.166778, acc.: 95.31%] [Generator loss: 7.478889]\n",
      "9045 [Discriminator loss: 0.291643, acc.: 87.50%] [Generator loss: 6.655556]\n",
      "9046 [Discriminator loss: 0.118952, acc.: 96.88%] [Generator loss: 6.219432]\n",
      "9047 [Discriminator loss: 0.092118, acc.: 95.31%] [Generator loss: 6.325912]\n",
      "9048 [Discriminator loss: 0.160845, acc.: 92.19%] [Generator loss: 6.710612]\n",
      "9049 [Discriminator loss: 0.070236, acc.: 96.88%] [Generator loss: 5.892313]\n",
      "9050 [Discriminator loss: 0.109258, acc.: 98.44%] [Generator loss: 6.751780]\n",
      "9051 [Discriminator loss: 0.254357, acc.: 89.06%] [Generator loss: 5.621452]\n",
      "9052 [Discriminator loss: 0.186390, acc.: 93.75%] [Generator loss: 6.060521]\n",
      "9053 [Discriminator loss: 0.111463, acc.: 93.75%] [Generator loss: 6.139272]\n",
      "9054 [Discriminator loss: 0.187900, acc.: 95.31%] [Generator loss: 5.235362]\n",
      "9055 [Discriminator loss: 0.079407, acc.: 96.88%] [Generator loss: 5.820112]\n",
      "9056 [Discriminator loss: 0.127374, acc.: 95.31%] [Generator loss: 5.621527]\n",
      "9057 [Discriminator loss: 0.226421, acc.: 90.62%] [Generator loss: 8.445236]\n",
      "9058 [Discriminator loss: 0.328845, acc.: 87.50%] [Generator loss: 6.041544]\n",
      "9059 [Discriminator loss: 0.121753, acc.: 96.88%] [Generator loss: 6.397742]\n",
      "9060 [Discriminator loss: 0.182410, acc.: 92.19%] [Generator loss: 7.302045]\n",
      "9061 [Discriminator loss: 0.208663, acc.: 93.75%] [Generator loss: 7.202675]\n",
      "9062 [Discriminator loss: 0.349534, acc.: 84.38%] [Generator loss: 5.890522]\n",
      "9063 [Discriminator loss: 0.275406, acc.: 90.62%] [Generator loss: 6.735621]\n",
      "9064 [Discriminator loss: 0.193786, acc.: 92.19%] [Generator loss: 8.664225]\n",
      "9065 [Discriminator loss: 0.086818, acc.: 95.31%] [Generator loss: 5.118628]\n",
      "9066 [Discriminator loss: 0.188604, acc.: 92.19%] [Generator loss: 4.664852]\n",
      "9067 [Discriminator loss: 0.130134, acc.: 95.31%] [Generator loss: 7.314620]\n",
      "9068 [Discriminator loss: 0.089348, acc.: 96.88%] [Generator loss: 7.846509]\n",
      "9069 [Discriminator loss: 0.215216, acc.: 92.19%] [Generator loss: 5.488143]\n",
      "9070 [Discriminator loss: 0.151136, acc.: 95.31%] [Generator loss: 6.648571]\n",
      "9071 [Discriminator loss: 0.272413, acc.: 90.62%] [Generator loss: 6.501004]\n",
      "9072 [Discriminator loss: 0.079877, acc.: 98.44%] [Generator loss: 6.494154]\n",
      "9073 [Discriminator loss: 0.385695, acc.: 82.81%] [Generator loss: 7.666767]\n",
      "9074 [Discriminator loss: 0.201986, acc.: 89.06%] [Generator loss: 6.836744]\n",
      "9075 [Discriminator loss: 0.131021, acc.: 93.75%] [Generator loss: 7.237373]\n",
      "9076 [Discriminator loss: 0.232592, acc.: 87.50%] [Generator loss: 8.256249]\n",
      "9077 [Discriminator loss: 0.056734, acc.: 98.44%] [Generator loss: 7.465881]\n",
      "9078 [Discriminator loss: 0.081831, acc.: 98.44%] [Generator loss: 7.121929]\n",
      "9079 [Discriminator loss: 0.257287, acc.: 90.62%] [Generator loss: 5.439291]\n",
      "9080 [Discriminator loss: 0.204644, acc.: 92.19%] [Generator loss: 6.960059]\n",
      "9081 [Discriminator loss: 0.176703, acc.: 92.19%] [Generator loss: 6.064776]\n",
      "9082 [Discriminator loss: 0.142858, acc.: 93.75%] [Generator loss: 5.225944]\n",
      "9083 [Discriminator loss: 0.095121, acc.: 95.31%] [Generator loss: 5.658500]\n",
      "9084 [Discriminator loss: 0.078112, acc.: 96.88%] [Generator loss: 7.648529]\n",
      "9085 [Discriminator loss: 0.151134, acc.: 93.75%] [Generator loss: 5.472603]\n",
      "9086 [Discriminator loss: 0.189361, acc.: 93.75%] [Generator loss: 5.838097]\n",
      "9087 [Discriminator loss: 0.199969, acc.: 92.19%] [Generator loss: 6.111794]\n",
      "9088 [Discriminator loss: 0.151346, acc.: 90.62%] [Generator loss: 7.110609]\n",
      "9089 [Discriminator loss: 0.059901, acc.: 100.00%] [Generator loss: 6.277652]\n",
      "9090 [Discriminator loss: 0.035083, acc.: 100.00%] [Generator loss: 6.701125]\n",
      "9091 [Discriminator loss: 0.065937, acc.: 98.44%] [Generator loss: 5.025227]\n",
      "9092 [Discriminator loss: 0.158933, acc.: 92.19%] [Generator loss: 5.143703]\n",
      "9093 [Discriminator loss: 0.154168, acc.: 92.19%] [Generator loss: 5.416840]\n",
      "9094 [Discriminator loss: 0.097442, acc.: 93.75%] [Generator loss: 5.778467]\n",
      "9095 [Discriminator loss: 0.095837, acc.: 98.44%] [Generator loss: 6.420640]\n",
      "9096 [Discriminator loss: 0.155752, acc.: 95.31%] [Generator loss: 5.479603]\n",
      "9097 [Discriminator loss: 0.155623, acc.: 93.75%] [Generator loss: 7.012691]\n",
      "9098 [Discriminator loss: 0.141301, acc.: 95.31%] [Generator loss: 6.196177]\n",
      "9099 [Discriminator loss: 0.101076, acc.: 96.88%] [Generator loss: 6.434748]\n",
      "9100 [Discriminator loss: 0.343690, acc.: 90.62%] [Generator loss: 6.189258]\n",
      "9101 [Discriminator loss: 0.042640, acc.: 100.00%] [Generator loss: 5.699340]\n",
      "9102 [Discriminator loss: 0.162532, acc.: 93.75%] [Generator loss: 6.445942]\n",
      "9103 [Discriminator loss: 0.186159, acc.: 95.31%] [Generator loss: 5.582890]\n",
      "9104 [Discriminator loss: 0.114244, acc.: 96.88%] [Generator loss: 5.143175]\n",
      "9105 [Discriminator loss: 0.099210, acc.: 96.88%] [Generator loss: 6.108465]\n",
      "9106 [Discriminator loss: 0.064682, acc.: 96.88%] [Generator loss: 6.454138]\n",
      "9107 [Discriminator loss: 0.127910, acc.: 95.31%] [Generator loss: 5.891891]\n",
      "9108 [Discriminator loss: 0.152428, acc.: 89.06%] [Generator loss: 7.126512]\n",
      "9109 [Discriminator loss: 0.094287, acc.: 93.75%] [Generator loss: 5.646902]\n",
      "9110 [Discriminator loss: 0.148799, acc.: 95.31%] [Generator loss: 6.827906]\n",
      "9111 [Discriminator loss: 0.237067, acc.: 89.06%] [Generator loss: 6.252256]\n",
      "9112 [Discriminator loss: 0.140613, acc.: 93.75%] [Generator loss: 6.175010]\n",
      "9113 [Discriminator loss: 0.122512, acc.: 93.75%] [Generator loss: 6.192146]\n",
      "9114 [Discriminator loss: 0.102330, acc.: 96.88%] [Generator loss: 5.780149]\n",
      "9115 [Discriminator loss: 0.050699, acc.: 100.00%] [Generator loss: 6.426835]\n",
      "9116 [Discriminator loss: 0.063911, acc.: 100.00%] [Generator loss: 5.126482]\n",
      "9117 [Discriminator loss: 0.083210, acc.: 96.88%] [Generator loss: 7.427156]\n",
      "9118 [Discriminator loss: 0.115007, acc.: 95.31%] [Generator loss: 5.923317]\n",
      "9119 [Discriminator loss: 0.355664, acc.: 89.06%] [Generator loss: 6.057454]\n",
      "9120 [Discriminator loss: 0.110993, acc.: 93.75%] [Generator loss: 5.263487]\n",
      "9121 [Discriminator loss: 0.173696, acc.: 93.75%] [Generator loss: 5.299011]\n",
      "9122 [Discriminator loss: 0.035090, acc.: 98.44%] [Generator loss: 7.977187]\n",
      "9123 [Discriminator loss: 0.127646, acc.: 93.75%] [Generator loss: 5.995892]\n",
      "9124 [Discriminator loss: 0.227371, acc.: 89.06%] [Generator loss: 6.074598]\n",
      "9125 [Discriminator loss: 0.128818, acc.: 93.75%] [Generator loss: 5.898452]\n",
      "9126 [Discriminator loss: 0.097977, acc.: 93.75%] [Generator loss: 6.602751]\n",
      "9127 [Discriminator loss: 0.270536, acc.: 89.06%] [Generator loss: 6.344654]\n",
      "9128 [Discriminator loss: 0.070163, acc.: 98.44%] [Generator loss: 6.382035]\n",
      "9129 [Discriminator loss: 0.119782, acc.: 96.88%] [Generator loss: 5.728512]\n",
      "9130 [Discriminator loss: 0.119591, acc.: 93.75%] [Generator loss: 6.939835]\n",
      "9131 [Discriminator loss: 0.273524, acc.: 89.06%] [Generator loss: 5.740425]\n",
      "9132 [Discriminator loss: 0.192261, acc.: 90.62%] [Generator loss: 7.305367]\n",
      "9133 [Discriminator loss: 0.064255, acc.: 96.88%] [Generator loss: 8.498447]\n",
      "9134 [Discriminator loss: 0.158385, acc.: 95.31%] [Generator loss: 7.251450]\n",
      "9135 [Discriminator loss: 0.123280, acc.: 95.31%] [Generator loss: 7.450200]\n",
      "9136 [Discriminator loss: 0.081590, acc.: 96.88%] [Generator loss: 6.931851]\n",
      "9137 [Discriminator loss: 0.101580, acc.: 95.31%] [Generator loss: 5.631971]\n",
      "9138 [Discriminator loss: 0.214424, acc.: 92.19%] [Generator loss: 6.464247]\n",
      "9139 [Discriminator loss: 0.092540, acc.: 96.88%] [Generator loss: 5.936459]\n",
      "9140 [Discriminator loss: 0.062404, acc.: 100.00%] [Generator loss: 5.304402]\n",
      "9141 [Discriminator loss: 0.248520, acc.: 90.62%] [Generator loss: 5.859300]\n",
      "9142 [Discriminator loss: 0.090177, acc.: 95.31%] [Generator loss: 6.998262]\n",
      "9143 [Discriminator loss: 0.264679, acc.: 89.06%] [Generator loss: 5.434247]\n",
      "9144 [Discriminator loss: 0.082915, acc.: 98.44%] [Generator loss: 6.949328]\n",
      "9145 [Discriminator loss: 0.131407, acc.: 93.75%] [Generator loss: 7.062803]\n",
      "9146 [Discriminator loss: 0.046843, acc.: 100.00%] [Generator loss: 6.530482]\n",
      "9147 [Discriminator loss: 0.059503, acc.: 100.00%] [Generator loss: 6.442512]\n",
      "9148 [Discriminator loss: 0.173848, acc.: 92.19%] [Generator loss: 5.446268]\n",
      "9149 [Discriminator loss: 0.150089, acc.: 93.75%] [Generator loss: 6.993644]\n",
      "9150 [Discriminator loss: 0.092957, acc.: 96.88%] [Generator loss: 6.668270]\n",
      "9151 [Discriminator loss: 0.161306, acc.: 95.31%] [Generator loss: 6.571141]\n",
      "9152 [Discriminator loss: 0.151480, acc.: 93.75%] [Generator loss: 7.090784]\n",
      "9153 [Discriminator loss: 0.184648, acc.: 89.06%] [Generator loss: 5.805949]\n",
      "9154 [Discriminator loss: 0.219418, acc.: 87.50%] [Generator loss: 4.966162]\n",
      "9155 [Discriminator loss: 0.059299, acc.: 98.44%] [Generator loss: 5.054979]\n",
      "9156 [Discriminator loss: 0.091362, acc.: 96.88%] [Generator loss: 6.526636]\n",
      "9157 [Discriminator loss: 0.144632, acc.: 93.75%] [Generator loss: 8.518143]\n",
      "9158 [Discriminator loss: 0.071976, acc.: 98.44%] [Generator loss: 6.034847]\n",
      "9159 [Discriminator loss: 0.133416, acc.: 95.31%] [Generator loss: 4.986192]\n",
      "9160 [Discriminator loss: 0.156512, acc.: 92.19%] [Generator loss: 4.470732]\n",
      "9161 [Discriminator loss: 0.129199, acc.: 95.31%] [Generator loss: 6.156975]\n",
      "9162 [Discriminator loss: 0.078344, acc.: 96.88%] [Generator loss: 6.018557]\n",
      "9163 [Discriminator loss: 0.142060, acc.: 93.75%] [Generator loss: 4.171350]\n",
      "9164 [Discriminator loss: 0.089945, acc.: 96.88%] [Generator loss: 5.980270]\n",
      "9165 [Discriminator loss: 0.268626, acc.: 89.06%] [Generator loss: 6.876760]\n",
      "9166 [Discriminator loss: 0.165592, acc.: 93.75%] [Generator loss: 7.170083]\n",
      "9167 [Discriminator loss: 0.151521, acc.: 93.75%] [Generator loss: 5.051685]\n",
      "9168 [Discriminator loss: 0.134460, acc.: 92.19%] [Generator loss: 6.990599]\n",
      "9169 [Discriminator loss: 0.071849, acc.: 96.88%] [Generator loss: 7.322246]\n",
      "9170 [Discriminator loss: 0.188689, acc.: 89.06%] [Generator loss: 5.898396]\n",
      "9171 [Discriminator loss: 0.061594, acc.: 98.44%] [Generator loss: 5.962403]\n",
      "9172 [Discriminator loss: 0.203190, acc.: 95.31%] [Generator loss: 5.716501]\n",
      "9173 [Discriminator loss: 0.064035, acc.: 98.44%] [Generator loss: 5.324327]\n",
      "9174 [Discriminator loss: 0.130098, acc.: 93.75%] [Generator loss: 6.178295]\n",
      "9175 [Discriminator loss: 0.115761, acc.: 96.88%] [Generator loss: 5.795683]\n",
      "9176 [Discriminator loss: 0.209229, acc.: 92.19%] [Generator loss: 4.259301]\n",
      "9177 [Discriminator loss: 0.125099, acc.: 93.75%] [Generator loss: 5.289544]\n",
      "9178 [Discriminator loss: 0.063254, acc.: 96.88%] [Generator loss: 5.560099]\n",
      "9179 [Discriminator loss: 0.070610, acc.: 93.75%] [Generator loss: 6.878335]\n",
      "9180 [Discriminator loss: 0.022777, acc.: 100.00%] [Generator loss: 5.992133]\n",
      "9181 [Discriminator loss: 0.107818, acc.: 95.31%] [Generator loss: 5.232001]\n",
      "9182 [Discriminator loss: 0.118009, acc.: 95.31%] [Generator loss: 6.634828]\n",
      "9183 [Discriminator loss: 0.272857, acc.: 89.06%] [Generator loss: 7.447595]\n",
      "9184 [Discriminator loss: 0.126514, acc.: 93.75%] [Generator loss: 6.559896]\n",
      "9185 [Discriminator loss: 0.107459, acc.: 93.75%] [Generator loss: 6.637873]\n",
      "9186 [Discriminator loss: 0.099496, acc.: 96.88%] [Generator loss: 5.684303]\n",
      "9187 [Discriminator loss: 0.041879, acc.: 100.00%] [Generator loss: 5.256927]\n",
      "9188 [Discriminator loss: 0.089635, acc.: 96.88%] [Generator loss: 6.387126]\n",
      "9189 [Discriminator loss: 0.104796, acc.: 93.75%] [Generator loss: 6.162050]\n",
      "9190 [Discriminator loss: 0.227672, acc.: 90.62%] [Generator loss: 5.838113]\n",
      "9191 [Discriminator loss: 0.061475, acc.: 98.44%] [Generator loss: 5.709947]\n",
      "9192 [Discriminator loss: 0.101695, acc.: 95.31%] [Generator loss: 5.804056]\n",
      "9193 [Discriminator loss: 0.059675, acc.: 98.44%] [Generator loss: 7.057264]\n",
      "9194 [Discriminator loss: 0.121662, acc.: 93.75%] [Generator loss: 6.143479]\n",
      "9195 [Discriminator loss: 0.099381, acc.: 95.31%] [Generator loss: 6.671774]\n",
      "9196 [Discriminator loss: 0.104112, acc.: 96.88%] [Generator loss: 6.388075]\n",
      "9197 [Discriminator loss: 0.190331, acc.: 90.62%] [Generator loss: 6.657074]\n",
      "9198 [Discriminator loss: 0.181189, acc.: 90.62%] [Generator loss: 7.893649]\n",
      "9199 [Discriminator loss: 0.233346, acc.: 84.38%] [Generator loss: 5.342075]\n",
      "9200 [Discriminator loss: 0.171974, acc.: 92.19%] [Generator loss: 5.674536]\n",
      "9201 [Discriminator loss: 0.051462, acc.: 98.44%] [Generator loss: 6.912930]\n",
      "9202 [Discriminator loss: 0.067918, acc.: 98.44%] [Generator loss: 6.133358]\n",
      "9203 [Discriminator loss: 0.226646, acc.: 90.62%] [Generator loss: 6.312343]\n",
      "9204 [Discriminator loss: 0.311067, acc.: 89.06%] [Generator loss: 5.425688]\n",
      "9205 [Discriminator loss: 0.113406, acc.: 95.31%] [Generator loss: 5.544424]\n",
      "9206 [Discriminator loss: 0.125880, acc.: 93.75%] [Generator loss: 6.676705]\n",
      "9207 [Discriminator loss: 0.060766, acc.: 96.88%] [Generator loss: 6.903895]\n",
      "9208 [Discriminator loss: 0.210379, acc.: 89.06%] [Generator loss: 7.148573]\n",
      "9209 [Discriminator loss: 0.134705, acc.: 95.31%] [Generator loss: 6.237595]\n",
      "9210 [Discriminator loss: 0.201920, acc.: 92.19%] [Generator loss: 6.410117]\n",
      "9211 [Discriminator loss: 0.090187, acc.: 96.88%] [Generator loss: 7.203524]\n",
      "9212 [Discriminator loss: 0.082990, acc.: 96.88%] [Generator loss: 5.221653]\n",
      "9213 [Discriminator loss: 0.209333, acc.: 89.06%] [Generator loss: 5.835997]\n",
      "9214 [Discriminator loss: 0.042029, acc.: 100.00%] [Generator loss: 6.322906]\n",
      "9215 [Discriminator loss: 0.150753, acc.: 93.75%] [Generator loss: 5.193438]\n",
      "9216 [Discriminator loss: 0.067758, acc.: 96.88%] [Generator loss: 6.001372]\n",
      "9217 [Discriminator loss: 0.112456, acc.: 95.31%] [Generator loss: 7.171564]\n",
      "9218 [Discriminator loss: 0.067070, acc.: 96.88%] [Generator loss: 5.422990]\n",
      "9219 [Discriminator loss: 0.099258, acc.: 95.31%] [Generator loss: 6.310644]\n",
      "9220 [Discriminator loss: 0.244385, acc.: 90.62%] [Generator loss: 6.157297]\n",
      "9221 [Discriminator loss: 0.094982, acc.: 95.31%] [Generator loss: 6.170685]\n",
      "9222 [Discriminator loss: 0.203078, acc.: 93.75%] [Generator loss: 7.158731]\n",
      "9223 [Discriminator loss: 0.145219, acc.: 96.88%] [Generator loss: 6.384937]\n",
      "9224 [Discriminator loss: 0.098566, acc.: 96.88%] [Generator loss: 5.971073]\n",
      "9225 [Discriminator loss: 0.085827, acc.: 93.75%] [Generator loss: 6.470008]\n",
      "9226 [Discriminator loss: 0.039288, acc.: 98.44%] [Generator loss: 6.499323]\n",
      "9227 [Discriminator loss: 0.144366, acc.: 93.75%] [Generator loss: 7.191327]\n",
      "9228 [Discriminator loss: 0.231269, acc.: 92.19%] [Generator loss: 7.825471]\n",
      "9229 [Discriminator loss: 0.167301, acc.: 90.62%] [Generator loss: 6.426292]\n",
      "9230 [Discriminator loss: 0.126210, acc.: 95.31%] [Generator loss: 5.109395]\n",
      "9231 [Discriminator loss: 0.072769, acc.: 98.44%] [Generator loss: 5.825432]\n",
      "9232 [Discriminator loss: 0.037461, acc.: 100.00%] [Generator loss: 5.601346]\n",
      "9233 [Discriminator loss: 0.127827, acc.: 95.31%] [Generator loss: 6.336447]\n",
      "9234 [Discriminator loss: 0.061790, acc.: 96.88%] [Generator loss: 6.876745]\n",
      "9235 [Discriminator loss: 0.258762, acc.: 89.06%] [Generator loss: 4.663117]\n",
      "9236 [Discriminator loss: 0.128874, acc.: 93.75%] [Generator loss: 6.161567]\n",
      "9237 [Discriminator loss: 0.165978, acc.: 92.19%] [Generator loss: 5.687948]\n",
      "9238 [Discriminator loss: 0.146939, acc.: 93.75%] [Generator loss: 6.582577]\n",
      "9239 [Discriminator loss: 0.148721, acc.: 93.75%] [Generator loss: 5.953794]\n",
      "9240 [Discriminator loss: 0.082264, acc.: 98.44%] [Generator loss: 5.296174]\n",
      "9241 [Discriminator loss: 0.097690, acc.: 96.88%] [Generator loss: 5.632861]\n",
      "9242 [Discriminator loss: 0.086962, acc.: 96.88%] [Generator loss: 6.491256]\n",
      "9243 [Discriminator loss: 0.173660, acc.: 92.19%] [Generator loss: 4.969928]\n",
      "9244 [Discriminator loss: 0.124560, acc.: 95.31%] [Generator loss: 5.781686]\n",
      "9245 [Discriminator loss: 0.197365, acc.: 93.75%] [Generator loss: 5.943336]\n",
      "9246 [Discriminator loss: 0.157245, acc.: 92.19%] [Generator loss: 7.411692]\n",
      "9247 [Discriminator loss: 0.491252, acc.: 78.12%] [Generator loss: 5.234733]\n",
      "9248 [Discriminator loss: 0.048230, acc.: 98.44%] [Generator loss: 5.964001]\n",
      "9249 [Discriminator loss: 0.106905, acc.: 95.31%] [Generator loss: 6.082847]\n",
      "9250 [Discriminator loss: 0.114085, acc.: 95.31%] [Generator loss: 6.431716]\n",
      "9251 [Discriminator loss: 0.119343, acc.: 96.88%] [Generator loss: 6.284132]\n",
      "9252 [Discriminator loss: 0.196048, acc.: 93.75%] [Generator loss: 6.161657]\n",
      "9253 [Discriminator loss: 0.087009, acc.: 96.88%] [Generator loss: 5.618640]\n",
      "9254 [Discriminator loss: 0.138939, acc.: 95.31%] [Generator loss: 5.683650]\n",
      "9255 [Discriminator loss: 0.118855, acc.: 96.88%] [Generator loss: 5.997163]\n",
      "9256 [Discriminator loss: 0.281018, acc.: 89.06%] [Generator loss: 6.571753]\n",
      "9257 [Discriminator loss: 0.163976, acc.: 95.31%] [Generator loss: 5.157844]\n",
      "9258 [Discriminator loss: 0.024250, acc.: 100.00%] [Generator loss: 4.547004]\n",
      "9259 [Discriminator loss: 0.136896, acc.: 96.88%] [Generator loss: 5.289806]\n",
      "9260 [Discriminator loss: 0.183876, acc.: 92.19%] [Generator loss: 7.033317]\n",
      "9261 [Discriminator loss: 0.191727, acc.: 92.19%] [Generator loss: 7.624723]\n",
      "9262 [Discriminator loss: 0.182539, acc.: 93.75%] [Generator loss: 7.885538]\n",
      "9263 [Discriminator loss: 0.196368, acc.: 93.75%] [Generator loss: 5.366351]\n",
      "9264 [Discriminator loss: 0.196388, acc.: 90.62%] [Generator loss: 5.092648]\n",
      "9265 [Discriminator loss: 0.117172, acc.: 96.88%] [Generator loss: 6.570114]\n",
      "9266 [Discriminator loss: 0.164577, acc.: 93.75%] [Generator loss: 5.160120]\n",
      "9267 [Discriminator loss: 0.109629, acc.: 92.19%] [Generator loss: 5.369014]\n",
      "9268 [Discriminator loss: 0.104471, acc.: 96.88%] [Generator loss: 6.768661]\n",
      "9269 [Discriminator loss: 0.167885, acc.: 93.75%] [Generator loss: 6.187486]\n",
      "9270 [Discriminator loss: 0.237529, acc.: 90.62%] [Generator loss: 6.533010]\n",
      "9271 [Discriminator loss: 0.080857, acc.: 95.31%] [Generator loss: 8.256277]\n",
      "9272 [Discriminator loss: 0.113877, acc.: 95.31%] [Generator loss: 6.987465]\n",
      "9273 [Discriminator loss: 0.207018, acc.: 93.75%] [Generator loss: 5.288315]\n",
      "9274 [Discriminator loss: 0.249181, acc.: 90.62%] [Generator loss: 5.732959]\n",
      "9275 [Discriminator loss: 0.160442, acc.: 92.19%] [Generator loss: 6.459191]\n",
      "9276 [Discriminator loss: 0.106208, acc.: 95.31%] [Generator loss: 6.080537]\n",
      "9277 [Discriminator loss: 0.097827, acc.: 96.88%] [Generator loss: 5.883266]\n",
      "9278 [Discriminator loss: 0.434371, acc.: 84.38%] [Generator loss: 6.282884]\n",
      "9279 [Discriminator loss: 0.065103, acc.: 98.44%] [Generator loss: 5.852894]\n",
      "9280 [Discriminator loss: 0.160714, acc.: 90.62%] [Generator loss: 5.660334]\n",
      "9281 [Discriminator loss: 0.113149, acc.: 93.75%] [Generator loss: 6.074838]\n",
      "9282 [Discriminator loss: 0.133783, acc.: 93.75%] [Generator loss: 6.423820]\n",
      "9283 [Discriminator loss: 0.154946, acc.: 90.62%] [Generator loss: 7.147152]\n",
      "9284 [Discriminator loss: 0.081370, acc.: 98.44%] [Generator loss: 6.459216]\n",
      "9285 [Discriminator loss: 0.237938, acc.: 87.50%] [Generator loss: 6.344511]\n",
      "9286 [Discriminator loss: 0.156143, acc.: 92.19%] [Generator loss: 7.733649]\n",
      "9287 [Discriminator loss: 0.064230, acc.: 98.44%] [Generator loss: 7.313653]\n",
      "9288 [Discriminator loss: 0.116056, acc.: 95.31%] [Generator loss: 6.238101]\n",
      "9289 [Discriminator loss: 0.258613, acc.: 87.50%] [Generator loss: 4.823813]\n",
      "9290 [Discriminator loss: 0.108344, acc.: 93.75%] [Generator loss: 6.958193]\n",
      "9291 [Discriminator loss: 0.153876, acc.: 92.19%] [Generator loss: 5.787940]\n",
      "9292 [Discriminator loss: 0.126560, acc.: 93.75%] [Generator loss: 5.540689]\n",
      "9293 [Discriminator loss: 0.141790, acc.: 92.19%] [Generator loss: 4.571867]\n",
      "9294 [Discriminator loss: 0.063425, acc.: 96.88%] [Generator loss: 5.322478]\n",
      "9295 [Discriminator loss: 0.103467, acc.: 96.88%] [Generator loss: 6.794917]\n",
      "9296 [Discriminator loss: 0.159335, acc.: 93.75%] [Generator loss: 5.972833]\n",
      "9297 [Discriminator loss: 0.093392, acc.: 96.88%] [Generator loss: 3.886576]\n",
      "9298 [Discriminator loss: 0.164503, acc.: 93.75%] [Generator loss: 5.972221]\n",
      "9299 [Discriminator loss: 0.199160, acc.: 92.19%] [Generator loss: 6.169716]\n",
      "9300 [Discriminator loss: 0.052317, acc.: 100.00%] [Generator loss: 6.322595]\n",
      "9301 [Discriminator loss: 0.070532, acc.: 96.88%] [Generator loss: 7.155012]\n",
      "9302 [Discriminator loss: 0.122742, acc.: 92.19%] [Generator loss: 6.261107]\n",
      "9303 [Discriminator loss: 0.066185, acc.: 100.00%] [Generator loss: 7.028164]\n",
      "9304 [Discriminator loss: 0.120144, acc.: 95.31%] [Generator loss: 6.568579]\n",
      "9305 [Discriminator loss: 0.067691, acc.: 96.88%] [Generator loss: 5.299140]\n",
      "9306 [Discriminator loss: 0.086957, acc.: 96.88%] [Generator loss: 5.367454]\n",
      "9307 [Discriminator loss: 0.043369, acc.: 98.44%] [Generator loss: 5.545031]\n",
      "9308 [Discriminator loss: 0.103468, acc.: 95.31%] [Generator loss: 5.492632]\n",
      "9309 [Discriminator loss: 0.154544, acc.: 95.31%] [Generator loss: 6.479458]\n",
      "9310 [Discriminator loss: 0.113002, acc.: 95.31%] [Generator loss: 5.634679]\n",
      "9311 [Discriminator loss: 0.103530, acc.: 95.31%] [Generator loss: 6.059005]\n",
      "9312 [Discriminator loss: 0.215415, acc.: 95.31%] [Generator loss: 8.133928]\n",
      "9313 [Discriminator loss: 0.120535, acc.: 96.88%] [Generator loss: 6.679000]\n",
      "9314 [Discriminator loss: 0.225868, acc.: 95.31%] [Generator loss: 5.371342]\n",
      "9315 [Discriminator loss: 0.117582, acc.: 93.75%] [Generator loss: 7.340790]\n",
      "9316 [Discriminator loss: 0.108080, acc.: 95.31%] [Generator loss: 6.187197]\n",
      "9317 [Discriminator loss: 0.187954, acc.: 90.62%] [Generator loss: 5.711621]\n",
      "9318 [Discriminator loss: 0.162072, acc.: 92.19%] [Generator loss: 7.175428]\n",
      "9319 [Discriminator loss: 0.066417, acc.: 98.44%] [Generator loss: 5.422375]\n",
      "9320 [Discriminator loss: 0.170110, acc.: 95.31%] [Generator loss: 5.445116]\n",
      "9321 [Discriminator loss: 0.121441, acc.: 93.75%] [Generator loss: 6.856555]\n",
      "9322 [Discriminator loss: 0.126762, acc.: 92.19%] [Generator loss: 7.097559]\n",
      "9323 [Discriminator loss: 0.241382, acc.: 90.62%] [Generator loss: 7.124634]\n",
      "9324 [Discriminator loss: 0.084064, acc.: 96.88%] [Generator loss: 5.984291]\n",
      "9325 [Discriminator loss: 0.061897, acc.: 98.44%] [Generator loss: 6.059836]\n",
      "9326 [Discriminator loss: 0.094454, acc.: 95.31%] [Generator loss: 4.436551]\n",
      "9327 [Discriminator loss: 0.130388, acc.: 95.31%] [Generator loss: 6.036876]\n",
      "9328 [Discriminator loss: 0.100941, acc.: 95.31%] [Generator loss: 5.549316]\n",
      "9329 [Discriminator loss: 0.119653, acc.: 96.88%] [Generator loss: 5.587317]\n",
      "9330 [Discriminator loss: 0.080884, acc.: 96.88%] [Generator loss: 6.327474]\n",
      "9331 [Discriminator loss: 0.098281, acc.: 96.88%] [Generator loss: 6.849328]\n",
      "9332 [Discriminator loss: 0.209632, acc.: 95.31%] [Generator loss: 5.544752]\n",
      "9333 [Discriminator loss: 0.139219, acc.: 93.75%] [Generator loss: 6.850955]\n",
      "9334 [Discriminator loss: 0.022157, acc.: 100.00%] [Generator loss: 6.856981]\n",
      "9335 [Discriminator loss: 0.055632, acc.: 98.44%] [Generator loss: 5.542705]\n",
      "9336 [Discriminator loss: 0.062987, acc.: 96.88%] [Generator loss: 5.846581]\n",
      "9337 [Discriminator loss: 0.045784, acc.: 98.44%] [Generator loss: 6.639915]\n",
      "9338 [Discriminator loss: 0.203799, acc.: 92.19%] [Generator loss: 5.260032]\n",
      "9339 [Discriminator loss: 0.224995, acc.: 92.19%] [Generator loss: 4.417404]\n",
      "9340 [Discriminator loss: 0.189230, acc.: 93.75%] [Generator loss: 6.028397]\n",
      "9341 [Discriminator loss: 0.116970, acc.: 95.31%] [Generator loss: 7.067433]\n",
      "9342 [Discriminator loss: 0.158197, acc.: 93.75%] [Generator loss: 6.415905]\n",
      "9343 [Discriminator loss: 0.116191, acc.: 95.31%] [Generator loss: 7.046978]\n",
      "9344 [Discriminator loss: 0.137796, acc.: 93.75%] [Generator loss: 5.912281]\n",
      "9345 [Discriminator loss: 0.071001, acc.: 98.44%] [Generator loss: 6.375875]\n",
      "9346 [Discriminator loss: 0.321306, acc.: 87.50%] [Generator loss: 6.956771]\n",
      "9347 [Discriminator loss: 0.082248, acc.: 95.31%] [Generator loss: 6.829098]\n",
      "9348 [Discriminator loss: 0.281479, acc.: 90.62%] [Generator loss: 6.764064]\n",
      "9349 [Discriminator loss: 0.069049, acc.: 96.88%] [Generator loss: 7.338738]\n",
      "9350 [Discriminator loss: 0.130951, acc.: 96.88%] [Generator loss: 5.499513]\n",
      "9351 [Discriminator loss: 0.266095, acc.: 87.50%] [Generator loss: 5.817091]\n",
      "9352 [Discriminator loss: 0.090837, acc.: 96.88%] [Generator loss: 7.152637]\n",
      "9353 [Discriminator loss: 0.185006, acc.: 89.06%] [Generator loss: 5.305910]\n",
      "9354 [Discriminator loss: 0.072644, acc.: 98.44%] [Generator loss: 6.138818]\n",
      "9355 [Discriminator loss: 0.130174, acc.: 95.31%] [Generator loss: 6.847878]\n",
      "9356 [Discriminator loss: 0.233444, acc.: 89.06%] [Generator loss: 6.597180]\n",
      "9357 [Discriminator loss: 0.161924, acc.: 92.19%] [Generator loss: 7.129435]\n",
      "9358 [Discriminator loss: 0.149383, acc.: 92.19%] [Generator loss: 8.220001]\n",
      "9359 [Discriminator loss: 0.133575, acc.: 93.75%] [Generator loss: 6.000897]\n",
      "9360 [Discriminator loss: 0.231700, acc.: 85.94%] [Generator loss: 6.458458]\n",
      "9361 [Discriminator loss: 0.214940, acc.: 93.75%] [Generator loss: 5.949249]\n",
      "9362 [Discriminator loss: 0.084369, acc.: 96.88%] [Generator loss: 6.378735]\n",
      "9363 [Discriminator loss: 0.093913, acc.: 95.31%] [Generator loss: 6.847853]\n",
      "9364 [Discriminator loss: 0.186631, acc.: 92.19%] [Generator loss: 6.103317]\n",
      "9365 [Discriminator loss: 0.120620, acc.: 95.31%] [Generator loss: 6.583560]\n",
      "9366 [Discriminator loss: 0.186337, acc.: 92.19%] [Generator loss: 6.352721]\n",
      "9367 [Discriminator loss: 0.058781, acc.: 98.44%] [Generator loss: 7.075384]\n",
      "9368 [Discriminator loss: 0.234954, acc.: 93.75%] [Generator loss: 5.495050]\n",
      "9369 [Discriminator loss: 0.085798, acc.: 96.88%] [Generator loss: 5.545885]\n",
      "9370 [Discriminator loss: 0.093565, acc.: 96.88%] [Generator loss: 5.426183]\n",
      "9371 [Discriminator loss: 0.078936, acc.: 96.88%] [Generator loss: 6.618808]\n",
      "9372 [Discriminator loss: 0.110564, acc.: 96.88%] [Generator loss: 6.886367]\n",
      "9373 [Discriminator loss: 0.083717, acc.: 96.88%] [Generator loss: 6.151000]\n",
      "9374 [Discriminator loss: 0.228721, acc.: 89.06%] [Generator loss: 6.351626]\n",
      "9375 [Discriminator loss: 0.097236, acc.: 96.88%] [Generator loss: 6.363899]\n",
      "9376 [Discriminator loss: 0.113790, acc.: 93.75%] [Generator loss: 5.483316]\n",
      "9377 [Discriminator loss: 0.138382, acc.: 96.88%] [Generator loss: 6.732596]\n",
      "9378 [Discriminator loss: 0.119812, acc.: 95.31%] [Generator loss: 6.401343]\n",
      "9379 [Discriminator loss: 0.101914, acc.: 95.31%] [Generator loss: 5.195524]\n",
      "9380 [Discriminator loss: 0.173694, acc.: 92.19%] [Generator loss: 5.997589]\n",
      "9381 [Discriminator loss: 0.078270, acc.: 96.88%] [Generator loss: 7.672337]\n",
      "9382 [Discriminator loss: 0.176405, acc.: 89.06%] [Generator loss: 8.093104]\n",
      "9383 [Discriminator loss: 0.150696, acc.: 95.31%] [Generator loss: 6.507770]\n",
      "9384 [Discriminator loss: 0.142963, acc.: 92.19%] [Generator loss: 4.983355]\n",
      "9385 [Discriminator loss: 0.154768, acc.: 93.75%] [Generator loss: 8.434875]\n",
      "9386 [Discriminator loss: 0.070997, acc.: 98.44%] [Generator loss: 7.365216]\n",
      "9387 [Discriminator loss: 0.199311, acc.: 92.19%] [Generator loss: 5.700940]\n",
      "9388 [Discriminator loss: 0.251493, acc.: 85.94%] [Generator loss: 8.509026]\n",
      "9389 [Discriminator loss: 0.227502, acc.: 89.06%] [Generator loss: 6.389320]\n",
      "9390 [Discriminator loss: 0.118569, acc.: 96.88%] [Generator loss: 6.801736]\n",
      "9391 [Discriminator loss: 0.260536, acc.: 89.06%] [Generator loss: 7.094975]\n",
      "9392 [Discriminator loss: 0.191004, acc.: 92.19%] [Generator loss: 5.779469]\n",
      "9393 [Discriminator loss: 0.249998, acc.: 90.62%] [Generator loss: 5.123297]\n",
      "9394 [Discriminator loss: 0.035143, acc.: 98.44%] [Generator loss: 6.749252]\n",
      "9395 [Discriminator loss: 0.056650, acc.: 98.44%] [Generator loss: 5.044910]\n",
      "9396 [Discriminator loss: 0.135020, acc.: 96.88%] [Generator loss: 6.912085]\n",
      "9397 [Discriminator loss: 0.164624, acc.: 93.75%] [Generator loss: 6.886889]\n",
      "9398 [Discriminator loss: 0.112966, acc.: 96.88%] [Generator loss: 6.431669]\n",
      "9399 [Discriminator loss: 0.101103, acc.: 93.75%] [Generator loss: 7.004875]\n",
      "9400 [Discriminator loss: 0.144783, acc.: 95.31%] [Generator loss: 6.817904]\n",
      "9401 [Discriminator loss: 0.127414, acc.: 96.88%] [Generator loss: 5.888687]\n",
      "9402 [Discriminator loss: 0.280159, acc.: 85.94%] [Generator loss: 7.777449]\n",
      "9403 [Discriminator loss: 0.150887, acc.: 96.88%] [Generator loss: 6.987857]\n",
      "9404 [Discriminator loss: 0.140939, acc.: 95.31%] [Generator loss: 6.051089]\n",
      "9405 [Discriminator loss: 0.165889, acc.: 92.19%] [Generator loss: 5.795825]\n",
      "9406 [Discriminator loss: 0.090361, acc.: 96.88%] [Generator loss: 6.950046]\n",
      "9407 [Discriminator loss: 0.088069, acc.: 98.44%] [Generator loss: 5.097905]\n",
      "9408 [Discriminator loss: 0.099807, acc.: 93.75%] [Generator loss: 6.701394]\n",
      "9409 [Discriminator loss: 0.127243, acc.: 93.75%] [Generator loss: 6.544693]\n",
      "9410 [Discriminator loss: 0.084297, acc.: 96.88%] [Generator loss: 6.199617]\n",
      "9411 [Discriminator loss: 0.138417, acc.: 92.19%] [Generator loss: 6.255620]\n",
      "9412 [Discriminator loss: 0.163103, acc.: 93.75%] [Generator loss: 5.750571]\n",
      "9413 [Discriminator loss: 0.197101, acc.: 93.75%] [Generator loss: 5.737377]\n",
      "9414 [Discriminator loss: 0.063908, acc.: 98.44%] [Generator loss: 5.276843]\n",
      "9415 [Discriminator loss: 0.071346, acc.: 98.44%] [Generator loss: 5.806238]\n",
      "9416 [Discriminator loss: 0.139551, acc.: 95.31%] [Generator loss: 5.771757]\n",
      "9417 [Discriminator loss: 0.254145, acc.: 92.19%] [Generator loss: 4.843276]\n",
      "9418 [Discriminator loss: 0.360669, acc.: 89.06%] [Generator loss: 6.978884]\n",
      "9419 [Discriminator loss: 0.127977, acc.: 93.75%] [Generator loss: 6.732283]\n",
      "9420 [Discriminator loss: 0.122804, acc.: 93.75%] [Generator loss: 4.734298]\n",
      "9421 [Discriminator loss: 0.089862, acc.: 96.88%] [Generator loss: 6.173028]\n",
      "9422 [Discriminator loss: 0.049235, acc.: 100.00%] [Generator loss: 6.540085]\n",
      "9423 [Discriminator loss: 0.248442, acc.: 87.50%] [Generator loss: 6.621718]\n",
      "9424 [Discriminator loss: 0.107054, acc.: 96.88%] [Generator loss: 7.749104]\n",
      "9425 [Discriminator loss: 0.146385, acc.: 89.06%] [Generator loss: 4.294894]\n",
      "9426 [Discriminator loss: 0.237505, acc.: 90.62%] [Generator loss: 8.112240]\n",
      "9427 [Discriminator loss: 0.043678, acc.: 98.44%] [Generator loss: 9.234755]\n",
      "9428 [Discriminator loss: 0.118851, acc.: 93.75%] [Generator loss: 7.811798]\n",
      "9429 [Discriminator loss: 0.156975, acc.: 92.19%] [Generator loss: 5.474041]\n",
      "9430 [Discriminator loss: 0.180009, acc.: 92.19%] [Generator loss: 6.599405]\n",
      "9431 [Discriminator loss: 0.022139, acc.: 100.00%] [Generator loss: 7.753608]\n",
      "9432 [Discriminator loss: 0.078832, acc.: 98.44%] [Generator loss: 6.172607]\n",
      "9433 [Discriminator loss: 0.183882, acc.: 92.19%] [Generator loss: 7.154759]\n",
      "9434 [Discriminator loss: 0.343501, acc.: 85.94%] [Generator loss: 5.840928]\n",
      "9435 [Discriminator loss: 0.132896, acc.: 95.31%] [Generator loss: 7.490543]\n",
      "9436 [Discriminator loss: 0.096990, acc.: 96.88%] [Generator loss: 6.475674]\n",
      "9437 [Discriminator loss: 0.260149, acc.: 90.62%] [Generator loss: 5.443375]\n",
      "9438 [Discriminator loss: 0.126254, acc.: 96.88%] [Generator loss: 5.841414]\n",
      "9439 [Discriminator loss: 0.107238, acc.: 95.31%] [Generator loss: 6.375403]\n",
      "9440 [Discriminator loss: 0.239557, acc.: 89.06%] [Generator loss: 5.980501]\n",
      "9441 [Discriminator loss: 0.115644, acc.: 95.31%] [Generator loss: 5.880187]\n",
      "9442 [Discriminator loss: 0.065778, acc.: 96.88%] [Generator loss: 5.894078]\n",
      "9443 [Discriminator loss: 0.204102, acc.: 93.75%] [Generator loss: 6.826660]\n",
      "9444 [Discriminator loss: 0.162567, acc.: 95.31%] [Generator loss: 6.260433]\n",
      "9445 [Discriminator loss: 0.271310, acc.: 90.62%] [Generator loss: 6.002905]\n",
      "9446 [Discriminator loss: 0.131587, acc.: 93.75%] [Generator loss: 7.407640]\n",
      "9447 [Discriminator loss: 0.129998, acc.: 95.31%] [Generator loss: 6.913707]\n",
      "9448 [Discriminator loss: 0.158181, acc.: 92.19%] [Generator loss: 6.121990]\n",
      "9449 [Discriminator loss: 0.114947, acc.: 95.31%] [Generator loss: 6.673031]\n",
      "9450 [Discriminator loss: 0.144459, acc.: 90.62%] [Generator loss: 6.586142]\n",
      "9451 [Discriminator loss: 0.160961, acc.: 93.75%] [Generator loss: 6.355855]\n",
      "9452 [Discriminator loss: 0.082948, acc.: 95.31%] [Generator loss: 5.887053]\n",
      "9453 [Discriminator loss: 0.101712, acc.: 95.31%] [Generator loss: 6.968277]\n",
      "9454 [Discriminator loss: 0.120050, acc.: 95.31%] [Generator loss: 6.513367]\n",
      "9455 [Discriminator loss: 0.095038, acc.: 95.31%] [Generator loss: 6.087983]\n",
      "9456 [Discriminator loss: 0.100695, acc.: 95.31%] [Generator loss: 6.553966]\n",
      "9457 [Discriminator loss: 0.157737, acc.: 92.19%] [Generator loss: 6.599867]\n",
      "9458 [Discriminator loss: 0.107716, acc.: 95.31%] [Generator loss: 7.119007]\n",
      "9459 [Discriminator loss: 0.099108, acc.: 96.88%] [Generator loss: 7.450265]\n",
      "9460 [Discriminator loss: 0.085919, acc.: 96.88%] [Generator loss: 7.182262]\n",
      "9461 [Discriminator loss: 0.060542, acc.: 98.44%] [Generator loss: 6.594629]\n",
      "9462 [Discriminator loss: 0.231973, acc.: 90.62%] [Generator loss: 6.075330]\n",
      "9463 [Discriminator loss: 0.076139, acc.: 96.88%] [Generator loss: 5.237458]\n",
      "9464 [Discriminator loss: 0.198641, acc.: 93.75%] [Generator loss: 6.625876]\n",
      "9465 [Discriminator loss: 0.086028, acc.: 95.31%] [Generator loss: 6.348865]\n",
      "9466 [Discriminator loss: 0.041590, acc.: 100.00%] [Generator loss: 6.233670]\n",
      "9467 [Discriminator loss: 0.121533, acc.: 95.31%] [Generator loss: 7.562884]\n",
      "9468 [Discriminator loss: 0.070118, acc.: 95.31%] [Generator loss: 7.062565]\n",
      "9469 [Discriminator loss: 0.194015, acc.: 89.06%] [Generator loss: 5.010151]\n",
      "9470 [Discriminator loss: 0.110878, acc.: 93.75%] [Generator loss: 5.836641]\n",
      "9471 [Discriminator loss: 0.089785, acc.: 96.88%] [Generator loss: 5.325139]\n",
      "9472 [Discriminator loss: 0.112020, acc.: 93.75%] [Generator loss: 5.813753]\n",
      "9473 [Discriminator loss: 0.151279, acc.: 95.31%] [Generator loss: 5.622152]\n",
      "9474 [Discriminator loss: 0.030628, acc.: 100.00%] [Generator loss: 6.229952]\n",
      "9475 [Discriminator loss: 0.075586, acc.: 98.44%] [Generator loss: 6.417111]\n",
      "9476 [Discriminator loss: 0.060629, acc.: 100.00%] [Generator loss: 5.934321]\n",
      "9477 [Discriminator loss: 0.136671, acc.: 95.31%] [Generator loss: 5.009118]\n",
      "9478 [Discriminator loss: 0.207144, acc.: 92.19%] [Generator loss: 6.238681]\n",
      "9479 [Discriminator loss: 0.063688, acc.: 96.88%] [Generator loss: 6.324896]\n",
      "9480 [Discriminator loss: 0.255225, acc.: 92.19%] [Generator loss: 5.248422]\n",
      "9481 [Discriminator loss: 0.087075, acc.: 95.31%] [Generator loss: 6.456988]\n",
      "9482 [Discriminator loss: 0.069564, acc.: 96.88%] [Generator loss: 5.993423]\n",
      "9483 [Discriminator loss: 0.086880, acc.: 98.44%] [Generator loss: 6.462685]\n",
      "9484 [Discriminator loss: 0.079154, acc.: 95.31%] [Generator loss: 7.021823]\n",
      "9485 [Discriminator loss: 0.069342, acc.: 98.44%] [Generator loss: 6.291573]\n",
      "9486 [Discriminator loss: 0.132851, acc.: 95.31%] [Generator loss: 5.710083]\n",
      "9487 [Discriminator loss: 0.142586, acc.: 92.19%] [Generator loss: 6.224269]\n",
      "9488 [Discriminator loss: 0.206501, acc.: 89.06%] [Generator loss: 7.865379]\n",
      "9489 [Discriminator loss: 0.113420, acc.: 93.75%] [Generator loss: 8.161700]\n",
      "9490 [Discriminator loss: 0.027087, acc.: 100.00%] [Generator loss: 7.030705]\n",
      "9491 [Discriminator loss: 0.161091, acc.: 90.62%] [Generator loss: 7.196799]\n",
      "9492 [Discriminator loss: 0.036427, acc.: 100.00%] [Generator loss: 6.799631]\n",
      "9493 [Discriminator loss: 0.050891, acc.: 98.44%] [Generator loss: 7.182470]\n",
      "9494 [Discriminator loss: 0.439321, acc.: 85.94%] [Generator loss: 6.057474]\n",
      "9495 [Discriminator loss: 0.048335, acc.: 98.44%] [Generator loss: 5.728230]\n",
      "9496 [Discriminator loss: 0.104190, acc.: 96.88%] [Generator loss: 5.963516]\n",
      "9497 [Discriminator loss: 0.094091, acc.: 98.44%] [Generator loss: 5.727724]\n",
      "9498 [Discriminator loss: 0.195843, acc.: 96.88%] [Generator loss: 6.240849]\n",
      "9499 [Discriminator loss: 0.152613, acc.: 93.75%] [Generator loss: 4.781356]\n",
      "9500 [Discriminator loss: 0.097307, acc.: 96.88%] [Generator loss: 6.226250]\n",
      "9501 [Discriminator loss: 0.053351, acc.: 100.00%] [Generator loss: 6.597493]\n",
      "9502 [Discriminator loss: 0.255148, acc.: 90.62%] [Generator loss: 7.433478]\n",
      "9503 [Discriminator loss: 0.140761, acc.: 93.75%] [Generator loss: 6.287083]\n",
      "9504 [Discriminator loss: 0.193953, acc.: 90.62%] [Generator loss: 6.523603]\n",
      "9505 [Discriminator loss: 0.185852, acc.: 89.06%] [Generator loss: 7.619734]\n",
      "9506 [Discriminator loss: 0.217997, acc.: 92.19%] [Generator loss: 5.350479]\n",
      "9507 [Discriminator loss: 0.156418, acc.: 93.75%] [Generator loss: 6.455652]\n",
      "9508 [Discriminator loss: 0.098199, acc.: 96.88%] [Generator loss: 7.172372]\n",
      "9509 [Discriminator loss: 0.112592, acc.: 96.88%] [Generator loss: 5.620161]\n",
      "9510 [Discriminator loss: 0.112592, acc.: 95.31%] [Generator loss: 5.363384]\n",
      "9511 [Discriminator loss: 0.289022, acc.: 87.50%] [Generator loss: 7.411018]\n",
      "9512 [Discriminator loss: 0.189957, acc.: 89.06%] [Generator loss: 6.851547]\n",
      "9513 [Discriminator loss: 0.084045, acc.: 95.31%] [Generator loss: 5.522117]\n",
      "9514 [Discriminator loss: 0.087603, acc.: 96.88%] [Generator loss: 5.748408]\n",
      "9515 [Discriminator loss: 0.365357, acc.: 89.06%] [Generator loss: 5.728690]\n",
      "9516 [Discriminator loss: 0.137712, acc.: 96.88%] [Generator loss: 6.684449]\n",
      "9517 [Discriminator loss: 0.101300, acc.: 96.88%] [Generator loss: 6.815659]\n",
      "9518 [Discriminator loss: 0.113056, acc.: 95.31%] [Generator loss: 6.257215]\n",
      "9519 [Discriminator loss: 0.223185, acc.: 92.19%] [Generator loss: 6.235294]\n",
      "9520 [Discriminator loss: 0.110639, acc.: 93.75%] [Generator loss: 7.529130]\n",
      "9521 [Discriminator loss: 0.208828, acc.: 92.19%] [Generator loss: 5.944416]\n",
      "9522 [Discriminator loss: 0.151007, acc.: 92.19%] [Generator loss: 5.805326]\n",
      "9523 [Discriminator loss: 0.066245, acc.: 98.44%] [Generator loss: 7.168189]\n",
      "9524 [Discriminator loss: 0.055231, acc.: 98.44%] [Generator loss: 6.132477]\n",
      "9525 [Discriminator loss: 0.237575, acc.: 92.19%] [Generator loss: 5.860980]\n",
      "9526 [Discriminator loss: 0.222977, acc.: 90.62%] [Generator loss: 4.866845]\n",
      "9527 [Discriminator loss: 0.155969, acc.: 93.75%] [Generator loss: 6.802449]\n",
      "9528 [Discriminator loss: 0.199661, acc.: 90.62%] [Generator loss: 7.199063]\n",
      "9529 [Discriminator loss: 0.316330, acc.: 85.94%] [Generator loss: 7.251452]\n",
      "9530 [Discriminator loss: 0.179935, acc.: 93.75%] [Generator loss: 7.123683]\n",
      "9531 [Discriminator loss: 0.148335, acc.: 90.62%] [Generator loss: 6.169834]\n",
      "9532 [Discriminator loss: 0.145030, acc.: 96.88%] [Generator loss: 6.375483]\n",
      "9533 [Discriminator loss: 0.082415, acc.: 96.88%] [Generator loss: 6.128865]\n",
      "9534 [Discriminator loss: 0.093111, acc.: 96.88%] [Generator loss: 6.639613]\n",
      "9535 [Discriminator loss: 0.222735, acc.: 89.06%] [Generator loss: 6.048950]\n",
      "9536 [Discriminator loss: 0.105723, acc.: 96.88%] [Generator loss: 6.399271]\n",
      "9537 [Discriminator loss: 0.265286, acc.: 87.50%] [Generator loss: 6.072872]\n",
      "9538 [Discriminator loss: 0.044471, acc.: 100.00%] [Generator loss: 6.102139]\n",
      "9539 [Discriminator loss: 0.100039, acc.: 95.31%] [Generator loss: 6.221276]\n",
      "9540 [Discriminator loss: 0.216930, acc.: 93.75%] [Generator loss: 6.332502]\n",
      "9541 [Discriminator loss: 0.193523, acc.: 93.75%] [Generator loss: 6.421147]\n",
      "9542 [Discriminator loss: 0.118539, acc.: 92.19%] [Generator loss: 4.685423]\n",
      "9543 [Discriminator loss: 0.110721, acc.: 96.88%] [Generator loss: 6.145073]\n",
      "9544 [Discriminator loss: 0.081541, acc.: 96.88%] [Generator loss: 5.587957]\n",
      "9545 [Discriminator loss: 0.300187, acc.: 81.25%] [Generator loss: 7.634725]\n",
      "9546 [Discriminator loss: 0.158069, acc.: 96.88%] [Generator loss: 5.568130]\n",
      "9547 [Discriminator loss: 0.130677, acc.: 95.31%] [Generator loss: 4.996173]\n",
      "9548 [Discriminator loss: 0.109644, acc.: 95.31%] [Generator loss: 6.834464]\n",
      "9549 [Discriminator loss: 0.186215, acc.: 92.19%] [Generator loss: 6.252431]\n",
      "9550 [Discriminator loss: 0.043185, acc.: 100.00%] [Generator loss: 7.112581]\n",
      "9551 [Discriminator loss: 0.252114, acc.: 89.06%] [Generator loss: 6.822071]\n",
      "9552 [Discriminator loss: 0.110332, acc.: 96.88%] [Generator loss: 6.760942]\n",
      "9553 [Discriminator loss: 0.202543, acc.: 84.38%] [Generator loss: 5.058401]\n",
      "9554 [Discriminator loss: 0.134628, acc.: 93.75%] [Generator loss: 5.618768]\n",
      "9555 [Discriminator loss: 0.151039, acc.: 92.19%] [Generator loss: 6.091814]\n",
      "9556 [Discriminator loss: 0.091035, acc.: 95.31%] [Generator loss: 7.353692]\n",
      "9557 [Discriminator loss: 0.133365, acc.: 95.31%] [Generator loss: 6.327734]\n",
      "9558 [Discriminator loss: 0.141424, acc.: 93.75%] [Generator loss: 6.054427]\n",
      "9559 [Discriminator loss: 0.250270, acc.: 89.06%] [Generator loss: 6.842282]\n",
      "9560 [Discriminator loss: 0.061957, acc.: 96.88%] [Generator loss: 7.524504]\n",
      "9561 [Discriminator loss: 0.146928, acc.: 95.31%] [Generator loss: 6.607889]\n",
      "9562 [Discriminator loss: 0.084276, acc.: 96.88%] [Generator loss: 7.144007]\n",
      "9563 [Discriminator loss: 0.272059, acc.: 93.75%] [Generator loss: 6.144632]\n",
      "9564 [Discriminator loss: 0.082238, acc.: 96.88%] [Generator loss: 5.876956]\n",
      "9565 [Discriminator loss: 0.125316, acc.: 93.75%] [Generator loss: 6.630937]\n",
      "9566 [Discriminator loss: 0.118333, acc.: 95.31%] [Generator loss: 6.660714]\n",
      "9567 [Discriminator loss: 0.050339, acc.: 96.88%] [Generator loss: 4.798646]\n",
      "9568 [Discriminator loss: 0.129569, acc.: 93.75%] [Generator loss: 6.395709]\n",
      "9569 [Discriminator loss: 0.175672, acc.: 92.19%] [Generator loss: 7.016420]\n",
      "9570 [Discriminator loss: 0.231661, acc.: 90.62%] [Generator loss: 6.742645]\n",
      "9571 [Discriminator loss: 0.231811, acc.: 87.50%] [Generator loss: 6.744450]\n",
      "9572 [Discriminator loss: 0.055172, acc.: 96.88%] [Generator loss: 6.839032]\n",
      "9573 [Discriminator loss: 0.092853, acc.: 98.44%] [Generator loss: 5.874912]\n",
      "9574 [Discriminator loss: 0.195498, acc.: 92.19%] [Generator loss: 6.576294]\n",
      "9575 [Discriminator loss: 0.297508, acc.: 87.50%] [Generator loss: 5.355776]\n",
      "9576 [Discriminator loss: 0.128777, acc.: 95.31%] [Generator loss: 8.193411]\n",
      "9577 [Discriminator loss: 0.061985, acc.: 96.88%] [Generator loss: 8.774925]\n",
      "9578 [Discriminator loss: 0.372016, acc.: 84.38%] [Generator loss: 6.746738]\n",
      "9579 [Discriminator loss: 0.046874, acc.: 100.00%] [Generator loss: 5.790148]\n",
      "9580 [Discriminator loss: 0.027099, acc.: 100.00%] [Generator loss: 6.964082]\n",
      "9581 [Discriminator loss: 0.286190, acc.: 82.81%] [Generator loss: 6.969666]\n",
      "9582 [Discriminator loss: 0.098841, acc.: 95.31%] [Generator loss: 7.994113]\n",
      "9583 [Discriminator loss: 0.081782, acc.: 96.88%] [Generator loss: 7.025829]\n",
      "9584 [Discriminator loss: 0.167152, acc.: 90.62%] [Generator loss: 5.768281]\n",
      "9585 [Discriminator loss: 0.087064, acc.: 96.88%] [Generator loss: 4.898744]\n",
      "9586 [Discriminator loss: 0.052747, acc.: 98.44%] [Generator loss: 5.274072]\n",
      "9587 [Discriminator loss: 0.118296, acc.: 96.88%] [Generator loss: 6.393987]\n",
      "9588 [Discriminator loss: 0.011875, acc.: 100.00%] [Generator loss: 5.663412]\n",
      "9589 [Discriminator loss: 0.126340, acc.: 93.75%] [Generator loss: 5.880312]\n",
      "9590 [Discriminator loss: 0.089823, acc.: 95.31%] [Generator loss: 6.330391]\n",
      "9591 [Discriminator loss: 0.286639, acc.: 90.62%] [Generator loss: 5.510531]\n",
      "9592 [Discriminator loss: 0.069742, acc.: 96.88%] [Generator loss: 7.139868]\n",
      "9593 [Discriminator loss: 0.146883, acc.: 93.75%] [Generator loss: 6.928781]\n",
      "9594 [Discriminator loss: 0.135322, acc.: 95.31%] [Generator loss: 5.780265]\n",
      "9595 [Discriminator loss: 0.115831, acc.: 93.75%] [Generator loss: 7.372955]\n",
      "9596 [Discriminator loss: 0.198833, acc.: 93.75%] [Generator loss: 6.209059]\n",
      "9597 [Discriminator loss: 0.197000, acc.: 95.31%] [Generator loss: 5.337851]\n",
      "9598 [Discriminator loss: 0.111443, acc.: 93.75%] [Generator loss: 6.458200]\n",
      "9599 [Discriminator loss: 0.093067, acc.: 95.31%] [Generator loss: 6.946918]\n",
      "9600 [Discriminator loss: 0.324855, acc.: 82.81%] [Generator loss: 6.189543]\n",
      "9601 [Discriminator loss: 0.089983, acc.: 95.31%] [Generator loss: 5.192564]\n",
      "9602 [Discriminator loss: 0.173836, acc.: 92.19%] [Generator loss: 6.041415]\n",
      "9603 [Discriminator loss: 0.078885, acc.: 98.44%] [Generator loss: 7.836226]\n",
      "9604 [Discriminator loss: 0.084025, acc.: 98.44%] [Generator loss: 6.928177]\n",
      "9605 [Discriminator loss: 0.103273, acc.: 96.88%] [Generator loss: 6.555097]\n",
      "9606 [Discriminator loss: 0.029953, acc.: 100.00%] [Generator loss: 6.972657]\n",
      "9607 [Discriminator loss: 0.116585, acc.: 96.88%] [Generator loss: 5.219794]\n",
      "9608 [Discriminator loss: 0.167652, acc.: 92.19%] [Generator loss: 6.500146]\n",
      "9609 [Discriminator loss: 0.094672, acc.: 96.88%] [Generator loss: 5.893961]\n",
      "9610 [Discriminator loss: 0.127295, acc.: 95.31%] [Generator loss: 5.507019]\n",
      "9611 [Discriminator loss: 0.094238, acc.: 98.44%] [Generator loss: 6.050673]\n",
      "9612 [Discriminator loss: 0.204909, acc.: 93.75%] [Generator loss: 6.467734]\n",
      "9613 [Discriminator loss: 0.099257, acc.: 98.44%] [Generator loss: 5.748591]\n",
      "9614 [Discriminator loss: 0.250150, acc.: 89.06%] [Generator loss: 6.826905]\n",
      "9615 [Discriminator loss: 0.064628, acc.: 98.44%] [Generator loss: 7.330664]\n",
      "9616 [Discriminator loss: 0.078832, acc.: 96.88%] [Generator loss: 5.835842]\n",
      "9617 [Discriminator loss: 0.179837, acc.: 95.31%] [Generator loss: 6.223116]\n",
      "9618 [Discriminator loss: 0.113279, acc.: 96.88%] [Generator loss: 7.692135]\n",
      "9619 [Discriminator loss: 0.291909, acc.: 87.50%] [Generator loss: 4.403128]\n",
      "9620 [Discriminator loss: 0.195023, acc.: 95.31%] [Generator loss: 6.214785]\n",
      "9621 [Discriminator loss: 0.031421, acc.: 100.00%] [Generator loss: 8.128380]\n",
      "9622 [Discriminator loss: 0.101348, acc.: 96.88%] [Generator loss: 5.467824]\n",
      "9623 [Discriminator loss: 0.099913, acc.: 93.75%] [Generator loss: 6.802411]\n",
      "9624 [Discriminator loss: 0.125794, acc.: 95.31%] [Generator loss: 6.664406]\n",
      "9625 [Discriminator loss: 0.059019, acc.: 100.00%] [Generator loss: 7.116424]\n",
      "9626 [Discriminator loss: 0.193016, acc.: 92.19%] [Generator loss: 5.919564]\n",
      "9627 [Discriminator loss: 0.108867, acc.: 93.75%] [Generator loss: 6.382312]\n",
      "9628 [Discriminator loss: 0.098304, acc.: 96.88%] [Generator loss: 6.138628]\n",
      "9629 [Discriminator loss: 0.104448, acc.: 98.44%] [Generator loss: 5.745130]\n",
      "9630 [Discriminator loss: 0.175400, acc.: 90.62%] [Generator loss: 6.492429]\n",
      "9631 [Discriminator loss: 0.135313, acc.: 95.31%] [Generator loss: 4.703819]\n",
      "9632 [Discriminator loss: 0.101760, acc.: 95.31%] [Generator loss: 6.792301]\n",
      "9633 [Discriminator loss: 0.213714, acc.: 92.19%] [Generator loss: 6.285019]\n",
      "9634 [Discriminator loss: 0.225007, acc.: 93.75%] [Generator loss: 6.097704]\n",
      "9635 [Discriminator loss: 0.187142, acc.: 90.62%] [Generator loss: 6.688154]\n",
      "9636 [Discriminator loss: 0.071455, acc.: 98.44%] [Generator loss: 7.139674]\n",
      "9637 [Discriminator loss: 0.166777, acc.: 96.88%] [Generator loss: 5.235759]\n",
      "9638 [Discriminator loss: 0.113164, acc.: 95.31%] [Generator loss: 5.594546]\n",
      "9639 [Discriminator loss: 0.078342, acc.: 96.88%] [Generator loss: 6.195106]\n",
      "9640 [Discriminator loss: 0.022488, acc.: 100.00%] [Generator loss: 5.774103]\n",
      "9641 [Discriminator loss: 0.206410, acc.: 87.50%] [Generator loss: 7.345293]\n",
      "9642 [Discriminator loss: 0.179478, acc.: 95.31%] [Generator loss: 7.358181]\n",
      "9643 [Discriminator loss: 0.147754, acc.: 96.88%] [Generator loss: 6.069718]\n",
      "9644 [Discriminator loss: 0.048521, acc.: 98.44%] [Generator loss: 4.493135]\n",
      "9645 [Discriminator loss: 0.164573, acc.: 95.31%] [Generator loss: 6.617205]\n",
      "9646 [Discriminator loss: 0.064021, acc.: 96.88%] [Generator loss: 7.494018]\n",
      "9647 [Discriminator loss: 0.118188, acc.: 95.31%] [Generator loss: 4.575816]\n",
      "9648 [Discriminator loss: 0.120148, acc.: 92.19%] [Generator loss: 7.251659]\n",
      "9649 [Discriminator loss: 0.038968, acc.: 100.00%] [Generator loss: 7.706526]\n",
      "9650 [Discriminator loss: 0.297872, acc.: 85.94%] [Generator loss: 6.753203]\n",
      "9651 [Discriminator loss: 0.106752, acc.: 96.88%] [Generator loss: 7.667970]\n",
      "9652 [Discriminator loss: 0.586256, acc.: 79.69%] [Generator loss: 4.660010]\n",
      "9653 [Discriminator loss: 0.111570, acc.: 95.31%] [Generator loss: 5.266873]\n",
      "9654 [Discriminator loss: 0.144444, acc.: 93.75%] [Generator loss: 6.085718]\n",
      "9655 [Discriminator loss: 0.114439, acc.: 95.31%] [Generator loss: 7.146051]\n",
      "9656 [Discriminator loss: 0.080901, acc.: 96.88%] [Generator loss: 6.539085]\n",
      "9657 [Discriminator loss: 0.161303, acc.: 95.31%] [Generator loss: 6.465467]\n",
      "9658 [Discriminator loss: 0.122055, acc.: 98.44%] [Generator loss: 6.205513]\n",
      "9659 [Discriminator loss: 0.070898, acc.: 96.88%] [Generator loss: 6.148898]\n",
      "9660 [Discriminator loss: 0.163698, acc.: 93.75%] [Generator loss: 5.778971]\n",
      "9661 [Discriminator loss: 0.082003, acc.: 96.88%] [Generator loss: 6.148709]\n",
      "9662 [Discriminator loss: 0.103851, acc.: 96.88%] [Generator loss: 6.167019]\n",
      "9663 [Discriminator loss: 0.054610, acc.: 98.44%] [Generator loss: 5.506368]\n",
      "9664 [Discriminator loss: 0.144706, acc.: 93.75%] [Generator loss: 8.216034]\n",
      "9665 [Discriminator loss: 0.090306, acc.: 95.31%] [Generator loss: 6.889868]\n",
      "9666 [Discriminator loss: 0.084467, acc.: 98.44%] [Generator loss: 5.733425]\n",
      "9667 [Discriminator loss: 0.160907, acc.: 93.75%] [Generator loss: 6.045724]\n",
      "9668 [Discriminator loss: 0.183673, acc.: 92.19%] [Generator loss: 6.484321]\n",
      "9669 [Discriminator loss: 0.104791, acc.: 96.88%] [Generator loss: 7.047095]\n",
      "9670 [Discriminator loss: 0.045539, acc.: 100.00%] [Generator loss: 5.144976]\n",
      "9671 [Discriminator loss: 0.107003, acc.: 98.44%] [Generator loss: 5.743430]\n",
      "9672 [Discriminator loss: 0.087923, acc.: 96.88%] [Generator loss: 5.651774]\n",
      "9673 [Discriminator loss: 0.147183, acc.: 96.88%] [Generator loss: 8.095573]\n",
      "9674 [Discriminator loss: 0.216843, acc.: 92.19%] [Generator loss: 5.334631]\n",
      "9675 [Discriminator loss: 0.104541, acc.: 96.88%] [Generator loss: 6.036673]\n",
      "9676 [Discriminator loss: 0.053551, acc.: 100.00%] [Generator loss: 5.527802]\n",
      "9677 [Discriminator loss: 0.080207, acc.: 96.88%] [Generator loss: 6.110274]\n",
      "9678 [Discriminator loss: 0.116506, acc.: 95.31%] [Generator loss: 6.131750]\n",
      "9679 [Discriminator loss: 0.106662, acc.: 95.31%] [Generator loss: 6.446939]\n",
      "9680 [Discriminator loss: 0.153870, acc.: 93.75%] [Generator loss: 7.182615]\n",
      "9681 [Discriminator loss: 0.079281, acc.: 95.31%] [Generator loss: 6.805527]\n",
      "9682 [Discriminator loss: 0.154215, acc.: 93.75%] [Generator loss: 5.931635]\n",
      "9683 [Discriminator loss: 0.051889, acc.: 100.00%] [Generator loss: 5.868595]\n",
      "9684 [Discriminator loss: 0.054691, acc.: 100.00%] [Generator loss: 5.789588]\n",
      "9685 [Discriminator loss: 0.076713, acc.: 95.31%] [Generator loss: 6.071810]\n",
      "9686 [Discriminator loss: 0.135365, acc.: 95.31%] [Generator loss: 7.042895]\n",
      "9687 [Discriminator loss: 0.184095, acc.: 93.75%] [Generator loss: 7.382226]\n",
      "9688 [Discriminator loss: 0.417416, acc.: 76.56%] [Generator loss: 5.709908]\n",
      "9689 [Discriminator loss: 0.137736, acc.: 92.19%] [Generator loss: 7.588964]\n",
      "9690 [Discriminator loss: 0.210881, acc.: 95.31%] [Generator loss: 5.629634]\n",
      "9691 [Discriminator loss: 0.227389, acc.: 92.19%] [Generator loss: 6.776130]\n",
      "9692 [Discriminator loss: 0.078807, acc.: 95.31%] [Generator loss: 6.401440]\n",
      "9693 [Discriminator loss: 0.184859, acc.: 90.62%] [Generator loss: 7.688376]\n",
      "9694 [Discriminator loss: 0.183295, acc.: 95.31%] [Generator loss: 7.292153]\n",
      "9695 [Discriminator loss: 0.074113, acc.: 96.88%] [Generator loss: 6.209690]\n",
      "9696 [Discriminator loss: 0.174263, acc.: 92.19%] [Generator loss: 5.620492]\n",
      "9697 [Discriminator loss: 0.247536, acc.: 95.31%] [Generator loss: 5.900327]\n",
      "9698 [Discriminator loss: 0.094030, acc.: 96.88%] [Generator loss: 7.345479]\n",
      "9699 [Discriminator loss: 0.146117, acc.: 92.19%] [Generator loss: 7.108735]\n",
      "9700 [Discriminator loss: 0.097700, acc.: 96.88%] [Generator loss: 6.065233]\n",
      "9701 [Discriminator loss: 0.115191, acc.: 93.75%] [Generator loss: 7.350009]\n",
      "9702 [Discriminator loss: 0.225091, acc.: 92.19%] [Generator loss: 7.040093]\n",
      "9703 [Discriminator loss: 0.106845, acc.: 95.31%] [Generator loss: 7.921070]\n",
      "9704 [Discriminator loss: 0.208735, acc.: 90.62%] [Generator loss: 5.456338]\n",
      "9705 [Discriminator loss: 0.267018, acc.: 90.62%] [Generator loss: 6.962242]\n",
      "9706 [Discriminator loss: 0.075219, acc.: 96.88%] [Generator loss: 8.061377]\n",
      "9707 [Discriminator loss: 0.275273, acc.: 89.06%] [Generator loss: 5.733418]\n",
      "9708 [Discriminator loss: 0.086924, acc.: 98.44%] [Generator loss: 6.454508]\n",
      "9709 [Discriminator loss: 0.041923, acc.: 98.44%] [Generator loss: 6.419257]\n",
      "9710 [Discriminator loss: 0.106607, acc.: 96.88%] [Generator loss: 7.040706]\n",
      "9711 [Discriminator loss: 0.214228, acc.: 90.62%] [Generator loss: 8.194878]\n",
      "9712 [Discriminator loss: 0.176725, acc.: 93.75%] [Generator loss: 6.337290]\n",
      "9713 [Discriminator loss: 0.380477, acc.: 89.06%] [Generator loss: 6.335866]\n",
      "9714 [Discriminator loss: 0.048513, acc.: 98.44%] [Generator loss: 7.534839]\n",
      "9715 [Discriminator loss: 0.128747, acc.: 93.75%] [Generator loss: 5.275369]\n",
      "9716 [Discriminator loss: 0.116946, acc.: 96.88%] [Generator loss: 5.265645]\n",
      "9717 [Discriminator loss: 0.047001, acc.: 96.88%] [Generator loss: 6.141159]\n",
      "9718 [Discriminator loss: 0.172443, acc.: 90.62%] [Generator loss: 5.352618]\n",
      "9719 [Discriminator loss: 0.137718, acc.: 92.19%] [Generator loss: 8.061074]\n",
      "9720 [Discriminator loss: 0.259834, acc.: 85.94%] [Generator loss: 6.034660]\n",
      "9721 [Discriminator loss: 0.169616, acc.: 93.75%] [Generator loss: 6.734225]\n",
      "9722 [Discriminator loss: 0.138374, acc.: 93.75%] [Generator loss: 5.644195]\n",
      "9723 [Discriminator loss: 0.044959, acc.: 98.44%] [Generator loss: 5.560494]\n",
      "9724 [Discriminator loss: 0.128977, acc.: 93.75%] [Generator loss: 7.491762]\n",
      "9725 [Discriminator loss: 0.307468, acc.: 89.06%] [Generator loss: 6.972964]\n",
      "9726 [Discriminator loss: 0.175314, acc.: 92.19%] [Generator loss: 5.851642]\n",
      "9727 [Discriminator loss: 0.092005, acc.: 96.88%] [Generator loss: 6.004348]\n",
      "9728 [Discriminator loss: 0.124976, acc.: 95.31%] [Generator loss: 6.361002]\n",
      "9729 [Discriminator loss: 0.156964, acc.: 92.19%] [Generator loss: 6.621627]\n",
      "9730 [Discriminator loss: 0.176780, acc.: 89.06%] [Generator loss: 7.389173]\n",
      "9731 [Discriminator loss: 0.105973, acc.: 96.88%] [Generator loss: 5.833127]\n",
      "9732 [Discriminator loss: 0.092819, acc.: 98.44%] [Generator loss: 6.210186]\n",
      "9733 [Discriminator loss: 0.111439, acc.: 93.75%] [Generator loss: 6.357160]\n",
      "9734 [Discriminator loss: 0.269889, acc.: 87.50%] [Generator loss: 7.099359]\n",
      "9735 [Discriminator loss: 0.118235, acc.: 93.75%] [Generator loss: 6.409179]\n",
      "9736 [Discriminator loss: 0.252476, acc.: 89.06%] [Generator loss: 6.634521]\n",
      "9737 [Discriminator loss: 0.174052, acc.: 93.75%] [Generator loss: 6.660429]\n",
      "9738 [Discriminator loss: 0.030167, acc.: 100.00%] [Generator loss: 5.743285]\n",
      "9739 [Discriminator loss: 0.160547, acc.: 95.31%] [Generator loss: 4.764853]\n",
      "9740 [Discriminator loss: 0.234538, acc.: 87.50%] [Generator loss: 6.239066]\n",
      "9741 [Discriminator loss: 0.132473, acc.: 95.31%] [Generator loss: 5.919430]\n",
      "9742 [Discriminator loss: 0.120094, acc.: 96.88%] [Generator loss: 5.727954]\n",
      "9743 [Discriminator loss: 0.072423, acc.: 98.44%] [Generator loss: 6.426353]\n",
      "9744 [Discriminator loss: 0.121507, acc.: 95.31%] [Generator loss: 5.129770]\n",
      "9745 [Discriminator loss: 0.150142, acc.: 95.31%] [Generator loss: 6.140877]\n",
      "9746 [Discriminator loss: 0.107229, acc.: 93.75%] [Generator loss: 6.072771]\n",
      "9747 [Discriminator loss: 0.090280, acc.: 96.88%] [Generator loss: 6.779650]\n",
      "9748 [Discriminator loss: 0.080746, acc.: 98.44%] [Generator loss: 5.838226]\n",
      "9749 [Discriminator loss: 0.078445, acc.: 96.88%] [Generator loss: 6.129357]\n",
      "9750 [Discriminator loss: 0.301780, acc.: 82.81%] [Generator loss: 5.517647]\n",
      "9751 [Discriminator loss: 0.139451, acc.: 92.19%] [Generator loss: 7.746830]\n",
      "9752 [Discriminator loss: 0.095112, acc.: 98.44%] [Generator loss: 6.058071]\n",
      "9753 [Discriminator loss: 0.271018, acc.: 87.50%] [Generator loss: 6.803453]\n",
      "9754 [Discriminator loss: 0.054305, acc.: 100.00%] [Generator loss: 7.545005]\n",
      "9755 [Discriminator loss: 0.103309, acc.: 93.75%] [Generator loss: 5.362991]\n",
      "9756 [Discriminator loss: 0.062574, acc.: 96.88%] [Generator loss: 6.790637]\n",
      "9757 [Discriminator loss: 0.051348, acc.: 100.00%] [Generator loss: 6.574361]\n",
      "9758 [Discriminator loss: 0.155889, acc.: 93.75%] [Generator loss: 7.182259]\n",
      "9759 [Discriminator loss: 0.052669, acc.: 98.44%] [Generator loss: 6.754158]\n",
      "9760 [Discriminator loss: 0.162814, acc.: 92.19%] [Generator loss: 6.207466]\n",
      "9761 [Discriminator loss: 0.086165, acc.: 98.44%] [Generator loss: 5.594820]\n",
      "9762 [Discriminator loss: 0.095572, acc.: 95.31%] [Generator loss: 6.901374]\n",
      "9763 [Discriminator loss: 0.263036, acc.: 84.38%] [Generator loss: 6.530326]\n",
      "9764 [Discriminator loss: 0.110740, acc.: 95.31%] [Generator loss: 6.797950]\n",
      "9765 [Discriminator loss: 0.070424, acc.: 96.88%] [Generator loss: 8.750265]\n",
      "9766 [Discriminator loss: 0.074139, acc.: 96.88%] [Generator loss: 7.433592]\n",
      "9767 [Discriminator loss: 0.077295, acc.: 96.88%] [Generator loss: 7.118022]\n",
      "9768 [Discriminator loss: 0.183432, acc.: 92.19%] [Generator loss: 6.619151]\n",
      "9769 [Discriminator loss: 0.070677, acc.: 100.00%] [Generator loss: 4.706808]\n",
      "9770 [Discriminator loss: 0.247058, acc.: 92.19%] [Generator loss: 7.298149]\n",
      "9771 [Discriminator loss: 0.064712, acc.: 96.88%] [Generator loss: 7.777808]\n",
      "9772 [Discriminator loss: 0.243681, acc.: 90.62%] [Generator loss: 5.581131]\n",
      "9773 [Discriminator loss: 0.207542, acc.: 93.75%] [Generator loss: 5.154539]\n",
      "9774 [Discriminator loss: 0.086722, acc.: 95.31%] [Generator loss: 5.532092]\n",
      "9775 [Discriminator loss: 0.072106, acc.: 93.75%] [Generator loss: 7.428660]\n",
      "9776 [Discriminator loss: 0.124292, acc.: 95.31%] [Generator loss: 7.742147]\n",
      "9777 [Discriminator loss: 0.178154, acc.: 93.75%] [Generator loss: 6.237933]\n",
      "9778 [Discriminator loss: 0.085472, acc.: 96.88%] [Generator loss: 5.583376]\n",
      "9779 [Discriminator loss: 0.198414, acc.: 96.88%] [Generator loss: 5.180137]\n",
      "9780 [Discriminator loss: 0.105125, acc.: 95.31%] [Generator loss: 7.137582]\n",
      "9781 [Discriminator loss: 0.112945, acc.: 95.31%] [Generator loss: 7.462420]\n",
      "9782 [Discriminator loss: 0.221846, acc.: 89.06%] [Generator loss: 5.543469]\n",
      "9783 [Discriminator loss: 0.119251, acc.: 93.75%] [Generator loss: 6.658682]\n",
      "9784 [Discriminator loss: 0.144569, acc.: 95.31%] [Generator loss: 7.023541]\n",
      "9785 [Discriminator loss: 0.261801, acc.: 89.06%] [Generator loss: 5.225731]\n",
      "9786 [Discriminator loss: 0.183942, acc.: 90.62%] [Generator loss: 7.123132]\n",
      "9787 [Discriminator loss: 0.057600, acc.: 98.44%] [Generator loss: 7.485454]\n",
      "9788 [Discriminator loss: 0.039841, acc.: 100.00%] [Generator loss: 7.662062]\n",
      "9789 [Discriminator loss: 0.142303, acc.: 93.75%] [Generator loss: 5.600092]\n",
      "9790 [Discriminator loss: 0.152094, acc.: 95.31%] [Generator loss: 7.909413]\n",
      "9791 [Discriminator loss: 0.099526, acc.: 95.31%] [Generator loss: 7.012522]\n",
      "9792 [Discriminator loss: 0.066330, acc.: 100.00%] [Generator loss: 5.121609]\n",
      "9793 [Discriminator loss: 0.160747, acc.: 95.31%] [Generator loss: 6.643370]\n",
      "9794 [Discriminator loss: 0.121911, acc.: 98.44%] [Generator loss: 5.960603]\n",
      "9795 [Discriminator loss: 0.075164, acc.: 98.44%] [Generator loss: 6.415138]\n",
      "9796 [Discriminator loss: 0.066357, acc.: 98.44%] [Generator loss: 6.744025]\n",
      "9797 [Discriminator loss: 0.094413, acc.: 95.31%] [Generator loss: 6.200042]\n",
      "9798 [Discriminator loss: 0.082230, acc.: 98.44%] [Generator loss: 5.983174]\n",
      "9799 [Discriminator loss: 0.121141, acc.: 96.88%] [Generator loss: 5.611707]\n",
      "9800 [Discriminator loss: 0.091060, acc.: 93.75%] [Generator loss: 5.927575]\n",
      "9801 [Discriminator loss: 0.052661, acc.: 100.00%] [Generator loss: 6.255197]\n",
      "9802 [Discriminator loss: 0.169640, acc.: 90.62%] [Generator loss: 6.090713]\n",
      "9803 [Discriminator loss: 0.160425, acc.: 93.75%] [Generator loss: 4.466101]\n",
      "9804 [Discriminator loss: 0.074248, acc.: 98.44%] [Generator loss: 6.618285]\n",
      "9805 [Discriminator loss: 0.120798, acc.: 95.31%] [Generator loss: 6.730431]\n",
      "9806 [Discriminator loss: 0.108628, acc.: 95.31%] [Generator loss: 5.851559]\n",
      "9807 [Discriminator loss: 0.199635, acc.: 92.19%] [Generator loss: 8.112612]\n",
      "9808 [Discriminator loss: 0.149860, acc.: 95.31%] [Generator loss: 7.557595]\n",
      "9809 [Discriminator loss: 0.051268, acc.: 98.44%] [Generator loss: 6.232156]\n",
      "9810 [Discriminator loss: 0.110659, acc.: 95.31%] [Generator loss: 7.462593]\n",
      "9811 [Discriminator loss: 0.053906, acc.: 98.44%] [Generator loss: 7.099168]\n",
      "9812 [Discriminator loss: 0.093440, acc.: 95.31%] [Generator loss: 7.012555]\n",
      "9813 [Discriminator loss: 0.202647, acc.: 90.62%] [Generator loss: 6.510317]\n",
      "9814 [Discriminator loss: 0.103726, acc.: 95.31%] [Generator loss: 5.707374]\n",
      "9815 [Discriminator loss: 0.079064, acc.: 96.88%] [Generator loss: 6.354744]\n",
      "9816 [Discriminator loss: 0.281472, acc.: 90.62%] [Generator loss: 6.260528]\n",
      "9817 [Discriminator loss: 0.185502, acc.: 93.75%] [Generator loss: 6.913490]\n",
      "9818 [Discriminator loss: 0.223946, acc.: 87.50%] [Generator loss: 5.681403]\n",
      "9819 [Discriminator loss: 0.053329, acc.: 100.00%] [Generator loss: 8.494166]\n",
      "9820 [Discriminator loss: 0.126132, acc.: 93.75%] [Generator loss: 5.293619]\n",
      "9821 [Discriminator loss: 0.085028, acc.: 96.88%] [Generator loss: 5.815758]\n",
      "9822 [Discriminator loss: 0.169974, acc.: 95.31%] [Generator loss: 5.987034]\n",
      "9823 [Discriminator loss: 0.065169, acc.: 98.44%] [Generator loss: 6.535453]\n",
      "9824 [Discriminator loss: 0.104579, acc.: 96.88%] [Generator loss: 6.197853]\n",
      "9825 [Discriminator loss: 0.202738, acc.: 93.75%] [Generator loss: 5.726737]\n",
      "9826 [Discriminator loss: 0.084988, acc.: 96.88%] [Generator loss: 7.507983]\n",
      "9827 [Discriminator loss: 0.136833, acc.: 96.88%] [Generator loss: 7.148211]\n",
      "9828 [Discriminator loss: 0.185496, acc.: 93.75%] [Generator loss: 6.448639]\n",
      "9829 [Discriminator loss: 0.121727, acc.: 96.88%] [Generator loss: 5.765220]\n",
      "9830 [Discriminator loss: 0.122848, acc.: 96.88%] [Generator loss: 5.851246]\n",
      "9831 [Discriminator loss: 0.077608, acc.: 98.44%] [Generator loss: 6.594057]\n",
      "9832 [Discriminator loss: 0.232550, acc.: 87.50%] [Generator loss: 5.856631]\n",
      "9833 [Discriminator loss: 0.102951, acc.: 98.44%] [Generator loss: 5.655594]\n",
      "9834 [Discriminator loss: 0.175939, acc.: 95.31%] [Generator loss: 7.545065]\n",
      "9835 [Discriminator loss: 0.217867, acc.: 90.62%] [Generator loss: 8.886568]\n",
      "9836 [Discriminator loss: 0.287357, acc.: 89.06%] [Generator loss: 7.210459]\n",
      "9837 [Discriminator loss: 0.122732, acc.: 93.75%] [Generator loss: 6.895916]\n",
      "9838 [Discriminator loss: 0.036229, acc.: 100.00%] [Generator loss: 7.216383]\n",
      "9839 [Discriminator loss: 0.128855, acc.: 95.31%] [Generator loss: 6.698678]\n",
      "9840 [Discriminator loss: 0.068027, acc.: 98.44%] [Generator loss: 7.672551]\n",
      "9841 [Discriminator loss: 0.113655, acc.: 93.75%] [Generator loss: 6.665668]\n",
      "9842 [Discriminator loss: 0.091442, acc.: 96.88%] [Generator loss: 7.624648]\n",
      "9843 [Discriminator loss: 0.121966, acc.: 96.88%] [Generator loss: 5.527589]\n",
      "9844 [Discriminator loss: 0.194851, acc.: 92.19%] [Generator loss: 7.649965]\n",
      "9845 [Discriminator loss: 0.202391, acc.: 92.19%] [Generator loss: 7.674848]\n",
      "9846 [Discriminator loss: 0.287515, acc.: 90.62%] [Generator loss: 6.896645]\n",
      "9847 [Discriminator loss: 0.358072, acc.: 85.94%] [Generator loss: 7.884766]\n",
      "9848 [Discriminator loss: 0.194435, acc.: 92.19%] [Generator loss: 7.984412]\n",
      "9849 [Discriminator loss: 0.063186, acc.: 96.88%] [Generator loss: 7.555748]\n",
      "9850 [Discriminator loss: 0.192111, acc.: 95.31%] [Generator loss: 4.846302]\n",
      "9851 [Discriminator loss: 0.326627, acc.: 89.06%] [Generator loss: 7.187829]\n",
      "9852 [Discriminator loss: 0.032282, acc.: 98.44%] [Generator loss: 9.601994]\n",
      "9853 [Discriminator loss: 0.234747, acc.: 87.50%] [Generator loss: 5.072341]\n",
      "9854 [Discriminator loss: 0.217921, acc.: 92.19%] [Generator loss: 7.147192]\n",
      "9855 [Discriminator loss: 0.135912, acc.: 93.75%] [Generator loss: 5.503706]\n",
      "9856 [Discriminator loss: 0.219493, acc.: 92.19%] [Generator loss: 6.966880]\n",
      "9857 [Discriminator loss: 0.171803, acc.: 93.75%] [Generator loss: 6.833863]\n",
      "9858 [Discriminator loss: 0.185893, acc.: 92.19%] [Generator loss: 7.155849]\n",
      "9859 [Discriminator loss: 0.061803, acc.: 96.88%] [Generator loss: 7.830621]\n",
      "9860 [Discriminator loss: 0.301992, acc.: 84.38%] [Generator loss: 5.527365]\n",
      "9861 [Discriminator loss: 0.201100, acc.: 92.19%] [Generator loss: 7.890594]\n",
      "9862 [Discriminator loss: 0.123029, acc.: 95.31%] [Generator loss: 7.611962]\n",
      "9863 [Discriminator loss: 0.108554, acc.: 96.88%] [Generator loss: 5.213465]\n",
      "9864 [Discriminator loss: 0.031979, acc.: 100.00%] [Generator loss: 4.477604]\n",
      "9865 [Discriminator loss: 0.226252, acc.: 89.06%] [Generator loss: 7.282251]\n",
      "9866 [Discriminator loss: 0.071399, acc.: 98.44%] [Generator loss: 8.023416]\n",
      "9867 [Discriminator loss: 0.137716, acc.: 95.31%] [Generator loss: 5.924967]\n",
      "9868 [Discriminator loss: 0.188371, acc.: 90.62%] [Generator loss: 6.799905]\n",
      "9869 [Discriminator loss: 0.087714, acc.: 96.88%] [Generator loss: 7.655604]\n",
      "9870 [Discriminator loss: 0.094607, acc.: 95.31%] [Generator loss: 7.265646]\n",
      "9871 [Discriminator loss: 0.111806, acc.: 93.75%] [Generator loss: 5.270066]\n",
      "9872 [Discriminator loss: 0.081757, acc.: 96.88%] [Generator loss: 6.016865]\n",
      "9873 [Discriminator loss: 0.162628, acc.: 92.19%] [Generator loss: 6.123330]\n",
      "9874 [Discriminator loss: 0.103010, acc.: 95.31%] [Generator loss: 6.313436]\n",
      "9875 [Discriminator loss: 0.033856, acc.: 98.44%] [Generator loss: 6.278419]\n",
      "9876 [Discriminator loss: 0.093554, acc.: 96.88%] [Generator loss: 5.749397]\n",
      "9877 [Discriminator loss: 0.084101, acc.: 96.88%] [Generator loss: 5.052664]\n",
      "9878 [Discriminator loss: 0.085627, acc.: 96.88%] [Generator loss: 6.572014]\n",
      "9879 [Discriminator loss: 0.092661, acc.: 95.31%] [Generator loss: 7.588626]\n",
      "9880 [Discriminator loss: 0.226370, acc.: 92.19%] [Generator loss: 8.093284]\n",
      "9881 [Discriminator loss: 0.225565, acc.: 87.50%] [Generator loss: 5.540248]\n",
      "9882 [Discriminator loss: 0.283118, acc.: 93.75%] [Generator loss: 6.847703]\n",
      "9883 [Discriminator loss: 0.134549, acc.: 95.31%] [Generator loss: 6.431434]\n",
      "9884 [Discriminator loss: 0.354406, acc.: 87.50%] [Generator loss: 6.931721]\n",
      "9885 [Discriminator loss: 0.133758, acc.: 93.75%] [Generator loss: 7.918952]\n",
      "9886 [Discriminator loss: 0.206962, acc.: 89.06%] [Generator loss: 5.813799]\n",
      "9887 [Discriminator loss: 0.169233, acc.: 92.19%] [Generator loss: 7.146732]\n",
      "9888 [Discriminator loss: 0.044400, acc.: 100.00%] [Generator loss: 7.157796]\n",
      "9889 [Discriminator loss: 0.116984, acc.: 93.75%] [Generator loss: 6.650336]\n",
      "9890 [Discriminator loss: 0.168982, acc.: 93.75%] [Generator loss: 7.042948]\n",
      "9891 [Discriminator loss: 0.032403, acc.: 98.44%] [Generator loss: 6.726204]\n",
      "9892 [Discriminator loss: 0.111082, acc.: 95.31%] [Generator loss: 7.253007]\n",
      "9893 [Discriminator loss: 0.094579, acc.: 95.31%] [Generator loss: 5.607074]\n",
      "9894 [Discriminator loss: 0.057301, acc.: 100.00%] [Generator loss: 5.442426]\n",
      "9895 [Discriminator loss: 0.067294, acc.: 96.88%] [Generator loss: 7.097072]\n",
      "9896 [Discriminator loss: 0.053594, acc.: 98.44%] [Generator loss: 6.363237]\n",
      "9897 [Discriminator loss: 0.091225, acc.: 95.31%] [Generator loss: 6.568141]\n",
      "9898 [Discriminator loss: 0.196899, acc.: 89.06%] [Generator loss: 6.349360]\n",
      "9899 [Discriminator loss: 0.028223, acc.: 100.00%] [Generator loss: 6.392087]\n",
      "9900 [Discriminator loss: 0.114899, acc.: 93.75%] [Generator loss: 5.494305]\n",
      "9901 [Discriminator loss: 0.049144, acc.: 98.44%] [Generator loss: 6.423464]\n",
      "9902 [Discriminator loss: 0.076099, acc.: 95.31%] [Generator loss: 7.708240]\n",
      "9903 [Discriminator loss: 0.187790, acc.: 92.19%] [Generator loss: 6.051765]\n",
      "9904 [Discriminator loss: 0.082978, acc.: 98.44%] [Generator loss: 5.871932]\n",
      "9905 [Discriminator loss: 0.313105, acc.: 92.19%] [Generator loss: 6.457523]\n",
      "9906 [Discriminator loss: 0.112574, acc.: 95.31%] [Generator loss: 5.956321]\n",
      "9907 [Discriminator loss: 0.160828, acc.: 87.50%] [Generator loss: 6.002195]\n",
      "9908 [Discriminator loss: 0.235280, acc.: 96.88%] [Generator loss: 7.078198]\n",
      "9909 [Discriminator loss: 0.232478, acc.: 95.31%] [Generator loss: 7.369919]\n",
      "9910 [Discriminator loss: 0.063528, acc.: 98.44%] [Generator loss: 6.190767]\n",
      "9911 [Discriminator loss: 0.037114, acc.: 100.00%] [Generator loss: 6.882539]\n",
      "9912 [Discriminator loss: 0.115665, acc.: 93.75%] [Generator loss: 6.695602]\n",
      "9913 [Discriminator loss: 0.072998, acc.: 96.88%] [Generator loss: 6.180824]\n",
      "9914 [Discriminator loss: 0.101754, acc.: 95.31%] [Generator loss: 5.397631]\n",
      "9915 [Discriminator loss: 0.101182, acc.: 95.31%] [Generator loss: 6.852914]\n",
      "9916 [Discriminator loss: 0.098534, acc.: 95.31%] [Generator loss: 6.558229]\n",
      "9917 [Discriminator loss: 0.246189, acc.: 90.62%] [Generator loss: 5.232466]\n",
      "9918 [Discriminator loss: 0.058137, acc.: 96.88%] [Generator loss: 8.049517]\n",
      "9919 [Discriminator loss: 0.220177, acc.: 90.62%] [Generator loss: 9.001578]\n",
      "9920 [Discriminator loss: 0.137068, acc.: 95.31%] [Generator loss: 7.465082]\n",
      "9921 [Discriminator loss: 0.152065, acc.: 93.75%] [Generator loss: 4.841021]\n",
      "9922 [Discriminator loss: 0.085258, acc.: 96.88%] [Generator loss: 4.677812]\n",
      "9923 [Discriminator loss: 0.097008, acc.: 96.88%] [Generator loss: 7.177955]\n",
      "9924 [Discriminator loss: 0.108194, acc.: 96.88%] [Generator loss: 5.607282]\n",
      "9925 [Discriminator loss: 0.083196, acc.: 96.88%] [Generator loss: 6.537842]\n",
      "9926 [Discriminator loss: 0.114896, acc.: 96.88%] [Generator loss: 7.229133]\n",
      "9927 [Discriminator loss: 0.077564, acc.: 95.31%] [Generator loss: 7.129793]\n",
      "9928 [Discriminator loss: 0.077755, acc.: 98.44%] [Generator loss: 5.708019]\n",
      "9929 [Discriminator loss: 0.211765, acc.: 90.62%] [Generator loss: 5.957984]\n",
      "9930 [Discriminator loss: 0.103795, acc.: 95.31%] [Generator loss: 6.385608]\n",
      "9931 [Discriminator loss: 0.058121, acc.: 98.44%] [Generator loss: 7.168357]\n",
      "9932 [Discriminator loss: 0.151843, acc.: 95.31%] [Generator loss: 5.518612]\n",
      "9933 [Discriminator loss: 0.078793, acc.: 96.88%] [Generator loss: 6.514640]\n",
      "9934 [Discriminator loss: 0.312328, acc.: 87.50%] [Generator loss: 8.149296]\n",
      "9935 [Discriminator loss: 0.068637, acc.: 100.00%] [Generator loss: 6.128845]\n",
      "9936 [Discriminator loss: 0.103936, acc.: 96.88%] [Generator loss: 5.793105]\n",
      "9937 [Discriminator loss: 0.029287, acc.: 100.00%] [Generator loss: 5.559000]\n",
      "9938 [Discriminator loss: 0.233277, acc.: 90.62%] [Generator loss: 6.004974]\n",
      "9939 [Discriminator loss: 0.066074, acc.: 98.44%] [Generator loss: 8.004576]\n",
      "9940 [Discriminator loss: 0.222052, acc.: 92.19%] [Generator loss: 6.241172]\n",
      "9941 [Discriminator loss: 0.095600, acc.: 96.88%] [Generator loss: 5.743941]\n",
      "9942 [Discriminator loss: 0.124535, acc.: 93.75%] [Generator loss: 6.679337]\n",
      "9943 [Discriminator loss: 0.035109, acc.: 98.44%] [Generator loss: 6.577013]\n",
      "9944 [Discriminator loss: 0.173752, acc.: 93.75%] [Generator loss: 7.787174]\n",
      "9945 [Discriminator loss: 0.173388, acc.: 90.62%] [Generator loss: 5.898338]\n",
      "9946 [Discriminator loss: 0.149149, acc.: 92.19%] [Generator loss: 7.568767]\n",
      "9947 [Discriminator loss: 0.093606, acc.: 96.88%] [Generator loss: 6.863009]\n",
      "9948 [Discriminator loss: 0.103365, acc.: 95.31%] [Generator loss: 6.140440]\n",
      "9949 [Discriminator loss: 0.035142, acc.: 98.44%] [Generator loss: 6.851165]\n",
      "9950 [Discriminator loss: 0.032509, acc.: 100.00%] [Generator loss: 5.238708]\n",
      "9951 [Discriminator loss: 0.277556, acc.: 90.62%] [Generator loss: 6.594518]\n",
      "9952 [Discriminator loss: 0.218275, acc.: 90.62%] [Generator loss: 6.469174]\n",
      "9953 [Discriminator loss: 0.188024, acc.: 92.19%] [Generator loss: 6.952659]\n",
      "9954 [Discriminator loss: 0.080235, acc.: 96.88%] [Generator loss: 5.695986]\n",
      "9955 [Discriminator loss: 0.101622, acc.: 95.31%] [Generator loss: 6.036928]\n",
      "9956 [Discriminator loss: 0.033741, acc.: 100.00%] [Generator loss: 6.523197]\n",
      "9957 [Discriminator loss: 0.299440, acc.: 92.19%] [Generator loss: 6.304198]\n",
      "9958 [Discriminator loss: 0.074553, acc.: 98.44%] [Generator loss: 6.843827]\n",
      "9959 [Discriminator loss: 0.288365, acc.: 85.94%] [Generator loss: 5.359061]\n",
      "9960 [Discriminator loss: 0.115403, acc.: 92.19%] [Generator loss: 8.855013]\n",
      "9961 [Discriminator loss: 0.215750, acc.: 90.62%] [Generator loss: 6.418950]\n",
      "9962 [Discriminator loss: 0.036940, acc.: 98.44%] [Generator loss: 5.058455]\n",
      "9963 [Discriminator loss: 0.099669, acc.: 96.88%] [Generator loss: 6.927011]\n",
      "9964 [Discriminator loss: 0.112217, acc.: 93.75%] [Generator loss: 6.530576]\n",
      "9965 [Discriminator loss: 0.124968, acc.: 96.88%] [Generator loss: 7.048203]\n",
      "9966 [Discriminator loss: 0.356206, acc.: 90.62%] [Generator loss: 6.464308]\n",
      "9967 [Discriminator loss: 0.290218, acc.: 85.94%] [Generator loss: 6.500083]\n",
      "9968 [Discriminator loss: 0.083401, acc.: 96.88%] [Generator loss: 6.947269]\n",
      "9969 [Discriminator loss: 0.238424, acc.: 90.62%] [Generator loss: 7.121347]\n",
      "9970 [Discriminator loss: 0.113449, acc.: 96.88%] [Generator loss: 7.236113]\n",
      "9971 [Discriminator loss: 0.087934, acc.: 98.44%] [Generator loss: 7.431639]\n",
      "9972 [Discriminator loss: 0.161076, acc.: 92.19%] [Generator loss: 6.225073]\n",
      "9973 [Discriminator loss: 0.089757, acc.: 96.88%] [Generator loss: 7.046222]\n",
      "9974 [Discriminator loss: 0.060579, acc.: 98.44%] [Generator loss: 6.862813]\n",
      "9975 [Discriminator loss: 0.200034, acc.: 95.31%] [Generator loss: 5.059826]\n",
      "9976 [Discriminator loss: 0.189045, acc.: 92.19%] [Generator loss: 6.357758]\n",
      "9977 [Discriminator loss: 0.152435, acc.: 92.19%] [Generator loss: 7.707877]\n",
      "9978 [Discriminator loss: 0.182959, acc.: 93.75%] [Generator loss: 6.903249]\n",
      "9979 [Discriminator loss: 0.087678, acc.: 96.88%] [Generator loss: 6.459951]\n",
      "9980 [Discriminator loss: 0.268522, acc.: 93.75%] [Generator loss: 6.043016]\n",
      "9981 [Discriminator loss: 0.059286, acc.: 96.88%] [Generator loss: 5.284860]\n",
      "9982 [Discriminator loss: 0.184011, acc.: 95.31%] [Generator loss: 5.668099]\n",
      "9983 [Discriminator loss: 0.145539, acc.: 93.75%] [Generator loss: 7.246115]\n",
      "9984 [Discriminator loss: 0.280393, acc.: 87.50%] [Generator loss: 7.058886]\n",
      "9985 [Discriminator loss: 0.170549, acc.: 92.19%] [Generator loss: 5.959084]\n",
      "9986 [Discriminator loss: 0.076602, acc.: 98.44%] [Generator loss: 6.869555]\n",
      "9987 [Discriminator loss: 0.221365, acc.: 90.62%] [Generator loss: 7.269056]\n",
      "9988 [Discriminator loss: 0.012417, acc.: 100.00%] [Generator loss: 8.047930]\n",
      "9989 [Discriminator loss: 0.114877, acc.: 95.31%] [Generator loss: 6.626801]\n",
      "9990 [Discriminator loss: 0.048348, acc.: 96.88%] [Generator loss: 5.806361]\n",
      "9991 [Discriminator loss: 0.256998, acc.: 90.62%] [Generator loss: 5.073711]\n",
      "9992 [Discriminator loss: 0.136129, acc.: 95.31%] [Generator loss: 6.668325]\n",
      "9993 [Discriminator loss: 0.060137, acc.: 100.00%] [Generator loss: 6.415719]\n",
      "9994 [Discriminator loss: 0.194975, acc.: 93.75%] [Generator loss: 7.337317]\n",
      "9995 [Discriminator loss: 0.056551, acc.: 98.44%] [Generator loss: 7.361720]\n",
      "9996 [Discriminator loss: 0.122381, acc.: 95.31%] [Generator loss: 5.684298]\n",
      "9997 [Discriminator loss: 0.094750, acc.: 95.31%] [Generator loss: 5.424444]\n",
      "9998 [Discriminator loss: 0.098881, acc.: 93.75%] [Generator loss: 4.214115]\n",
      "9999 [Discriminator loss: 0.295501, acc.: 85.94%] [Generator loss: 8.287392]\n",
      "10000 [Discriminator loss: 0.066331, acc.: 98.44%] [Generator loss: 8.711756]\n",
      "10001 [Discriminator loss: 0.291348, acc.: 85.94%] [Generator loss: 4.572896]\n",
      "10002 [Discriminator loss: 0.091864, acc.: 95.31%] [Generator loss: 6.386055]\n",
      "10003 [Discriminator loss: 0.058434, acc.: 98.44%] [Generator loss: 7.140500]\n",
      "10004 [Discriminator loss: 0.154972, acc.: 93.75%] [Generator loss: 5.593863]\n",
      "10005 [Discriminator loss: 0.202565, acc.: 93.75%] [Generator loss: 6.344846]\n",
      "10006 [Discriminator loss: 0.075011, acc.: 96.88%] [Generator loss: 6.656616]\n",
      "10007 [Discriminator loss: 0.162257, acc.: 95.31%] [Generator loss: 7.173518]\n",
      "10008 [Discriminator loss: 0.063975, acc.: 96.88%] [Generator loss: 7.477286]\n",
      "10009 [Discriminator loss: 0.053049, acc.: 100.00%] [Generator loss: 6.308529]\n",
      "10010 [Discriminator loss: 0.119306, acc.: 95.31%] [Generator loss: 5.895177]\n",
      "10011 [Discriminator loss: 0.221628, acc.: 87.50%] [Generator loss: 5.788495]\n",
      "10012 [Discriminator loss: 0.105929, acc.: 96.88%] [Generator loss: 7.221277]\n",
      "10013 [Discriminator loss: 0.067809, acc.: 96.88%] [Generator loss: 7.584417]\n",
      "10014 [Discriminator loss: 0.180692, acc.: 93.75%] [Generator loss: 5.251793]\n",
      "10015 [Discriminator loss: 0.084914, acc.: 98.44%] [Generator loss: 5.059578]\n",
      "10016 [Discriminator loss: 0.199384, acc.: 90.62%] [Generator loss: 5.642472]\n",
      "10017 [Discriminator loss: 0.069752, acc.: 96.88%] [Generator loss: 7.451473]\n",
      "10018 [Discriminator loss: 0.084935, acc.: 96.88%] [Generator loss: 5.997158]\n",
      "10019 [Discriminator loss: 0.180827, acc.: 92.19%] [Generator loss: 6.599167]\n",
      "10020 [Discriminator loss: 0.078223, acc.: 95.31%] [Generator loss: 7.108434]\n",
      "10021 [Discriminator loss: 0.173970, acc.: 92.19%] [Generator loss: 5.210384]\n",
      "10022 [Discriminator loss: 0.064904, acc.: 96.88%] [Generator loss: 6.555284]\n",
      "10023 [Discriminator loss: 0.087453, acc.: 96.88%] [Generator loss: 6.254260]\n",
      "10024 [Discriminator loss: 0.135445, acc.: 95.31%] [Generator loss: 7.885321]\n",
      "10025 [Discriminator loss: 0.186219, acc.: 92.19%] [Generator loss: 6.205311]\n",
      "10026 [Discriminator loss: 0.261915, acc.: 87.50%] [Generator loss: 6.060524]\n",
      "10027 [Discriminator loss: 0.057732, acc.: 96.88%] [Generator loss: 6.022575]\n",
      "10028 [Discriminator loss: 0.098404, acc.: 98.44%] [Generator loss: 6.129330]\n",
      "10029 [Discriminator loss: 0.254584, acc.: 84.38%] [Generator loss: 8.075883]\n",
      "10030 [Discriminator loss: 0.073939, acc.: 96.88%] [Generator loss: 8.251532]\n",
      "10031 [Discriminator loss: 0.419615, acc.: 81.25%] [Generator loss: 6.772576]\n",
      "10032 [Discriminator loss: 0.064211, acc.: 98.44%] [Generator loss: 7.517404]\n",
      "10033 [Discriminator loss: 0.184883, acc.: 92.19%] [Generator loss: 5.734092]\n",
      "10034 [Discriminator loss: 0.123804, acc.: 95.31%] [Generator loss: 7.895885]\n",
      "10035 [Discriminator loss: 0.154283, acc.: 93.75%] [Generator loss: 7.083692]\n",
      "10036 [Discriminator loss: 0.086787, acc.: 95.31%] [Generator loss: 6.644977]\n",
      "10037 [Discriminator loss: 0.131005, acc.: 92.19%] [Generator loss: 5.964191]\n",
      "10038 [Discriminator loss: 0.083491, acc.: 98.44%] [Generator loss: 5.302896]\n",
      "10039 [Discriminator loss: 0.118871, acc.: 92.19%] [Generator loss: 6.702557]\n",
      "10040 [Discriminator loss: 0.230678, acc.: 93.75%] [Generator loss: 6.220339]\n",
      "10041 [Discriminator loss: 0.067261, acc.: 98.44%] [Generator loss: 7.757497]\n",
      "10042 [Discriminator loss: 0.203772, acc.: 89.06%] [Generator loss: 6.117456]\n",
      "10043 [Discriminator loss: 0.328269, acc.: 89.06%] [Generator loss: 8.317902]\n",
      "10044 [Discriminator loss: 0.204250, acc.: 92.19%] [Generator loss: 6.960384]\n",
      "10045 [Discriminator loss: 0.098175, acc.: 96.88%] [Generator loss: 6.342993]\n",
      "10046 [Discriminator loss: 0.163449, acc.: 93.75%] [Generator loss: 8.022507]\n",
      "10047 [Discriminator loss: 0.039495, acc.: 100.00%] [Generator loss: 6.949005]\n",
      "10048 [Discriminator loss: 0.190937, acc.: 90.62%] [Generator loss: 6.144375]\n",
      "10049 [Discriminator loss: 0.083810, acc.: 98.44%] [Generator loss: 7.713602]\n",
      "10050 [Discriminator loss: 0.340336, acc.: 82.81%] [Generator loss: 7.503780]\n",
      "10051 [Discriminator loss: 0.078628, acc.: 96.88%] [Generator loss: 6.273950]\n",
      "10052 [Discriminator loss: 0.132056, acc.: 92.19%] [Generator loss: 6.993413]\n",
      "10053 [Discriminator loss: 0.093826, acc.: 96.88%] [Generator loss: 6.438745]\n",
      "10054 [Discriminator loss: 0.254265, acc.: 90.62%] [Generator loss: 5.016899]\n",
      "10055 [Discriminator loss: 0.176032, acc.: 95.31%] [Generator loss: 7.154998]\n",
      "10056 [Discriminator loss: 0.060417, acc.: 98.44%] [Generator loss: 6.596917]\n",
      "10057 [Discriminator loss: 0.089457, acc.: 98.44%] [Generator loss: 6.166023]\n",
      "10058 [Discriminator loss: 0.256734, acc.: 90.62%] [Generator loss: 6.717279]\n",
      "10059 [Discriminator loss: 0.166379, acc.: 93.75%] [Generator loss: 7.016725]\n",
      "10060 [Discriminator loss: 0.070718, acc.: 96.88%] [Generator loss: 7.421065]\n",
      "10061 [Discriminator loss: 0.065814, acc.: 98.44%] [Generator loss: 6.832500]\n",
      "10062 [Discriminator loss: 0.177637, acc.: 93.75%] [Generator loss: 5.194011]\n",
      "10063 [Discriminator loss: 0.212090, acc.: 87.50%] [Generator loss: 7.483258]\n",
      "10064 [Discriminator loss: 0.234507, acc.: 90.62%] [Generator loss: 7.115318]\n",
      "10065 [Discriminator loss: 0.076250, acc.: 96.88%] [Generator loss: 5.221331]\n",
      "10066 [Discriminator loss: 0.175035, acc.: 90.62%] [Generator loss: 7.405226]\n",
      "10067 [Discriminator loss: 0.185354, acc.: 90.62%] [Generator loss: 5.223194]\n",
      "10068 [Discriminator loss: 0.174067, acc.: 92.19%] [Generator loss: 6.699047]\n",
      "10069 [Discriminator loss: 0.325034, acc.: 92.19%] [Generator loss: 5.884843]\n",
      "10070 [Discriminator loss: 0.098347, acc.: 96.88%] [Generator loss: 6.391328]\n",
      "10071 [Discriminator loss: 0.240565, acc.: 87.50%] [Generator loss: 7.076125]\n",
      "10072 [Discriminator loss: 0.065718, acc.: 98.44%] [Generator loss: 6.889702]\n",
      "10073 [Discriminator loss: 0.068527, acc.: 98.44%] [Generator loss: 6.544004]\n",
      "10074 [Discriminator loss: 0.167262, acc.: 93.75%] [Generator loss: 6.165821]\n",
      "10075 [Discriminator loss: 0.036559, acc.: 98.44%] [Generator loss: 6.758807]\n",
      "10076 [Discriminator loss: 0.332712, acc.: 85.94%] [Generator loss: 7.195584]\n",
      "10077 [Discriminator loss: 0.086079, acc.: 96.88%] [Generator loss: 6.542804]\n",
      "10078 [Discriminator loss: 0.205523, acc.: 95.31%] [Generator loss: 7.165135]\n",
      "10079 [Discriminator loss: 0.086010, acc.: 96.88%] [Generator loss: 6.792635]\n",
      "10080 [Discriminator loss: 0.069240, acc.: 98.44%] [Generator loss: 5.110894]\n",
      "10081 [Discriminator loss: 0.048518, acc.: 98.44%] [Generator loss: 7.038397]\n",
      "10082 [Discriminator loss: 0.140734, acc.: 95.31%] [Generator loss: 5.991060]\n",
      "10083 [Discriminator loss: 0.197929, acc.: 93.75%] [Generator loss: 5.255123]\n",
      "10084 [Discriminator loss: 0.134573, acc.: 95.31%] [Generator loss: 6.481351]\n",
      "10085 [Discriminator loss: 0.135324, acc.: 93.75%] [Generator loss: 5.862038]\n",
      "10086 [Discriminator loss: 0.110960, acc.: 96.88%] [Generator loss: 4.273374]\n",
      "10087 [Discriminator loss: 0.068867, acc.: 96.88%] [Generator loss: 7.075808]\n",
      "10088 [Discriminator loss: 0.145148, acc.: 93.75%] [Generator loss: 6.473537]\n",
      "10089 [Discriminator loss: 0.131930, acc.: 95.31%] [Generator loss: 5.003675]\n",
      "10090 [Discriminator loss: 0.108705, acc.: 96.88%] [Generator loss: 6.109031]\n",
      "10091 [Discriminator loss: 0.170395, acc.: 92.19%] [Generator loss: 6.961111]\n",
      "10092 [Discriminator loss: 0.137602, acc.: 92.19%] [Generator loss: 5.434971]\n",
      "10093 [Discriminator loss: 0.063888, acc.: 96.88%] [Generator loss: 6.631543]\n",
      "10094 [Discriminator loss: 0.108273, acc.: 93.75%] [Generator loss: 6.919100]\n",
      "10095 [Discriminator loss: 0.045487, acc.: 98.44%] [Generator loss: 7.300284]\n",
      "10096 [Discriminator loss: 0.207928, acc.: 92.19%] [Generator loss: 6.325143]\n",
      "10097 [Discriminator loss: 0.037420, acc.: 100.00%] [Generator loss: 6.192732]\n",
      "10098 [Discriminator loss: 0.164331, acc.: 92.19%] [Generator loss: 8.266420]\n",
      "10099 [Discriminator loss: 0.134594, acc.: 95.31%] [Generator loss: 5.988179]\n",
      "10100 [Discriminator loss: 0.107397, acc.: 96.88%] [Generator loss: 5.939338]\n",
      "10101 [Discriminator loss: 0.069476, acc.: 96.88%] [Generator loss: 6.305169]\n",
      "10102 [Discriminator loss: 0.047429, acc.: 98.44%] [Generator loss: 6.412902]\n",
      "10103 [Discriminator loss: 0.241813, acc.: 92.19%] [Generator loss: 8.867144]\n",
      "10104 [Discriminator loss: 0.224219, acc.: 92.19%] [Generator loss: 5.668939]\n",
      "10105 [Discriminator loss: 0.303275, acc.: 92.19%] [Generator loss: 7.736378]\n",
      "10106 [Discriminator loss: 0.338562, acc.: 90.62%] [Generator loss: 6.628591]\n",
      "10107 [Discriminator loss: 0.034192, acc.: 100.00%] [Generator loss: 8.696230]\n",
      "10108 [Discriminator loss: 0.254320, acc.: 85.94%] [Generator loss: 6.921053]\n",
      "10109 [Discriminator loss: 0.108601, acc.: 96.88%] [Generator loss: 5.383826]\n",
      "10110 [Discriminator loss: 0.237055, acc.: 92.19%] [Generator loss: 7.230394]\n",
      "10111 [Discriminator loss: 0.131084, acc.: 92.19%] [Generator loss: 6.331284]\n",
      "10112 [Discriminator loss: 0.139713, acc.: 95.31%] [Generator loss: 6.522797]\n",
      "10113 [Discriminator loss: 0.154147, acc.: 96.88%] [Generator loss: 5.570635]\n",
      "10114 [Discriminator loss: 0.112297, acc.: 95.31%] [Generator loss: 7.054195]\n",
      "10115 [Discriminator loss: 0.077845, acc.: 96.88%] [Generator loss: 7.003935]\n",
      "10116 [Discriminator loss: 0.110260, acc.: 95.31%] [Generator loss: 5.591994]\n",
      "10117 [Discriminator loss: 0.254870, acc.: 92.19%] [Generator loss: 8.010141]\n",
      "10118 [Discriminator loss: 0.094406, acc.: 95.31%] [Generator loss: 6.718552]\n",
      "10119 [Discriminator loss: 0.134329, acc.: 93.75%] [Generator loss: 5.236495]\n",
      "10120 [Discriminator loss: 0.090925, acc.: 96.88%] [Generator loss: 6.106124]\n",
      "10121 [Discriminator loss: 0.157659, acc.: 93.75%] [Generator loss: 7.809702]\n",
      "10122 [Discriminator loss: 0.131616, acc.: 90.62%] [Generator loss: 8.980013]\n",
      "10123 [Discriminator loss: 0.423435, acc.: 82.81%] [Generator loss: 5.324738]\n",
      "10124 [Discriminator loss: 0.491135, acc.: 84.38%] [Generator loss: 8.754326]\n",
      "10125 [Discriminator loss: 0.128607, acc.: 93.75%] [Generator loss: 9.533565]\n",
      "10126 [Discriminator loss: 0.080438, acc.: 96.88%] [Generator loss: 7.473888]\n",
      "10127 [Discriminator loss: 0.200551, acc.: 89.06%] [Generator loss: 6.688887]\n",
      "10128 [Discriminator loss: 0.117150, acc.: 93.75%] [Generator loss: 8.100008]\n",
      "10129 [Discriminator loss: 0.193535, acc.: 92.19%] [Generator loss: 7.178918]\n",
      "10130 [Discriminator loss: 0.058831, acc.: 95.31%] [Generator loss: 8.033984]\n",
      "10131 [Discriminator loss: 0.264835, acc.: 85.94%] [Generator loss: 5.586534]\n",
      "10132 [Discriminator loss: 0.143885, acc.: 95.31%] [Generator loss: 6.011357]\n",
      "10133 [Discriminator loss: 0.159865, acc.: 92.19%] [Generator loss: 6.430228]\n",
      "10134 [Discriminator loss: 0.038219, acc.: 100.00%] [Generator loss: 5.887306]\n",
      "10135 [Discriminator loss: 0.198292, acc.: 92.19%] [Generator loss: 5.807541]\n",
      "10136 [Discriminator loss: 0.136993, acc.: 92.19%] [Generator loss: 6.554514]\n",
      "10137 [Discriminator loss: 0.172638, acc.: 93.75%] [Generator loss: 7.333433]\n",
      "10138 [Discriminator loss: 0.089563, acc.: 95.31%] [Generator loss: 8.276528]\n",
      "10139 [Discriminator loss: 0.164606, acc.: 92.19%] [Generator loss: 6.896602]\n",
      "10140 [Discriminator loss: 0.068385, acc.: 96.88%] [Generator loss: 6.280458]\n",
      "10141 [Discriminator loss: 0.096868, acc.: 95.31%] [Generator loss: 5.838815]\n",
      "10142 [Discriminator loss: 0.194705, acc.: 90.62%] [Generator loss: 7.065136]\n",
      "10143 [Discriminator loss: 0.022439, acc.: 100.00%] [Generator loss: 7.492218]\n",
      "10144 [Discriminator loss: 0.153793, acc.: 95.31%] [Generator loss: 4.976027]\n",
      "10145 [Discriminator loss: 0.258374, acc.: 90.62%] [Generator loss: 7.576943]\n",
      "10146 [Discriminator loss: 0.032803, acc.: 100.00%] [Generator loss: 9.015173]\n",
      "10147 [Discriminator loss: 0.293963, acc.: 89.06%] [Generator loss: 6.058523]\n",
      "10148 [Discriminator loss: 0.061802, acc.: 98.44%] [Generator loss: 6.387305]\n",
      "10149 [Discriminator loss: 0.148111, acc.: 93.75%] [Generator loss: 6.073244]\n",
      "10150 [Discriminator loss: 0.133207, acc.: 93.75%] [Generator loss: 7.605197]\n",
      "10151 [Discriminator loss: 0.304083, acc.: 89.06%] [Generator loss: 7.518135]\n",
      "10152 [Discriminator loss: 0.094694, acc.: 95.31%] [Generator loss: 7.770052]\n",
      "10153 [Discriminator loss: 0.172523, acc.: 95.31%] [Generator loss: 5.864727]\n",
      "10154 [Discriminator loss: 0.258259, acc.: 92.19%] [Generator loss: 6.015181]\n",
      "10155 [Discriminator loss: 0.118237, acc.: 96.88%] [Generator loss: 7.572271]\n",
      "10156 [Discriminator loss: 0.088141, acc.: 96.88%] [Generator loss: 7.262398]\n",
      "10157 [Discriminator loss: 0.105776, acc.: 95.31%] [Generator loss: 5.664270]\n",
      "10158 [Discriminator loss: 0.142907, acc.: 93.75%] [Generator loss: 7.439659]\n",
      "10159 [Discriminator loss: 0.161238, acc.: 92.19%] [Generator loss: 6.269183]\n",
      "10160 [Discriminator loss: 0.181182, acc.: 92.19%] [Generator loss: 7.778702]\n",
      "10161 [Discriminator loss: 0.088674, acc.: 98.44%] [Generator loss: 7.362395]\n",
      "10162 [Discriminator loss: 0.080529, acc.: 95.31%] [Generator loss: 4.691365]\n",
      "10163 [Discriminator loss: 0.090009, acc.: 98.44%] [Generator loss: 5.420896]\n",
      "10164 [Discriminator loss: 0.078088, acc.: 98.44%] [Generator loss: 6.863120]\n",
      "10165 [Discriminator loss: 0.034244, acc.: 100.00%] [Generator loss: 6.905457]\n",
      "10166 [Discriminator loss: 0.209031, acc.: 90.62%] [Generator loss: 6.835588]\n",
      "10167 [Discriminator loss: 0.103876, acc.: 96.88%] [Generator loss: 6.983196]\n",
      "10168 [Discriminator loss: 0.147368, acc.: 92.19%] [Generator loss: 4.955258]\n",
      "10169 [Discriminator loss: 0.107623, acc.: 95.31%] [Generator loss: 5.296024]\n",
      "10170 [Discriminator loss: 0.039999, acc.: 98.44%] [Generator loss: 7.285428]\n",
      "10171 [Discriminator loss: 0.125746, acc.: 96.88%] [Generator loss: 7.801907]\n",
      "10172 [Discriminator loss: 0.081258, acc.: 98.44%] [Generator loss: 6.635732]\n",
      "10173 [Discriminator loss: 0.223658, acc.: 89.06%] [Generator loss: 6.457497]\n",
      "10174 [Discriminator loss: 0.149121, acc.: 95.31%] [Generator loss: 6.279063]\n",
      "10175 [Discriminator loss: 0.074417, acc.: 96.88%] [Generator loss: 6.615943]\n",
      "10176 [Discriminator loss: 0.176156, acc.: 93.75%] [Generator loss: 6.069178]\n",
      "10177 [Discriminator loss: 0.200932, acc.: 89.06%] [Generator loss: 7.498783]\n",
      "10178 [Discriminator loss: 0.058718, acc.: 95.31%] [Generator loss: 7.534242]\n",
      "10179 [Discriminator loss: 0.035561, acc.: 96.88%] [Generator loss: 8.610012]\n",
      "10180 [Discriminator loss: 0.147369, acc.: 95.31%] [Generator loss: 6.526242]\n",
      "10181 [Discriminator loss: 0.318013, acc.: 89.06%] [Generator loss: 6.732152]\n",
      "10182 [Discriminator loss: 0.021684, acc.: 100.00%] [Generator loss: 6.258897]\n",
      "10183 [Discriminator loss: 0.176055, acc.: 92.19%] [Generator loss: 6.865338]\n",
      "10184 [Discriminator loss: 0.053727, acc.: 100.00%] [Generator loss: 6.114780]\n",
      "10185 [Discriminator loss: 0.070428, acc.: 100.00%] [Generator loss: 5.985162]\n",
      "10186 [Discriminator loss: 0.197334, acc.: 93.75%] [Generator loss: 7.232123]\n",
      "10187 [Discriminator loss: 0.087409, acc.: 96.88%] [Generator loss: 5.309714]\n",
      "10188 [Discriminator loss: 0.097842, acc.: 96.88%] [Generator loss: 5.731712]\n",
      "10189 [Discriminator loss: 0.095668, acc.: 93.75%] [Generator loss: 5.505027]\n",
      "10190 [Discriminator loss: 0.193337, acc.: 93.75%] [Generator loss: 6.757323]\n",
      "10191 [Discriminator loss: 0.142355, acc.: 96.88%] [Generator loss: 6.961118]\n",
      "10192 [Discriminator loss: 0.116398, acc.: 93.75%] [Generator loss: 7.706827]\n",
      "10193 [Discriminator loss: 0.271517, acc.: 89.06%] [Generator loss: 5.481617]\n",
      "10194 [Discriminator loss: 0.225424, acc.: 87.50%] [Generator loss: 7.309367]\n",
      "10195 [Discriminator loss: 0.109944, acc.: 95.31%] [Generator loss: 8.598903]\n",
      "10196 [Discriminator loss: 0.096768, acc.: 95.31%] [Generator loss: 6.309733]\n",
      "10197 [Discriminator loss: 0.065058, acc.: 96.88%] [Generator loss: 6.290488]\n",
      "10198 [Discriminator loss: 0.066083, acc.: 96.88%] [Generator loss: 6.345435]\n",
      "10199 [Discriminator loss: 0.093036, acc.: 98.44%] [Generator loss: 5.405834]\n",
      "10200 [Discriminator loss: 0.167847, acc.: 95.31%] [Generator loss: 6.034363]\n",
      "10201 [Discriminator loss: 0.031066, acc.: 98.44%] [Generator loss: 6.803924]\n",
      "10202 [Discriminator loss: 0.115218, acc.: 95.31%] [Generator loss: 7.164208]\n",
      "10203 [Discriminator loss: 0.119081, acc.: 95.31%] [Generator loss: 7.723579]\n",
      "10204 [Discriminator loss: 0.104662, acc.: 95.31%] [Generator loss: 6.972259]\n",
      "10205 [Discriminator loss: 0.052170, acc.: 98.44%] [Generator loss: 6.486300]\n",
      "10206 [Discriminator loss: 0.095790, acc.: 96.88%] [Generator loss: 6.025977]\n",
      "10207 [Discriminator loss: 0.163106, acc.: 90.62%] [Generator loss: 6.208607]\n",
      "10208 [Discriminator loss: 0.118045, acc.: 96.88%] [Generator loss: 6.842763]\n",
      "10209 [Discriminator loss: 0.134207, acc.: 93.75%] [Generator loss: 5.410496]\n",
      "10210 [Discriminator loss: 0.105674, acc.: 95.31%] [Generator loss: 6.763573]\n",
      "10211 [Discriminator loss: 0.075656, acc.: 98.44%] [Generator loss: 5.794527]\n",
      "10212 [Discriminator loss: 0.136482, acc.: 93.75%] [Generator loss: 6.984883]\n",
      "10213 [Discriminator loss: 0.051324, acc.: 98.44%] [Generator loss: 6.854565]\n",
      "10214 [Discriminator loss: 0.191120, acc.: 90.62%] [Generator loss: 5.550800]\n",
      "10215 [Discriminator loss: 0.129121, acc.: 92.19%] [Generator loss: 6.866862]\n",
      "10216 [Discriminator loss: 0.082628, acc.: 96.88%] [Generator loss: 6.422182]\n",
      "10217 [Discriminator loss: 0.055478, acc.: 98.44%] [Generator loss: 6.530872]\n",
      "10218 [Discriminator loss: 0.196355, acc.: 95.31%] [Generator loss: 6.605719]\n",
      "10219 [Discriminator loss: 0.115147, acc.: 95.31%] [Generator loss: 7.821148]\n",
      "10220 [Discriminator loss: 0.109683, acc.: 96.88%] [Generator loss: 6.796307]\n",
      "10221 [Discriminator loss: 0.109187, acc.: 98.44%] [Generator loss: 7.249929]\n",
      "10222 [Discriminator loss: 0.125957, acc.: 92.19%] [Generator loss: 6.916002]\n",
      "10223 [Discriminator loss: 0.073169, acc.: 96.88%] [Generator loss: 6.334573]\n",
      "10224 [Discriminator loss: 0.319465, acc.: 90.62%] [Generator loss: 5.365626]\n",
      "10225 [Discriminator loss: 0.148372, acc.: 95.31%] [Generator loss: 7.693473]\n",
      "10226 [Discriminator loss: 0.134280, acc.: 93.75%] [Generator loss: 6.922941]\n",
      "10227 [Discriminator loss: 0.034240, acc.: 100.00%] [Generator loss: 7.046792]\n",
      "10228 [Discriminator loss: 0.086700, acc.: 96.88%] [Generator loss: 6.360676]\n",
      "10229 [Discriminator loss: 0.165087, acc.: 95.31%] [Generator loss: 6.218704]\n",
      "10230 [Discriminator loss: 0.064825, acc.: 100.00%] [Generator loss: 4.883791]\n",
      "10231 [Discriminator loss: 0.069334, acc.: 98.44%] [Generator loss: 5.179993]\n",
      "10232 [Discriminator loss: 0.113658, acc.: 95.31%] [Generator loss: 4.929039]\n",
      "10233 [Discriminator loss: 0.226394, acc.: 90.62%] [Generator loss: 8.703339]\n",
      "10234 [Discriminator loss: 0.194072, acc.: 90.62%] [Generator loss: 7.741623]\n",
      "10235 [Discriminator loss: 0.132465, acc.: 93.75%] [Generator loss: 7.352404]\n",
      "10236 [Discriminator loss: 0.141278, acc.: 92.19%] [Generator loss: 6.614632]\n",
      "10237 [Discriminator loss: 0.077723, acc.: 96.88%] [Generator loss: 6.666420]\n",
      "10238 [Discriminator loss: 0.197352, acc.: 90.62%] [Generator loss: 7.327241]\n",
      "10239 [Discriminator loss: 0.107481, acc.: 96.88%] [Generator loss: 6.704584]\n",
      "10240 [Discriminator loss: 0.385614, acc.: 87.50%] [Generator loss: 7.222563]\n",
      "10241 [Discriminator loss: 0.065994, acc.: 96.88%] [Generator loss: 8.016164]\n",
      "10242 [Discriminator loss: 0.177762, acc.: 92.19%] [Generator loss: 7.788491]\n",
      "10243 [Discriminator loss: 0.056833, acc.: 98.44%] [Generator loss: 6.117671]\n",
      "10244 [Discriminator loss: 0.178942, acc.: 96.88%] [Generator loss: 6.702274]\n",
      "10245 [Discriminator loss: 0.250741, acc.: 89.06%] [Generator loss: 7.151501]\n",
      "10246 [Discriminator loss: 0.174870, acc.: 93.75%] [Generator loss: 6.522698]\n",
      "10247 [Discriminator loss: 0.111258, acc.: 93.75%] [Generator loss: 5.693020]\n",
      "10248 [Discriminator loss: 0.122658, acc.: 96.88%] [Generator loss: 7.262317]\n",
      "10249 [Discriminator loss: 0.128437, acc.: 93.75%] [Generator loss: 7.063913]\n",
      "10250 [Discriminator loss: 0.276134, acc.: 90.62%] [Generator loss: 6.993150]\n",
      "10251 [Discriminator loss: 0.089032, acc.: 96.88%] [Generator loss: 7.106999]\n",
      "10252 [Discriminator loss: 0.248234, acc.: 85.94%] [Generator loss: 7.118168]\n",
      "10253 [Discriminator loss: 0.125609, acc.: 95.31%] [Generator loss: 6.908025]\n",
      "10254 [Discriminator loss: 0.025374, acc.: 100.00%] [Generator loss: 5.337998]\n",
      "10255 [Discriminator loss: 0.105598, acc.: 96.88%] [Generator loss: 6.885009]\n",
      "10256 [Discriminator loss: 0.153861, acc.: 93.75%] [Generator loss: 6.497439]\n",
      "10257 [Discriminator loss: 0.091289, acc.: 96.88%] [Generator loss: 6.273187]\n",
      "10258 [Discriminator loss: 0.125966, acc.: 93.75%] [Generator loss: 7.078974]\n",
      "10259 [Discriminator loss: 0.100285, acc.: 98.44%] [Generator loss: 7.289234]\n",
      "10260 [Discriminator loss: 0.025750, acc.: 98.44%] [Generator loss: 5.248047]\n",
      "10261 [Discriminator loss: 0.083821, acc.: 96.88%] [Generator loss: 5.672703]\n",
      "10262 [Discriminator loss: 0.247936, acc.: 89.06%] [Generator loss: 7.624947]\n",
      "10263 [Discriminator loss: 0.122361, acc.: 95.31%] [Generator loss: 8.441351]\n",
      "10264 [Discriminator loss: 0.088057, acc.: 96.88%] [Generator loss: 6.367237]\n",
      "10265 [Discriminator loss: 0.181350, acc.: 93.75%] [Generator loss: 6.789222]\n",
      "10266 [Discriminator loss: 0.439182, acc.: 89.06%] [Generator loss: 6.601295]\n",
      "10267 [Discriminator loss: 0.067460, acc.: 96.88%] [Generator loss: 6.974482]\n",
      "10268 [Discriminator loss: 0.143787, acc.: 93.75%] [Generator loss: 8.283354]\n",
      "10269 [Discriminator loss: 0.056069, acc.: 98.44%] [Generator loss: 7.585013]\n",
      "10270 [Discriminator loss: 0.146567, acc.: 93.75%] [Generator loss: 6.652561]\n",
      "10271 [Discriminator loss: 0.132526, acc.: 96.88%] [Generator loss: 6.597012]\n",
      "10272 [Discriminator loss: 0.047359, acc.: 100.00%] [Generator loss: 7.103916]\n",
      "10273 [Discriminator loss: 0.315541, acc.: 87.50%] [Generator loss: 5.969548]\n",
      "10274 [Discriminator loss: 0.111142, acc.: 95.31%] [Generator loss: 7.913305]\n",
      "10275 [Discriminator loss: 0.176732, acc.: 92.19%] [Generator loss: 5.476454]\n",
      "10276 [Discriminator loss: 0.141769, acc.: 93.75%] [Generator loss: 6.722038]\n",
      "10277 [Discriminator loss: 0.161760, acc.: 95.31%] [Generator loss: 6.631551]\n",
      "10278 [Discriminator loss: 0.170802, acc.: 90.62%] [Generator loss: 7.711489]\n",
      "10279 [Discriminator loss: 0.108352, acc.: 92.19%] [Generator loss: 6.783918]\n",
      "10280 [Discriminator loss: 0.130861, acc.: 96.88%] [Generator loss: 5.852520]\n",
      "10281 [Discriminator loss: 0.182423, acc.: 93.75%] [Generator loss: 5.100207]\n",
      "10282 [Discriminator loss: 0.106393, acc.: 96.88%] [Generator loss: 7.221069]\n",
      "10283 [Discriminator loss: 0.151211, acc.: 93.75%] [Generator loss: 5.862980]\n",
      "10284 [Discriminator loss: 0.187951, acc.: 92.19%] [Generator loss: 5.788175]\n",
      "10285 [Discriminator loss: 0.021451, acc.: 100.00%] [Generator loss: 6.388204]\n",
      "10286 [Discriminator loss: 0.259697, acc.: 87.50%] [Generator loss: 7.325327]\n",
      "10287 [Discriminator loss: 0.117685, acc.: 96.88%] [Generator loss: 5.278654]\n",
      "10288 [Discriminator loss: 0.251212, acc.: 89.06%] [Generator loss: 6.514084]\n",
      "10289 [Discriminator loss: 0.129480, acc.: 95.31%] [Generator loss: 8.579210]\n",
      "10290 [Discriminator loss: 0.136383, acc.: 95.31%] [Generator loss: 5.979713]\n",
      "10291 [Discriminator loss: 0.104715, acc.: 96.88%] [Generator loss: 7.191746]\n",
      "10292 [Discriminator loss: 0.076249, acc.: 96.88%] [Generator loss: 7.771759]\n",
      "10293 [Discriminator loss: 0.136381, acc.: 93.75%] [Generator loss: 7.002429]\n",
      "10294 [Discriminator loss: 0.258202, acc.: 90.62%] [Generator loss: 7.410243]\n",
      "10295 [Discriminator loss: 0.207764, acc.: 90.62%] [Generator loss: 4.746597]\n",
      "10296 [Discriminator loss: 0.132122, acc.: 96.88%] [Generator loss: 5.717822]\n",
      "10297 [Discriminator loss: 0.048438, acc.: 100.00%] [Generator loss: 6.965641]\n",
      "10298 [Discriminator loss: 0.090163, acc.: 96.88%] [Generator loss: 6.064169]\n",
      "10299 [Discriminator loss: 0.104315, acc.: 93.75%] [Generator loss: 6.078357]\n",
      "10300 [Discriminator loss: 0.052467, acc.: 98.44%] [Generator loss: 7.643914]\n",
      "10301 [Discriminator loss: 0.116529, acc.: 95.31%] [Generator loss: 6.239758]\n",
      "10302 [Discriminator loss: 0.088794, acc.: 96.88%] [Generator loss: 6.240346]\n",
      "10303 [Discriminator loss: 0.062606, acc.: 98.44%] [Generator loss: 7.215354]\n",
      "10304 [Discriminator loss: 0.136717, acc.: 92.19%] [Generator loss: 5.264838]\n",
      "10305 [Discriminator loss: 0.071663, acc.: 96.88%] [Generator loss: 6.208590]\n",
      "10306 [Discriminator loss: 0.078061, acc.: 96.88%] [Generator loss: 6.745970]\n",
      "10307 [Discriminator loss: 0.089725, acc.: 96.88%] [Generator loss: 6.945219]\n",
      "10308 [Discriminator loss: 0.048424, acc.: 100.00%] [Generator loss: 6.873603]\n",
      "10309 [Discriminator loss: 0.050368, acc.: 98.44%] [Generator loss: 5.747119]\n",
      "10310 [Discriminator loss: 0.048893, acc.: 100.00%] [Generator loss: 7.046239]\n",
      "10311 [Discriminator loss: 0.034586, acc.: 100.00%] [Generator loss: 7.035025]\n",
      "10312 [Discriminator loss: 0.121650, acc.: 96.88%] [Generator loss: 7.324364]\n",
      "10313 [Discriminator loss: 0.154877, acc.: 92.19%] [Generator loss: 5.660277]\n",
      "10314 [Discriminator loss: 0.030971, acc.: 100.00%] [Generator loss: 5.976289]\n",
      "10315 [Discriminator loss: 0.069815, acc.: 100.00%] [Generator loss: 5.740910]\n",
      "10316 [Discriminator loss: 0.121483, acc.: 92.19%] [Generator loss: 7.538161]\n",
      "10317 [Discriminator loss: 0.147348, acc.: 95.31%] [Generator loss: 7.578066]\n",
      "10318 [Discriminator loss: 0.146489, acc.: 95.31%] [Generator loss: 5.586102]\n",
      "10319 [Discriminator loss: 0.081181, acc.: 96.88%] [Generator loss: 6.458835]\n",
      "10320 [Discriminator loss: 0.187035, acc.: 92.19%] [Generator loss: 6.462188]\n",
      "10321 [Discriminator loss: 0.069627, acc.: 98.44%] [Generator loss: 7.089632]\n",
      "10322 [Discriminator loss: 0.164829, acc.: 96.88%] [Generator loss: 7.098723]\n",
      "10323 [Discriminator loss: 0.154913, acc.: 93.75%] [Generator loss: 8.057980]\n",
      "10324 [Discriminator loss: 0.114566, acc.: 93.75%] [Generator loss: 5.058865]\n",
      "10325 [Discriminator loss: 0.153997, acc.: 93.75%] [Generator loss: 6.924184]\n",
      "10326 [Discriminator loss: 0.194628, acc.: 95.31%] [Generator loss: 6.614594]\n",
      "10327 [Discriminator loss: 0.104766, acc.: 96.88%] [Generator loss: 7.681514]\n",
      "10328 [Discriminator loss: 0.109156, acc.: 95.31%] [Generator loss: 6.900748]\n",
      "10329 [Discriminator loss: 0.077487, acc.: 96.88%] [Generator loss: 6.147964]\n",
      "10330 [Discriminator loss: 0.058282, acc.: 96.88%] [Generator loss: 5.824142]\n",
      "10331 [Discriminator loss: 0.316927, acc.: 93.75%] [Generator loss: 8.627052]\n",
      "10332 [Discriminator loss: 0.066196, acc.: 98.44%] [Generator loss: 8.307247]\n",
      "10333 [Discriminator loss: 0.126945, acc.: 95.31%] [Generator loss: 6.789618]\n",
      "10334 [Discriminator loss: 0.370439, acc.: 78.12%] [Generator loss: 7.839527]\n",
      "10335 [Discriminator loss: 0.055900, acc.: 98.44%] [Generator loss: 7.399176]\n",
      "10336 [Discriminator loss: 0.209108, acc.: 93.75%] [Generator loss: 7.099110]\n",
      "10337 [Discriminator loss: 0.150812, acc.: 93.75%] [Generator loss: 6.381135]\n",
      "10338 [Discriminator loss: 0.064401, acc.: 100.00%] [Generator loss: 7.351347]\n",
      "10339 [Discriminator loss: 0.080427, acc.: 98.44%] [Generator loss: 6.157074]\n",
      "10340 [Discriminator loss: 0.147645, acc.: 95.31%] [Generator loss: 6.588662]\n",
      "10341 [Discriminator loss: 0.140601, acc.: 95.31%] [Generator loss: 6.925743]\n",
      "10342 [Discriminator loss: 0.041135, acc.: 100.00%] [Generator loss: 6.867272]\n",
      "10343 [Discriminator loss: 0.158947, acc.: 92.19%] [Generator loss: 6.366529]\n",
      "10344 [Discriminator loss: 0.079874, acc.: 96.88%] [Generator loss: 6.652523]\n",
      "10345 [Discriminator loss: 0.124088, acc.: 98.44%] [Generator loss: 6.083641]\n",
      "10346 [Discriminator loss: 0.204330, acc.: 92.19%] [Generator loss: 7.444567]\n",
      "10347 [Discriminator loss: 0.148179, acc.: 93.75%] [Generator loss: 6.815574]\n",
      "10348 [Discriminator loss: 0.031352, acc.: 98.44%] [Generator loss: 6.367935]\n",
      "10349 [Discriminator loss: 0.045330, acc.: 98.44%] [Generator loss: 6.387453]\n",
      "10350 [Discriminator loss: 0.203999, acc.: 90.62%] [Generator loss: 7.368007]\n",
      "10351 [Discriminator loss: 0.078354, acc.: 95.31%] [Generator loss: 7.845572]\n",
      "10352 [Discriminator loss: 0.471853, acc.: 82.81%] [Generator loss: 5.910021]\n",
      "10353 [Discriminator loss: 0.047320, acc.: 98.44%] [Generator loss: 6.637635]\n",
      "10354 [Discriminator loss: 0.086832, acc.: 96.88%] [Generator loss: 5.265416]\n",
      "10355 [Discriminator loss: 0.056444, acc.: 98.44%] [Generator loss: 6.030702]\n",
      "10356 [Discriminator loss: 0.103254, acc.: 95.31%] [Generator loss: 7.670838]\n",
      "10357 [Discriminator loss: 0.071169, acc.: 96.88%] [Generator loss: 7.252950]\n",
      "10358 [Discriminator loss: 0.273006, acc.: 89.06%] [Generator loss: 7.727191]\n",
      "10359 [Discriminator loss: 0.110764, acc.: 96.88%] [Generator loss: 6.932177]\n",
      "10360 [Discriminator loss: 0.134057, acc.: 89.06%] [Generator loss: 6.690774]\n",
      "10361 [Discriminator loss: 0.171293, acc.: 92.19%] [Generator loss: 7.748753]\n",
      "10362 [Discriminator loss: 0.058095, acc.: 96.88%] [Generator loss: 7.478065]\n",
      "10363 [Discriminator loss: 0.050666, acc.: 96.88%] [Generator loss: 5.844444]\n",
      "10364 [Discriminator loss: 0.114248, acc.: 95.31%] [Generator loss: 5.131318]\n",
      "10365 [Discriminator loss: 0.105130, acc.: 95.31%] [Generator loss: 6.923052]\n",
      "10366 [Discriminator loss: 0.163343, acc.: 98.44%] [Generator loss: 6.590529]\n",
      "10367 [Discriminator loss: 0.171333, acc.: 92.19%] [Generator loss: 8.264524]\n",
      "10368 [Discriminator loss: 0.089833, acc.: 96.88%] [Generator loss: 7.102901]\n",
      "10369 [Discriminator loss: 0.141891, acc.: 93.75%] [Generator loss: 6.457366]\n",
      "10370 [Discriminator loss: 0.078824, acc.: 96.88%] [Generator loss: 7.009640]\n",
      "10371 [Discriminator loss: 0.028440, acc.: 100.00%] [Generator loss: 5.569838]\n",
      "10372 [Discriminator loss: 0.121064, acc.: 95.31%] [Generator loss: 5.967871]\n",
      "10373 [Discriminator loss: 0.096184, acc.: 96.88%] [Generator loss: 7.352620]\n",
      "10374 [Discriminator loss: 0.127786, acc.: 95.31%] [Generator loss: 6.964556]\n",
      "10375 [Discriminator loss: 0.057535, acc.: 98.44%] [Generator loss: 5.373024]\n",
      "10376 [Discriminator loss: 0.194146, acc.: 92.19%] [Generator loss: 5.117404]\n",
      "10377 [Discriminator loss: 0.063094, acc.: 98.44%] [Generator loss: 6.345027]\n",
      "10378 [Discriminator loss: 0.150883, acc.: 92.19%] [Generator loss: 5.844660]\n",
      "10379 [Discriminator loss: 0.082804, acc.: 98.44%] [Generator loss: 5.217700]\n",
      "10380 [Discriminator loss: 0.053784, acc.: 98.44%] [Generator loss: 5.725538]\n",
      "10381 [Discriminator loss: 0.285735, acc.: 90.62%] [Generator loss: 8.043365]\n",
      "10382 [Discriminator loss: 0.041292, acc.: 98.44%] [Generator loss: 8.858681]\n",
      "10383 [Discriminator loss: 0.071545, acc.: 95.31%] [Generator loss: 7.116791]\n",
      "10384 [Discriminator loss: 0.120679, acc.: 95.31%] [Generator loss: 5.206179]\n",
      "10385 [Discriminator loss: 0.172302, acc.: 95.31%] [Generator loss: 7.744251]\n",
      "10386 [Discriminator loss: 0.120012, acc.: 95.31%] [Generator loss: 6.760422]\n",
      "10387 [Discriminator loss: 0.029307, acc.: 98.44%] [Generator loss: 5.435884]\n",
      "10388 [Discriminator loss: 0.129067, acc.: 92.19%] [Generator loss: 4.919683]\n",
      "10389 [Discriminator loss: 0.122825, acc.: 93.75%] [Generator loss: 6.062163]\n",
      "10390 [Discriminator loss: 0.060153, acc.: 98.44%] [Generator loss: 6.704069]\n",
      "10391 [Discriminator loss: 0.058447, acc.: 96.88%] [Generator loss: 5.548096]\n",
      "10392 [Discriminator loss: 0.092599, acc.: 96.88%] [Generator loss: 8.075501]\n",
      "10393 [Discriminator loss: 0.093673, acc.: 96.88%] [Generator loss: 7.433325]\n",
      "10394 [Discriminator loss: 0.161392, acc.: 93.75%] [Generator loss: 6.412710]\n",
      "10395 [Discriminator loss: 0.348409, acc.: 84.38%] [Generator loss: 4.823765]\n",
      "10396 [Discriminator loss: 0.113639, acc.: 92.19%] [Generator loss: 7.942084]\n",
      "10397 [Discriminator loss: 0.053974, acc.: 100.00%] [Generator loss: 6.661133]\n",
      "10398 [Discriminator loss: 0.213492, acc.: 89.06%] [Generator loss: 7.203126]\n",
      "10399 [Discriminator loss: 0.136403, acc.: 93.75%] [Generator loss: 6.282433]\n",
      "10400 [Discriminator loss: 0.049576, acc.: 98.44%] [Generator loss: 6.197491]\n",
      "10401 [Discriminator loss: 0.266366, acc.: 89.06%] [Generator loss: 6.034037]\n",
      "10402 [Discriminator loss: 0.051313, acc.: 98.44%] [Generator loss: 7.198464]\n",
      "10403 [Discriminator loss: 0.264945, acc.: 85.94%] [Generator loss: 6.314494]\n",
      "10404 [Discriminator loss: 0.116826, acc.: 95.31%] [Generator loss: 6.746162]\n",
      "10405 [Discriminator loss: 0.087640, acc.: 95.31%] [Generator loss: 8.075508]\n",
      "10406 [Discriminator loss: 0.032496, acc.: 100.00%] [Generator loss: 7.089972]\n",
      "10407 [Discriminator loss: 0.174020, acc.: 93.75%] [Generator loss: 6.476646]\n",
      "10408 [Discriminator loss: 0.145058, acc.: 93.75%] [Generator loss: 5.170888]\n",
      "10409 [Discriminator loss: 0.032637, acc.: 100.00%] [Generator loss: 6.229334]\n",
      "10410 [Discriminator loss: 0.045735, acc.: 98.44%] [Generator loss: 5.475451]\n",
      "10411 [Discriminator loss: 0.117268, acc.: 96.88%] [Generator loss: 7.068765]\n",
      "10412 [Discriminator loss: 0.165732, acc.: 93.75%] [Generator loss: 5.522243]\n",
      "10413 [Discriminator loss: 0.046921, acc.: 100.00%] [Generator loss: 7.612759]\n",
      "10414 [Discriminator loss: 0.069194, acc.: 98.44%] [Generator loss: 5.284990]\n",
      "10415 [Discriminator loss: 0.116633, acc.: 93.75%] [Generator loss: 5.481709]\n",
      "10416 [Discriminator loss: 0.108229, acc.: 95.31%] [Generator loss: 7.877524]\n",
      "10417 [Discriminator loss: 0.231516, acc.: 85.94%] [Generator loss: 7.483234]\n",
      "10418 [Discriminator loss: 0.131036, acc.: 95.31%] [Generator loss: 6.798893]\n",
      "10419 [Discriminator loss: 0.121478, acc.: 95.31%] [Generator loss: 7.287515]\n",
      "10420 [Discriminator loss: 0.087460, acc.: 93.75%] [Generator loss: 6.372488]\n",
      "10421 [Discriminator loss: 0.134625, acc.: 92.19%] [Generator loss: 6.108830]\n",
      "10422 [Discriminator loss: 0.211722, acc.: 92.19%] [Generator loss: 8.599134]\n",
      "10423 [Discriminator loss: 0.103703, acc.: 96.88%] [Generator loss: 8.238462]\n",
      "10424 [Discriminator loss: 0.206024, acc.: 90.62%] [Generator loss: 7.362754]\n",
      "10425 [Discriminator loss: 0.090753, acc.: 98.44%] [Generator loss: 7.176980]\n",
      "10426 [Discriminator loss: 0.218420, acc.: 90.62%] [Generator loss: 5.843708]\n",
      "10427 [Discriminator loss: 0.065318, acc.: 96.88%] [Generator loss: 6.829677]\n",
      "10428 [Discriminator loss: 0.051445, acc.: 100.00%] [Generator loss: 6.509625]\n",
      "10429 [Discriminator loss: 0.135936, acc.: 93.75%] [Generator loss: 6.592526]\n",
      "10430 [Discriminator loss: 0.109462, acc.: 95.31%] [Generator loss: 6.498284]\n",
      "10431 [Discriminator loss: 0.265908, acc.: 89.06%] [Generator loss: 4.574432]\n",
      "10432 [Discriminator loss: 0.030796, acc.: 100.00%] [Generator loss: 5.789777]\n",
      "10433 [Discriminator loss: 0.119358, acc.: 96.88%] [Generator loss: 6.600529]\n",
      "10434 [Discriminator loss: 0.062046, acc.: 96.88%] [Generator loss: 6.270677]\n",
      "10435 [Discriminator loss: 0.225823, acc.: 90.62%] [Generator loss: 6.345358]\n",
      "10436 [Discriminator loss: 0.054949, acc.: 98.44%] [Generator loss: 6.584550]\n",
      "10437 [Discriminator loss: 0.177309, acc.: 90.62%] [Generator loss: 8.107569]\n",
      "10438 [Discriminator loss: 0.092123, acc.: 98.44%] [Generator loss: 7.898890]\n",
      "10439 [Discriminator loss: 0.183919, acc.: 93.75%] [Generator loss: 7.739039]\n",
      "10440 [Discriminator loss: 0.108689, acc.: 93.75%] [Generator loss: 5.815239]\n",
      "10441 [Discriminator loss: 0.083343, acc.: 95.31%] [Generator loss: 6.804118]\n",
      "10442 [Discriminator loss: 0.118200, acc.: 92.19%] [Generator loss: 5.507985]\n",
      "10443 [Discriminator loss: 0.082112, acc.: 96.88%] [Generator loss: 7.445375]\n",
      "10444 [Discriminator loss: 0.122554, acc.: 95.31%] [Generator loss: 6.826109]\n",
      "10445 [Discriminator loss: 0.060336, acc.: 98.44%] [Generator loss: 6.709507]\n",
      "10446 [Discriminator loss: 0.090214, acc.: 96.88%] [Generator loss: 5.797760]\n",
      "10447 [Discriminator loss: 0.261432, acc.: 89.06%] [Generator loss: 8.667409]\n",
      "10448 [Discriminator loss: 0.160553, acc.: 93.75%] [Generator loss: 8.995906]\n",
      "10449 [Discriminator loss: 0.123166, acc.: 95.31%] [Generator loss: 5.301133]\n",
      "10450 [Discriminator loss: 0.058441, acc.: 96.88%] [Generator loss: 6.312361]\n",
      "10451 [Discriminator loss: 0.144579, acc.: 95.31%] [Generator loss: 7.423971]\n",
      "10452 [Discriminator loss: 0.056551, acc.: 98.44%] [Generator loss: 6.622872]\n",
      "10453 [Discriminator loss: 0.033239, acc.: 100.00%] [Generator loss: 5.873552]\n",
      "10454 [Discriminator loss: 0.158076, acc.: 90.62%] [Generator loss: 5.346287]\n",
      "10455 [Discriminator loss: 0.204069, acc.: 95.31%] [Generator loss: 7.119462]\n",
      "10456 [Discriminator loss: 0.039636, acc.: 100.00%] [Generator loss: 7.557294]\n",
      "10457 [Discriminator loss: 0.256620, acc.: 87.50%] [Generator loss: 5.302557]\n",
      "10458 [Discriminator loss: 0.080044, acc.: 96.88%] [Generator loss: 6.563975]\n",
      "10459 [Discriminator loss: 0.138610, acc.: 95.31%] [Generator loss: 6.878679]\n",
      "10460 [Discriminator loss: 0.180003, acc.: 87.50%] [Generator loss: 6.708030]\n",
      "10461 [Discriminator loss: 0.027806, acc.: 100.00%] [Generator loss: 7.302028]\n",
      "10462 [Discriminator loss: 0.206604, acc.: 93.75%] [Generator loss: 7.209342]\n",
      "10463 [Discriminator loss: 0.122034, acc.: 92.19%] [Generator loss: 7.401645]\n",
      "10464 [Discriminator loss: 0.041374, acc.: 100.00%] [Generator loss: 7.476407]\n",
      "10465 [Discriminator loss: 0.143169, acc.: 92.19%] [Generator loss: 5.627063]\n",
      "10466 [Discriminator loss: 0.268220, acc.: 89.06%] [Generator loss: 5.841974]\n",
      "10467 [Discriminator loss: 0.030104, acc.: 100.00%] [Generator loss: 7.075803]\n",
      "10468 [Discriminator loss: 0.066695, acc.: 95.31%] [Generator loss: 5.042649]\n",
      "10469 [Discriminator loss: 0.285265, acc.: 84.38%] [Generator loss: 7.708857]\n",
      "10470 [Discriminator loss: 0.068799, acc.: 98.44%] [Generator loss: 8.777767]\n",
      "10471 [Discriminator loss: 0.256228, acc.: 90.62%] [Generator loss: 6.297310]\n",
      "10472 [Discriminator loss: 0.088035, acc.: 96.88%] [Generator loss: 6.195267]\n",
      "10473 [Discriminator loss: 0.050679, acc.: 98.44%] [Generator loss: 7.038460]\n",
      "10474 [Discriminator loss: 0.150944, acc.: 95.31%] [Generator loss: 7.419595]\n",
      "10475 [Discriminator loss: 0.047309, acc.: 98.44%] [Generator loss: 6.743218]\n",
      "10476 [Discriminator loss: 0.164197, acc.: 95.31%] [Generator loss: 7.685646]\n",
      "10477 [Discriminator loss: 0.117454, acc.: 95.31%] [Generator loss: 6.184088]\n",
      "10478 [Discriminator loss: 0.146116, acc.: 96.88%] [Generator loss: 5.648577]\n",
      "10479 [Discriminator loss: 0.135204, acc.: 96.88%] [Generator loss: 7.748252]\n",
      "10480 [Discriminator loss: 0.198714, acc.: 92.19%] [Generator loss: 5.533275]\n",
      "10481 [Discriminator loss: 0.238803, acc.: 87.50%] [Generator loss: 7.147989]\n",
      "10482 [Discriminator loss: 0.068696, acc.: 98.44%] [Generator loss: 8.681581]\n",
      "10483 [Discriminator loss: 0.312073, acc.: 87.50%] [Generator loss: 6.164371]\n",
      "10484 [Discriminator loss: 0.099144, acc.: 93.75%] [Generator loss: 6.608748]\n",
      "10485 [Discriminator loss: 0.221675, acc.: 93.75%] [Generator loss: 5.751232]\n",
      "10486 [Discriminator loss: 0.067614, acc.: 98.44%] [Generator loss: 5.990980]\n",
      "10487 [Discriminator loss: 0.179975, acc.: 95.31%] [Generator loss: 5.717053]\n",
      "10488 [Discriminator loss: 0.063394, acc.: 95.31%] [Generator loss: 7.144674]\n",
      "10489 [Discriminator loss: 0.131337, acc.: 95.31%] [Generator loss: 6.978778]\n",
      "10490 [Discriminator loss: 0.247126, acc.: 89.06%] [Generator loss: 5.844394]\n",
      "10491 [Discriminator loss: 0.152055, acc.: 96.88%] [Generator loss: 6.201762]\n",
      "10492 [Discriminator loss: 0.074218, acc.: 96.88%] [Generator loss: 7.076221]\n",
      "10493 [Discriminator loss: 0.073001, acc.: 95.31%] [Generator loss: 5.477523]\n",
      "10494 [Discriminator loss: 0.175243, acc.: 95.31%] [Generator loss: 6.229443]\n",
      "10495 [Discriminator loss: 0.095954, acc.: 96.88%] [Generator loss: 6.693676]\n",
      "10496 [Discriminator loss: 0.155354, acc.: 93.75%] [Generator loss: 5.647023]\n",
      "10497 [Discriminator loss: 0.141994, acc.: 92.19%] [Generator loss: 7.523475]\n",
      "10498 [Discriminator loss: 0.085639, acc.: 95.31%] [Generator loss: 7.306226]\n",
      "10499 [Discriminator loss: 0.094792, acc.: 95.31%] [Generator loss: 5.792200]\n",
      "10500 [Discriminator loss: 0.194029, acc.: 92.19%] [Generator loss: 5.690326]\n",
      "10501 [Discriminator loss: 0.035165, acc.: 98.44%] [Generator loss: 7.806254]\n",
      "10502 [Discriminator loss: 0.088422, acc.: 96.88%] [Generator loss: 7.014053]\n",
      "10503 [Discriminator loss: 0.034370, acc.: 100.00%] [Generator loss: 6.670349]\n",
      "10504 [Discriminator loss: 0.178126, acc.: 92.19%] [Generator loss: 4.908288]\n",
      "10505 [Discriminator loss: 0.293790, acc.: 92.19%] [Generator loss: 8.429436]\n",
      "10506 [Discriminator loss: 0.197660, acc.: 93.75%] [Generator loss: 7.791662]\n",
      "10507 [Discriminator loss: 0.091431, acc.: 95.31%] [Generator loss: 6.689445]\n",
      "10508 [Discriminator loss: 0.038010, acc.: 100.00%] [Generator loss: 6.324169]\n",
      "10509 [Discriminator loss: 0.113008, acc.: 95.31%] [Generator loss: 6.185430]\n",
      "10510 [Discriminator loss: 0.120134, acc.: 95.31%] [Generator loss: 6.740744]\n",
      "10511 [Discriminator loss: 0.053837, acc.: 98.44%] [Generator loss: 5.982272]\n",
      "10512 [Discriminator loss: 0.069108, acc.: 96.88%] [Generator loss: 7.702275]\n",
      "10513 [Discriminator loss: 0.233787, acc.: 89.06%] [Generator loss: 5.623936]\n",
      "10514 [Discriminator loss: 0.090992, acc.: 96.88%] [Generator loss: 7.073938]\n",
      "10515 [Discriminator loss: 0.088601, acc.: 98.44%] [Generator loss: 6.551190]\n",
      "10516 [Discriminator loss: 0.217182, acc.: 92.19%] [Generator loss: 6.135886]\n",
      "10517 [Discriminator loss: 0.091187, acc.: 96.88%] [Generator loss: 5.623124]\n",
      "10518 [Discriminator loss: 0.398459, acc.: 81.25%] [Generator loss: 9.185825]\n",
      "10519 [Discriminator loss: 0.236008, acc.: 89.06%] [Generator loss: 7.349118]\n",
      "10520 [Discriminator loss: 0.161223, acc.: 95.31%] [Generator loss: 6.685267]\n",
      "10521 [Discriminator loss: 0.179677, acc.: 95.31%] [Generator loss: 5.879908]\n",
      "10522 [Discriminator loss: 0.145122, acc.: 95.31%] [Generator loss: 6.596125]\n",
      "10523 [Discriminator loss: 0.075081, acc.: 96.88%] [Generator loss: 6.824881]\n",
      "10524 [Discriminator loss: 0.088725, acc.: 96.88%] [Generator loss: 7.609510]\n",
      "10525 [Discriminator loss: 0.185139, acc.: 93.75%] [Generator loss: 7.032865]\n",
      "10526 [Discriminator loss: 0.064946, acc.: 98.44%] [Generator loss: 6.333695]\n",
      "10527 [Discriminator loss: 0.044733, acc.: 98.44%] [Generator loss: 5.990029]\n",
      "10528 [Discriminator loss: 0.160852, acc.: 92.19%] [Generator loss: 6.877193]\n",
      "10529 [Discriminator loss: 0.056312, acc.: 98.44%] [Generator loss: 5.764113]\n",
      "10530 [Discriminator loss: 0.122209, acc.: 95.31%] [Generator loss: 5.111878]\n",
      "10531 [Discriminator loss: 0.355395, acc.: 85.94%] [Generator loss: 6.547270]\n",
      "10532 [Discriminator loss: 0.068640, acc.: 98.44%] [Generator loss: 7.364150]\n",
      "10533 [Discriminator loss: 0.102641, acc.: 95.31%] [Generator loss: 6.029243]\n",
      "10534 [Discriminator loss: 0.147102, acc.: 93.75%] [Generator loss: 6.290990]\n",
      "10535 [Discriminator loss: 0.072916, acc.: 98.44%] [Generator loss: 7.543636]\n",
      "10536 [Discriminator loss: 0.056200, acc.: 98.44%] [Generator loss: 7.103708]\n",
      "10537 [Discriminator loss: 0.103363, acc.: 95.31%] [Generator loss: 6.461214]\n",
      "10538 [Discriminator loss: 0.113003, acc.: 96.88%] [Generator loss: 7.800586]\n",
      "10539 [Discriminator loss: 0.064028, acc.: 98.44%] [Generator loss: 7.222499]\n",
      "10540 [Discriminator loss: 0.153457, acc.: 95.31%] [Generator loss: 5.617054]\n",
      "10541 [Discriminator loss: 0.051546, acc.: 98.44%] [Generator loss: 6.014139]\n",
      "10542 [Discriminator loss: 0.078334, acc.: 95.31%] [Generator loss: 6.322581]\n",
      "10543 [Discriminator loss: 0.036780, acc.: 98.44%] [Generator loss: 7.870701]\n",
      "10544 [Discriminator loss: 0.179415, acc.: 95.31%] [Generator loss: 6.026507]\n",
      "10545 [Discriminator loss: 0.112083, acc.: 96.88%] [Generator loss: 6.512755]\n",
      "10546 [Discriminator loss: 0.065852, acc.: 96.88%] [Generator loss: 5.557909]\n",
      "10547 [Discriminator loss: 0.145266, acc.: 95.31%] [Generator loss: 5.061151]\n",
      "10548 [Discriminator loss: 0.180397, acc.: 95.31%] [Generator loss: 7.005440]\n",
      "10549 [Discriminator loss: 0.111331, acc.: 92.19%] [Generator loss: 8.447414]\n",
      "10550 [Discriminator loss: 0.191271, acc.: 92.19%] [Generator loss: 5.753513]\n",
      "10551 [Discriminator loss: 0.144888, acc.: 96.88%] [Generator loss: 7.305072]\n",
      "10552 [Discriminator loss: 0.038711, acc.: 98.44%] [Generator loss: 9.206596]\n",
      "10553 [Discriminator loss: 0.209675, acc.: 96.88%] [Generator loss: 6.099053]\n",
      "10554 [Discriminator loss: 0.092198, acc.: 98.44%] [Generator loss: 6.807493]\n",
      "10555 [Discriminator loss: 0.120998, acc.: 93.75%] [Generator loss: 6.808226]\n",
      "10556 [Discriminator loss: 0.032078, acc.: 100.00%] [Generator loss: 6.099021]\n",
      "10557 [Discriminator loss: 0.160052, acc.: 93.75%] [Generator loss: 6.085097]\n",
      "10558 [Discriminator loss: 0.077225, acc.: 96.88%] [Generator loss: 6.792465]\n",
      "10559 [Discriminator loss: 0.070788, acc.: 96.88%] [Generator loss: 6.778361]\n",
      "10560 [Discriminator loss: 0.087790, acc.: 96.88%] [Generator loss: 5.492257]\n",
      "10561 [Discriminator loss: 0.121361, acc.: 93.75%] [Generator loss: 5.965223]\n",
      "10562 [Discriminator loss: 0.156535, acc.: 92.19%] [Generator loss: 6.464498]\n",
      "10563 [Discriminator loss: 0.070296, acc.: 98.44%] [Generator loss: 7.802849]\n",
      "10564 [Discriminator loss: 0.124932, acc.: 95.31%] [Generator loss: 7.322360]\n",
      "10565 [Discriminator loss: 0.032782, acc.: 100.00%] [Generator loss: 7.827570]\n",
      "10566 [Discriminator loss: 0.038177, acc.: 100.00%] [Generator loss: 6.561274]\n",
      "10567 [Discriminator loss: 0.147062, acc.: 98.44%] [Generator loss: 5.583851]\n",
      "10568 [Discriminator loss: 0.157255, acc.: 92.19%] [Generator loss: 6.453059]\n",
      "10569 [Discriminator loss: 0.111919, acc.: 95.31%] [Generator loss: 7.349797]\n",
      "10570 [Discriminator loss: 0.217460, acc.: 92.19%] [Generator loss: 5.730157]\n",
      "10571 [Discriminator loss: 0.141533, acc.: 93.75%] [Generator loss: 8.228596]\n",
      "10572 [Discriminator loss: 0.183657, acc.: 92.19%] [Generator loss: 5.253554]\n",
      "10573 [Discriminator loss: 0.114087, acc.: 95.31%] [Generator loss: 7.169331]\n",
      "10574 [Discriminator loss: 0.175101, acc.: 92.19%] [Generator loss: 5.981433]\n",
      "10575 [Discriminator loss: 0.126951, acc.: 93.75%] [Generator loss: 5.398175]\n",
      "10576 [Discriminator loss: 0.139431, acc.: 93.75%] [Generator loss: 5.633814]\n",
      "10577 [Discriminator loss: 0.153397, acc.: 92.19%] [Generator loss: 6.638499]\n",
      "10578 [Discriminator loss: 0.187105, acc.: 89.06%] [Generator loss: 8.765471]\n",
      "10579 [Discriminator loss: 0.348987, acc.: 84.38%] [Generator loss: 5.875937]\n",
      "10580 [Discriminator loss: 0.234373, acc.: 93.75%] [Generator loss: 7.114926]\n",
      "10581 [Discriminator loss: 0.108093, acc.: 98.44%] [Generator loss: 7.692026]\n",
      "10582 [Discriminator loss: 0.081861, acc.: 96.88%] [Generator loss: 6.743387]\n",
      "10583 [Discriminator loss: 0.045204, acc.: 98.44%] [Generator loss: 5.093837]\n",
      "10584 [Discriminator loss: 0.105390, acc.: 95.31%] [Generator loss: 7.527018]\n",
      "10585 [Discriminator loss: 0.124616, acc.: 92.19%] [Generator loss: 6.890100]\n",
      "10586 [Discriminator loss: 0.174647, acc.: 95.31%] [Generator loss: 3.666561]\n",
      "10587 [Discriminator loss: 0.332768, acc.: 85.94%] [Generator loss: 7.269933]\n",
      "10588 [Discriminator loss: 0.034407, acc.: 98.44%] [Generator loss: 10.070337]\n",
      "10589 [Discriminator loss: 0.231699, acc.: 90.62%] [Generator loss: 6.184680]\n",
      "10590 [Discriminator loss: 0.097009, acc.: 96.88%] [Generator loss: 6.570958]\n",
      "10591 [Discriminator loss: 0.235673, acc.: 93.75%] [Generator loss: 6.957803]\n",
      "10592 [Discriminator loss: 0.053493, acc.: 98.44%] [Generator loss: 7.028453]\n",
      "10593 [Discriminator loss: 0.039245, acc.: 98.44%] [Generator loss: 6.664049]\n",
      "10594 [Discriminator loss: 0.172536, acc.: 93.75%] [Generator loss: 7.203987]\n",
      "10595 [Discriminator loss: 0.139775, acc.: 93.75%] [Generator loss: 6.380462]\n",
      "10596 [Discriminator loss: 0.047678, acc.: 100.00%] [Generator loss: 6.957841]\n",
      "10597 [Discriminator loss: 0.196631, acc.: 95.31%] [Generator loss: 6.319790]\n",
      "10598 [Discriminator loss: 0.166162, acc.: 93.75%] [Generator loss: 5.683212]\n",
      "10599 [Discriminator loss: 0.117450, acc.: 95.31%] [Generator loss: 6.014249]\n",
      "10600 [Discriminator loss: 0.060258, acc.: 96.88%] [Generator loss: 6.700901]\n",
      "10601 [Discriminator loss: 0.197065, acc.: 90.62%] [Generator loss: 6.661841]\n",
      "10602 [Discriminator loss: 0.076790, acc.: 96.88%] [Generator loss: 6.791608]\n",
      "10603 [Discriminator loss: 0.214182, acc.: 90.62%] [Generator loss: 5.666887]\n",
      "10604 [Discriminator loss: 0.181424, acc.: 89.06%] [Generator loss: 9.056965]\n",
      "10605 [Discriminator loss: 0.101309, acc.: 95.31%] [Generator loss: 8.673581]\n",
      "10606 [Discriminator loss: 0.280941, acc.: 84.38%] [Generator loss: 6.900018]\n",
      "10607 [Discriminator loss: 0.114336, acc.: 95.31%] [Generator loss: 6.089128]\n",
      "10608 [Discriminator loss: 0.143985, acc.: 92.19%] [Generator loss: 7.992078]\n",
      "10609 [Discriminator loss: 0.105669, acc.: 96.88%] [Generator loss: 6.362370]\n",
      "10610 [Discriminator loss: 0.174426, acc.: 90.62%] [Generator loss: 5.700986]\n",
      "10611 [Discriminator loss: 0.175626, acc.: 92.19%] [Generator loss: 7.368149]\n",
      "10612 [Discriminator loss: 0.151131, acc.: 96.88%] [Generator loss: 6.532507]\n",
      "10613 [Discriminator loss: 0.041519, acc.: 98.44%] [Generator loss: 8.392248]\n",
      "10614 [Discriminator loss: 0.128106, acc.: 93.75%] [Generator loss: 6.193242]\n",
      "10615 [Discriminator loss: 0.147018, acc.: 92.19%] [Generator loss: 6.797214]\n",
      "10616 [Discriminator loss: 0.049976, acc.: 96.88%] [Generator loss: 7.267086]\n",
      "10617 [Discriminator loss: 0.168668, acc.: 95.31%] [Generator loss: 6.011766]\n",
      "10618 [Discriminator loss: 0.078259, acc.: 95.31%] [Generator loss: 6.875525]\n",
      "10619 [Discriminator loss: 0.073997, acc.: 96.88%] [Generator loss: 6.892930]\n",
      "10620 [Discriminator loss: 0.058237, acc.: 98.44%] [Generator loss: 5.773649]\n",
      "10621 [Discriminator loss: 0.292968, acc.: 85.94%] [Generator loss: 7.794980]\n",
      "10622 [Discriminator loss: 0.154187, acc.: 95.31%] [Generator loss: 7.234592]\n",
      "10623 [Discriminator loss: 0.074238, acc.: 96.88%] [Generator loss: 7.606753]\n",
      "10624 [Discriminator loss: 0.206675, acc.: 90.62%] [Generator loss: 6.932941]\n",
      "10625 [Discriminator loss: 0.052214, acc.: 98.44%] [Generator loss: 8.488426]\n",
      "10626 [Discriminator loss: 0.055262, acc.: 98.44%] [Generator loss: 7.209873]\n",
      "10627 [Discriminator loss: 0.194097, acc.: 90.62%] [Generator loss: 8.202281]\n",
      "10628 [Discriminator loss: 0.121764, acc.: 93.75%] [Generator loss: 6.749133]\n",
      "10629 [Discriminator loss: 0.166647, acc.: 89.06%] [Generator loss: 6.072176]\n",
      "10630 [Discriminator loss: 0.027550, acc.: 100.00%] [Generator loss: 5.871884]\n",
      "10631 [Discriminator loss: 0.218629, acc.: 93.75%] [Generator loss: 7.123651]\n",
      "10632 [Discriminator loss: 0.165199, acc.: 93.75%] [Generator loss: 6.587723]\n",
      "10633 [Discriminator loss: 0.183774, acc.: 92.19%] [Generator loss: 6.067472]\n",
      "10634 [Discriminator loss: 0.118035, acc.: 95.31%] [Generator loss: 5.305112]\n",
      "10635 [Discriminator loss: 0.115218, acc.: 95.31%] [Generator loss: 5.853101]\n",
      "10636 [Discriminator loss: 0.047041, acc.: 96.88%] [Generator loss: 6.978614]\n",
      "10637 [Discriminator loss: 0.226022, acc.: 92.19%] [Generator loss: 6.352647]\n",
      "10638 [Discriminator loss: 0.086646, acc.: 95.31%] [Generator loss: 6.126471]\n",
      "10639 [Discriminator loss: 0.058111, acc.: 98.44%] [Generator loss: 7.375588]\n",
      "10640 [Discriminator loss: 0.129557, acc.: 93.75%] [Generator loss: 7.179145]\n",
      "10641 [Discriminator loss: 0.143736, acc.: 93.75%] [Generator loss: 5.465321]\n",
      "10642 [Discriminator loss: 0.083308, acc.: 96.88%] [Generator loss: 6.523328]\n",
      "10643 [Discriminator loss: 0.018006, acc.: 100.00%] [Generator loss: 7.626011]\n",
      "10644 [Discriminator loss: 0.193774, acc.: 92.19%] [Generator loss: 7.992114]\n",
      "10645 [Discriminator loss: 0.094985, acc.: 95.31%] [Generator loss: 5.706222]\n",
      "10646 [Discriminator loss: 0.042584, acc.: 100.00%] [Generator loss: 6.314765]\n",
      "10647 [Discriminator loss: 0.125083, acc.: 90.62%] [Generator loss: 6.585944]\n",
      "10648 [Discriminator loss: 0.168050, acc.: 90.62%] [Generator loss: 5.564610]\n",
      "10649 [Discriminator loss: 0.105198, acc.: 95.31%] [Generator loss: 6.966618]\n",
      "10650 [Discriminator loss: 0.056554, acc.: 98.44%] [Generator loss: 7.948630]\n",
      "10651 [Discriminator loss: 0.090684, acc.: 98.44%] [Generator loss: 6.719649]\n",
      "10652 [Discriminator loss: 0.092919, acc.: 96.88%] [Generator loss: 6.128590]\n",
      "10653 [Discriminator loss: 0.035429, acc.: 100.00%] [Generator loss: 6.220447]\n",
      "10654 [Discriminator loss: 0.152335, acc.: 90.62%] [Generator loss: 7.066567]\n",
      "10655 [Discriminator loss: 0.210876, acc.: 93.75%] [Generator loss: 5.374338]\n",
      "10656 [Discriminator loss: 0.056658, acc.: 98.44%] [Generator loss: 7.704026]\n",
      "10657 [Discriminator loss: 0.218227, acc.: 95.31%] [Generator loss: 8.219074]\n",
      "10658 [Discriminator loss: 0.254469, acc.: 87.50%] [Generator loss: 4.765246]\n",
      "10659 [Discriminator loss: 0.179787, acc.: 95.31%] [Generator loss: 6.870019]\n",
      "10660 [Discriminator loss: 0.031915, acc.: 100.00%] [Generator loss: 8.084913]\n",
      "10661 [Discriminator loss: 0.216996, acc.: 92.19%] [Generator loss: 7.483171]\n",
      "10662 [Discriminator loss: 0.086050, acc.: 96.88%] [Generator loss: 6.541610]\n",
      "10663 [Discriminator loss: 0.059118, acc.: 96.88%] [Generator loss: 6.702234]\n",
      "10664 [Discriminator loss: 0.054440, acc.: 98.44%] [Generator loss: 6.543728]\n",
      "10665 [Discriminator loss: 0.140264, acc.: 95.31%] [Generator loss: 6.055024]\n",
      "10666 [Discriminator loss: 0.094345, acc.: 95.31%] [Generator loss: 8.276819]\n",
      "10667 [Discriminator loss: 0.340912, acc.: 89.06%] [Generator loss: 7.093322]\n",
      "10668 [Discriminator loss: 0.166786, acc.: 93.75%] [Generator loss: 5.861645]\n",
      "10669 [Discriminator loss: 0.077476, acc.: 96.88%] [Generator loss: 7.527281]\n",
      "10670 [Discriminator loss: 0.132858, acc.: 98.44%] [Generator loss: 6.043144]\n",
      "10671 [Discriminator loss: 0.104618, acc.: 93.75%] [Generator loss: 6.155340]\n",
      "10672 [Discriminator loss: 0.067121, acc.: 96.88%] [Generator loss: 6.192599]\n",
      "10673 [Discriminator loss: 0.140397, acc.: 93.75%] [Generator loss: 6.451495]\n",
      "10674 [Discriminator loss: 0.178272, acc.: 90.62%] [Generator loss: 8.188940]\n",
      "10675 [Discriminator loss: 0.150957, acc.: 93.75%] [Generator loss: 6.632411]\n",
      "10676 [Discriminator loss: 0.112889, acc.: 96.88%] [Generator loss: 6.265068]\n",
      "10677 [Discriminator loss: 0.117824, acc.: 96.88%] [Generator loss: 5.337232]\n",
      "10678 [Discriminator loss: 0.062112, acc.: 98.44%] [Generator loss: 5.742674]\n",
      "10679 [Discriminator loss: 0.156939, acc.: 92.19%] [Generator loss: 7.194722]\n",
      "10680 [Discriminator loss: 0.096983, acc.: 95.31%] [Generator loss: 6.986327]\n",
      "10681 [Discriminator loss: 0.098949, acc.: 98.44%] [Generator loss: 6.107834]\n",
      "10682 [Discriminator loss: 0.075994, acc.: 95.31%] [Generator loss: 6.061226]\n",
      "10683 [Discriminator loss: 0.166301, acc.: 93.75%] [Generator loss: 6.054401]\n",
      "10684 [Discriminator loss: 0.050194, acc.: 96.88%] [Generator loss: 4.553805]\n",
      "10685 [Discriminator loss: 0.156571, acc.: 90.62%] [Generator loss: 7.870852]\n",
      "10686 [Discriminator loss: 0.061054, acc.: 98.44%] [Generator loss: 8.325438]\n",
      "10687 [Discriminator loss: 0.094744, acc.: 96.88%] [Generator loss: 6.969138]\n",
      "10688 [Discriminator loss: 0.121184, acc.: 93.75%] [Generator loss: 5.978875]\n",
      "10689 [Discriminator loss: 0.032279, acc.: 100.00%] [Generator loss: 6.457423]\n",
      "10690 [Discriminator loss: 0.032412, acc.: 100.00%] [Generator loss: 6.044353]\n",
      "10691 [Discriminator loss: 0.198128, acc.: 92.19%] [Generator loss: 7.313062]\n",
      "10692 [Discriminator loss: 0.171776, acc.: 96.88%] [Generator loss: 5.956016]\n",
      "10693 [Discriminator loss: 0.303845, acc.: 87.50%] [Generator loss: 7.013651]\n",
      "10694 [Discriminator loss: 0.013938, acc.: 100.00%] [Generator loss: 7.042554]\n",
      "10695 [Discriminator loss: 0.095693, acc.: 96.88%] [Generator loss: 8.021823]\n",
      "10696 [Discriminator loss: 0.238119, acc.: 87.50%] [Generator loss: 6.946043]\n",
      "10697 [Discriminator loss: 0.053431, acc.: 98.44%] [Generator loss: 6.864561]\n",
      "10698 [Discriminator loss: 0.067591, acc.: 96.88%] [Generator loss: 7.664870]\n",
      "10699 [Discriminator loss: 0.304145, acc.: 90.62%] [Generator loss: 6.290257]\n",
      "10700 [Discriminator loss: 0.087404, acc.: 93.75%] [Generator loss: 5.888032]\n",
      "10701 [Discriminator loss: 0.107635, acc.: 93.75%] [Generator loss: 6.936274]\n",
      "10702 [Discriminator loss: 0.124088, acc.: 96.88%] [Generator loss: 7.279447]\n",
      "10703 [Discriminator loss: 0.128370, acc.: 98.44%] [Generator loss: 6.281794]\n",
      "10704 [Discriminator loss: 0.118565, acc.: 93.75%] [Generator loss: 6.416947]\n",
      "10705 [Discriminator loss: 0.078611, acc.: 96.88%] [Generator loss: 7.242080]\n",
      "10706 [Discriminator loss: 0.140729, acc.: 93.75%] [Generator loss: 5.425022]\n",
      "10707 [Discriminator loss: 0.095495, acc.: 96.88%] [Generator loss: 7.402039]\n",
      "10708 [Discriminator loss: 0.207492, acc.: 92.19%] [Generator loss: 5.916724]\n",
      "10709 [Discriminator loss: 0.034857, acc.: 100.00%] [Generator loss: 5.976963]\n",
      "10710 [Discriminator loss: 0.042809, acc.: 100.00%] [Generator loss: 5.150107]\n",
      "10711 [Discriminator loss: 0.170157, acc.: 92.19%] [Generator loss: 5.924374]\n",
      "10712 [Discriminator loss: 0.039999, acc.: 98.44%] [Generator loss: 6.605026]\n",
      "10713 [Discriminator loss: 0.116724, acc.: 95.31%] [Generator loss: 5.910197]\n",
      "10714 [Discriminator loss: 0.166036, acc.: 93.75%] [Generator loss: 7.378422]\n",
      "10715 [Discriminator loss: 0.105168, acc.: 93.75%] [Generator loss: 7.556411]\n",
      "10716 [Discriminator loss: 0.147645, acc.: 93.75%] [Generator loss: 6.872122]\n",
      "10717 [Discriminator loss: 0.135405, acc.: 96.88%] [Generator loss: 5.874815]\n",
      "10718 [Discriminator loss: 0.068379, acc.: 96.88%] [Generator loss: 7.207563]\n",
      "10719 [Discriminator loss: 0.090381, acc.: 96.88%] [Generator loss: 7.585044]\n",
      "10720 [Discriminator loss: 0.233352, acc.: 90.62%] [Generator loss: 5.894855]\n",
      "10721 [Discriminator loss: 0.148768, acc.: 92.19%] [Generator loss: 7.536945]\n",
      "10722 [Discriminator loss: 0.029098, acc.: 100.00%] [Generator loss: 6.293795]\n",
      "10723 [Discriminator loss: 0.365247, acc.: 89.06%] [Generator loss: 5.923539]\n",
      "10724 [Discriminator loss: 0.105736, acc.: 92.19%] [Generator loss: 7.622000]\n",
      "10725 [Discriminator loss: 0.086688, acc.: 96.88%] [Generator loss: 7.622516]\n",
      "10726 [Discriminator loss: 0.209630, acc.: 95.31%] [Generator loss: 6.661028]\n",
      "10727 [Discriminator loss: 0.184809, acc.: 95.31%] [Generator loss: 6.611000]\n",
      "10728 [Discriminator loss: 0.084155, acc.: 96.88%] [Generator loss: 6.413266]\n",
      "10729 [Discriminator loss: 0.208952, acc.: 93.75%] [Generator loss: 6.015226]\n",
      "10730 [Discriminator loss: 0.046263, acc.: 100.00%] [Generator loss: 7.151439]\n",
      "10731 [Discriminator loss: 0.093008, acc.: 95.31%] [Generator loss: 6.442511]\n",
      "10732 [Discriminator loss: 0.215941, acc.: 93.75%] [Generator loss: 4.667041]\n",
      "10733 [Discriminator loss: 0.276717, acc.: 90.62%] [Generator loss: 6.057889]\n",
      "10734 [Discriminator loss: 0.023795, acc.: 100.00%] [Generator loss: 7.154343]\n",
      "10735 [Discriminator loss: 0.385345, acc.: 79.69%] [Generator loss: 6.641280]\n",
      "10736 [Discriminator loss: 0.048784, acc.: 100.00%] [Generator loss: 7.534255]\n",
      "10737 [Discriminator loss: 0.028167, acc.: 100.00%] [Generator loss: 7.168703]\n",
      "10738 [Discriminator loss: 0.111654, acc.: 96.88%] [Generator loss: 6.748356]\n",
      "10739 [Discriminator loss: 0.130480, acc.: 95.31%] [Generator loss: 6.941804]\n",
      "10740 [Discriminator loss: 0.079399, acc.: 96.88%] [Generator loss: 7.141126]\n",
      "10741 [Discriminator loss: 0.100023, acc.: 96.88%] [Generator loss: 8.412342]\n",
      "10742 [Discriminator loss: 0.071317, acc.: 98.44%] [Generator loss: 6.563467]\n",
      "10743 [Discriminator loss: 0.292638, acc.: 87.50%] [Generator loss: 5.240869]\n",
      "10744 [Discriminator loss: 0.192606, acc.: 90.62%] [Generator loss: 5.743385]\n",
      "10745 [Discriminator loss: 0.054471, acc.: 98.44%] [Generator loss: 6.500069]\n",
      "10746 [Discriminator loss: 0.104228, acc.: 96.88%] [Generator loss: 6.231675]\n",
      "10747 [Discriminator loss: 0.109383, acc.: 95.31%] [Generator loss: 6.456438]\n",
      "10748 [Discriminator loss: 0.039597, acc.: 98.44%] [Generator loss: 8.016464]\n",
      "10749 [Discriminator loss: 0.136217, acc.: 95.31%] [Generator loss: 5.592358]\n",
      "10750 [Discriminator loss: 0.065558, acc.: 96.88%] [Generator loss: 5.869051]\n",
      "10751 [Discriminator loss: 0.153865, acc.: 92.19%] [Generator loss: 5.987899]\n",
      "10752 [Discriminator loss: 0.112396, acc.: 96.88%] [Generator loss: 7.548607]\n",
      "10753 [Discriminator loss: 0.081254, acc.: 96.88%] [Generator loss: 6.488223]\n",
      "10754 [Discriminator loss: 0.093478, acc.: 95.31%] [Generator loss: 8.067077]\n",
      "10755 [Discriminator loss: 0.099209, acc.: 95.31%] [Generator loss: 6.778862]\n",
      "10756 [Discriminator loss: 0.291224, acc.: 89.06%] [Generator loss: 6.757876]\n",
      "10757 [Discriminator loss: 0.180957, acc.: 93.75%] [Generator loss: 6.615564]\n",
      "10758 [Discriminator loss: 0.185781, acc.: 93.75%] [Generator loss: 8.097913]\n",
      "10759 [Discriminator loss: 0.203491, acc.: 92.19%] [Generator loss: 8.092110]\n",
      "10760 [Discriminator loss: 0.044893, acc.: 100.00%] [Generator loss: 8.378210]\n",
      "10761 [Discriminator loss: 0.036051, acc.: 100.00%] [Generator loss: 6.908757]\n",
      "10762 [Discriminator loss: 0.103506, acc.: 95.31%] [Generator loss: 6.340011]\n",
      "10763 [Discriminator loss: 0.054395, acc.: 98.44%] [Generator loss: 6.564352]\n",
      "10764 [Discriminator loss: 0.045151, acc.: 98.44%] [Generator loss: 6.791334]\n",
      "10765 [Discriminator loss: 0.097238, acc.: 96.88%] [Generator loss: 6.332937]\n",
      "10766 [Discriminator loss: 0.097812, acc.: 95.31%] [Generator loss: 6.439035]\n",
      "10767 [Discriminator loss: 0.063427, acc.: 96.88%] [Generator loss: 7.024795]\n",
      "10768 [Discriminator loss: 0.212547, acc.: 90.62%] [Generator loss: 5.601316]\n",
      "10769 [Discriminator loss: 0.343776, acc.: 89.06%] [Generator loss: 7.171398]\n",
      "10770 [Discriminator loss: 0.041591, acc.: 98.44%] [Generator loss: 8.560823]\n",
      "10771 [Discriminator loss: 0.221464, acc.: 90.62%] [Generator loss: 5.654557]\n",
      "10772 [Discriminator loss: 0.177411, acc.: 92.19%] [Generator loss: 6.426219]\n",
      "10773 [Discriminator loss: 0.036362, acc.: 100.00%] [Generator loss: 6.508063]\n",
      "10774 [Discriminator loss: 0.062204, acc.: 96.88%] [Generator loss: 4.858225]\n",
      "10775 [Discriminator loss: 0.344673, acc.: 87.50%] [Generator loss: 9.123974]\n",
      "10776 [Discriminator loss: 0.219431, acc.: 90.62%] [Generator loss: 6.963480]\n",
      "10777 [Discriminator loss: 0.110459, acc.: 96.88%] [Generator loss: 6.902834]\n",
      "10778 [Discriminator loss: 0.065837, acc.: 96.88%] [Generator loss: 6.452456]\n",
      "10779 [Discriminator loss: 0.150281, acc.: 92.19%] [Generator loss: 6.285197]\n",
      "10780 [Discriminator loss: 0.294372, acc.: 87.50%] [Generator loss: 7.112509]\n",
      "10781 [Discriminator loss: 0.134104, acc.: 93.75%] [Generator loss: 6.866827]\n",
      "10782 [Discriminator loss: 0.067532, acc.: 96.88%] [Generator loss: 7.386990]\n",
      "10783 [Discriminator loss: 0.090879, acc.: 98.44%] [Generator loss: 7.239714]\n",
      "10784 [Discriminator loss: 0.031705, acc.: 100.00%] [Generator loss: 6.090474]\n",
      "10785 [Discriminator loss: 0.050546, acc.: 100.00%] [Generator loss: 6.920160]\n",
      "10786 [Discriminator loss: 0.039410, acc.: 98.44%] [Generator loss: 5.902950]\n",
      "10787 [Discriminator loss: 0.078575, acc.: 96.88%] [Generator loss: 6.963373]\n",
      "10788 [Discriminator loss: 0.164747, acc.: 96.88%] [Generator loss: 5.762164]\n",
      "10789 [Discriminator loss: 0.066152, acc.: 98.44%] [Generator loss: 6.881980]\n",
      "10790 [Discriminator loss: 0.075628, acc.: 96.88%] [Generator loss: 6.207139]\n",
      "10791 [Discriminator loss: 0.042102, acc.: 100.00%] [Generator loss: 7.846260]\n",
      "10792 [Discriminator loss: 0.218408, acc.: 89.06%] [Generator loss: 6.366545]\n",
      "10793 [Discriminator loss: 0.112233, acc.: 96.88%] [Generator loss: 6.795362]\n",
      "10794 [Discriminator loss: 0.062349, acc.: 98.44%] [Generator loss: 6.779598]\n",
      "10795 [Discriminator loss: 0.323590, acc.: 89.06%] [Generator loss: 6.454465]\n",
      "10796 [Discriminator loss: 0.068970, acc.: 98.44%] [Generator loss: 8.976364]\n",
      "10797 [Discriminator loss: 0.110078, acc.: 96.88%] [Generator loss: 6.294048]\n",
      "10798 [Discriminator loss: 0.058194, acc.: 100.00%] [Generator loss: 5.416770]\n",
      "10799 [Discriminator loss: 0.471675, acc.: 85.94%] [Generator loss: 9.169291]\n",
      "10800 [Discriminator loss: 0.629060, acc.: 75.00%] [Generator loss: 6.086302]\n",
      "10801 [Discriminator loss: 0.257945, acc.: 89.06%] [Generator loss: 8.067379]\n",
      "10802 [Discriminator loss: 0.042304, acc.: 98.44%] [Generator loss: 9.230419]\n",
      "10803 [Discriminator loss: 0.130292, acc.: 93.75%] [Generator loss: 7.862753]\n",
      "10804 [Discriminator loss: 0.166881, acc.: 93.75%] [Generator loss: 6.179102]\n",
      "10805 [Discriminator loss: 0.067209, acc.: 96.88%] [Generator loss: 6.505151]\n",
      "10806 [Discriminator loss: 0.174284, acc.: 95.31%] [Generator loss: 6.164313]\n",
      "10807 [Discriminator loss: 0.093243, acc.: 96.88%] [Generator loss: 6.847997]\n",
      "10808 [Discriminator loss: 0.132423, acc.: 96.88%] [Generator loss: 6.435642]\n",
      "10809 [Discriminator loss: 0.112553, acc.: 93.75%] [Generator loss: 6.107738]\n",
      "10810 [Discriminator loss: 0.120588, acc.: 98.44%] [Generator loss: 5.350282]\n",
      "10811 [Discriminator loss: 0.100663, acc.: 96.88%] [Generator loss: 5.346417]\n",
      "10812 [Discriminator loss: 0.311859, acc.: 85.94%] [Generator loss: 7.193546]\n",
      "10813 [Discriminator loss: 0.017681, acc.: 100.00%] [Generator loss: 8.312469]\n",
      "10814 [Discriminator loss: 0.513126, acc.: 84.38%] [Generator loss: 5.604361]\n",
      "10815 [Discriminator loss: 0.033013, acc.: 100.00%] [Generator loss: 6.013754]\n",
      "10816 [Discriminator loss: 0.113408, acc.: 95.31%] [Generator loss: 6.040050]\n",
      "10817 [Discriminator loss: 0.094790, acc.: 96.88%] [Generator loss: 5.924454]\n",
      "10818 [Discriminator loss: 0.233094, acc.: 87.50%] [Generator loss: 8.669882]\n",
      "10819 [Discriminator loss: 0.252545, acc.: 87.50%] [Generator loss: 7.014977]\n",
      "10820 [Discriminator loss: 0.125949, acc.: 96.88%] [Generator loss: 7.612242]\n",
      "10821 [Discriminator loss: 0.067361, acc.: 95.31%] [Generator loss: 8.149995]\n",
      "10822 [Discriminator loss: 0.090908, acc.: 96.88%] [Generator loss: 7.234672]\n",
      "10823 [Discriminator loss: 0.148631, acc.: 96.88%] [Generator loss: 5.937428]\n",
      "10824 [Discriminator loss: 0.277501, acc.: 89.06%] [Generator loss: 5.725975]\n",
      "10825 [Discriminator loss: 0.065231, acc.: 98.44%] [Generator loss: 6.502031]\n",
      "10826 [Discriminator loss: 0.140434, acc.: 93.75%] [Generator loss: 7.820056]\n",
      "10827 [Discriminator loss: 0.048251, acc.: 100.00%] [Generator loss: 6.645682]\n",
      "10828 [Discriminator loss: 0.219377, acc.: 90.62%] [Generator loss: 5.818771]\n",
      "10829 [Discriminator loss: 0.091385, acc.: 95.31%] [Generator loss: 6.271635]\n",
      "10830 [Discriminator loss: 0.071920, acc.: 96.88%] [Generator loss: 6.814465]\n",
      "10831 [Discriminator loss: 0.169758, acc.: 92.19%] [Generator loss: 5.771720]\n",
      "10832 [Discriminator loss: 0.226944, acc.: 87.50%] [Generator loss: 7.045245]\n",
      "10833 [Discriminator loss: 0.103658, acc.: 95.31%] [Generator loss: 8.008050]\n",
      "10834 [Discriminator loss: 0.196784, acc.: 87.50%] [Generator loss: 8.419312]\n",
      "10835 [Discriminator loss: 0.307156, acc.: 87.50%] [Generator loss: 5.767542]\n",
      "10836 [Discriminator loss: 0.074959, acc.: 95.31%] [Generator loss: 5.190585]\n",
      "10837 [Discriminator loss: 0.154866, acc.: 93.75%] [Generator loss: 5.419616]\n",
      "10838 [Discriminator loss: 0.020352, acc.: 100.00%] [Generator loss: 7.333489]\n",
      "10839 [Discriminator loss: 0.154041, acc.: 93.75%] [Generator loss: 5.508951]\n",
      "10840 [Discriminator loss: 0.181217, acc.: 93.75%] [Generator loss: 6.536182]\n",
      "10841 [Discriminator loss: 0.082511, acc.: 98.44%] [Generator loss: 6.278686]\n",
      "10842 [Discriminator loss: 0.155989, acc.: 95.31%] [Generator loss: 5.438160]\n",
      "10843 [Discriminator loss: 0.053957, acc.: 98.44%] [Generator loss: 5.528766]\n",
      "10844 [Discriminator loss: 0.302930, acc.: 85.94%] [Generator loss: 9.176046]\n",
      "10845 [Discriminator loss: 0.036385, acc.: 100.00%] [Generator loss: 9.750923]\n",
      "10846 [Discriminator loss: 0.320867, acc.: 85.94%] [Generator loss: 4.230797]\n",
      "10847 [Discriminator loss: 0.513400, acc.: 81.25%] [Generator loss: 7.790591]\n",
      "10848 [Discriminator loss: 0.074410, acc.: 95.31%] [Generator loss: 10.485708]\n",
      "10849 [Discriminator loss: 0.237166, acc.: 89.06%] [Generator loss: 6.229615]\n",
      "10850 [Discriminator loss: 0.128701, acc.: 92.19%] [Generator loss: 6.472980]\n",
      "10851 [Discriminator loss: 0.084186, acc.: 96.88%] [Generator loss: 6.295025]\n",
      "10852 [Discriminator loss: 0.026481, acc.: 100.00%] [Generator loss: 6.200172]\n",
      "10853 [Discriminator loss: 0.299879, acc.: 82.81%] [Generator loss: 5.859274]\n",
      "10854 [Discriminator loss: 0.037069, acc.: 100.00%] [Generator loss: 8.569059]\n",
      "10855 [Discriminator loss: 0.099889, acc.: 96.88%] [Generator loss: 6.816262]\n",
      "10856 [Discriminator loss: 0.142935, acc.: 96.88%] [Generator loss: 6.551137]\n",
      "10857 [Discriminator loss: 0.051209, acc.: 96.88%] [Generator loss: 7.248205]\n",
      "10858 [Discriminator loss: 0.066640, acc.: 98.44%] [Generator loss: 6.015437]\n",
      "10859 [Discriminator loss: 0.278348, acc.: 90.62%] [Generator loss: 5.214041]\n",
      "10860 [Discriminator loss: 0.059855, acc.: 98.44%] [Generator loss: 7.713493]\n",
      "10861 [Discriminator loss: 0.069386, acc.: 98.44%] [Generator loss: 5.917578]\n",
      "10862 [Discriminator loss: 0.183287, acc.: 92.19%] [Generator loss: 6.942841]\n",
      "10863 [Discriminator loss: 0.149469, acc.: 93.75%] [Generator loss: 5.855385]\n",
      "10864 [Discriminator loss: 0.344474, acc.: 85.94%] [Generator loss: 5.410987]\n",
      "10865 [Discriminator loss: 0.095236, acc.: 93.75%] [Generator loss: 6.424845]\n",
      "10866 [Discriminator loss: 0.085398, acc.: 96.88%] [Generator loss: 7.695788]\n",
      "10867 [Discriminator loss: 0.105533, acc.: 95.31%] [Generator loss: 4.740537]\n",
      "10868 [Discriminator loss: 0.069537, acc.: 98.44%] [Generator loss: 5.478004]\n",
      "10869 [Discriminator loss: 0.068869, acc.: 96.88%] [Generator loss: 6.821066]\n",
      "10870 [Discriminator loss: 0.198596, acc.: 96.88%] [Generator loss: 5.968464]\n",
      "10871 [Discriminator loss: 0.088909, acc.: 96.88%] [Generator loss: 8.207841]\n",
      "10872 [Discriminator loss: 0.097606, acc.: 96.88%] [Generator loss: 6.187782]\n",
      "10873 [Discriminator loss: 0.227815, acc.: 89.06%] [Generator loss: 5.857516]\n",
      "10874 [Discriminator loss: 0.108111, acc.: 98.44%] [Generator loss: 8.879954]\n",
      "10875 [Discriminator loss: 0.055464, acc.: 98.44%] [Generator loss: 8.080867]\n",
      "10876 [Discriminator loss: 0.107204, acc.: 95.31%] [Generator loss: 6.466229]\n",
      "10877 [Discriminator loss: 0.106435, acc.: 95.31%] [Generator loss: 6.060981]\n",
      "10878 [Discriminator loss: 0.190875, acc.: 92.19%] [Generator loss: 6.832170]\n",
      "10879 [Discriminator loss: 0.159580, acc.: 92.19%] [Generator loss: 6.286448]\n",
      "10880 [Discriminator loss: 0.042043, acc.: 100.00%] [Generator loss: 5.952431]\n",
      "10881 [Discriminator loss: 0.156291, acc.: 95.31%] [Generator loss: 6.753542]\n",
      "10882 [Discriminator loss: 0.099621, acc.: 96.88%] [Generator loss: 6.736125]\n",
      "10883 [Discriminator loss: 0.062031, acc.: 98.44%] [Generator loss: 6.796835]\n",
      "10884 [Discriminator loss: 0.149693, acc.: 90.62%] [Generator loss: 7.756763]\n",
      "10885 [Discriminator loss: 0.096009, acc.: 95.31%] [Generator loss: 7.802325]\n",
      "10886 [Discriminator loss: 0.359731, acc.: 87.50%] [Generator loss: 5.524215]\n",
      "10887 [Discriminator loss: 0.043650, acc.: 100.00%] [Generator loss: 6.995901]\n",
      "10888 [Discriminator loss: 0.060101, acc.: 96.88%] [Generator loss: 7.155858]\n",
      "10889 [Discriminator loss: 0.027677, acc.: 98.44%] [Generator loss: 8.092945]\n",
      "10890 [Discriminator loss: 0.143945, acc.: 93.75%] [Generator loss: 6.611566]\n",
      "10891 [Discriminator loss: 0.156921, acc.: 93.75%] [Generator loss: 6.950316]\n",
      "10892 [Discriminator loss: 0.080889, acc.: 98.44%] [Generator loss: 7.128651]\n",
      "10893 [Discriminator loss: 0.085730, acc.: 95.31%] [Generator loss: 8.086874]\n",
      "10894 [Discriminator loss: 0.055641, acc.: 98.44%] [Generator loss: 6.788757]\n",
      "10895 [Discriminator loss: 0.098569, acc.: 95.31%] [Generator loss: 5.798398]\n",
      "10896 [Discriminator loss: 0.194314, acc.: 90.62%] [Generator loss: 5.984073]\n",
      "10897 [Discriminator loss: 0.045722, acc.: 98.44%] [Generator loss: 7.080058]\n",
      "10898 [Discriminator loss: 0.092525, acc.: 93.75%] [Generator loss: 6.639376]\n",
      "10899 [Discriminator loss: 0.072682, acc.: 96.88%] [Generator loss: 6.426700]\n",
      "10900 [Discriminator loss: 0.116502, acc.: 96.88%] [Generator loss: 6.703471]\n",
      "10901 [Discriminator loss: 0.115966, acc.: 93.75%] [Generator loss: 5.867794]\n",
      "10902 [Discriminator loss: 0.071351, acc.: 98.44%] [Generator loss: 5.952070]\n",
      "10903 [Discriminator loss: 0.241393, acc.: 92.19%] [Generator loss: 5.339518]\n",
      "10904 [Discriminator loss: 0.074600, acc.: 98.44%] [Generator loss: 6.119474]\n",
      "10905 [Discriminator loss: 0.128339, acc.: 95.31%] [Generator loss: 8.046191]\n",
      "10906 [Discriminator loss: 0.048990, acc.: 98.44%] [Generator loss: 7.514280]\n",
      "10907 [Discriminator loss: 0.156638, acc.: 90.62%] [Generator loss: 7.037482]\n",
      "10908 [Discriminator loss: 0.058988, acc.: 98.44%] [Generator loss: 6.761351]\n",
      "10909 [Discriminator loss: 0.111897, acc.: 96.88%] [Generator loss: 6.732716]\n",
      "10910 [Discriminator loss: 0.079003, acc.: 96.88%] [Generator loss: 6.501108]\n",
      "10911 [Discriminator loss: 0.200696, acc.: 89.06%] [Generator loss: 6.539180]\n",
      "10912 [Discriminator loss: 0.118054, acc.: 96.88%] [Generator loss: 8.520151]\n",
      "10913 [Discriminator loss: 0.087317, acc.: 96.88%] [Generator loss: 6.351060]\n",
      "10914 [Discriminator loss: 0.120915, acc.: 93.75%] [Generator loss: 7.164673]\n",
      "10915 [Discriminator loss: 0.149338, acc.: 96.88%] [Generator loss: 6.266470]\n",
      "10916 [Discriminator loss: 0.080788, acc.: 98.44%] [Generator loss: 6.161756]\n",
      "10917 [Discriminator loss: 0.089140, acc.: 96.88%] [Generator loss: 6.306511]\n",
      "10918 [Discriminator loss: 0.091744, acc.: 96.88%] [Generator loss: 6.636205]\n",
      "10919 [Discriminator loss: 0.114671, acc.: 96.88%] [Generator loss: 8.144283]\n",
      "10920 [Discriminator loss: 0.193819, acc.: 93.75%] [Generator loss: 6.413507]\n",
      "10921 [Discriminator loss: 0.081386, acc.: 96.88%] [Generator loss: 6.834903]\n",
      "10922 [Discriminator loss: 0.062118, acc.: 95.31%] [Generator loss: 6.566907]\n",
      "10923 [Discriminator loss: 0.166139, acc.: 92.19%] [Generator loss: 6.961190]\n",
      "10924 [Discriminator loss: 0.148915, acc.: 93.75%] [Generator loss: 6.829250]\n",
      "10925 [Discriminator loss: 0.116606, acc.: 95.31%] [Generator loss: 6.588612]\n",
      "10926 [Discriminator loss: 0.155548, acc.: 96.88%] [Generator loss: 5.065810]\n",
      "10927 [Discriminator loss: 0.094942, acc.: 95.31%] [Generator loss: 5.744735]\n",
      "10928 [Discriminator loss: 0.191644, acc.: 93.75%] [Generator loss: 6.365584]\n",
      "10929 [Discriminator loss: 0.196451, acc.: 89.06%] [Generator loss: 8.488111]\n",
      "10930 [Discriminator loss: 0.107249, acc.: 95.31%] [Generator loss: 7.285527]\n",
      "10931 [Discriminator loss: 0.046145, acc.: 98.44%] [Generator loss: 6.674739]\n",
      "10932 [Discriminator loss: 0.052631, acc.: 98.44%] [Generator loss: 6.073794]\n",
      "10933 [Discriminator loss: 0.163621, acc.: 93.75%] [Generator loss: 6.553946]\n",
      "10934 [Discriminator loss: 0.054116, acc.: 98.44%] [Generator loss: 7.966103]\n",
      "10935 [Discriminator loss: 0.087807, acc.: 98.44%] [Generator loss: 6.337412]\n",
      "10936 [Discriminator loss: 0.054370, acc.: 100.00%] [Generator loss: 6.620308]\n",
      "10937 [Discriminator loss: 0.153931, acc.: 90.62%] [Generator loss: 5.768749]\n",
      "10938 [Discriminator loss: 0.108104, acc.: 95.31%] [Generator loss: 5.759343]\n",
      "10939 [Discriminator loss: 0.272409, acc.: 89.06%] [Generator loss: 6.841688]\n",
      "10940 [Discriminator loss: 0.064919, acc.: 95.31%] [Generator loss: 7.730437]\n",
      "10941 [Discriminator loss: 0.148053, acc.: 93.75%] [Generator loss: 5.275418]\n",
      "10942 [Discriminator loss: 0.104790, acc.: 95.31%] [Generator loss: 6.846049]\n",
      "10943 [Discriminator loss: 0.043205, acc.: 98.44%] [Generator loss: 6.518764]\n",
      "10944 [Discriminator loss: 0.019368, acc.: 100.00%] [Generator loss: 6.768256]\n",
      "10945 [Discriminator loss: 0.083271, acc.: 98.44%] [Generator loss: 5.511754]\n",
      "10946 [Discriminator loss: 0.047246, acc.: 98.44%] [Generator loss: 6.894157]\n",
      "10947 [Discriminator loss: 0.098434, acc.: 96.88%] [Generator loss: 7.093478]\n",
      "10948 [Discriminator loss: 0.092682, acc.: 96.88%] [Generator loss: 6.820362]\n",
      "10949 [Discriminator loss: 0.217255, acc.: 89.06%] [Generator loss: 6.697369]\n",
      "10950 [Discriminator loss: 0.032002, acc.: 100.00%] [Generator loss: 7.694857]\n",
      "10951 [Discriminator loss: 0.052026, acc.: 98.44%] [Generator loss: 6.743312]\n",
      "10952 [Discriminator loss: 0.080824, acc.: 96.88%] [Generator loss: 7.611598]\n",
      "10953 [Discriminator loss: 0.047329, acc.: 100.00%] [Generator loss: 6.558019]\n",
      "10954 [Discriminator loss: 0.120920, acc.: 95.31%] [Generator loss: 6.594467]\n",
      "10955 [Discriminator loss: 0.055772, acc.: 98.44%] [Generator loss: 7.987440]\n",
      "10956 [Discriminator loss: 0.419012, acc.: 82.81%] [Generator loss: 8.002945]\n",
      "10957 [Discriminator loss: 0.029815, acc.: 100.00%] [Generator loss: 8.474908]\n",
      "10958 [Discriminator loss: 0.142280, acc.: 93.75%] [Generator loss: 7.439585]\n",
      "10959 [Discriminator loss: 0.093938, acc.: 95.31%] [Generator loss: 7.032462]\n",
      "10960 [Discriminator loss: 0.114846, acc.: 95.31%] [Generator loss: 7.110417]\n",
      "10961 [Discriminator loss: 0.072000, acc.: 96.88%] [Generator loss: 6.813559]\n",
      "10962 [Discriminator loss: 0.162874, acc.: 95.31%] [Generator loss: 6.092603]\n",
      "10963 [Discriminator loss: 0.077719, acc.: 98.44%] [Generator loss: 6.698935]\n",
      "10964 [Discriminator loss: 0.165074, acc.: 93.75%] [Generator loss: 6.567999]\n",
      "10965 [Discriminator loss: 0.210571, acc.: 93.75%] [Generator loss: 6.491947]\n",
      "10966 [Discriminator loss: 0.097114, acc.: 98.44%] [Generator loss: 6.716248]\n",
      "10967 [Discriminator loss: 0.058236, acc.: 96.88%] [Generator loss: 6.173119]\n",
      "10968 [Discriminator loss: 0.143267, acc.: 95.31%] [Generator loss: 7.238260]\n",
      "10969 [Discriminator loss: 0.048119, acc.: 96.88%] [Generator loss: 7.254210]\n",
      "10970 [Discriminator loss: 0.052469, acc.: 98.44%] [Generator loss: 7.082780]\n",
      "10971 [Discriminator loss: 0.192395, acc.: 93.75%] [Generator loss: 4.528019]\n",
      "10972 [Discriminator loss: 0.068571, acc.: 98.44%] [Generator loss: 6.283409]\n",
      "10973 [Discriminator loss: 0.128373, acc.: 96.88%] [Generator loss: 7.069893]\n",
      "10974 [Discriminator loss: 0.147646, acc.: 95.31%] [Generator loss: 7.131200]\n",
      "10975 [Discriminator loss: 0.083617, acc.: 96.88%] [Generator loss: 7.738536]\n",
      "10976 [Discriminator loss: 0.184253, acc.: 92.19%] [Generator loss: 7.371625]\n",
      "10977 [Discriminator loss: 0.221222, acc.: 89.06%] [Generator loss: 4.960460]\n",
      "10978 [Discriminator loss: 0.163011, acc.: 93.75%] [Generator loss: 6.224406]\n",
      "10979 [Discriminator loss: 0.107810, acc.: 96.88%] [Generator loss: 7.326379]\n",
      "10980 [Discriminator loss: 0.090260, acc.: 95.31%] [Generator loss: 5.961866]\n",
      "10981 [Discriminator loss: 0.039509, acc.: 98.44%] [Generator loss: 5.801317]\n",
      "10982 [Discriminator loss: 0.143062, acc.: 96.88%] [Generator loss: 7.394930]\n",
      "10983 [Discriminator loss: 0.057530, acc.: 96.88%] [Generator loss: 6.963381]\n",
      "10984 [Discriminator loss: 0.167699, acc.: 92.19%] [Generator loss: 5.914666]\n",
      "10985 [Discriminator loss: 0.088429, acc.: 96.88%] [Generator loss: 7.508102]\n",
      "10986 [Discriminator loss: 0.068151, acc.: 100.00%] [Generator loss: 5.011971]\n",
      "10987 [Discriminator loss: 0.057699, acc.: 96.88%] [Generator loss: 6.825659]\n",
      "10988 [Discriminator loss: 0.092887, acc.: 96.88%] [Generator loss: 6.286427]\n",
      "10989 [Discriminator loss: 0.285183, acc.: 87.50%] [Generator loss: 7.219961]\n",
      "10990 [Discriminator loss: 0.051903, acc.: 98.44%] [Generator loss: 6.903047]\n",
      "10991 [Discriminator loss: 0.102357, acc.: 93.75%] [Generator loss: 6.981227]\n",
      "10992 [Discriminator loss: 0.108664, acc.: 93.75%] [Generator loss: 5.754278]\n",
      "10993 [Discriminator loss: 0.060066, acc.: 98.44%] [Generator loss: 5.029059]\n",
      "10994 [Discriminator loss: 0.111215, acc.: 95.31%] [Generator loss: 6.843057]\n",
      "10995 [Discriminator loss: 0.185856, acc.: 96.88%] [Generator loss: 7.201392]\n",
      "10996 [Discriminator loss: 0.065595, acc.: 98.44%] [Generator loss: 7.668889]\n",
      "10997 [Discriminator loss: 0.118627, acc.: 93.75%] [Generator loss: 6.770964]\n",
      "10998 [Discriminator loss: 0.031445, acc.: 100.00%] [Generator loss: 5.644185]\n",
      "10999 [Discriminator loss: 0.065721, acc.: 98.44%] [Generator loss: 5.962790]\n",
      "11000 [Discriminator loss: 0.137130, acc.: 93.75%] [Generator loss: 6.668901]\n",
      "11001 [Discriminator loss: 0.126368, acc.: 93.75%] [Generator loss: 7.944829]\n",
      "11002 [Discriminator loss: 0.285177, acc.: 89.06%] [Generator loss: 6.228971]\n",
      "11003 [Discriminator loss: 0.105402, acc.: 98.44%] [Generator loss: 6.187155]\n",
      "11004 [Discriminator loss: 0.024535, acc.: 100.00%] [Generator loss: 5.618263]\n",
      "11005 [Discriminator loss: 0.114634, acc.: 95.31%] [Generator loss: 6.880531]\n",
      "11006 [Discriminator loss: 0.192495, acc.: 92.19%] [Generator loss: 6.400117]\n",
      "11007 [Discriminator loss: 0.052632, acc.: 98.44%] [Generator loss: 6.558355]\n",
      "11008 [Discriminator loss: 0.128698, acc.: 95.31%] [Generator loss: 7.042723]\n",
      "11009 [Discriminator loss: 0.072803, acc.: 96.88%] [Generator loss: 6.575491]\n",
      "11010 [Discriminator loss: 0.066996, acc.: 96.88%] [Generator loss: 5.329919]\n",
      "11011 [Discriminator loss: 0.047390, acc.: 100.00%] [Generator loss: 6.957202]\n",
      "11012 [Discriminator loss: 0.107086, acc.: 95.31%] [Generator loss: 6.532165]\n",
      "11013 [Discriminator loss: 0.239499, acc.: 93.75%] [Generator loss: 6.283768]\n",
      "11014 [Discriminator loss: 0.108939, acc.: 93.75%] [Generator loss: 8.242309]\n",
      "11015 [Discriminator loss: 0.065736, acc.: 96.88%] [Generator loss: 7.747702]\n",
      "11016 [Discriminator loss: 0.118898, acc.: 95.31%] [Generator loss: 6.251267]\n",
      "11017 [Discriminator loss: 0.115770, acc.: 96.88%] [Generator loss: 6.525352]\n",
      "11018 [Discriminator loss: 0.168841, acc.: 93.75%] [Generator loss: 5.832725]\n",
      "11019 [Discriminator loss: 0.080434, acc.: 98.44%] [Generator loss: 7.310790]\n",
      "11020 [Discriminator loss: 0.019559, acc.: 100.00%] [Generator loss: 8.383841]\n",
      "11021 [Discriminator loss: 0.111665, acc.: 93.75%] [Generator loss: 7.638159]\n",
      "11022 [Discriminator loss: 0.049395, acc.: 96.88%] [Generator loss: 6.012679]\n",
      "11023 [Discriminator loss: 0.105896, acc.: 95.31%] [Generator loss: 7.099398]\n",
      "11024 [Discriminator loss: 0.036724, acc.: 100.00%] [Generator loss: 6.755786]\n",
      "11025 [Discriminator loss: 0.043450, acc.: 98.44%] [Generator loss: 7.015691]\n",
      "11026 [Discriminator loss: 0.132075, acc.: 95.31%] [Generator loss: 6.658096]\n",
      "11027 [Discriminator loss: 0.077719, acc.: 96.88%] [Generator loss: 7.644711]\n",
      "11028 [Discriminator loss: 0.433689, acc.: 84.38%] [Generator loss: 7.139052]\n",
      "11029 [Discriminator loss: 0.147864, acc.: 95.31%] [Generator loss: 5.863567]\n",
      "11030 [Discriminator loss: 0.135806, acc.: 95.31%] [Generator loss: 6.008433]\n",
      "11031 [Discriminator loss: 0.260141, acc.: 90.62%] [Generator loss: 6.699140]\n",
      "11032 [Discriminator loss: 0.072685, acc.: 98.44%] [Generator loss: 6.813199]\n",
      "11033 [Discriminator loss: 0.110803, acc.: 95.31%] [Generator loss: 5.986389]\n",
      "11034 [Discriminator loss: 0.227942, acc.: 90.62%] [Generator loss: 7.075711]\n",
      "11035 [Discriminator loss: 0.263894, acc.: 92.19%] [Generator loss: 6.152038]\n",
      "11036 [Discriminator loss: 0.231603, acc.: 89.06%] [Generator loss: 6.430105]\n",
      "11037 [Discriminator loss: 0.082321, acc.: 96.88%] [Generator loss: 9.028380]\n",
      "11038 [Discriminator loss: 0.114707, acc.: 93.75%] [Generator loss: 5.148648]\n",
      "11039 [Discriminator loss: 0.319329, acc.: 87.50%] [Generator loss: 8.096458]\n",
      "11040 [Discriminator loss: 0.164863, acc.: 89.06%] [Generator loss: 7.826868]\n",
      "11041 [Discriminator loss: 0.133970, acc.: 96.88%] [Generator loss: 6.396679]\n",
      "11042 [Discriminator loss: 0.084928, acc.: 95.31%] [Generator loss: 6.332484]\n",
      "11043 [Discriminator loss: 0.091444, acc.: 96.88%] [Generator loss: 6.791107]\n",
      "11044 [Discriminator loss: 0.167075, acc.: 92.19%] [Generator loss: 6.515794]\n",
      "11045 [Discriminator loss: 0.028340, acc.: 100.00%] [Generator loss: 6.498152]\n",
      "11046 [Discriminator loss: 0.106806, acc.: 95.31%] [Generator loss: 5.117166]\n",
      "11047 [Discriminator loss: 0.080307, acc.: 96.88%] [Generator loss: 7.524300]\n",
      "11048 [Discriminator loss: 0.140689, acc.: 96.88%] [Generator loss: 7.255975]\n",
      "11049 [Discriminator loss: 0.091222, acc.: 95.31%] [Generator loss: 7.020334]\n",
      "11050 [Discriminator loss: 0.224797, acc.: 90.62%] [Generator loss: 6.347771]\n",
      "11051 [Discriminator loss: 0.118948, acc.: 96.88%] [Generator loss: 6.724490]\n",
      "11052 [Discriminator loss: 0.079133, acc.: 98.44%] [Generator loss: 6.382966]\n",
      "11053 [Discriminator loss: 0.103661, acc.: 95.31%] [Generator loss: 6.950385]\n",
      "11054 [Discriminator loss: 0.094771, acc.: 96.88%] [Generator loss: 6.650700]\n",
      "11055 [Discriminator loss: 0.069260, acc.: 98.44%] [Generator loss: 6.943190]\n",
      "11056 [Discriminator loss: 0.095978, acc.: 95.31%] [Generator loss: 6.863564]\n",
      "11057 [Discriminator loss: 0.041413, acc.: 98.44%] [Generator loss: 6.775930]\n",
      "11058 [Discriminator loss: 0.110424, acc.: 95.31%] [Generator loss: 6.772757]\n",
      "11059 [Discriminator loss: 0.041656, acc.: 100.00%] [Generator loss: 8.055535]\n",
      "11060 [Discriminator loss: 0.146046, acc.: 95.31%] [Generator loss: 6.295325]\n",
      "11061 [Discriminator loss: 0.108486, acc.: 96.88%] [Generator loss: 7.465617]\n",
      "11062 [Discriminator loss: 0.070139, acc.: 98.44%] [Generator loss: 8.354992]\n",
      "11063 [Discriminator loss: 0.084038, acc.: 95.31%] [Generator loss: 6.969396]\n",
      "11064 [Discriminator loss: 0.097180, acc.: 96.88%] [Generator loss: 5.984007]\n",
      "11065 [Discriminator loss: 0.115681, acc.: 96.88%] [Generator loss: 6.720105]\n",
      "11066 [Discriminator loss: 0.146598, acc.: 92.19%] [Generator loss: 5.238284]\n",
      "11067 [Discriminator loss: 0.047776, acc.: 98.44%] [Generator loss: 6.257489]\n",
      "11068 [Discriminator loss: 0.047705, acc.: 100.00%] [Generator loss: 6.778725]\n",
      "11069 [Discriminator loss: 0.130065, acc.: 95.31%] [Generator loss: 6.804169]\n",
      "11070 [Discriminator loss: 0.088448, acc.: 95.31%] [Generator loss: 6.588146]\n",
      "11071 [Discriminator loss: 0.265324, acc.: 89.06%] [Generator loss: 7.979181]\n",
      "11072 [Discriminator loss: 0.054603, acc.: 100.00%] [Generator loss: 6.945064]\n",
      "11073 [Discriminator loss: 0.083318, acc.: 96.88%] [Generator loss: 7.525973]\n",
      "11074 [Discriminator loss: 0.140651, acc.: 95.31%] [Generator loss: 5.552583]\n",
      "11075 [Discriminator loss: 0.147063, acc.: 95.31%] [Generator loss: 6.320982]\n",
      "11076 [Discriminator loss: 0.085613, acc.: 96.88%] [Generator loss: 7.269006]\n",
      "11077 [Discriminator loss: 0.120446, acc.: 95.31%] [Generator loss: 6.553459]\n",
      "11078 [Discriminator loss: 0.113395, acc.: 93.75%] [Generator loss: 6.231425]\n",
      "11079 [Discriminator loss: 0.056854, acc.: 96.88%] [Generator loss: 6.395713]\n",
      "11080 [Discriminator loss: 0.081072, acc.: 95.31%] [Generator loss: 6.653424]\n",
      "11081 [Discriminator loss: 0.228016, acc.: 84.38%] [Generator loss: 8.124968]\n",
      "11082 [Discriminator loss: 0.041693, acc.: 100.00%] [Generator loss: 8.636608]\n",
      "11083 [Discriminator loss: 0.236908, acc.: 89.06%] [Generator loss: 5.898493]\n",
      "11084 [Discriminator loss: 0.070385, acc.: 96.88%] [Generator loss: 5.976294]\n",
      "11085 [Discriminator loss: 0.025205, acc.: 100.00%] [Generator loss: 6.328085]\n",
      "11086 [Discriminator loss: 0.281901, acc.: 89.06%] [Generator loss: 5.669088]\n",
      "11087 [Discriminator loss: 0.069709, acc.: 98.44%] [Generator loss: 7.600806]\n",
      "11088 [Discriminator loss: 0.120156, acc.: 95.31%] [Generator loss: 6.664265]\n",
      "11089 [Discriminator loss: 0.091626, acc.: 96.88%] [Generator loss: 7.441365]\n",
      "11090 [Discriminator loss: 0.042928, acc.: 98.44%] [Generator loss: 7.567988]\n",
      "11091 [Discriminator loss: 0.110055, acc.: 96.88%] [Generator loss: 6.083408]\n",
      "11092 [Discriminator loss: 0.213613, acc.: 93.75%] [Generator loss: 5.664791]\n",
      "11093 [Discriminator loss: 0.088238, acc.: 96.88%] [Generator loss: 5.999474]\n",
      "11094 [Discriminator loss: 0.048214, acc.: 98.44%] [Generator loss: 7.372847]\n",
      "11095 [Discriminator loss: 0.082881, acc.: 96.88%] [Generator loss: 6.712316]\n",
      "11096 [Discriminator loss: 0.075044, acc.: 98.44%] [Generator loss: 6.059746]\n",
      "11097 [Discriminator loss: 0.109875, acc.: 96.88%] [Generator loss: 6.043472]\n",
      "11098 [Discriminator loss: 0.171919, acc.: 92.19%] [Generator loss: 5.555453]\n",
      "11099 [Discriminator loss: 0.128936, acc.: 95.31%] [Generator loss: 7.306773]\n",
      "11100 [Discriminator loss: 0.030096, acc.: 100.00%] [Generator loss: 6.433946]\n",
      "11101 [Discriminator loss: 0.250382, acc.: 92.19%] [Generator loss: 6.900684]\n",
      "11102 [Discriminator loss: 0.066186, acc.: 96.88%] [Generator loss: 6.700932]\n",
      "11103 [Discriminator loss: 0.043231, acc.: 98.44%] [Generator loss: 4.837025]\n",
      "11104 [Discriminator loss: 0.074612, acc.: 96.88%] [Generator loss: 5.650907]\n",
      "11105 [Discriminator loss: 0.060117, acc.: 96.88%] [Generator loss: 7.369562]\n",
      "11106 [Discriminator loss: 0.072018, acc.: 96.88%] [Generator loss: 6.799837]\n",
      "11107 [Discriminator loss: 0.077026, acc.: 96.88%] [Generator loss: 5.788267]\n",
      "11108 [Discriminator loss: 0.197567, acc.: 93.75%] [Generator loss: 6.190749]\n",
      "11109 [Discriminator loss: 0.138328, acc.: 96.88%] [Generator loss: 7.262349]\n",
      "11110 [Discriminator loss: 0.086208, acc.: 96.88%] [Generator loss: 6.562247]\n",
      "11111 [Discriminator loss: 0.050264, acc.: 100.00%] [Generator loss: 7.010006]\n",
      "11112 [Discriminator loss: 0.173353, acc.: 92.19%] [Generator loss: 7.243420]\n",
      "11113 [Discriminator loss: 0.067917, acc.: 96.88%] [Generator loss: 7.619571]\n",
      "11114 [Discriminator loss: 0.058798, acc.: 96.88%] [Generator loss: 5.615817]\n",
      "11115 [Discriminator loss: 0.062349, acc.: 98.44%] [Generator loss: 5.706696]\n",
      "11116 [Discriminator loss: 0.081965, acc.: 98.44%] [Generator loss: 6.761235]\n",
      "11117 [Discriminator loss: 0.095479, acc.: 93.75%] [Generator loss: 6.600934]\n",
      "11118 [Discriminator loss: 0.194744, acc.: 87.50%] [Generator loss: 5.843730]\n",
      "11119 [Discriminator loss: 0.076806, acc.: 98.44%] [Generator loss: 7.760788]\n",
      "11120 [Discriminator loss: 0.096187, acc.: 96.88%] [Generator loss: 7.252297]\n",
      "11121 [Discriminator loss: 0.061635, acc.: 98.44%] [Generator loss: 5.947692]\n",
      "11122 [Discriminator loss: 0.170921, acc.: 92.19%] [Generator loss: 8.599274]\n",
      "11123 [Discriminator loss: 0.257495, acc.: 90.62%] [Generator loss: 7.638274]\n",
      "11124 [Discriminator loss: 0.100780, acc.: 98.44%] [Generator loss: 6.649384]\n",
      "11125 [Discriminator loss: 0.167057, acc.: 93.75%] [Generator loss: 6.145657]\n",
      "11126 [Discriminator loss: 0.146476, acc.: 95.31%] [Generator loss: 6.973548]\n",
      "11127 [Discriminator loss: 0.143198, acc.: 93.75%] [Generator loss: 5.957799]\n",
      "11128 [Discriminator loss: 0.245748, acc.: 89.06%] [Generator loss: 7.683182]\n",
      "11129 [Discriminator loss: 0.079839, acc.: 98.44%] [Generator loss: 6.352754]\n",
      "11130 [Discriminator loss: 0.147118, acc.: 92.19%] [Generator loss: 5.331973]\n",
      "11131 [Discriminator loss: 0.097029, acc.: 95.31%] [Generator loss: 7.702130]\n",
      "11132 [Discriminator loss: 0.102729, acc.: 95.31%] [Generator loss: 7.398720]\n",
      "11133 [Discriminator loss: 0.046681, acc.: 98.44%] [Generator loss: 6.684883]\n",
      "11134 [Discriminator loss: 0.124717, acc.: 92.19%] [Generator loss: 7.974721]\n",
      "11135 [Discriminator loss: 0.033226, acc.: 100.00%] [Generator loss: 6.555793]\n",
      "11136 [Discriminator loss: 0.076253, acc.: 98.44%] [Generator loss: 5.820648]\n",
      "11137 [Discriminator loss: 0.126526, acc.: 95.31%] [Generator loss: 5.317742]\n",
      "11138 [Discriminator loss: 0.087412, acc.: 96.88%] [Generator loss: 7.728054]\n",
      "11139 [Discriminator loss: 0.110993, acc.: 95.31%] [Generator loss: 7.945179]\n",
      "11140 [Discriminator loss: 0.082330, acc.: 95.31%] [Generator loss: 7.096430]\n",
      "11141 [Discriminator loss: 0.054086, acc.: 96.88%] [Generator loss: 5.348859]\n",
      "11142 [Discriminator loss: 0.102775, acc.: 96.88%] [Generator loss: 5.325130]\n",
      "11143 [Discriminator loss: 0.088112, acc.: 96.88%] [Generator loss: 7.014926]\n",
      "11144 [Discriminator loss: 0.108885, acc.: 93.75%] [Generator loss: 6.637810]\n",
      "11145 [Discriminator loss: 0.159818, acc.: 92.19%] [Generator loss: 5.674707]\n",
      "11146 [Discriminator loss: 0.064631, acc.: 96.88%] [Generator loss: 6.327403]\n",
      "11147 [Discriminator loss: 0.118393, acc.: 95.31%] [Generator loss: 7.145230]\n",
      "11148 [Discriminator loss: 0.048106, acc.: 96.88%] [Generator loss: 7.217697]\n",
      "11149 [Discriminator loss: 0.125438, acc.: 95.31%] [Generator loss: 7.159929]\n",
      "11150 [Discriminator loss: 0.077537, acc.: 98.44%] [Generator loss: 7.169892]\n",
      "11151 [Discriminator loss: 0.030760, acc.: 100.00%] [Generator loss: 7.102604]\n",
      "11152 [Discriminator loss: 0.079998, acc.: 98.44%] [Generator loss: 6.839661]\n",
      "11153 [Discriminator loss: 0.070620, acc.: 96.88%] [Generator loss: 5.844613]\n",
      "11154 [Discriminator loss: 0.122415, acc.: 95.31%] [Generator loss: 6.917764]\n",
      "11155 [Discriminator loss: 0.498877, acc.: 78.12%] [Generator loss: 6.174154]\n",
      "11156 [Discriminator loss: 0.146719, acc.: 93.75%] [Generator loss: 8.263767]\n",
      "11157 [Discriminator loss: 0.046081, acc.: 98.44%] [Generator loss: 8.414257]\n",
      "11158 [Discriminator loss: 0.187119, acc.: 90.62%] [Generator loss: 6.295846]\n",
      "11159 [Discriminator loss: 0.163648, acc.: 93.75%] [Generator loss: 7.527534]\n",
      "11160 [Discriminator loss: 0.048506, acc.: 98.44%] [Generator loss: 6.389060]\n",
      "11161 [Discriminator loss: 0.195343, acc.: 95.31%] [Generator loss: 6.251793]\n",
      "11162 [Discriminator loss: 0.055558, acc.: 98.44%] [Generator loss: 8.328498]\n",
      "11163 [Discriminator loss: 0.092342, acc.: 98.44%] [Generator loss: 6.553851]\n",
      "11164 [Discriminator loss: 0.111363, acc.: 95.31%] [Generator loss: 7.117380]\n",
      "11165 [Discriminator loss: 0.091138, acc.: 96.88%] [Generator loss: 6.706726]\n",
      "11166 [Discriminator loss: 0.121970, acc.: 96.88%] [Generator loss: 6.571137]\n",
      "11167 [Discriminator loss: 0.128681, acc.: 93.75%] [Generator loss: 7.280755]\n",
      "11168 [Discriminator loss: 0.115862, acc.: 98.44%] [Generator loss: 7.510940]\n",
      "11169 [Discriminator loss: 0.209425, acc.: 92.19%] [Generator loss: 8.399597]\n",
      "11170 [Discriminator loss: 0.188850, acc.: 92.19%] [Generator loss: 7.343347]\n",
      "11171 [Discriminator loss: 0.115502, acc.: 95.31%] [Generator loss: 6.390761]\n",
      "11172 [Discriminator loss: 0.049764, acc.: 98.44%] [Generator loss: 5.611131]\n",
      "11173 [Discriminator loss: 0.044676, acc.: 98.44%] [Generator loss: 7.736106]\n",
      "11174 [Discriminator loss: 0.093757, acc.: 96.88%] [Generator loss: 4.998897]\n",
      "11175 [Discriminator loss: 0.057094, acc.: 98.44%] [Generator loss: 7.060805]\n",
      "11176 [Discriminator loss: 0.120170, acc.: 93.75%] [Generator loss: 6.414272]\n",
      "11177 [Discriminator loss: 0.180267, acc.: 92.19%] [Generator loss: 6.343006]\n",
      "11178 [Discriminator loss: 0.173074, acc.: 92.19%] [Generator loss: 7.581665]\n",
      "11179 [Discriminator loss: 0.131675, acc.: 96.88%] [Generator loss: 6.791267]\n",
      "11180 [Discriminator loss: 0.026004, acc.: 100.00%] [Generator loss: 6.687064]\n",
      "11181 [Discriminator loss: 0.102838, acc.: 93.75%] [Generator loss: 5.838953]\n",
      "11182 [Discriminator loss: 0.128011, acc.: 93.75%] [Generator loss: 7.148057]\n",
      "11183 [Discriminator loss: 0.049094, acc.: 98.44%] [Generator loss: 7.751039]\n",
      "11184 [Discriminator loss: 0.047369, acc.: 100.00%] [Generator loss: 7.238698]\n",
      "11185 [Discriminator loss: 0.117841, acc.: 93.75%] [Generator loss: 7.375195]\n",
      "11186 [Discriminator loss: 0.229458, acc.: 92.19%] [Generator loss: 5.050193]\n",
      "11187 [Discriminator loss: 0.083622, acc.: 96.88%] [Generator loss: 6.116628]\n",
      "11188 [Discriminator loss: 0.103826, acc.: 95.31%] [Generator loss: 8.880117]\n",
      "11189 [Discriminator loss: 0.169654, acc.: 93.75%] [Generator loss: 6.617429]\n",
      "11190 [Discriminator loss: 0.061569, acc.: 96.88%] [Generator loss: 5.385213]\n",
      "11191 [Discriminator loss: 0.100354, acc.: 93.75%] [Generator loss: 6.587967]\n",
      "11192 [Discriminator loss: 0.240337, acc.: 89.06%] [Generator loss: 6.510758]\n",
      "11193 [Discriminator loss: 0.041368, acc.: 98.44%] [Generator loss: 6.893802]\n",
      "11194 [Discriminator loss: 0.180778, acc.: 95.31%] [Generator loss: 5.671635]\n",
      "11195 [Discriminator loss: 0.065015, acc.: 98.44%] [Generator loss: 6.396341]\n",
      "11196 [Discriminator loss: 0.041557, acc.: 98.44%] [Generator loss: 6.746835]\n",
      "11197 [Discriminator loss: 0.087368, acc.: 98.44%] [Generator loss: 6.990476]\n",
      "11198 [Discriminator loss: 0.076414, acc.: 98.44%] [Generator loss: 6.286962]\n",
      "11199 [Discriminator loss: 0.094374, acc.: 96.88%] [Generator loss: 6.739761]\n",
      "11200 [Discriminator loss: 0.073238, acc.: 96.88%] [Generator loss: 6.888715]\n",
      "11201 [Discriminator loss: 0.028637, acc.: 100.00%] [Generator loss: 8.496374]\n",
      "11202 [Discriminator loss: 0.117593, acc.: 95.31%] [Generator loss: 7.044303]\n",
      "11203 [Discriminator loss: 0.038454, acc.: 100.00%] [Generator loss: 6.395026]\n",
      "11204 [Discriminator loss: 0.141257, acc.: 92.19%] [Generator loss: 7.088820]\n",
      "11205 [Discriminator loss: 0.103839, acc.: 93.75%] [Generator loss: 6.926609]\n",
      "11206 [Discriminator loss: 0.068901, acc.: 96.88%] [Generator loss: 6.713593]\n",
      "11207 [Discriminator loss: 0.021959, acc.: 100.00%] [Generator loss: 7.546093]\n",
      "11208 [Discriminator loss: 0.046351, acc.: 98.44%] [Generator loss: 6.426956]\n",
      "11209 [Discriminator loss: 0.113240, acc.: 95.31%] [Generator loss: 7.717644]\n",
      "11210 [Discriminator loss: 0.040631, acc.: 98.44%] [Generator loss: 7.812670]\n",
      "11211 [Discriminator loss: 0.132672, acc.: 96.88%] [Generator loss: 6.112902]\n",
      "11212 [Discriminator loss: 0.081245, acc.: 95.31%] [Generator loss: 6.203534]\n",
      "11213 [Discriminator loss: 0.067021, acc.: 98.44%] [Generator loss: 7.090679]\n",
      "11214 [Discriminator loss: 0.061101, acc.: 96.88%] [Generator loss: 7.405763]\n",
      "11215 [Discriminator loss: 0.152122, acc.: 93.75%] [Generator loss: 5.233984]\n",
      "11216 [Discriminator loss: 0.198981, acc.: 93.75%] [Generator loss: 6.187708]\n",
      "11217 [Discriminator loss: 0.071701, acc.: 96.88%] [Generator loss: 7.228358]\n",
      "11218 [Discriminator loss: 0.093570, acc.: 96.88%] [Generator loss: 6.553716]\n",
      "11219 [Discriminator loss: 0.035879, acc.: 100.00%] [Generator loss: 7.024446]\n",
      "11220 [Discriminator loss: 0.146183, acc.: 95.31%] [Generator loss: 4.846234]\n",
      "11221 [Discriminator loss: 0.266206, acc.: 92.19%] [Generator loss: 7.078625]\n",
      "11222 [Discriminator loss: 0.040723, acc.: 98.44%] [Generator loss: 7.223425]\n",
      "11223 [Discriminator loss: 0.174668, acc.: 93.75%] [Generator loss: 6.928388]\n",
      "11224 [Discriminator loss: 0.083545, acc.: 96.88%] [Generator loss: 8.631064]\n",
      "11225 [Discriminator loss: 0.069146, acc.: 98.44%] [Generator loss: 8.177307]\n",
      "11226 [Discriminator loss: 0.298086, acc.: 87.50%] [Generator loss: 6.553758]\n",
      "11227 [Discriminator loss: 0.043926, acc.: 100.00%] [Generator loss: 6.253800]\n",
      "11228 [Discriminator loss: 0.038044, acc.: 100.00%] [Generator loss: 6.386968]\n",
      "11229 [Discriminator loss: 0.032089, acc.: 100.00%] [Generator loss: 6.413046]\n",
      "11230 [Discriminator loss: 0.153523, acc.: 92.19%] [Generator loss: 6.944886]\n",
      "11231 [Discriminator loss: 0.119001, acc.: 95.31%] [Generator loss: 7.280608]\n",
      "11232 [Discriminator loss: 0.128085, acc.: 96.88%] [Generator loss: 7.056994]\n",
      "11233 [Discriminator loss: 0.151057, acc.: 93.75%] [Generator loss: 5.908941]\n",
      "11234 [Discriminator loss: 0.061385, acc.: 96.88%] [Generator loss: 6.293421]\n",
      "11235 [Discriminator loss: 0.287272, acc.: 87.50%] [Generator loss: 8.552146]\n",
      "11236 [Discriminator loss: 0.121343, acc.: 93.75%] [Generator loss: 8.631004]\n",
      "11237 [Discriminator loss: 0.083062, acc.: 95.31%] [Generator loss: 7.336453]\n",
      "11238 [Discriminator loss: 0.081400, acc.: 96.88%] [Generator loss: 6.315528]\n",
      "11239 [Discriminator loss: 0.106243, acc.: 95.31%] [Generator loss: 9.065253]\n",
      "11240 [Discriminator loss: 0.130036, acc.: 96.88%] [Generator loss: 6.507055]\n",
      "11241 [Discriminator loss: 0.102162, acc.: 93.75%] [Generator loss: 6.778639]\n",
      "11242 [Discriminator loss: 0.228313, acc.: 92.19%] [Generator loss: 6.980338]\n",
      "11243 [Discriminator loss: 0.066837, acc.: 95.31%] [Generator loss: 7.689815]\n",
      "11244 [Discriminator loss: 0.093106, acc.: 98.44%] [Generator loss: 9.558062]\n",
      "11245 [Discriminator loss: 0.137233, acc.: 95.31%] [Generator loss: 7.202493]\n",
      "11246 [Discriminator loss: 0.171863, acc.: 92.19%] [Generator loss: 7.491327]\n",
      "11247 [Discriminator loss: 0.015532, acc.: 100.00%] [Generator loss: 7.703372]\n",
      "11248 [Discriminator loss: 0.229694, acc.: 93.75%] [Generator loss: 6.983833]\n",
      "11249 [Discriminator loss: 0.235905, acc.: 90.62%] [Generator loss: 7.815161]\n",
      "11250 [Discriminator loss: 0.408231, acc.: 84.38%] [Generator loss: 6.002927]\n",
      "11251 [Discriminator loss: 0.143592, acc.: 95.31%] [Generator loss: 8.704585]\n",
      "11252 [Discriminator loss: 0.046637, acc.: 98.44%] [Generator loss: 7.436435]\n",
      "11253 [Discriminator loss: 0.073608, acc.: 95.31%] [Generator loss: 8.246964]\n",
      "11254 [Discriminator loss: 0.074933, acc.: 96.88%] [Generator loss: 6.916675]\n",
      "11255 [Discriminator loss: 0.113636, acc.: 95.31%] [Generator loss: 6.840822]\n",
      "11256 [Discriminator loss: 0.204603, acc.: 92.19%] [Generator loss: 7.844689]\n",
      "11257 [Discriminator loss: 0.098252, acc.: 95.31%] [Generator loss: 7.621991]\n",
      "11258 [Discriminator loss: 0.163702, acc.: 92.19%] [Generator loss: 6.910553]\n",
      "11259 [Discriminator loss: 0.127997, acc.: 92.19%] [Generator loss: 6.815608]\n",
      "11260 [Discriminator loss: 0.136515, acc.: 95.31%] [Generator loss: 7.349441]\n",
      "11261 [Discriminator loss: 0.129279, acc.: 93.75%] [Generator loss: 7.254392]\n",
      "11262 [Discriminator loss: 0.116726, acc.: 93.75%] [Generator loss: 6.131794]\n",
      "11263 [Discriminator loss: 0.071662, acc.: 96.88%] [Generator loss: 6.721623]\n",
      "11264 [Discriminator loss: 0.033819, acc.: 98.44%] [Generator loss: 6.771142]\n",
      "11265 [Discriminator loss: 0.218125, acc.: 92.19%] [Generator loss: 5.869795]\n",
      "11266 [Discriminator loss: 0.139860, acc.: 92.19%] [Generator loss: 7.581729]\n",
      "11267 [Discriminator loss: 0.058006, acc.: 96.88%] [Generator loss: 6.411381]\n",
      "11268 [Discriminator loss: 0.117770, acc.: 93.75%] [Generator loss: 6.428409]\n",
      "11269 [Discriminator loss: 0.217321, acc.: 89.06%] [Generator loss: 7.499735]\n",
      "11270 [Discriminator loss: 0.085223, acc.: 98.44%] [Generator loss: 6.187028]\n",
      "11271 [Discriminator loss: 0.042066, acc.: 100.00%] [Generator loss: 7.704798]\n",
      "11272 [Discriminator loss: 0.079232, acc.: 98.44%] [Generator loss: 7.267172]\n",
      "11273 [Discriminator loss: 0.150320, acc.: 95.31%] [Generator loss: 6.300582]\n",
      "11274 [Discriminator loss: 0.193243, acc.: 92.19%] [Generator loss: 7.331202]\n",
      "11275 [Discriminator loss: 0.158094, acc.: 95.31%] [Generator loss: 6.022899]\n",
      "11276 [Discriminator loss: 0.296157, acc.: 89.06%] [Generator loss: 8.798129]\n",
      "11277 [Discriminator loss: 0.219463, acc.: 92.19%] [Generator loss: 6.662790]\n",
      "11278 [Discriminator loss: 0.161855, acc.: 92.19%] [Generator loss: 6.620921]\n",
      "11279 [Discriminator loss: 0.098677, acc.: 96.88%] [Generator loss: 6.489811]\n",
      "11280 [Discriminator loss: 0.063441, acc.: 96.88%] [Generator loss: 7.739852]\n",
      "11281 [Discriminator loss: 0.180823, acc.: 89.06%] [Generator loss: 6.708808]\n",
      "11282 [Discriminator loss: 0.082839, acc.: 95.31%] [Generator loss: 7.029887]\n",
      "11283 [Discriminator loss: 0.029406, acc.: 100.00%] [Generator loss: 6.529142]\n",
      "11284 [Discriminator loss: 0.043228, acc.: 98.44%] [Generator loss: 6.538245]\n",
      "11285 [Discriminator loss: 0.106162, acc.: 95.31%] [Generator loss: 6.563168]\n",
      "11286 [Discriminator loss: 0.051170, acc.: 100.00%] [Generator loss: 6.294227]\n",
      "11287 [Discriminator loss: 0.111263, acc.: 95.31%] [Generator loss: 5.300210]\n",
      "11288 [Discriminator loss: 0.148826, acc.: 93.75%] [Generator loss: 7.800586]\n",
      "11289 [Discriminator loss: 0.048449, acc.: 98.44%] [Generator loss: 8.795719]\n",
      "11290 [Discriminator loss: 0.285511, acc.: 92.19%] [Generator loss: 6.285858]\n",
      "11291 [Discriminator loss: 0.128039, acc.: 93.75%] [Generator loss: 7.294323]\n",
      "11292 [Discriminator loss: 0.066113, acc.: 96.88%] [Generator loss: 7.626153]\n",
      "11293 [Discriminator loss: 0.108275, acc.: 93.75%] [Generator loss: 7.347233]\n",
      "11294 [Discriminator loss: 0.102004, acc.: 95.31%] [Generator loss: 4.027164]\n",
      "11295 [Discriminator loss: 0.233733, acc.: 87.50%] [Generator loss: 7.960947]\n",
      "11296 [Discriminator loss: 0.303912, acc.: 89.06%] [Generator loss: 8.238258]\n",
      "11297 [Discriminator loss: 0.054397, acc.: 96.88%] [Generator loss: 7.778512]\n",
      "11298 [Discriminator loss: 0.289555, acc.: 89.06%] [Generator loss: 5.327935]\n",
      "11299 [Discriminator loss: 0.196957, acc.: 90.62%] [Generator loss: 5.312436]\n",
      "11300 [Discriminator loss: 0.102911, acc.: 95.31%] [Generator loss: 5.666713]\n",
      "11301 [Discriminator loss: 0.144706, acc.: 90.62%] [Generator loss: 6.352839]\n",
      "11302 [Discriminator loss: 0.121909, acc.: 96.88%] [Generator loss: 6.824574]\n",
      "11303 [Discriminator loss: 0.200004, acc.: 93.75%] [Generator loss: 6.388290]\n",
      "11304 [Discriminator loss: 0.163936, acc.: 95.31%] [Generator loss: 6.684080]\n",
      "11305 [Discriminator loss: 0.080715, acc.: 95.31%] [Generator loss: 5.179886]\n",
      "11306 [Discriminator loss: 0.117718, acc.: 95.31%] [Generator loss: 6.419343]\n",
      "11307 [Discriminator loss: 0.276079, acc.: 95.31%] [Generator loss: 6.664428]\n",
      "11308 [Discriminator loss: 0.061129, acc.: 98.44%] [Generator loss: 6.335383]\n",
      "11309 [Discriminator loss: 0.223645, acc.: 92.19%] [Generator loss: 5.350477]\n",
      "11310 [Discriminator loss: 0.072407, acc.: 98.44%] [Generator loss: 5.627730]\n",
      "11311 [Discriminator loss: 0.280753, acc.: 89.06%] [Generator loss: 8.250593]\n",
      "11312 [Discriminator loss: 0.096494, acc.: 95.31%] [Generator loss: 7.340009]\n",
      "11313 [Discriminator loss: 0.072601, acc.: 96.88%] [Generator loss: 7.266092]\n",
      "11314 [Discriminator loss: 0.104850, acc.: 96.88%] [Generator loss: 7.639814]\n",
      "11315 [Discriminator loss: 0.041328, acc.: 100.00%] [Generator loss: 6.881318]\n",
      "11316 [Discriminator loss: 0.095007, acc.: 95.31%] [Generator loss: 7.466501]\n",
      "11317 [Discriminator loss: 0.198479, acc.: 92.19%] [Generator loss: 6.297060]\n",
      "11318 [Discriminator loss: 0.102177, acc.: 96.88%] [Generator loss: 6.342723]\n",
      "11319 [Discriminator loss: 0.173315, acc.: 95.31%] [Generator loss: 7.143455]\n",
      "11320 [Discriminator loss: 0.067153, acc.: 98.44%] [Generator loss: 6.918383]\n",
      "11321 [Discriminator loss: 0.035199, acc.: 100.00%] [Generator loss: 6.806039]\n",
      "11322 [Discriminator loss: 0.122693, acc.: 95.31%] [Generator loss: 5.461834]\n",
      "11323 [Discriminator loss: 0.290379, acc.: 87.50%] [Generator loss: 8.912209]\n",
      "11324 [Discriminator loss: 0.138224, acc.: 90.62%] [Generator loss: 7.376612]\n",
      "11325 [Discriminator loss: 0.071080, acc.: 96.88%] [Generator loss: 7.453076]\n",
      "11326 [Discriminator loss: 0.093167, acc.: 95.31%] [Generator loss: 7.197070]\n",
      "11327 [Discriminator loss: 0.164437, acc.: 92.19%] [Generator loss: 8.191177]\n",
      "11328 [Discriminator loss: 0.097002, acc.: 96.88%] [Generator loss: 7.818702]\n",
      "11329 [Discriminator loss: 0.024977, acc.: 100.00%] [Generator loss: 8.167708]\n",
      "11330 [Discriminator loss: 0.234139, acc.: 92.19%] [Generator loss: 6.587174]\n",
      "11331 [Discriminator loss: 0.213988, acc.: 90.62%] [Generator loss: 6.968640]\n",
      "11332 [Discriminator loss: 0.044559, acc.: 100.00%] [Generator loss: 7.985372]\n",
      "11333 [Discriminator loss: 0.132826, acc.: 95.31%] [Generator loss: 7.218433]\n",
      "11334 [Discriminator loss: 0.251649, acc.: 92.19%] [Generator loss: 6.739546]\n",
      "11335 [Discriminator loss: 0.064672, acc.: 98.44%] [Generator loss: 7.515497]\n",
      "11336 [Discriminator loss: 0.108146, acc.: 95.31%] [Generator loss: 7.515683]\n",
      "11337 [Discriminator loss: 0.109392, acc.: 95.31%] [Generator loss: 5.586233]\n",
      "11338 [Discriminator loss: 0.134040, acc.: 96.88%] [Generator loss: 6.299528]\n",
      "11339 [Discriminator loss: 0.101964, acc.: 96.88%] [Generator loss: 6.760204]\n",
      "11340 [Discriminator loss: 0.048606, acc.: 96.88%] [Generator loss: 6.143855]\n",
      "11341 [Discriminator loss: 0.159429, acc.: 90.62%] [Generator loss: 7.236297]\n",
      "11342 [Discriminator loss: 0.128335, acc.: 95.31%] [Generator loss: 7.086526]\n",
      "11343 [Discriminator loss: 0.158809, acc.: 95.31%] [Generator loss: 6.194554]\n",
      "11344 [Discriminator loss: 0.034130, acc.: 98.44%] [Generator loss: 6.249633]\n",
      "11345 [Discriminator loss: 0.076964, acc.: 96.88%] [Generator loss: 7.007829]\n",
      "11346 [Discriminator loss: 0.041169, acc.: 98.44%] [Generator loss: 6.263408]\n",
      "11347 [Discriminator loss: 0.190716, acc.: 90.62%] [Generator loss: 7.835926]\n",
      "11348 [Discriminator loss: 0.205658, acc.: 92.19%] [Generator loss: 4.961636]\n",
      "11349 [Discriminator loss: 0.085174, acc.: 96.88%] [Generator loss: 7.934250]\n",
      "11350 [Discriminator loss: 0.052515, acc.: 100.00%] [Generator loss: 8.364008]\n",
      "11351 [Discriminator loss: 0.079527, acc.: 98.44%] [Generator loss: 8.033681]\n",
      "11352 [Discriminator loss: 0.059431, acc.: 98.44%] [Generator loss: 7.240482]\n",
      "11353 [Discriminator loss: 0.035587, acc.: 100.00%] [Generator loss: 6.636674]\n",
      "11354 [Discriminator loss: 0.137044, acc.: 93.75%] [Generator loss: 6.701286]\n",
      "11355 [Discriminator loss: 0.098573, acc.: 98.44%] [Generator loss: 7.172994]\n",
      "11356 [Discriminator loss: 0.050209, acc.: 98.44%] [Generator loss: 5.434156]\n",
      "11357 [Discriminator loss: 0.080595, acc.: 96.88%] [Generator loss: 5.225104]\n",
      "11358 [Discriminator loss: 0.100984, acc.: 95.31%] [Generator loss: 8.287440]\n",
      "11359 [Discriminator loss: 0.576544, acc.: 78.12%] [Generator loss: 6.699741]\n",
      "11360 [Discriminator loss: 0.124654, acc.: 96.88%] [Generator loss: 7.203383]\n",
      "11361 [Discriminator loss: 0.056713, acc.: 98.44%] [Generator loss: 6.839977]\n",
      "11362 [Discriminator loss: 0.124144, acc.: 93.75%] [Generator loss: 5.783302]\n",
      "11363 [Discriminator loss: 0.069729, acc.: 96.88%] [Generator loss: 6.078712]\n",
      "11364 [Discriminator loss: 0.193715, acc.: 92.19%] [Generator loss: 4.422047]\n",
      "11365 [Discriminator loss: 0.241455, acc.: 90.62%] [Generator loss: 6.951159]\n",
      "11366 [Discriminator loss: 0.126556, acc.: 95.31%] [Generator loss: 7.253752]\n",
      "11367 [Discriminator loss: 0.107407, acc.: 95.31%] [Generator loss: 8.347271]\n",
      "11368 [Discriminator loss: 0.073310, acc.: 98.44%] [Generator loss: 6.208485]\n",
      "11369 [Discriminator loss: 0.142027, acc.: 93.75%] [Generator loss: 5.638380]\n",
      "11370 [Discriminator loss: 0.085004, acc.: 96.88%] [Generator loss: 6.155475]\n",
      "11371 [Discriminator loss: 0.086508, acc.: 95.31%] [Generator loss: 7.936324]\n",
      "11372 [Discriminator loss: 0.033815, acc.: 100.00%] [Generator loss: 7.399972]\n",
      "11373 [Discriminator loss: 0.089239, acc.: 96.88%] [Generator loss: 5.562750]\n",
      "11374 [Discriminator loss: 0.068699, acc.: 96.88%] [Generator loss: 6.419106]\n",
      "11375 [Discriminator loss: 0.152092, acc.: 92.19%] [Generator loss: 6.362508]\n",
      "11376 [Discriminator loss: 0.163795, acc.: 93.75%] [Generator loss: 7.193953]\n",
      "11377 [Discriminator loss: 0.067908, acc.: 96.88%] [Generator loss: 6.343546]\n",
      "11378 [Discriminator loss: 0.049298, acc.: 98.44%] [Generator loss: 7.229708]\n",
      "11379 [Discriminator loss: 0.123371, acc.: 93.75%] [Generator loss: 6.414401]\n",
      "11380 [Discriminator loss: 0.078066, acc.: 95.31%] [Generator loss: 6.684430]\n",
      "11381 [Discriminator loss: 0.048810, acc.: 98.44%] [Generator loss: 7.006114]\n",
      "11382 [Discriminator loss: 0.176171, acc.: 92.19%] [Generator loss: 6.871342]\n",
      "11383 [Discriminator loss: 0.124334, acc.: 93.75%] [Generator loss: 7.614918]\n",
      "11384 [Discriminator loss: 0.168848, acc.: 90.62%] [Generator loss: 6.484481]\n",
      "11385 [Discriminator loss: 0.047862, acc.: 98.44%] [Generator loss: 6.644254]\n",
      "11386 [Discriminator loss: 0.035070, acc.: 98.44%] [Generator loss: 7.431498]\n",
      "11387 [Discriminator loss: 0.176241, acc.: 90.62%] [Generator loss: 7.073462]\n",
      "11388 [Discriminator loss: 0.071466, acc.: 96.88%] [Generator loss: 6.850018]\n",
      "11389 [Discriminator loss: 0.072944, acc.: 96.88%] [Generator loss: 7.638494]\n",
      "11390 [Discriminator loss: 0.031446, acc.: 100.00%] [Generator loss: 7.848392]\n",
      "11391 [Discriminator loss: 0.105397, acc.: 96.88%] [Generator loss: 5.889373]\n",
      "11392 [Discriminator loss: 0.113736, acc.: 92.19%] [Generator loss: 6.840991]\n",
      "11393 [Discriminator loss: 0.159375, acc.: 92.19%] [Generator loss: 7.679728]\n",
      "11394 [Discriminator loss: 0.201814, acc.: 89.06%] [Generator loss: 9.040901]\n",
      "11395 [Discriminator loss: 0.105790, acc.: 95.31%] [Generator loss: 8.329669]\n",
      "11396 [Discriminator loss: 0.029599, acc.: 100.00%] [Generator loss: 7.016158]\n",
      "11397 [Discriminator loss: 0.166921, acc.: 95.31%] [Generator loss: 6.325488]\n",
      "11398 [Discriminator loss: 0.103531, acc.: 95.31%] [Generator loss: 7.156590]\n",
      "11399 [Discriminator loss: 0.031024, acc.: 100.00%] [Generator loss: 7.559359]\n",
      "11400 [Discriminator loss: 0.117783, acc.: 93.75%] [Generator loss: 6.432570]\n",
      "11401 [Discriminator loss: 0.178980, acc.: 95.31%] [Generator loss: 6.091643]\n",
      "11402 [Discriminator loss: 0.092700, acc.: 98.44%] [Generator loss: 7.268806]\n",
      "11403 [Discriminator loss: 0.164144, acc.: 90.62%] [Generator loss: 6.029145]\n",
      "11404 [Discriminator loss: 0.174891, acc.: 95.31%] [Generator loss: 5.631544]\n",
      "11405 [Discriminator loss: 0.099763, acc.: 96.88%] [Generator loss: 7.722959]\n",
      "11406 [Discriminator loss: 0.053798, acc.: 98.44%] [Generator loss: 7.524465]\n",
      "11407 [Discriminator loss: 0.094897, acc.: 95.31%] [Generator loss: 7.180184]\n",
      "11408 [Discriminator loss: 0.066512, acc.: 95.31%] [Generator loss: 8.175383]\n",
      "11409 [Discriminator loss: 0.205105, acc.: 92.19%] [Generator loss: 5.197227]\n",
      "11410 [Discriminator loss: 0.121722, acc.: 96.88%] [Generator loss: 5.427533]\n",
      "11411 [Discriminator loss: 0.033216, acc.: 98.44%] [Generator loss: 7.284333]\n",
      "11412 [Discriminator loss: 0.092946, acc.: 98.44%] [Generator loss: 6.974223]\n",
      "11413 [Discriminator loss: 0.156271, acc.: 92.19%] [Generator loss: 6.492174]\n",
      "11414 [Discriminator loss: 0.129470, acc.: 93.75%] [Generator loss: 6.849168]\n",
      "11415 [Discriminator loss: 0.132930, acc.: 95.31%] [Generator loss: 8.342609]\n",
      "11416 [Discriminator loss: 0.132019, acc.: 93.75%] [Generator loss: 6.778193]\n",
      "11417 [Discriminator loss: 0.079281, acc.: 93.75%] [Generator loss: 8.186371]\n",
      "11418 [Discriminator loss: 0.236207, acc.: 89.06%] [Generator loss: 6.898811]\n",
      "11419 [Discriminator loss: 0.151226, acc.: 93.75%] [Generator loss: 6.855845]\n",
      "11420 [Discriminator loss: 0.031949, acc.: 98.44%] [Generator loss: 5.148455]\n",
      "11421 [Discriminator loss: 0.163189, acc.: 92.19%] [Generator loss: 7.560999]\n",
      "11422 [Discriminator loss: 0.162666, acc.: 95.31%] [Generator loss: 7.463243]\n",
      "11423 [Discriminator loss: 0.168163, acc.: 92.19%] [Generator loss: 6.739228]\n",
      "11424 [Discriminator loss: 0.032721, acc.: 100.00%] [Generator loss: 6.623864]\n",
      "11425 [Discriminator loss: 0.150979, acc.: 92.19%] [Generator loss: 7.447071]\n",
      "11426 [Discriminator loss: 0.054774, acc.: 98.44%] [Generator loss: 8.102514]\n",
      "11427 [Discriminator loss: 0.176078, acc.: 92.19%] [Generator loss: 7.459724]\n",
      "11428 [Discriminator loss: 0.180601, acc.: 92.19%] [Generator loss: 6.986975]\n",
      "11429 [Discriminator loss: 0.189711, acc.: 93.75%] [Generator loss: 7.451479]\n",
      "11430 [Discriminator loss: 0.055308, acc.: 98.44%] [Generator loss: 5.709024]\n",
      "11431 [Discriminator loss: 0.224647, acc.: 89.06%] [Generator loss: 6.995402]\n",
      "11432 [Discriminator loss: 0.095288, acc.: 96.88%] [Generator loss: 6.861368]\n",
      "11433 [Discriminator loss: 0.224179, acc.: 90.62%] [Generator loss: 7.774400]\n",
      "11434 [Discriminator loss: 0.039781, acc.: 98.44%] [Generator loss: 7.929228]\n",
      "11435 [Discriminator loss: 0.312969, acc.: 84.38%] [Generator loss: 7.160498]\n",
      "11436 [Discriminator loss: 0.291782, acc.: 95.31%] [Generator loss: 8.877983]\n",
      "11437 [Discriminator loss: 0.249672, acc.: 85.94%] [Generator loss: 4.698386]\n",
      "11438 [Discriminator loss: 0.123866, acc.: 93.75%] [Generator loss: 8.889164]\n",
      "11439 [Discriminator loss: 0.103593, acc.: 96.88%] [Generator loss: 7.889250]\n",
      "11440 [Discriminator loss: 0.308710, acc.: 85.94%] [Generator loss: 4.882629]\n",
      "11441 [Discriminator loss: 0.145636, acc.: 93.75%] [Generator loss: 7.867070]\n",
      "11442 [Discriminator loss: 0.015183, acc.: 100.00%] [Generator loss: 7.725425]\n",
      "11443 [Discriminator loss: 0.164702, acc.: 93.75%] [Generator loss: 6.444694]\n",
      "11444 [Discriminator loss: 0.028801, acc.: 100.00%] [Generator loss: 6.283884]\n",
      "11445 [Discriminator loss: 0.059427, acc.: 98.44%] [Generator loss: 7.448846]\n",
      "11446 [Discriminator loss: 0.077136, acc.: 98.44%] [Generator loss: 4.334206]\n",
      "11447 [Discriminator loss: 0.148981, acc.: 96.88%] [Generator loss: 6.558936]\n",
      "11448 [Discriminator loss: 0.242511, acc.: 92.19%] [Generator loss: 9.552040]\n",
      "11449 [Discriminator loss: 0.136248, acc.: 95.31%] [Generator loss: 8.157925]\n",
      "11450 [Discriminator loss: 0.208117, acc.: 92.19%] [Generator loss: 5.720695]\n",
      "11451 [Discriminator loss: 0.308770, acc.: 89.06%] [Generator loss: 9.050877]\n",
      "11452 [Discriminator loss: 0.047383, acc.: 98.44%] [Generator loss: 7.525866]\n",
      "11453 [Discriminator loss: 0.114747, acc.: 95.31%] [Generator loss: 7.286147]\n",
      "11454 [Discriminator loss: 0.257707, acc.: 93.75%] [Generator loss: 7.405890]\n",
      "11455 [Discriminator loss: 0.087293, acc.: 96.88%] [Generator loss: 6.446853]\n",
      "11456 [Discriminator loss: 0.100747, acc.: 95.31%] [Generator loss: 6.748839]\n",
      "11457 [Discriminator loss: 0.137185, acc.: 93.75%] [Generator loss: 6.799477]\n",
      "11458 [Discriminator loss: 0.113259, acc.: 95.31%] [Generator loss: 7.520629]\n",
      "11459 [Discriminator loss: 0.060198, acc.: 98.44%] [Generator loss: 7.468504]\n",
      "11460 [Discriminator loss: 0.109428, acc.: 96.88%] [Generator loss: 5.851259]\n",
      "11461 [Discriminator loss: 0.115492, acc.: 95.31%] [Generator loss: 7.087365]\n",
      "11462 [Discriminator loss: 0.191385, acc.: 92.19%] [Generator loss: 7.337664]\n",
      "11463 [Discriminator loss: 0.023288, acc.: 100.00%] [Generator loss: 7.079047]\n",
      "11464 [Discriminator loss: 0.145087, acc.: 92.19%] [Generator loss: 6.829166]\n",
      "11465 [Discriminator loss: 0.021690, acc.: 100.00%] [Generator loss: 5.939938]\n",
      "11466 [Discriminator loss: 0.290972, acc.: 85.94%] [Generator loss: 9.679914]\n",
      "11467 [Discriminator loss: 0.176686, acc.: 90.62%] [Generator loss: 8.549385]\n",
      "11468 [Discriminator loss: 0.068541, acc.: 96.88%] [Generator loss: 6.830460]\n",
      "11469 [Discriminator loss: 0.185747, acc.: 93.75%] [Generator loss: 6.080042]\n",
      "11470 [Discriminator loss: 0.105982, acc.: 95.31%] [Generator loss: 8.093005]\n",
      "11471 [Discriminator loss: 0.138020, acc.: 93.75%] [Generator loss: 7.714144]\n",
      "11472 [Discriminator loss: 0.095179, acc.: 96.88%] [Generator loss: 7.581193]\n",
      "11473 [Discriminator loss: 0.098673, acc.: 95.31%] [Generator loss: 7.177640]\n",
      "11474 [Discriminator loss: 0.108139, acc.: 93.75%] [Generator loss: 6.496795]\n",
      "11475 [Discriminator loss: 0.050593, acc.: 98.44%] [Generator loss: 7.671721]\n",
      "11476 [Discriminator loss: 0.129954, acc.: 96.88%] [Generator loss: 7.593646]\n",
      "11477 [Discriminator loss: 0.121273, acc.: 93.75%] [Generator loss: 6.889943]\n",
      "11478 [Discriminator loss: 0.295004, acc.: 89.06%] [Generator loss: 8.306692]\n",
      "11479 [Discriminator loss: 0.067050, acc.: 98.44%] [Generator loss: 7.388381]\n",
      "11480 [Discriminator loss: 0.261348, acc.: 89.06%] [Generator loss: 8.617983]\n",
      "11481 [Discriminator loss: 0.120415, acc.: 95.31%] [Generator loss: 7.510925]\n",
      "11482 [Discriminator loss: 0.135195, acc.: 95.31%] [Generator loss: 7.319559]\n",
      "11483 [Discriminator loss: 0.166549, acc.: 93.75%] [Generator loss: 6.133646]\n",
      "11484 [Discriminator loss: 0.309581, acc.: 87.50%] [Generator loss: 8.170904]\n",
      "11485 [Discriminator loss: 0.104374, acc.: 96.88%] [Generator loss: 7.309878]\n",
      "11486 [Discriminator loss: 0.166746, acc.: 93.75%] [Generator loss: 6.414003]\n",
      "11487 [Discriminator loss: 0.099539, acc.: 95.31%] [Generator loss: 5.941647]\n",
      "11488 [Discriminator loss: 0.074756, acc.: 98.44%] [Generator loss: 7.121747]\n",
      "11489 [Discriminator loss: 0.131141, acc.: 96.88%] [Generator loss: 7.368485]\n",
      "11490 [Discriminator loss: 0.197763, acc.: 90.62%] [Generator loss: 8.862175]\n",
      "11491 [Discriminator loss: 0.149577, acc.: 95.31%] [Generator loss: 7.969252]\n",
      "11492 [Discriminator loss: 0.117991, acc.: 92.19%] [Generator loss: 7.398303]\n",
      "11493 [Discriminator loss: 0.090168, acc.: 96.88%] [Generator loss: 7.263670]\n",
      "11494 [Discriminator loss: 0.081912, acc.: 100.00%] [Generator loss: 6.551700]\n",
      "11495 [Discriminator loss: 0.146954, acc.: 90.62%] [Generator loss: 7.614021]\n",
      "11496 [Discriminator loss: 0.055481, acc.: 96.88%] [Generator loss: 7.309211]\n",
      "11497 [Discriminator loss: 0.143447, acc.: 93.75%] [Generator loss: 7.340878]\n",
      "11498 [Discriminator loss: 0.131937, acc.: 96.88%] [Generator loss: 6.325151]\n",
      "11499 [Discriminator loss: 0.176775, acc.: 90.62%] [Generator loss: 7.251163]\n",
      "11500 [Discriminator loss: 0.194627, acc.: 89.06%] [Generator loss: 6.979676]\n",
      "11501 [Discriminator loss: 0.156569, acc.: 95.31%] [Generator loss: 8.213567]\n",
      "11502 [Discriminator loss: 0.101131, acc.: 96.88%] [Generator loss: 6.914302]\n",
      "11503 [Discriminator loss: 0.245254, acc.: 85.94%] [Generator loss: 6.760722]\n",
      "11504 [Discriminator loss: 0.241022, acc.: 90.62%] [Generator loss: 7.681319]\n",
      "11505 [Discriminator loss: 0.130157, acc.: 93.75%] [Generator loss: 7.254074]\n",
      "11506 [Discriminator loss: 0.056047, acc.: 98.44%] [Generator loss: 6.752234]\n",
      "11507 [Discriminator loss: 0.058198, acc.: 98.44%] [Generator loss: 6.121784]\n",
      "11508 [Discriminator loss: 0.200286, acc.: 90.62%] [Generator loss: 5.601360]\n",
      "11509 [Discriminator loss: 0.115305, acc.: 93.75%] [Generator loss: 7.800589]\n",
      "11510 [Discriminator loss: 0.027373, acc.: 100.00%] [Generator loss: 7.485816]\n",
      "11511 [Discriminator loss: 0.173607, acc.: 89.06%] [Generator loss: 4.350470]\n",
      "11512 [Discriminator loss: 0.219045, acc.: 90.62%] [Generator loss: 6.816133]\n",
      "11513 [Discriminator loss: 0.084781, acc.: 95.31%] [Generator loss: 7.991602]\n",
      "11514 [Discriminator loss: 0.095447, acc.: 95.31%] [Generator loss: 6.043455]\n",
      "11515 [Discriminator loss: 0.155004, acc.: 92.19%] [Generator loss: 7.409884]\n",
      "11516 [Discriminator loss: 0.093779, acc.: 96.88%] [Generator loss: 6.423395]\n",
      "11517 [Discriminator loss: 0.045961, acc.: 98.44%] [Generator loss: 6.464105]\n",
      "11518 [Discriminator loss: 0.099927, acc.: 96.88%] [Generator loss: 7.856525]\n",
      "11519 [Discriminator loss: 0.054760, acc.: 98.44%] [Generator loss: 6.736616]\n",
      "11520 [Discriminator loss: 0.106837, acc.: 98.44%] [Generator loss: 7.790071]\n",
      "11521 [Discriminator loss: 0.117248, acc.: 98.44%] [Generator loss: 8.306984]\n",
      "11522 [Discriminator loss: 0.084309, acc.: 98.44%] [Generator loss: 6.528637]\n",
      "11523 [Discriminator loss: 0.069003, acc.: 98.44%] [Generator loss: 6.967664]\n",
      "11524 [Discriminator loss: 0.125010, acc.: 93.75%] [Generator loss: 7.102850]\n",
      "11525 [Discriminator loss: 0.054057, acc.: 98.44%] [Generator loss: 7.376596]\n",
      "11526 [Discriminator loss: 0.114618, acc.: 95.31%] [Generator loss: 6.552770]\n",
      "11527 [Discriminator loss: 0.046709, acc.: 100.00%] [Generator loss: 6.543816]\n",
      "11528 [Discriminator loss: 0.137362, acc.: 95.31%] [Generator loss: 5.422443]\n",
      "11529 [Discriminator loss: 0.230289, acc.: 89.06%] [Generator loss: 6.134307]\n",
      "11530 [Discriminator loss: 0.027539, acc.: 100.00%] [Generator loss: 6.155083]\n",
      "11531 [Discriminator loss: 0.063901, acc.: 96.88%] [Generator loss: 7.152473]\n",
      "11532 [Discriminator loss: 0.149565, acc.: 93.75%] [Generator loss: 6.531881]\n",
      "11533 [Discriminator loss: 0.204402, acc.: 90.62%] [Generator loss: 6.819835]\n",
      "11534 [Discriminator loss: 0.091776, acc.: 96.88%] [Generator loss: 7.400966]\n",
      "11535 [Discriminator loss: 0.122238, acc.: 93.75%] [Generator loss: 6.796409]\n",
      "11536 [Discriminator loss: 0.137440, acc.: 95.31%] [Generator loss: 7.851314]\n",
      "11537 [Discriminator loss: 0.139534, acc.: 93.75%] [Generator loss: 6.540407]\n",
      "11538 [Discriminator loss: 0.073476, acc.: 98.44%] [Generator loss: 5.708656]\n",
      "11539 [Discriminator loss: 0.292428, acc.: 95.31%] [Generator loss: 7.257457]\n",
      "11540 [Discriminator loss: 0.106508, acc.: 96.88%] [Generator loss: 6.795784]\n",
      "11541 [Discriminator loss: 0.013469, acc.: 100.00%] [Generator loss: 6.641336]\n",
      "11542 [Discriminator loss: 0.094332, acc.: 93.75%] [Generator loss: 7.360398]\n",
      "11543 [Discriminator loss: 0.174363, acc.: 92.19%] [Generator loss: 5.435043]\n",
      "11544 [Discriminator loss: 0.167384, acc.: 95.31%] [Generator loss: 6.859662]\n",
      "11545 [Discriminator loss: 0.107538, acc.: 96.88%] [Generator loss: 6.925569]\n",
      "11546 [Discriminator loss: 0.178377, acc.: 92.19%] [Generator loss: 6.669773]\n",
      "11547 [Discriminator loss: 0.150557, acc.: 95.31%] [Generator loss: 8.277143]\n",
      "11548 [Discriminator loss: 0.074228, acc.: 96.88%] [Generator loss: 8.595569]\n",
      "11549 [Discriminator loss: 0.138905, acc.: 95.31%] [Generator loss: 8.208400]\n",
      "11550 [Discriminator loss: 0.158596, acc.: 96.88%] [Generator loss: 7.017539]\n",
      "11551 [Discriminator loss: 0.171719, acc.: 93.75%] [Generator loss: 7.202540]\n",
      "11552 [Discriminator loss: 0.075015, acc.: 96.88%] [Generator loss: 6.578223]\n",
      "11553 [Discriminator loss: 0.058019, acc.: 96.88%] [Generator loss: 7.207571]\n",
      "11554 [Discriminator loss: 0.087709, acc.: 95.31%] [Generator loss: 7.386425]\n",
      "11555 [Discriminator loss: 0.149418, acc.: 93.75%] [Generator loss: 6.847957]\n",
      "11556 [Discriminator loss: 0.071838, acc.: 100.00%] [Generator loss: 6.927998]\n",
      "11557 [Discriminator loss: 0.033684, acc.: 98.44%] [Generator loss: 7.903006]\n",
      "11558 [Discriminator loss: 0.115454, acc.: 95.31%] [Generator loss: 4.706062]\n",
      "11559 [Discriminator loss: 0.075106, acc.: 98.44%] [Generator loss: 6.661434]\n",
      "11560 [Discriminator loss: 0.026974, acc.: 100.00%] [Generator loss: 7.132401]\n",
      "11561 [Discriminator loss: 0.156806, acc.: 92.19%] [Generator loss: 5.786349]\n",
      "11562 [Discriminator loss: 0.057830, acc.: 98.44%] [Generator loss: 6.044181]\n",
      "11563 [Discriminator loss: 0.026847, acc.: 100.00%] [Generator loss: 6.648177]\n",
      "11564 [Discriminator loss: 0.125046, acc.: 92.19%] [Generator loss: 6.833159]\n",
      "11565 [Discriminator loss: 0.087936, acc.: 95.31%] [Generator loss: 8.180819]\n",
      "11566 [Discriminator loss: 0.072284, acc.: 95.31%] [Generator loss: 6.890701]\n",
      "11567 [Discriminator loss: 0.103920, acc.: 93.75%] [Generator loss: 5.562161]\n",
      "11568 [Discriminator loss: 0.192784, acc.: 95.31%] [Generator loss: 6.821754]\n",
      "11569 [Discriminator loss: 0.101213, acc.: 95.31%] [Generator loss: 7.264956]\n",
      "11570 [Discriminator loss: 0.175181, acc.: 93.75%] [Generator loss: 6.289935]\n",
      "11571 [Discriminator loss: 0.070511, acc.: 95.31%] [Generator loss: 6.046392]\n",
      "11572 [Discriminator loss: 0.106375, acc.: 95.31%] [Generator loss: 6.917553]\n",
      "11573 [Discriminator loss: 0.071289, acc.: 96.88%] [Generator loss: 8.136738]\n",
      "11574 [Discriminator loss: 0.034611, acc.: 98.44%] [Generator loss: 7.949089]\n",
      "11575 [Discriminator loss: 0.180238, acc.: 92.19%] [Generator loss: 6.243259]\n",
      "11576 [Discriminator loss: 0.133273, acc.: 95.31%] [Generator loss: 6.560752]\n",
      "11577 [Discriminator loss: 0.068284, acc.: 98.44%] [Generator loss: 6.441348]\n",
      "11578 [Discriminator loss: 0.063062, acc.: 98.44%] [Generator loss: 7.027276]\n",
      "11579 [Discriminator loss: 0.022488, acc.: 100.00%] [Generator loss: 7.526013]\n",
      "11580 [Discriminator loss: 0.101905, acc.: 92.19%] [Generator loss: 6.055301]\n",
      "11581 [Discriminator loss: 0.038086, acc.: 98.44%] [Generator loss: 7.498880]\n",
      "11582 [Discriminator loss: 0.095047, acc.: 92.19%] [Generator loss: 5.434339]\n",
      "11583 [Discriminator loss: 0.261753, acc.: 92.19%] [Generator loss: 6.708687]\n",
      "11584 [Discriminator loss: 0.126420, acc.: 95.31%] [Generator loss: 7.031884]\n",
      "11585 [Discriminator loss: 0.091715, acc.: 98.44%] [Generator loss: 5.224163]\n",
      "11586 [Discriminator loss: 0.211870, acc.: 90.62%] [Generator loss: 6.975329]\n",
      "11587 [Discriminator loss: 0.021108, acc.: 100.00%] [Generator loss: 7.530643]\n",
      "11588 [Discriminator loss: 0.172857, acc.: 90.62%] [Generator loss: 7.035257]\n",
      "11589 [Discriminator loss: 0.051413, acc.: 96.88%] [Generator loss: 6.378264]\n",
      "11590 [Discriminator loss: 0.052745, acc.: 98.44%] [Generator loss: 5.611144]\n",
      "11591 [Discriminator loss: 0.115483, acc.: 95.31%] [Generator loss: 7.462836]\n",
      "11592 [Discriminator loss: 0.111704, acc.: 96.88%] [Generator loss: 5.626513]\n",
      "11593 [Discriminator loss: 0.086394, acc.: 96.88%] [Generator loss: 5.611022]\n",
      "11594 [Discriminator loss: 0.101408, acc.: 96.88%] [Generator loss: 6.380282]\n",
      "11595 [Discriminator loss: 0.044444, acc.: 98.44%] [Generator loss: 5.913742]\n",
      "11596 [Discriminator loss: 0.264259, acc.: 90.62%] [Generator loss: 7.877512]\n",
      "11597 [Discriminator loss: 0.123753, acc.: 95.31%] [Generator loss: 7.591842]\n",
      "11598 [Discriminator loss: 0.110799, acc.: 93.75%] [Generator loss: 7.373123]\n",
      "11599 [Discriminator loss: 0.111183, acc.: 95.31%] [Generator loss: 7.252708]\n",
      "11600 [Discriminator loss: 0.143181, acc.: 93.75%] [Generator loss: 7.869529]\n",
      "11601 [Discriminator loss: 0.071526, acc.: 98.44%] [Generator loss: 7.316882]\n",
      "11602 [Discriminator loss: 0.137961, acc.: 96.88%] [Generator loss: 6.729678]\n",
      "11603 [Discriminator loss: 0.233146, acc.: 93.75%] [Generator loss: 6.608702]\n",
      "11604 [Discriminator loss: 0.089611, acc.: 98.44%] [Generator loss: 6.060378]\n",
      "11605 [Discriminator loss: 0.106439, acc.: 95.31%] [Generator loss: 7.026328]\n",
      "11606 [Discriminator loss: 0.054940, acc.: 98.44%] [Generator loss: 6.878962]\n",
      "11607 [Discriminator loss: 0.120228, acc.: 98.44%] [Generator loss: 5.912827]\n",
      "11608 [Discriminator loss: 0.185873, acc.: 90.62%] [Generator loss: 7.390354]\n",
      "11609 [Discriminator loss: 0.178051, acc.: 90.62%] [Generator loss: 7.010131]\n",
      "11610 [Discriminator loss: 0.098616, acc.: 96.88%] [Generator loss: 6.955082]\n",
      "11611 [Discriminator loss: 0.024201, acc.: 100.00%] [Generator loss: 6.995646]\n",
      "11612 [Discriminator loss: 0.105466, acc.: 96.88%] [Generator loss: 7.230574]\n",
      "11613 [Discriminator loss: 0.066317, acc.: 98.44%] [Generator loss: 6.978249]\n",
      "11614 [Discriminator loss: 0.032859, acc.: 100.00%] [Generator loss: 6.789512]\n",
      "11615 [Discriminator loss: 0.045961, acc.: 100.00%] [Generator loss: 7.472285]\n",
      "11616 [Discriminator loss: 0.204122, acc.: 92.19%] [Generator loss: 6.727305]\n",
      "11617 [Discriminator loss: 0.066056, acc.: 96.88%] [Generator loss: 8.222946]\n",
      "11618 [Discriminator loss: 0.117350, acc.: 96.88%] [Generator loss: 6.821957]\n",
      "11619 [Discriminator loss: 0.314997, acc.: 85.94%] [Generator loss: 6.038529]\n",
      "11620 [Discriminator loss: 0.050439, acc.: 98.44%] [Generator loss: 7.070552]\n",
      "11621 [Discriminator loss: 0.094317, acc.: 93.75%] [Generator loss: 6.767435]\n",
      "11622 [Discriminator loss: 0.048792, acc.: 98.44%] [Generator loss: 7.283240]\n",
      "11623 [Discriminator loss: 0.026866, acc.: 98.44%] [Generator loss: 6.131760]\n",
      "11624 [Discriminator loss: 0.057114, acc.: 100.00%] [Generator loss: 5.297007]\n",
      "11625 [Discriminator loss: 0.099264, acc.: 95.31%] [Generator loss: 6.444783]\n",
      "11626 [Discriminator loss: 0.126146, acc.: 95.31%] [Generator loss: 6.550375]\n",
      "11627 [Discriminator loss: 0.091798, acc.: 96.88%] [Generator loss: 6.261435]\n",
      "11628 [Discriminator loss: 0.334455, acc.: 89.06%] [Generator loss: 7.556413]\n",
      "11629 [Discriminator loss: 0.053173, acc.: 100.00%] [Generator loss: 8.265055]\n",
      "11630 [Discriminator loss: 0.076234, acc.: 96.88%] [Generator loss: 7.366421]\n",
      "11631 [Discriminator loss: 0.071405, acc.: 96.88%] [Generator loss: 7.718370]\n",
      "11632 [Discriminator loss: 0.107786, acc.: 96.88%] [Generator loss: 6.757434]\n",
      "11633 [Discriminator loss: 0.086184, acc.: 96.88%] [Generator loss: 5.488649]\n",
      "11634 [Discriminator loss: 0.052844, acc.: 98.44%] [Generator loss: 4.387704]\n",
      "11635 [Discriminator loss: 0.205540, acc.: 92.19%] [Generator loss: 6.110423]\n",
      "11636 [Discriminator loss: 0.137047, acc.: 92.19%] [Generator loss: 7.279083]\n",
      "11637 [Discriminator loss: 0.042843, acc.: 100.00%] [Generator loss: 8.017385]\n",
      "11638 [Discriminator loss: 0.071044, acc.: 98.44%] [Generator loss: 8.905684]\n",
      "11639 [Discriminator loss: 0.158036, acc.: 93.75%] [Generator loss: 6.881187]\n",
      "11640 [Discriminator loss: 0.107303, acc.: 96.88%] [Generator loss: 6.807280]\n",
      "11641 [Discriminator loss: 0.035693, acc.: 98.44%] [Generator loss: 6.971084]\n",
      "11642 [Discriminator loss: 0.118735, acc.: 96.88%] [Generator loss: 7.816902]\n",
      "11643 [Discriminator loss: 0.112766, acc.: 95.31%] [Generator loss: 5.789694]\n",
      "11644 [Discriminator loss: 0.183753, acc.: 92.19%] [Generator loss: 10.429441]\n",
      "11645 [Discriminator loss: 0.180233, acc.: 95.31%] [Generator loss: 8.827232]\n",
      "11646 [Discriminator loss: 0.212897, acc.: 90.62%] [Generator loss: 5.976161]\n",
      "11647 [Discriminator loss: 0.133650, acc.: 95.31%] [Generator loss: 6.500904]\n",
      "11648 [Discriminator loss: 0.038953, acc.: 100.00%] [Generator loss: 6.986901]\n",
      "11649 [Discriminator loss: 0.022877, acc.: 100.00%] [Generator loss: 8.247128]\n",
      "11650 [Discriminator loss: 0.180028, acc.: 93.75%] [Generator loss: 6.745502]\n",
      "11651 [Discriminator loss: 0.111907, acc.: 93.75%] [Generator loss: 6.779438]\n",
      "11652 [Discriminator loss: 0.105365, acc.: 98.44%] [Generator loss: 8.390593]\n",
      "11653 [Discriminator loss: 0.061357, acc.: 98.44%] [Generator loss: 6.119441]\n",
      "11654 [Discriminator loss: 0.150311, acc.: 93.75%] [Generator loss: 9.040596]\n",
      "11655 [Discriminator loss: 0.188082, acc.: 95.31%] [Generator loss: 6.728069]\n",
      "11656 [Discriminator loss: 0.087651, acc.: 96.88%] [Generator loss: 7.222197]\n",
      "11657 [Discriminator loss: 0.169696, acc.: 93.75%] [Generator loss: 8.279698]\n",
      "11658 [Discriminator loss: 0.143637, acc.: 95.31%] [Generator loss: 7.784639]\n",
      "11659 [Discriminator loss: 0.117782, acc.: 95.31%] [Generator loss: 8.375720]\n",
      "11660 [Discriminator loss: 0.104860, acc.: 95.31%] [Generator loss: 5.661424]\n",
      "11661 [Discriminator loss: 0.062113, acc.: 98.44%] [Generator loss: 6.120988]\n",
      "11662 [Discriminator loss: 0.055779, acc.: 98.44%] [Generator loss: 7.345516]\n",
      "11663 [Discriminator loss: 0.057452, acc.: 98.44%] [Generator loss: 6.277067]\n",
      "11664 [Discriminator loss: 0.116438, acc.: 95.31%] [Generator loss: 6.300142]\n",
      "11665 [Discriminator loss: 0.059718, acc.: 98.44%] [Generator loss: 7.460494]\n",
      "11666 [Discriminator loss: 0.163415, acc.: 90.62%] [Generator loss: 9.217033]\n",
      "11667 [Discriminator loss: 0.073524, acc.: 98.44%] [Generator loss: 9.684218]\n",
      "11668 [Discriminator loss: 0.307592, acc.: 85.94%] [Generator loss: 5.607354]\n",
      "11669 [Discriminator loss: 0.046601, acc.: 98.44%] [Generator loss: 5.710726]\n",
      "11670 [Discriminator loss: 0.109864, acc.: 95.31%] [Generator loss: 6.999816]\n",
      "11671 [Discriminator loss: 0.141307, acc.: 92.19%] [Generator loss: 6.560052]\n",
      "11672 [Discriminator loss: 0.036233, acc.: 100.00%] [Generator loss: 7.987143]\n",
      "11673 [Discriminator loss: 0.117972, acc.: 96.88%] [Generator loss: 5.843161]\n",
      "11674 [Discriminator loss: 0.123829, acc.: 96.88%] [Generator loss: 5.481544]\n",
      "11675 [Discriminator loss: 0.294683, acc.: 92.19%] [Generator loss: 7.011622]\n",
      "11676 [Discriminator loss: 0.152457, acc.: 95.31%] [Generator loss: 6.595775]\n",
      "11677 [Discriminator loss: 0.017535, acc.: 100.00%] [Generator loss: 6.834050]\n",
      "11678 [Discriminator loss: 0.124864, acc.: 96.88%] [Generator loss: 6.150133]\n",
      "11679 [Discriminator loss: 0.047391, acc.: 100.00%] [Generator loss: 6.639588]\n",
      "11680 [Discriminator loss: 0.120842, acc.: 93.75%] [Generator loss: 7.059230]\n",
      "11681 [Discriminator loss: 0.066918, acc.: 96.88%] [Generator loss: 6.655898]\n",
      "11682 [Discriminator loss: 0.201754, acc.: 89.06%] [Generator loss: 7.426871]\n",
      "11683 [Discriminator loss: 0.089815, acc.: 98.44%] [Generator loss: 8.819346]\n",
      "11684 [Discriminator loss: 0.153246, acc.: 92.19%] [Generator loss: 7.269024]\n",
      "11685 [Discriminator loss: 0.235238, acc.: 89.06%] [Generator loss: 6.811528]\n",
      "11686 [Discriminator loss: 0.105830, acc.: 95.31%] [Generator loss: 7.868717]\n",
      "11687 [Discriminator loss: 0.075846, acc.: 98.44%] [Generator loss: 5.594630]\n",
      "11688 [Discriminator loss: 0.094538, acc.: 96.88%] [Generator loss: 7.425233]\n",
      "11689 [Discriminator loss: 0.151609, acc.: 93.75%] [Generator loss: 7.278261]\n",
      "11690 [Discriminator loss: 0.061908, acc.: 98.44%] [Generator loss: 6.999188]\n",
      "11691 [Discriminator loss: 0.121923, acc.: 95.31%] [Generator loss: 5.570414]\n",
      "11692 [Discriminator loss: 0.041545, acc.: 98.44%] [Generator loss: 7.727551]\n",
      "11693 [Discriminator loss: 0.090181, acc.: 96.88%] [Generator loss: 6.562979]\n",
      "11694 [Discriminator loss: 0.150067, acc.: 93.75%] [Generator loss: 6.945682]\n",
      "11695 [Discriminator loss: 0.162200, acc.: 95.31%] [Generator loss: 6.166571]\n",
      "11696 [Discriminator loss: 0.046821, acc.: 98.44%] [Generator loss: 6.685097]\n",
      "11697 [Discriminator loss: 0.146701, acc.: 93.75%] [Generator loss: 7.159390]\n",
      "11698 [Discriminator loss: 0.032435, acc.: 98.44%] [Generator loss: 7.432672]\n",
      "11699 [Discriminator loss: 0.100249, acc.: 96.88%] [Generator loss: 7.273617]\n",
      "11700 [Discriminator loss: 0.083879, acc.: 98.44%] [Generator loss: 5.189632]\n",
      "11701 [Discriminator loss: 0.134077, acc.: 93.75%] [Generator loss: 5.008659]\n",
      "11702 [Discriminator loss: 0.031459, acc.: 100.00%] [Generator loss: 8.564660]\n",
      "11703 [Discriminator loss: 0.059427, acc.: 98.44%] [Generator loss: 6.848605]\n",
      "11704 [Discriminator loss: 0.128948, acc.: 96.88%] [Generator loss: 7.610367]\n",
      "11705 [Discriminator loss: 0.075528, acc.: 96.88%] [Generator loss: 7.620155]\n",
      "11706 [Discriminator loss: 0.128127, acc.: 96.88%] [Generator loss: 6.948870]\n",
      "11707 [Discriminator loss: 0.276684, acc.: 87.50%] [Generator loss: 6.305861]\n",
      "11708 [Discriminator loss: 0.050896, acc.: 96.88%] [Generator loss: 7.115555]\n",
      "11709 [Discriminator loss: 0.068158, acc.: 98.44%] [Generator loss: 5.499509]\n",
      "11710 [Discriminator loss: 0.201773, acc.: 90.62%] [Generator loss: 7.211513]\n",
      "11711 [Discriminator loss: 0.101329, acc.: 95.31%] [Generator loss: 7.676596]\n",
      "11712 [Discriminator loss: 0.087691, acc.: 96.88%] [Generator loss: 7.565917]\n",
      "11713 [Discriminator loss: 0.224769, acc.: 90.62%] [Generator loss: 6.498324]\n",
      "11714 [Discriminator loss: 0.072351, acc.: 98.44%] [Generator loss: 6.756911]\n",
      "11715 [Discriminator loss: 0.071244, acc.: 95.31%] [Generator loss: 6.149702]\n",
      "11716 [Discriminator loss: 0.208684, acc.: 90.62%] [Generator loss: 4.744513]\n",
      "11717 [Discriminator loss: 0.062579, acc.: 96.88%] [Generator loss: 4.714274]\n",
      "11718 [Discriminator loss: 0.064976, acc.: 96.88%] [Generator loss: 6.320719]\n",
      "11719 [Discriminator loss: 0.197647, acc.: 89.06%] [Generator loss: 6.442963]\n",
      "11720 [Discriminator loss: 0.179906, acc.: 90.62%] [Generator loss: 6.137556]\n",
      "11721 [Discriminator loss: 0.210734, acc.: 93.75%] [Generator loss: 7.831027]\n",
      "11722 [Discriminator loss: 0.053789, acc.: 98.44%] [Generator loss: 7.164307]\n",
      "11723 [Discriminator loss: 0.286093, acc.: 84.38%] [Generator loss: 8.471785]\n",
      "11724 [Discriminator loss: 0.051763, acc.: 98.44%] [Generator loss: 7.784450]\n",
      "11725 [Discriminator loss: 0.089369, acc.: 95.31%] [Generator loss: 7.657509]\n",
      "11726 [Discriminator loss: 0.179147, acc.: 95.31%] [Generator loss: 8.273813]\n",
      "11727 [Discriminator loss: 0.058036, acc.: 100.00%] [Generator loss: 7.680297]\n",
      "11728 [Discriminator loss: 0.086489, acc.: 95.31%] [Generator loss: 6.065971]\n",
      "11729 [Discriminator loss: 0.118326, acc.: 95.31%] [Generator loss: 7.745905]\n",
      "11730 [Discriminator loss: 0.074908, acc.: 95.31%] [Generator loss: 6.706347]\n",
      "11731 [Discriminator loss: 0.132735, acc.: 95.31%] [Generator loss: 6.643132]\n",
      "11732 [Discriminator loss: 0.063550, acc.: 98.44%] [Generator loss: 6.297295]\n",
      "11733 [Discriminator loss: 0.196362, acc.: 90.62%] [Generator loss: 7.770450]\n",
      "11734 [Discriminator loss: 0.039655, acc.: 98.44%] [Generator loss: 7.326815]\n",
      "11735 [Discriminator loss: 0.145496, acc.: 92.19%] [Generator loss: 6.857704]\n",
      "11736 [Discriminator loss: 0.156550, acc.: 90.62%] [Generator loss: 7.163219]\n",
      "11737 [Discriminator loss: 0.316327, acc.: 84.38%] [Generator loss: 8.104544]\n",
      "11738 [Discriminator loss: 0.192140, acc.: 89.06%] [Generator loss: 6.611673]\n",
      "11739 [Discriminator loss: 0.095451, acc.: 96.88%] [Generator loss: 7.565899]\n",
      "11740 [Discriminator loss: 0.051807, acc.: 100.00%] [Generator loss: 7.793078]\n",
      "11741 [Discriminator loss: 0.188605, acc.: 92.19%] [Generator loss: 5.609721]\n",
      "11742 [Discriminator loss: 0.161513, acc.: 93.75%] [Generator loss: 8.558268]\n",
      "11743 [Discriminator loss: 0.090111, acc.: 96.88%] [Generator loss: 8.629727]\n",
      "11744 [Discriminator loss: 0.207334, acc.: 92.19%] [Generator loss: 5.696773]\n",
      "11745 [Discriminator loss: 0.141172, acc.: 92.19%] [Generator loss: 7.201379]\n",
      "11746 [Discriminator loss: 0.296672, acc.: 89.06%] [Generator loss: 6.210508]\n",
      "11747 [Discriminator loss: 0.083469, acc.: 95.31%] [Generator loss: 5.566310]\n",
      "11748 [Discriminator loss: 0.154672, acc.: 93.75%] [Generator loss: 7.341934]\n",
      "11749 [Discriminator loss: 0.068898, acc.: 98.44%] [Generator loss: 7.865718]\n",
      "11750 [Discriminator loss: 0.112545, acc.: 96.88%] [Generator loss: 6.902088]\n",
      "11751 [Discriminator loss: 0.120380, acc.: 93.75%] [Generator loss: 6.778885]\n",
      "11752 [Discriminator loss: 0.151985, acc.: 95.31%] [Generator loss: 6.099257]\n",
      "11753 [Discriminator loss: 0.114761, acc.: 95.31%] [Generator loss: 7.659872]\n",
      "11754 [Discriminator loss: 0.194624, acc.: 90.62%] [Generator loss: 6.309674]\n",
      "11755 [Discriminator loss: 0.043068, acc.: 96.88%] [Generator loss: 6.986334]\n",
      "11756 [Discriminator loss: 0.043630, acc.: 100.00%] [Generator loss: 7.393015]\n",
      "11757 [Discriminator loss: 0.201225, acc.: 90.62%] [Generator loss: 6.875130]\n",
      "11758 [Discriminator loss: 0.159758, acc.: 95.31%] [Generator loss: 7.083201]\n",
      "11759 [Discriminator loss: 0.051300, acc.: 98.44%] [Generator loss: 7.252555]\n",
      "11760 [Discriminator loss: 0.202058, acc.: 89.06%] [Generator loss: 7.995320]\n",
      "11761 [Discriminator loss: 0.064898, acc.: 98.44%] [Generator loss: 7.655473]\n",
      "11762 [Discriminator loss: 0.222271, acc.: 90.62%] [Generator loss: 9.317570]\n",
      "11763 [Discriminator loss: 0.104673, acc.: 95.31%] [Generator loss: 8.344193]\n",
      "11764 [Discriminator loss: 0.235648, acc.: 92.19%] [Generator loss: 6.434732]\n",
      "11765 [Discriminator loss: 0.215379, acc.: 92.19%] [Generator loss: 5.686594]\n",
      "11766 [Discriminator loss: 0.046031, acc.: 100.00%] [Generator loss: 6.818937]\n",
      "11767 [Discriminator loss: 0.080250, acc.: 96.88%] [Generator loss: 5.692585]\n",
      "11768 [Discriminator loss: 0.131295, acc.: 93.75%] [Generator loss: 7.765816]\n",
      "11769 [Discriminator loss: 0.027311, acc.: 98.44%] [Generator loss: 7.653549]\n",
      "11770 [Discriminator loss: 0.139993, acc.: 95.31%] [Generator loss: 6.535550]\n",
      "11771 [Discriminator loss: 0.133293, acc.: 93.75%] [Generator loss: 6.916064]\n",
      "11772 [Discriminator loss: 0.088111, acc.: 95.31%] [Generator loss: 6.751172]\n",
      "11773 [Discriminator loss: 0.143361, acc.: 95.31%] [Generator loss: 5.790229]\n",
      "11774 [Discriminator loss: 0.161579, acc.: 96.88%] [Generator loss: 7.785429]\n",
      "11775 [Discriminator loss: 0.151740, acc.: 93.75%] [Generator loss: 7.183800]\n",
      "11776 [Discriminator loss: 0.149367, acc.: 95.31%] [Generator loss: 7.045255]\n",
      "11777 [Discriminator loss: 0.103478, acc.: 98.44%] [Generator loss: 7.769606]\n",
      "11778 [Discriminator loss: 0.061403, acc.: 96.88%] [Generator loss: 7.423962]\n",
      "11779 [Discriminator loss: 0.073966, acc.: 98.44%] [Generator loss: 6.561577]\n",
      "11780 [Discriminator loss: 0.227766, acc.: 92.19%] [Generator loss: 6.274493]\n",
      "11781 [Discriminator loss: 0.038038, acc.: 98.44%] [Generator loss: 6.521621]\n",
      "11782 [Discriminator loss: 0.105883, acc.: 95.31%] [Generator loss: 5.884942]\n",
      "11783 [Discriminator loss: 0.332804, acc.: 85.94%] [Generator loss: 7.477176]\n",
      "11784 [Discriminator loss: 0.186839, acc.: 93.75%] [Generator loss: 5.564900]\n",
      "11785 [Discriminator loss: 0.177426, acc.: 92.19%] [Generator loss: 6.496240]\n",
      "11786 [Discriminator loss: 0.030016, acc.: 100.00%] [Generator loss: 6.467924]\n",
      "11787 [Discriminator loss: 0.159492, acc.: 92.19%] [Generator loss: 8.167426]\n",
      "11788 [Discriminator loss: 0.151943, acc.: 95.31%] [Generator loss: 7.732622]\n",
      "11789 [Discriminator loss: 0.082147, acc.: 95.31%] [Generator loss: 6.854939]\n",
      "11790 [Discriminator loss: 0.120359, acc.: 95.31%] [Generator loss: 8.000431]\n",
      "11791 [Discriminator loss: 0.299757, acc.: 90.62%] [Generator loss: 6.465152]\n",
      "11792 [Discriminator loss: 0.041774, acc.: 96.88%] [Generator loss: 7.585390]\n",
      "11793 [Discriminator loss: 0.133811, acc.: 95.31%] [Generator loss: 7.194991]\n",
      "11794 [Discriminator loss: 0.258036, acc.: 87.50%] [Generator loss: 7.189794]\n",
      "11795 [Discriminator loss: 0.090258, acc.: 96.88%] [Generator loss: 7.909043]\n",
      "11796 [Discriminator loss: 0.082821, acc.: 96.88%] [Generator loss: 6.629013]\n",
      "11797 [Discriminator loss: 0.492204, acc.: 81.25%] [Generator loss: 6.441267]\n",
      "11798 [Discriminator loss: 0.098125, acc.: 96.88%] [Generator loss: 6.720284]\n",
      "11799 [Discriminator loss: 0.051896, acc.: 98.44%] [Generator loss: 5.982629]\n",
      "11800 [Discriminator loss: 0.135129, acc.: 95.31%] [Generator loss: 6.246975]\n",
      "11801 [Discriminator loss: 0.091286, acc.: 96.88%] [Generator loss: 7.047127]\n",
      "11802 [Discriminator loss: 0.295667, acc.: 84.38%] [Generator loss: 5.703546]\n",
      "11803 [Discriminator loss: 0.050224, acc.: 100.00%] [Generator loss: 7.696901]\n",
      "11804 [Discriminator loss: 0.073214, acc.: 95.31%] [Generator loss: 7.026878]\n",
      "11805 [Discriminator loss: 0.058098, acc.: 98.44%] [Generator loss: 5.952232]\n",
      "11806 [Discriminator loss: 0.148769, acc.: 93.75%] [Generator loss: 7.949265]\n",
      "11807 [Discriminator loss: 0.423796, acc.: 87.50%] [Generator loss: 7.911595]\n",
      "11808 [Discriminator loss: 0.160091, acc.: 93.75%] [Generator loss: 6.527413]\n",
      "11809 [Discriminator loss: 0.091064, acc.: 95.31%] [Generator loss: 8.126240]\n",
      "11810 [Discriminator loss: 0.213183, acc.: 93.75%] [Generator loss: 7.580091]\n",
      "11811 [Discriminator loss: 0.241673, acc.: 95.31%] [Generator loss: 7.295667]\n",
      "11812 [Discriminator loss: 0.068427, acc.: 98.44%] [Generator loss: 5.927329]\n",
      "11813 [Discriminator loss: 0.137862, acc.: 92.19%] [Generator loss: 6.219309]\n",
      "11814 [Discriminator loss: 0.126106, acc.: 98.44%] [Generator loss: 7.294504]\n",
      "11815 [Discriminator loss: 0.100661, acc.: 95.31%] [Generator loss: 6.314224]\n",
      "11816 [Discriminator loss: 0.104691, acc.: 95.31%] [Generator loss: 5.730676]\n",
      "11817 [Discriminator loss: 0.141131, acc.: 93.75%] [Generator loss: 5.826890]\n",
      "11818 [Discriminator loss: 0.040603, acc.: 98.44%] [Generator loss: 8.227854]\n",
      "11819 [Discriminator loss: 0.216464, acc.: 89.06%] [Generator loss: 6.634834]\n",
      "11820 [Discriminator loss: 0.215038, acc.: 89.06%] [Generator loss: 7.194415]\n",
      "11821 [Discriminator loss: 0.235538, acc.: 87.50%] [Generator loss: 8.695139]\n",
      "11822 [Discriminator loss: 0.111704, acc.: 93.75%] [Generator loss: 7.603310]\n",
      "11823 [Discriminator loss: 0.099495, acc.: 92.19%] [Generator loss: 7.259094]\n",
      "11824 [Discriminator loss: 0.085116, acc.: 95.31%] [Generator loss: 6.730339]\n",
      "11825 [Discriminator loss: 0.049624, acc.: 96.88%] [Generator loss: 5.594729]\n",
      "11826 [Discriminator loss: 0.232326, acc.: 90.62%] [Generator loss: 7.382259]\n",
      "11827 [Discriminator loss: 0.054573, acc.: 98.44%] [Generator loss: 8.182927]\n",
      "11828 [Discriminator loss: 0.074696, acc.: 96.88%] [Generator loss: 7.789929]\n",
      "11829 [Discriminator loss: 0.399292, acc.: 81.25%] [Generator loss: 6.795354]\n",
      "11830 [Discriminator loss: 0.030843, acc.: 98.44%] [Generator loss: 6.607715]\n",
      "11831 [Discriminator loss: 0.218630, acc.: 90.62%] [Generator loss: 8.860498]\n",
      "11832 [Discriminator loss: 0.077940, acc.: 98.44%] [Generator loss: 7.408006]\n",
      "11833 [Discriminator loss: 0.122857, acc.: 96.88%] [Generator loss: 6.712646]\n",
      "11834 [Discriminator loss: 0.188596, acc.: 93.75%] [Generator loss: 6.329688]\n",
      "11835 [Discriminator loss: 0.072986, acc.: 98.44%] [Generator loss: 5.364157]\n",
      "11836 [Discriminator loss: 0.135389, acc.: 92.19%] [Generator loss: 6.002986]\n",
      "11837 [Discriminator loss: 0.116818, acc.: 95.31%] [Generator loss: 6.731461]\n",
      "11838 [Discriminator loss: 0.046715, acc.: 100.00%] [Generator loss: 6.169430]\n",
      "11839 [Discriminator loss: 0.253651, acc.: 87.50%] [Generator loss: 6.198636]\n",
      "11840 [Discriminator loss: 0.066373, acc.: 95.31%] [Generator loss: 7.885908]\n",
      "11841 [Discriminator loss: 0.102512, acc.: 95.31%] [Generator loss: 4.919851]\n",
      "11842 [Discriminator loss: 0.203802, acc.: 90.62%] [Generator loss: 7.413057]\n",
      "11843 [Discriminator loss: 0.086595, acc.: 95.31%] [Generator loss: 8.024049]\n",
      "11844 [Discriminator loss: 0.084258, acc.: 96.88%] [Generator loss: 6.442227]\n",
      "11845 [Discriminator loss: 0.195757, acc.: 90.62%] [Generator loss: 8.399935]\n",
      "11846 [Discriminator loss: 0.292041, acc.: 90.62%] [Generator loss: 8.296144]\n",
      "11847 [Discriminator loss: 0.111059, acc.: 96.88%] [Generator loss: 7.336136]\n",
      "11848 [Discriminator loss: 0.127249, acc.: 95.31%] [Generator loss: 6.973128]\n",
      "11849 [Discriminator loss: 0.151225, acc.: 93.75%] [Generator loss: 6.502790]\n",
      "11850 [Discriminator loss: 0.057875, acc.: 98.44%] [Generator loss: 7.128383]\n",
      "11851 [Discriminator loss: 0.157861, acc.: 92.19%] [Generator loss: 6.758593]\n",
      "11852 [Discriminator loss: 0.086349, acc.: 95.31%] [Generator loss: 6.681047]\n",
      "11853 [Discriminator loss: 0.065615, acc.: 98.44%] [Generator loss: 6.929564]\n",
      "11854 [Discriminator loss: 0.126094, acc.: 93.75%] [Generator loss: 5.575876]\n",
      "11855 [Discriminator loss: 0.114996, acc.: 92.19%] [Generator loss: 7.153798]\n",
      "11856 [Discriminator loss: 0.148124, acc.: 92.19%] [Generator loss: 8.493156]\n",
      "11857 [Discriminator loss: 0.159825, acc.: 95.31%] [Generator loss: 7.840456]\n",
      "11858 [Discriminator loss: 0.058174, acc.: 96.88%] [Generator loss: 7.587057]\n",
      "11859 [Discriminator loss: 0.102735, acc.: 96.88%] [Generator loss: 6.080304]\n",
      "11860 [Discriminator loss: 0.118347, acc.: 95.31%] [Generator loss: 6.775264]\n",
      "11861 [Discriminator loss: 0.158971, acc.: 92.19%] [Generator loss: 8.318872]\n",
      "11862 [Discriminator loss: 0.204430, acc.: 92.19%] [Generator loss: 7.418827]\n",
      "11863 [Discriminator loss: 0.295806, acc.: 90.62%] [Generator loss: 6.410768]\n",
      "11864 [Discriminator loss: 0.072579, acc.: 96.88%] [Generator loss: 6.107683]\n",
      "11865 [Discriminator loss: 0.138448, acc.: 93.75%] [Generator loss: 7.328679]\n",
      "11866 [Discriminator loss: 0.152362, acc.: 95.31%] [Generator loss: 7.684981]\n",
      "11867 [Discriminator loss: 0.135959, acc.: 93.75%] [Generator loss: 6.660584]\n",
      "11868 [Discriminator loss: 0.035191, acc.: 100.00%] [Generator loss: 6.181681]\n",
      "11869 [Discriminator loss: 0.091309, acc.: 93.75%] [Generator loss: 7.114917]\n",
      "11870 [Discriminator loss: 0.075535, acc.: 98.44%] [Generator loss: 6.075335]\n",
      "11871 [Discriminator loss: 0.190798, acc.: 90.62%] [Generator loss: 5.440397]\n",
      "11872 [Discriminator loss: 0.045076, acc.: 100.00%] [Generator loss: 6.365347]\n",
      "11873 [Discriminator loss: 0.040246, acc.: 100.00%] [Generator loss: 7.442506]\n",
      "11874 [Discriminator loss: 0.130936, acc.: 93.75%] [Generator loss: 6.500661]\n",
      "11875 [Discriminator loss: 0.154559, acc.: 96.88%] [Generator loss: 7.087886]\n",
      "11876 [Discriminator loss: 0.127293, acc.: 93.75%] [Generator loss: 8.365719]\n",
      "11877 [Discriminator loss: 0.120595, acc.: 93.75%] [Generator loss: 6.697017]\n",
      "11878 [Discriminator loss: 0.131900, acc.: 95.31%] [Generator loss: 8.191423]\n",
      "11879 [Discriminator loss: 0.046768, acc.: 96.88%] [Generator loss: 7.656385]\n",
      "11880 [Discriminator loss: 0.149132, acc.: 93.75%] [Generator loss: 7.371854]\n",
      "11881 [Discriminator loss: 0.060455, acc.: 98.44%] [Generator loss: 6.260861]\n",
      "11882 [Discriminator loss: 0.160167, acc.: 95.31%] [Generator loss: 6.215909]\n",
      "11883 [Discriminator loss: 0.127483, acc.: 93.75%] [Generator loss: 7.959876]\n",
      "11884 [Discriminator loss: 0.039343, acc.: 98.44%] [Generator loss: 7.641567]\n",
      "11885 [Discriminator loss: 0.117800, acc.: 95.31%] [Generator loss: 7.177193]\n",
      "11886 [Discriminator loss: 0.197157, acc.: 93.75%] [Generator loss: 5.921649]\n",
      "11887 [Discriminator loss: 0.107502, acc.: 96.88%] [Generator loss: 7.380925]\n",
      "11888 [Discriminator loss: 0.131705, acc.: 93.75%] [Generator loss: 5.862792]\n",
      "11889 [Discriminator loss: 0.214336, acc.: 92.19%] [Generator loss: 9.498545]\n",
      "11890 [Discriminator loss: 0.138743, acc.: 96.88%] [Generator loss: 7.263291]\n",
      "11891 [Discriminator loss: 0.128401, acc.: 93.75%] [Generator loss: 6.358379]\n",
      "11892 [Discriminator loss: 0.112001, acc.: 93.75%] [Generator loss: 6.609222]\n",
      "11893 [Discriminator loss: 0.121865, acc.: 95.31%] [Generator loss: 6.675580]\n",
      "11894 [Discriminator loss: 0.165605, acc.: 95.31%] [Generator loss: 7.209338]\n",
      "11895 [Discriminator loss: 0.127791, acc.: 95.31%] [Generator loss: 6.882441]\n",
      "11896 [Discriminator loss: 0.155868, acc.: 95.31%] [Generator loss: 6.406534]\n",
      "11897 [Discriminator loss: 0.117908, acc.: 93.75%] [Generator loss: 6.703428]\n",
      "11898 [Discriminator loss: 0.145161, acc.: 93.75%] [Generator loss: 6.392740]\n",
      "11899 [Discriminator loss: 0.095463, acc.: 96.88%] [Generator loss: 6.964056]\n",
      "11900 [Discriminator loss: 0.064494, acc.: 96.88%] [Generator loss: 6.685364]\n",
      "11901 [Discriminator loss: 0.079055, acc.: 98.44%] [Generator loss: 6.562663]\n",
      "11902 [Discriminator loss: 0.047803, acc.: 96.88%] [Generator loss: 7.511551]\n",
      "11903 [Discriminator loss: 0.056520, acc.: 96.88%] [Generator loss: 5.921877]\n",
      "11904 [Discriminator loss: 0.167731, acc.: 92.19%] [Generator loss: 7.080343]\n",
      "11905 [Discriminator loss: 0.074543, acc.: 96.88%] [Generator loss: 8.274262]\n",
      "11906 [Discriminator loss: 0.093332, acc.: 96.88%] [Generator loss: 7.490355]\n",
      "11907 [Discriminator loss: 0.082227, acc.: 98.44%] [Generator loss: 6.361794]\n",
      "11908 [Discriminator loss: 0.237514, acc.: 90.62%] [Generator loss: 5.527331]\n",
      "11909 [Discriminator loss: 0.072328, acc.: 96.88%] [Generator loss: 6.763098]\n",
      "11910 [Discriminator loss: 0.109836, acc.: 96.88%] [Generator loss: 7.093388]\n",
      "11911 [Discriminator loss: 0.122381, acc.: 95.31%] [Generator loss: 7.520898]\n",
      "11912 [Discriminator loss: 0.125338, acc.: 93.75%] [Generator loss: 6.729981]\n",
      "11913 [Discriminator loss: 0.031175, acc.: 100.00%] [Generator loss: 7.303170]\n",
      "11914 [Discriminator loss: 0.092312, acc.: 98.44%] [Generator loss: 6.723950]\n",
      "11915 [Discriminator loss: 0.021896, acc.: 100.00%] [Generator loss: 7.835537]\n",
      "11916 [Discriminator loss: 0.103697, acc.: 98.44%] [Generator loss: 7.423303]\n",
      "11917 [Discriminator loss: 0.315631, acc.: 89.06%] [Generator loss: 6.323878]\n",
      "11918 [Discriminator loss: 0.177243, acc.: 95.31%] [Generator loss: 6.122221]\n",
      "11919 [Discriminator loss: 0.037240, acc.: 100.00%] [Generator loss: 6.081702]\n",
      "11920 [Discriminator loss: 0.085357, acc.: 96.88%] [Generator loss: 6.705702]\n",
      "11921 [Discriminator loss: 0.049390, acc.: 98.44%] [Generator loss: 6.804182]\n",
      "11922 [Discriminator loss: 0.197098, acc.: 92.19%] [Generator loss: 6.677979]\n",
      "11923 [Discriminator loss: 0.127561, acc.: 93.75%] [Generator loss: 6.314988]\n",
      "11924 [Discriminator loss: 0.175922, acc.: 93.75%] [Generator loss: 6.927283]\n",
      "11925 [Discriminator loss: 0.089271, acc.: 96.88%] [Generator loss: 6.560365]\n",
      "11926 [Discriminator loss: 0.133864, acc.: 92.19%] [Generator loss: 7.627386]\n",
      "11927 [Discriminator loss: 0.041567, acc.: 98.44%] [Generator loss: 7.851905]\n",
      "11928 [Discriminator loss: 0.102154, acc.: 93.75%] [Generator loss: 6.796906]\n",
      "11929 [Discriminator loss: 0.154052, acc.: 93.75%] [Generator loss: 6.892697]\n",
      "11930 [Discriminator loss: 0.091333, acc.: 96.88%] [Generator loss: 6.289010]\n",
      "11931 [Discriminator loss: 0.218995, acc.: 90.62%] [Generator loss: 7.150226]\n",
      "11932 [Discriminator loss: 0.056231, acc.: 98.44%] [Generator loss: 6.342214]\n",
      "11933 [Discriminator loss: 0.103505, acc.: 93.75%] [Generator loss: 6.644072]\n",
      "11934 [Discriminator loss: 0.061113, acc.: 96.88%] [Generator loss: 6.632721]\n",
      "11935 [Discriminator loss: 0.157613, acc.: 93.75%] [Generator loss: 7.105212]\n",
      "11936 [Discriminator loss: 0.138023, acc.: 93.75%] [Generator loss: 6.716711]\n",
      "11937 [Discriminator loss: 0.158411, acc.: 93.75%] [Generator loss: 5.464915]\n",
      "11938 [Discriminator loss: 0.069374, acc.: 96.88%] [Generator loss: 6.900898]\n",
      "11939 [Discriminator loss: 0.067113, acc.: 96.88%] [Generator loss: 7.889609]\n",
      "11940 [Discriminator loss: 0.136565, acc.: 93.75%] [Generator loss: 7.548422]\n",
      "11941 [Discriminator loss: 0.051458, acc.: 98.44%] [Generator loss: 6.795762]\n",
      "11942 [Discriminator loss: 0.047184, acc.: 98.44%] [Generator loss: 6.936961]\n",
      "11943 [Discriminator loss: 0.160519, acc.: 92.19%] [Generator loss: 8.012611]\n",
      "11944 [Discriminator loss: 0.071547, acc.: 96.88%] [Generator loss: 9.121187]\n",
      "11945 [Discriminator loss: 0.125926, acc.: 96.88%] [Generator loss: 7.136161]\n",
      "11946 [Discriminator loss: 0.249952, acc.: 90.62%] [Generator loss: 7.496654]\n",
      "11947 [Discriminator loss: 0.062165, acc.: 96.88%] [Generator loss: 9.227418]\n",
      "11948 [Discriminator loss: 0.250109, acc.: 93.75%] [Generator loss: 6.322737]\n",
      "11949 [Discriminator loss: 0.054825, acc.: 98.44%] [Generator loss: 6.464186]\n",
      "11950 [Discriminator loss: 0.281575, acc.: 89.06%] [Generator loss: 6.000159]\n",
      "11951 [Discriminator loss: 0.081551, acc.: 96.88%] [Generator loss: 7.596338]\n",
      "11952 [Discriminator loss: 0.051357, acc.: 100.00%] [Generator loss: 7.963119]\n",
      "11953 [Discriminator loss: 0.082467, acc.: 96.88%] [Generator loss: 7.060824]\n",
      "11954 [Discriminator loss: 0.057501, acc.: 98.44%] [Generator loss: 7.832480]\n",
      "11955 [Discriminator loss: 0.219936, acc.: 92.19%] [Generator loss: 7.116482]\n",
      "11956 [Discriminator loss: 0.163848, acc.: 93.75%] [Generator loss: 7.172629]\n",
      "11957 [Discriminator loss: 0.161134, acc.: 93.75%] [Generator loss: 7.671507]\n",
      "11958 [Discriminator loss: 0.032280, acc.: 100.00%] [Generator loss: 6.623708]\n",
      "11959 [Discriminator loss: 0.096045, acc.: 95.31%] [Generator loss: 7.056362]\n",
      "11960 [Discriminator loss: 0.111498, acc.: 96.88%] [Generator loss: 7.206958]\n",
      "11961 [Discriminator loss: 0.132391, acc.: 93.75%] [Generator loss: 6.421995]\n",
      "11962 [Discriminator loss: 0.061442, acc.: 98.44%] [Generator loss: 6.330345]\n",
      "11963 [Discriminator loss: 0.066412, acc.: 98.44%] [Generator loss: 7.327866]\n",
      "11964 [Discriminator loss: 0.035070, acc.: 98.44%] [Generator loss: 6.314600]\n",
      "11965 [Discriminator loss: 0.167986, acc.: 90.62%] [Generator loss: 6.422363]\n",
      "11966 [Discriminator loss: 0.084104, acc.: 96.88%] [Generator loss: 7.381623]\n",
      "11967 [Discriminator loss: 0.097068, acc.: 93.75%] [Generator loss: 8.228777]\n",
      "11968 [Discriminator loss: 0.126494, acc.: 95.31%] [Generator loss: 7.992934]\n",
      "11969 [Discriminator loss: 0.137703, acc.: 95.31%] [Generator loss: 7.337897]\n",
      "11970 [Discriminator loss: 0.035657, acc.: 100.00%] [Generator loss: 7.776723]\n",
      "11971 [Discriminator loss: 0.073581, acc.: 98.44%] [Generator loss: 7.413124]\n",
      "11972 [Discriminator loss: 0.153116, acc.: 95.31%] [Generator loss: 9.173745]\n",
      "11973 [Discriminator loss: 0.172710, acc.: 90.62%] [Generator loss: 7.186498]\n",
      "11974 [Discriminator loss: 0.165546, acc.: 93.75%] [Generator loss: 8.088058]\n",
      "11975 [Discriminator loss: 0.149273, acc.: 95.31%] [Generator loss: 7.739244]\n",
      "11976 [Discriminator loss: 0.130864, acc.: 92.19%] [Generator loss: 6.970111]\n",
      "11977 [Discriminator loss: 0.097363, acc.: 96.88%] [Generator loss: 7.097814]\n",
      "11978 [Discriminator loss: 0.104975, acc.: 95.31%] [Generator loss: 7.374378]\n",
      "11979 [Discriminator loss: 0.179719, acc.: 90.62%] [Generator loss: 8.347999]\n",
      "11980 [Discriminator loss: 0.090180, acc.: 98.44%] [Generator loss: 7.538709]\n",
      "11981 [Discriminator loss: 0.187670, acc.: 90.62%] [Generator loss: 6.494291]\n",
      "11982 [Discriminator loss: 0.030949, acc.: 98.44%] [Generator loss: 6.527398]\n",
      "11983 [Discriminator loss: 0.200925, acc.: 89.06%] [Generator loss: 8.397351]\n",
      "11984 [Discriminator loss: 0.423349, acc.: 81.25%] [Generator loss: 7.268461]\n",
      "11985 [Discriminator loss: 0.157518, acc.: 90.62%] [Generator loss: 8.130518]\n",
      "11986 [Discriminator loss: 0.034263, acc.: 100.00%] [Generator loss: 8.078113]\n",
      "11987 [Discriminator loss: 0.075069, acc.: 96.88%] [Generator loss: 6.184826]\n",
      "11988 [Discriminator loss: 0.149710, acc.: 95.31%] [Generator loss: 8.035988]\n",
      "11989 [Discriminator loss: 0.129710, acc.: 95.31%] [Generator loss: 6.369766]\n",
      "11990 [Discriminator loss: 0.055578, acc.: 98.44%] [Generator loss: 7.123630]\n",
      "11991 [Discriminator loss: 0.071223, acc.: 98.44%] [Generator loss: 6.866290]\n",
      "11992 [Discriminator loss: 0.091814, acc.: 98.44%] [Generator loss: 3.970518]\n",
      "11993 [Discriminator loss: 0.188001, acc.: 93.75%] [Generator loss: 7.966650]\n",
      "11994 [Discriminator loss: 0.165631, acc.: 95.31%] [Generator loss: 6.976613]\n",
      "11995 [Discriminator loss: 0.077614, acc.: 98.44%] [Generator loss: 8.037155]\n",
      "11996 [Discriminator loss: 0.126721, acc.: 96.88%] [Generator loss: 5.326465]\n",
      "11997 [Discriminator loss: 0.191447, acc.: 92.19%] [Generator loss: 6.526152]\n",
      "11998 [Discriminator loss: 0.151359, acc.: 95.31%] [Generator loss: 6.417435]\n",
      "11999 [Discriminator loss: 0.188242, acc.: 90.62%] [Generator loss: 7.756304]\n",
      "12000 [Discriminator loss: 0.074459, acc.: 96.88%] [Generator loss: 6.831222]\n",
      "12001 [Discriminator loss: 0.163905, acc.: 92.19%] [Generator loss: 6.634066]\n",
      "12002 [Discriminator loss: 0.065448, acc.: 98.44%] [Generator loss: 7.101573]\n",
      "12003 [Discriminator loss: 0.131422, acc.: 95.31%] [Generator loss: 6.347274]\n",
      "12004 [Discriminator loss: 0.024007, acc.: 100.00%] [Generator loss: 5.784564]\n",
      "12005 [Discriminator loss: 0.153722, acc.: 90.62%] [Generator loss: 6.844968]\n",
      "12006 [Discriminator loss: 0.249879, acc.: 90.62%] [Generator loss: 8.268955]\n",
      "12007 [Discriminator loss: 0.073217, acc.: 96.88%] [Generator loss: 8.483327]\n",
      "12008 [Discriminator loss: 0.163261, acc.: 89.06%] [Generator loss: 6.222443]\n",
      "12009 [Discriminator loss: 0.100535, acc.: 96.88%] [Generator loss: 5.970922]\n",
      "12010 [Discriminator loss: 0.176129, acc.: 92.19%] [Generator loss: 7.261863]\n",
      "12011 [Discriminator loss: 0.065030, acc.: 96.88%] [Generator loss: 7.919746]\n",
      "12012 [Discriminator loss: 0.185459, acc.: 92.19%] [Generator loss: 7.687178]\n",
      "12013 [Discriminator loss: 0.088619, acc.: 98.44%] [Generator loss: 7.227736]\n",
      "12014 [Discriminator loss: 0.087969, acc.: 96.88%] [Generator loss: 6.194385]\n",
      "12015 [Discriminator loss: 0.192088, acc.: 93.75%] [Generator loss: 6.316644]\n",
      "12016 [Discriminator loss: 0.145841, acc.: 95.31%] [Generator loss: 4.885811]\n",
      "12017 [Discriminator loss: 0.043594, acc.: 98.44%] [Generator loss: 6.828303]\n",
      "12018 [Discriminator loss: 0.048430, acc.: 98.44%] [Generator loss: 6.257422]\n",
      "12019 [Discriminator loss: 0.107874, acc.: 96.88%] [Generator loss: 6.324360]\n",
      "12020 [Discriminator loss: 0.042907, acc.: 100.00%] [Generator loss: 6.236472]\n",
      "12021 [Discriminator loss: 0.243565, acc.: 89.06%] [Generator loss: 8.617778]\n",
      "12022 [Discriminator loss: 0.057023, acc.: 98.44%] [Generator loss: 8.900297]\n",
      "12023 [Discriminator loss: 0.114778, acc.: 96.88%] [Generator loss: 8.231731]\n",
      "12024 [Discriminator loss: 0.119337, acc.: 96.88%] [Generator loss: 7.338953]\n",
      "12025 [Discriminator loss: 0.164244, acc.: 92.19%] [Generator loss: 7.299058]\n",
      "12026 [Discriminator loss: 0.144998, acc.: 93.75%] [Generator loss: 7.173307]\n",
      "12027 [Discriminator loss: 0.052518, acc.: 98.44%] [Generator loss: 9.561217]\n",
      "12028 [Discriminator loss: 0.127103, acc.: 93.75%] [Generator loss: 6.905185]\n",
      "12029 [Discriminator loss: 0.111992, acc.: 96.88%] [Generator loss: 5.824716]\n",
      "12030 [Discriminator loss: 0.131388, acc.: 92.19%] [Generator loss: 9.106258]\n",
      "12031 [Discriminator loss: 0.171706, acc.: 92.19%] [Generator loss: 7.842112]\n",
      "12032 [Discriminator loss: 0.330408, acc.: 93.75%] [Generator loss: 6.762244]\n",
      "12033 [Discriminator loss: 0.124589, acc.: 96.88%] [Generator loss: 5.973149]\n",
      "12034 [Discriminator loss: 0.179536, acc.: 92.19%] [Generator loss: 7.469340]\n",
      "12035 [Discriminator loss: 0.066867, acc.: 98.44%] [Generator loss: 8.033432]\n",
      "12036 [Discriminator loss: 0.140358, acc.: 93.75%] [Generator loss: 7.426197]\n",
      "12037 [Discriminator loss: 0.118107, acc.: 95.31%] [Generator loss: 5.716713]\n",
      "12038 [Discriminator loss: 0.112134, acc.: 95.31%] [Generator loss: 7.510001]\n",
      "12039 [Discriminator loss: 0.050299, acc.: 98.44%] [Generator loss: 8.084427]\n",
      "12040 [Discriminator loss: 0.123095, acc.: 93.75%] [Generator loss: 6.745879]\n",
      "12041 [Discriminator loss: 0.074774, acc.: 96.88%] [Generator loss: 6.464293]\n",
      "12042 [Discriminator loss: 0.171324, acc.: 90.62%] [Generator loss: 7.464130]\n",
      "12043 [Discriminator loss: 0.053807, acc.: 98.44%] [Generator loss: 8.386742]\n",
      "12044 [Discriminator loss: 0.209245, acc.: 93.75%] [Generator loss: 5.876568]\n",
      "12045 [Discriminator loss: 0.041812, acc.: 100.00%] [Generator loss: 7.397476]\n",
      "12046 [Discriminator loss: 0.150968, acc.: 95.31%] [Generator loss: 7.594189]\n",
      "12047 [Discriminator loss: 0.035983, acc.: 100.00%] [Generator loss: 8.108107]\n",
      "12048 [Discriminator loss: 0.132502, acc.: 95.31%] [Generator loss: 7.666718]\n",
      "12049 [Discriminator loss: 0.084944, acc.: 95.31%] [Generator loss: 7.822599]\n",
      "12050 [Discriminator loss: 0.111063, acc.: 96.88%] [Generator loss: 7.244826]\n",
      "12051 [Discriminator loss: 0.149015, acc.: 90.62%] [Generator loss: 6.456214]\n",
      "12052 [Discriminator loss: 0.045463, acc.: 98.44%] [Generator loss: 8.517679]\n",
      "12053 [Discriminator loss: 0.059550, acc.: 98.44%] [Generator loss: 5.400078]\n",
      "12054 [Discriminator loss: 0.160472, acc.: 92.19%] [Generator loss: 5.437850]\n",
      "12055 [Discriminator loss: 0.082415, acc.: 98.44%] [Generator loss: 6.736696]\n",
      "12056 [Discriminator loss: 0.044132, acc.: 96.88%] [Generator loss: 6.868874]\n",
      "12057 [Discriminator loss: 0.072887, acc.: 98.44%] [Generator loss: 6.915522]\n",
      "12058 [Discriminator loss: 0.123537, acc.: 96.88%] [Generator loss: 6.615678]\n",
      "12059 [Discriminator loss: 0.122279, acc.: 93.75%] [Generator loss: 5.506209]\n",
      "12060 [Discriminator loss: 0.071520, acc.: 96.88%] [Generator loss: 5.787818]\n",
      "12061 [Discriminator loss: 0.099199, acc.: 93.75%] [Generator loss: 7.576404]\n",
      "12062 [Discriminator loss: 0.124926, acc.: 93.75%] [Generator loss: 6.834583]\n",
      "12063 [Discriminator loss: 0.126630, acc.: 92.19%] [Generator loss: 8.716881]\n",
      "12064 [Discriminator loss: 0.056161, acc.: 98.44%] [Generator loss: 6.638275]\n",
      "12065 [Discriminator loss: 0.155907, acc.: 90.62%] [Generator loss: 7.762563]\n",
      "12066 [Discriminator loss: 0.069246, acc.: 96.88%] [Generator loss: 7.301967]\n",
      "12067 [Discriminator loss: 0.079539, acc.: 96.88%] [Generator loss: 6.559731]\n",
      "12068 [Discriminator loss: 0.225639, acc.: 90.62%] [Generator loss: 8.398899]\n",
      "12069 [Discriminator loss: 0.184946, acc.: 98.44%] [Generator loss: 8.183269]\n",
      "12070 [Discriminator loss: 0.104021, acc.: 95.31%] [Generator loss: 7.468472]\n",
      "12071 [Discriminator loss: 0.078488, acc.: 98.44%] [Generator loss: 6.911912]\n",
      "12072 [Discriminator loss: 0.035132, acc.: 98.44%] [Generator loss: 7.098323]\n",
      "12073 [Discriminator loss: 0.178908, acc.: 93.75%] [Generator loss: 5.766026]\n",
      "12074 [Discriminator loss: 0.169824, acc.: 93.75%] [Generator loss: 6.116024]\n",
      "12075 [Discriminator loss: 0.035230, acc.: 100.00%] [Generator loss: 6.716653]\n",
      "12076 [Discriminator loss: 0.222726, acc.: 90.62%] [Generator loss: 7.659930]\n",
      "12077 [Discriminator loss: 0.076561, acc.: 96.88%] [Generator loss: 7.583919]\n",
      "12078 [Discriminator loss: 0.109541, acc.: 93.75%] [Generator loss: 8.374063]\n",
      "12079 [Discriminator loss: 0.065581, acc.: 98.44%] [Generator loss: 6.894746]\n",
      "12080 [Discriminator loss: 0.079265, acc.: 96.88%] [Generator loss: 8.156620]\n",
      "12081 [Discriminator loss: 0.287766, acc.: 93.75%] [Generator loss: 6.607337]\n",
      "12082 [Discriminator loss: 0.043854, acc.: 100.00%] [Generator loss: 5.589089]\n",
      "12083 [Discriminator loss: 0.085673, acc.: 98.44%] [Generator loss: 8.046753]\n",
      "12084 [Discriminator loss: 0.066448, acc.: 98.44%] [Generator loss: 7.301332]\n",
      "12085 [Discriminator loss: 0.159234, acc.: 96.88%] [Generator loss: 5.509634]\n",
      "12086 [Discriminator loss: 0.076365, acc.: 98.44%] [Generator loss: 6.642346]\n",
      "12087 [Discriminator loss: 0.049127, acc.: 100.00%] [Generator loss: 8.035072]\n",
      "12088 [Discriminator loss: 0.368576, acc.: 85.94%] [Generator loss: 6.556526]\n",
      "12089 [Discriminator loss: 0.197517, acc.: 93.75%] [Generator loss: 7.714849]\n",
      "12090 [Discriminator loss: 0.102222, acc.: 93.75%] [Generator loss: 8.101160]\n",
      "12091 [Discriminator loss: 0.075889, acc.: 96.88%] [Generator loss: 7.819640]\n",
      "12092 [Discriminator loss: 0.012395, acc.: 100.00%] [Generator loss: 7.956567]\n",
      "12093 [Discriminator loss: 0.177780, acc.: 92.19%] [Generator loss: 6.023738]\n",
      "12094 [Discriminator loss: 0.067003, acc.: 96.88%] [Generator loss: 6.107347]\n",
      "12095 [Discriminator loss: 0.182245, acc.: 92.19%] [Generator loss: 6.970830]\n",
      "12096 [Discriminator loss: 0.053887, acc.: 98.44%] [Generator loss: 6.616765]\n",
      "12097 [Discriminator loss: 0.047310, acc.: 100.00%] [Generator loss: 7.125561]\n",
      "12098 [Discriminator loss: 0.076330, acc.: 98.44%] [Generator loss: 5.357473]\n",
      "12099 [Discriminator loss: 0.115997, acc.: 92.19%] [Generator loss: 6.950305]\n",
      "12100 [Discriminator loss: 0.208879, acc.: 90.62%] [Generator loss: 7.054353]\n",
      "12101 [Discriminator loss: 0.111766, acc.: 98.44%] [Generator loss: 7.315355]\n",
      "12102 [Discriminator loss: 0.165346, acc.: 92.19%] [Generator loss: 6.906012]\n",
      "12103 [Discriminator loss: 0.073307, acc.: 96.88%] [Generator loss: 6.548580]\n",
      "12104 [Discriminator loss: 0.027173, acc.: 100.00%] [Generator loss: 7.152613]\n",
      "12105 [Discriminator loss: 0.114810, acc.: 93.75%] [Generator loss: 6.786530]\n",
      "12106 [Discriminator loss: 0.122515, acc.: 95.31%] [Generator loss: 8.332527]\n",
      "12107 [Discriminator loss: 0.039789, acc.: 98.44%] [Generator loss: 7.354704]\n",
      "12108 [Discriminator loss: 0.048390, acc.: 100.00%] [Generator loss: 6.961299]\n",
      "12109 [Discriminator loss: 0.119112, acc.: 95.31%] [Generator loss: 8.182636]\n",
      "12110 [Discriminator loss: 0.061603, acc.: 98.44%] [Generator loss: 7.251477]\n",
      "12111 [Discriminator loss: 0.098413, acc.: 96.88%] [Generator loss: 7.467030]\n",
      "12112 [Discriminator loss: 0.085852, acc.: 96.88%] [Generator loss: 7.665182]\n",
      "12113 [Discriminator loss: 0.057495, acc.: 96.88%] [Generator loss: 6.057826]\n",
      "12114 [Discriminator loss: 0.224099, acc.: 89.06%] [Generator loss: 7.420852]\n",
      "12115 [Discriminator loss: 0.096544, acc.: 96.88%] [Generator loss: 7.753423]\n",
      "12116 [Discriminator loss: 0.031391, acc.: 100.00%] [Generator loss: 7.261588]\n",
      "12117 [Discriminator loss: 0.169581, acc.: 92.19%] [Generator loss: 7.243922]\n",
      "12118 [Discriminator loss: 0.076397, acc.: 96.88%] [Generator loss: 7.734353]\n",
      "12119 [Discriminator loss: 0.139239, acc.: 96.88%] [Generator loss: 6.168561]\n",
      "12120 [Discriminator loss: 0.084839, acc.: 98.44%] [Generator loss: 6.771686]\n",
      "12121 [Discriminator loss: 0.072760, acc.: 96.88%] [Generator loss: 7.946797]\n",
      "12122 [Discriminator loss: 0.154930, acc.: 90.62%] [Generator loss: 6.665389]\n",
      "12123 [Discriminator loss: 0.108376, acc.: 95.31%] [Generator loss: 6.683866]\n",
      "12124 [Discriminator loss: 0.085501, acc.: 98.44%] [Generator loss: 7.008267]\n",
      "12125 [Discriminator loss: 0.077229, acc.: 96.88%] [Generator loss: 5.945912]\n",
      "12126 [Discriminator loss: 0.102845, acc.: 95.31%] [Generator loss: 7.477649]\n",
      "12127 [Discriminator loss: 0.133625, acc.: 96.88%] [Generator loss: 8.343971]\n",
      "12128 [Discriminator loss: 0.130105, acc.: 95.31%] [Generator loss: 8.366820]\n",
      "12129 [Discriminator loss: 0.089912, acc.: 96.88%] [Generator loss: 7.693310]\n",
      "12130 [Discriminator loss: 0.176339, acc.: 93.75%] [Generator loss: 5.840975]\n",
      "12131 [Discriminator loss: 0.114164, acc.: 96.88%] [Generator loss: 7.078771]\n",
      "12132 [Discriminator loss: 0.031934, acc.: 100.00%] [Generator loss: 7.966557]\n",
      "12133 [Discriminator loss: 0.176037, acc.: 93.75%] [Generator loss: 6.203942]\n",
      "12134 [Discriminator loss: 0.076813, acc.: 96.88%] [Generator loss: 6.841547]\n",
      "12135 [Discriminator loss: 0.155699, acc.: 92.19%] [Generator loss: 7.795647]\n",
      "12136 [Discriminator loss: 0.086520, acc.: 96.88%] [Generator loss: 7.288511]\n",
      "12137 [Discriminator loss: 0.141104, acc.: 93.75%] [Generator loss: 8.658648]\n",
      "12138 [Discriminator loss: 0.121785, acc.: 95.31%] [Generator loss: 7.066332]\n",
      "12139 [Discriminator loss: 0.111660, acc.: 93.75%] [Generator loss: 6.570762]\n",
      "12140 [Discriminator loss: 0.090473, acc.: 95.31%] [Generator loss: 8.446934]\n",
      "12141 [Discriminator loss: 0.162339, acc.: 93.75%] [Generator loss: 7.976912]\n",
      "12142 [Discriminator loss: 0.040574, acc.: 100.00%] [Generator loss: 7.774146]\n",
      "12143 [Discriminator loss: 0.151125, acc.: 93.75%] [Generator loss: 5.781579]\n",
      "12144 [Discriminator loss: 0.051869, acc.: 96.88%] [Generator loss: 5.300029]\n",
      "12145 [Discriminator loss: 0.082955, acc.: 96.88%] [Generator loss: 6.483232]\n",
      "12146 [Discriminator loss: 0.100286, acc.: 95.31%] [Generator loss: 6.758123]\n",
      "12147 [Discriminator loss: 0.073261, acc.: 95.31%] [Generator loss: 8.042117]\n",
      "12148 [Discriminator loss: 0.035931, acc.: 100.00%] [Generator loss: 9.115211]\n",
      "12149 [Discriminator loss: 0.077149, acc.: 96.88%] [Generator loss: 7.291224]\n",
      "12150 [Discriminator loss: 0.227832, acc.: 90.62%] [Generator loss: 6.172231]\n",
      "12151 [Discriminator loss: 0.081952, acc.: 98.44%] [Generator loss: 8.012956]\n",
      "12152 [Discriminator loss: 0.097009, acc.: 95.31%] [Generator loss: 7.990396]\n",
      "12153 [Discriminator loss: 0.231059, acc.: 92.19%] [Generator loss: 7.816020]\n",
      "12154 [Discriminator loss: 0.040753, acc.: 100.00%] [Generator loss: 6.822296]\n",
      "12155 [Discriminator loss: 0.103282, acc.: 96.88%] [Generator loss: 8.120142]\n",
      "12156 [Discriminator loss: 0.160954, acc.: 93.75%] [Generator loss: 8.919355]\n",
      "12157 [Discriminator loss: 0.180545, acc.: 90.62%] [Generator loss: 7.119544]\n",
      "12158 [Discriminator loss: 0.063320, acc.: 98.44%] [Generator loss: 6.463308]\n",
      "12159 [Discriminator loss: 0.022753, acc.: 100.00%] [Generator loss: 6.911750]\n",
      "12160 [Discriminator loss: 0.084737, acc.: 96.88%] [Generator loss: 6.886257]\n",
      "12161 [Discriminator loss: 0.142890, acc.: 95.31%] [Generator loss: 8.311907]\n",
      "12162 [Discriminator loss: 0.142007, acc.: 92.19%] [Generator loss: 7.023233]\n",
      "12163 [Discriminator loss: 0.195745, acc.: 92.19%] [Generator loss: 8.129977]\n",
      "12164 [Discriminator loss: 0.041373, acc.: 98.44%] [Generator loss: 6.987083]\n",
      "12165 [Discriminator loss: 0.273178, acc.: 90.62%] [Generator loss: 6.043985]\n",
      "12166 [Discriminator loss: 0.050634, acc.: 98.44%] [Generator loss: 6.930749]\n",
      "12167 [Discriminator loss: 0.071648, acc.: 95.31%] [Generator loss: 7.578851]\n",
      "12168 [Discriminator loss: 0.041897, acc.: 100.00%] [Generator loss: 8.954128]\n",
      "12169 [Discriminator loss: 0.095164, acc.: 96.88%] [Generator loss: 6.420356]\n",
      "12170 [Discriminator loss: 0.135353, acc.: 92.19%] [Generator loss: 7.404801]\n",
      "12171 [Discriminator loss: 0.085722, acc.: 96.88%] [Generator loss: 7.271234]\n",
      "12172 [Discriminator loss: 0.101833, acc.: 98.44%] [Generator loss: 6.610045]\n",
      "12173 [Discriminator loss: 0.118521, acc.: 96.88%] [Generator loss: 7.652896]\n",
      "12174 [Discriminator loss: 0.018136, acc.: 100.00%] [Generator loss: 7.270362]\n",
      "12175 [Discriminator loss: 0.073043, acc.: 96.88%] [Generator loss: 8.046279]\n",
      "12176 [Discriminator loss: 0.061421, acc.: 96.88%] [Generator loss: 7.221272]\n",
      "12177 [Discriminator loss: 0.041085, acc.: 98.44%] [Generator loss: 6.597545]\n",
      "12178 [Discriminator loss: 0.093404, acc.: 96.88%] [Generator loss: 6.861711]\n",
      "12179 [Discriminator loss: 0.092671, acc.: 96.88%] [Generator loss: 7.455068]\n",
      "12180 [Discriminator loss: 0.068335, acc.: 96.88%] [Generator loss: 7.114388]\n",
      "12181 [Discriminator loss: 0.042288, acc.: 98.44%] [Generator loss: 6.673036]\n",
      "12182 [Discriminator loss: 0.042347, acc.: 98.44%] [Generator loss: 6.979540]\n",
      "12183 [Discriminator loss: 0.193719, acc.: 90.62%] [Generator loss: 5.765997]\n",
      "12184 [Discriminator loss: 0.088777, acc.: 96.88%] [Generator loss: 6.975834]\n",
      "12185 [Discriminator loss: 0.039848, acc.: 100.00%] [Generator loss: 7.823151]\n",
      "12186 [Discriminator loss: 0.160144, acc.: 95.31%] [Generator loss: 7.533521]\n",
      "12187 [Discriminator loss: 0.207726, acc.: 95.31%] [Generator loss: 5.708277]\n",
      "12188 [Discriminator loss: 0.145107, acc.: 93.75%] [Generator loss: 6.360609]\n",
      "12189 [Discriminator loss: 0.232642, acc.: 92.19%] [Generator loss: 7.337990]\n",
      "12190 [Discriminator loss: 0.065858, acc.: 98.44%] [Generator loss: 7.880524]\n",
      "12191 [Discriminator loss: 0.104045, acc.: 96.88%] [Generator loss: 8.189512]\n",
      "12192 [Discriminator loss: 0.169993, acc.: 96.88%] [Generator loss: 6.823525]\n",
      "12193 [Discriminator loss: 0.085957, acc.: 95.31%] [Generator loss: 7.147321]\n",
      "12194 [Discriminator loss: 0.059281, acc.: 100.00%] [Generator loss: 6.375988]\n",
      "12195 [Discriminator loss: 0.036678, acc.: 100.00%] [Generator loss: 7.452467]\n",
      "12196 [Discriminator loss: 0.047231, acc.: 98.44%] [Generator loss: 6.686940]\n",
      "12197 [Discriminator loss: 0.072005, acc.: 96.88%] [Generator loss: 6.080257]\n",
      "12198 [Discriminator loss: 0.055141, acc.: 98.44%] [Generator loss: 6.415353]\n",
      "12199 [Discriminator loss: 0.218415, acc.: 89.06%] [Generator loss: 7.539089]\n",
      "12200 [Discriminator loss: 0.111157, acc.: 96.88%] [Generator loss: 7.489838]\n",
      "12201 [Discriminator loss: 0.242710, acc.: 95.31%] [Generator loss: 6.407393]\n",
      "12202 [Discriminator loss: 0.267576, acc.: 90.62%] [Generator loss: 7.334361]\n",
      "12203 [Discriminator loss: 0.128746, acc.: 95.31%] [Generator loss: 7.955132]\n",
      "12204 [Discriminator loss: 0.185470, acc.: 93.75%] [Generator loss: 7.867848]\n",
      "12205 [Discriminator loss: 0.164001, acc.: 95.31%] [Generator loss: 8.884592]\n",
      "12206 [Discriminator loss: 0.188165, acc.: 89.06%] [Generator loss: 5.773116]\n",
      "12207 [Discriminator loss: 0.142932, acc.: 93.75%] [Generator loss: 6.468979]\n",
      "12208 [Discriminator loss: 0.136036, acc.: 96.88%] [Generator loss: 7.050401]\n",
      "12209 [Discriminator loss: 0.069042, acc.: 98.44%] [Generator loss: 6.968968]\n",
      "12210 [Discriminator loss: 0.205910, acc.: 89.06%] [Generator loss: 4.867054]\n",
      "12211 [Discriminator loss: 0.029510, acc.: 98.44%] [Generator loss: 5.634578]\n",
      "12212 [Discriminator loss: 0.176753, acc.: 92.19%] [Generator loss: 7.875605]\n",
      "12213 [Discriminator loss: 0.250738, acc.: 89.06%] [Generator loss: 6.308455]\n",
      "12214 [Discriminator loss: 0.048862, acc.: 98.44%] [Generator loss: 6.812399]\n",
      "12215 [Discriminator loss: 0.027352, acc.: 100.00%] [Generator loss: 6.740507]\n",
      "12216 [Discriminator loss: 0.284777, acc.: 95.31%] [Generator loss: 7.673114]\n",
      "12217 [Discriminator loss: 0.154703, acc.: 95.31%] [Generator loss: 7.236394]\n",
      "12218 [Discriminator loss: 0.274753, acc.: 90.62%] [Generator loss: 6.934221]\n",
      "12219 [Discriminator loss: 0.115335, acc.: 95.31%] [Generator loss: 7.261871]\n",
      "12220 [Discriminator loss: 0.156895, acc.: 90.62%] [Generator loss: 6.265489]\n",
      "12221 [Discriminator loss: 0.027651, acc.: 98.44%] [Generator loss: 6.505864]\n",
      "12222 [Discriminator loss: 0.189260, acc.: 93.75%] [Generator loss: 7.007785]\n",
      "12223 [Discriminator loss: 0.057295, acc.: 100.00%] [Generator loss: 6.191449]\n",
      "12224 [Discriminator loss: 0.129684, acc.: 95.31%] [Generator loss: 7.564946]\n",
      "12225 [Discriminator loss: 0.159939, acc.: 93.75%] [Generator loss: 7.201077]\n",
      "12226 [Discriminator loss: 0.122002, acc.: 90.62%] [Generator loss: 6.300282]\n",
      "12227 [Discriminator loss: 0.139885, acc.: 95.31%] [Generator loss: 6.543440]\n",
      "12228 [Discriminator loss: 0.175981, acc.: 89.06%] [Generator loss: 7.405349]\n",
      "12229 [Discriminator loss: 0.050707, acc.: 98.44%] [Generator loss: 7.746002]\n",
      "12230 [Discriminator loss: 0.101174, acc.: 96.88%] [Generator loss: 7.089763]\n",
      "12231 [Discriminator loss: 0.087944, acc.: 98.44%] [Generator loss: 7.971394]\n",
      "12232 [Discriminator loss: 0.180639, acc.: 92.19%] [Generator loss: 6.437576]\n",
      "12233 [Discriminator loss: 0.087171, acc.: 96.88%] [Generator loss: 6.995839]\n",
      "12234 [Discriminator loss: 0.081427, acc.: 96.88%] [Generator loss: 7.913748]\n",
      "12235 [Discriminator loss: 0.275387, acc.: 93.75%] [Generator loss: 6.516340]\n",
      "12236 [Discriminator loss: 0.095064, acc.: 96.88%] [Generator loss: 6.447319]\n",
      "12237 [Discriminator loss: 0.013365, acc.: 100.00%] [Generator loss: 7.318185]\n",
      "12238 [Discriminator loss: 0.020731, acc.: 100.00%] [Generator loss: 6.884744]\n",
      "12239 [Discriminator loss: 0.074463, acc.: 96.88%] [Generator loss: 6.890326]\n",
      "12240 [Discriminator loss: 0.071940, acc.: 96.88%] [Generator loss: 6.596715]\n",
      "12241 [Discriminator loss: 0.026961, acc.: 100.00%] [Generator loss: 6.011018]\n",
      "12242 [Discriminator loss: 0.090476, acc.: 96.88%] [Generator loss: 6.600087]\n",
      "12243 [Discriminator loss: 0.119785, acc.: 95.31%] [Generator loss: 6.509356]\n",
      "12244 [Discriminator loss: 0.286516, acc.: 89.06%] [Generator loss: 6.225803]\n",
      "12245 [Discriminator loss: 0.116191, acc.: 95.31%] [Generator loss: 6.214180]\n",
      "12246 [Discriminator loss: 0.105857, acc.: 95.31%] [Generator loss: 7.829424]\n",
      "12247 [Discriminator loss: 0.019124, acc.: 98.44%] [Generator loss: 8.463910]\n",
      "12248 [Discriminator loss: 0.061340, acc.: 96.88%] [Generator loss: 7.991601]\n",
      "12249 [Discriminator loss: 0.203534, acc.: 95.31%] [Generator loss: 5.881084]\n",
      "12250 [Discriminator loss: 0.170805, acc.: 92.19%] [Generator loss: 8.354181]\n",
      "12251 [Discriminator loss: 0.060829, acc.: 98.44%] [Generator loss: 6.464575]\n",
      "12252 [Discriminator loss: 0.048357, acc.: 98.44%] [Generator loss: 6.856436]\n",
      "12253 [Discriminator loss: 0.166426, acc.: 93.75%] [Generator loss: 6.541722]\n",
      "12254 [Discriminator loss: 0.089391, acc.: 95.31%] [Generator loss: 6.892111]\n",
      "12255 [Discriminator loss: 0.100281, acc.: 96.88%] [Generator loss: 8.509839]\n",
      "12256 [Discriminator loss: 0.110616, acc.: 92.19%] [Generator loss: 5.868303]\n",
      "12257 [Discriminator loss: 0.081461, acc.: 96.88%] [Generator loss: 5.726234]\n",
      "12258 [Discriminator loss: 0.076346, acc.: 96.88%] [Generator loss: 7.400990]\n",
      "12259 [Discriminator loss: 0.055506, acc.: 98.44%] [Generator loss: 6.003346]\n",
      "12260 [Discriminator loss: 0.138403, acc.: 95.31%] [Generator loss: 7.267096]\n",
      "12261 [Discriminator loss: 0.214778, acc.: 90.62%] [Generator loss: 5.148067]\n",
      "12262 [Discriminator loss: 0.075099, acc.: 96.88%] [Generator loss: 6.415687]\n",
      "12263 [Discriminator loss: 0.066078, acc.: 98.44%] [Generator loss: 6.937225]\n",
      "12264 [Discriminator loss: 0.098367, acc.: 96.88%] [Generator loss: 7.196796]\n",
      "12265 [Discriminator loss: 0.055787, acc.: 98.44%] [Generator loss: 6.291828]\n",
      "12266 [Discriminator loss: 0.065919, acc.: 98.44%] [Generator loss: 7.410023]\n",
      "12267 [Discriminator loss: 0.174836, acc.: 95.31%] [Generator loss: 6.975576]\n",
      "12268 [Discriminator loss: 0.075449, acc.: 96.88%] [Generator loss: 6.954832]\n",
      "12269 [Discriminator loss: 0.110518, acc.: 93.75%] [Generator loss: 7.785645]\n",
      "12270 [Discriminator loss: 0.007613, acc.: 100.00%] [Generator loss: 8.551323]\n",
      "12271 [Discriminator loss: 0.210324, acc.: 93.75%] [Generator loss: 5.028850]\n",
      "12272 [Discriminator loss: 0.045073, acc.: 98.44%] [Generator loss: 6.463419]\n",
      "12273 [Discriminator loss: 0.080867, acc.: 96.88%] [Generator loss: 6.860521]\n",
      "12274 [Discriminator loss: 0.103731, acc.: 96.88%] [Generator loss: 7.442986]\n",
      "12275 [Discriminator loss: 0.038455, acc.: 98.44%] [Generator loss: 7.635782]\n",
      "12276 [Discriminator loss: 0.124081, acc.: 95.31%] [Generator loss: 6.500287]\n",
      "12277 [Discriminator loss: 0.116484, acc.: 96.88%] [Generator loss: 7.530486]\n",
      "12278 [Discriminator loss: 0.044674, acc.: 100.00%] [Generator loss: 8.194506]\n",
      "12279 [Discriminator loss: 0.032168, acc.: 100.00%] [Generator loss: 7.805419]\n",
      "12280 [Discriminator loss: 0.269767, acc.: 87.50%] [Generator loss: 5.649394]\n",
      "12281 [Discriminator loss: 0.114062, acc.: 96.88%] [Generator loss: 6.555202]\n",
      "12282 [Discriminator loss: 0.028307, acc.: 100.00%] [Generator loss: 5.728415]\n",
      "12283 [Discriminator loss: 0.233974, acc.: 90.62%] [Generator loss: 5.674253]\n",
      "12284 [Discriminator loss: 0.044518, acc.: 98.44%] [Generator loss: 7.008986]\n",
      "12285 [Discriminator loss: 0.027176, acc.: 100.00%] [Generator loss: 7.408774]\n",
      "12286 [Discriminator loss: 0.068151, acc.: 98.44%] [Generator loss: 6.396910]\n",
      "12287 [Discriminator loss: 0.061936, acc.: 100.00%] [Generator loss: 5.974843]\n",
      "12288 [Discriminator loss: 0.081700, acc.: 95.31%] [Generator loss: 6.992969]\n",
      "12289 [Discriminator loss: 0.192235, acc.: 92.19%] [Generator loss: 7.443971]\n",
      "12290 [Discriminator loss: 0.104162, acc.: 95.31%] [Generator loss: 8.862261]\n",
      "12291 [Discriminator loss: 0.063248, acc.: 98.44%] [Generator loss: 7.150093]\n",
      "12292 [Discriminator loss: 0.145561, acc.: 93.75%] [Generator loss: 7.652612]\n",
      "12293 [Discriminator loss: 0.056680, acc.: 98.44%] [Generator loss: 8.536472]\n",
      "12294 [Discriminator loss: 0.046694, acc.: 100.00%] [Generator loss: 7.307267]\n",
      "12295 [Discriminator loss: 0.173877, acc.: 92.19%] [Generator loss: 7.055507]\n",
      "12296 [Discriminator loss: 0.051434, acc.: 98.44%] [Generator loss: 7.641236]\n",
      "12297 [Discriminator loss: 0.044490, acc.: 100.00%] [Generator loss: 6.751131]\n",
      "12298 [Discriminator loss: 0.150932, acc.: 92.19%] [Generator loss: 4.569533]\n",
      "12299 [Discriminator loss: 0.084424, acc.: 96.88%] [Generator loss: 7.147965]\n",
      "12300 [Discriminator loss: 0.184432, acc.: 92.19%] [Generator loss: 7.863592]\n",
      "12301 [Discriminator loss: 0.096254, acc.: 96.88%] [Generator loss: 6.481023]\n",
      "12302 [Discriminator loss: 0.117154, acc.: 93.75%] [Generator loss: 6.892301]\n",
      "12303 [Discriminator loss: 0.031745, acc.: 100.00%] [Generator loss: 6.528296]\n",
      "12304 [Discriminator loss: 0.028309, acc.: 100.00%] [Generator loss: 6.634368]\n",
      "12305 [Discriminator loss: 0.237981, acc.: 85.94%] [Generator loss: 10.491364]\n",
      "12306 [Discriminator loss: 0.158202, acc.: 90.62%] [Generator loss: 5.626806]\n",
      "12307 [Discriminator loss: 0.114636, acc.: 93.75%] [Generator loss: 6.084493]\n",
      "12308 [Discriminator loss: 0.012678, acc.: 100.00%] [Generator loss: 5.900262]\n",
      "12309 [Discriminator loss: 0.037776, acc.: 98.44%] [Generator loss: 6.716979]\n",
      "12310 [Discriminator loss: 0.115592, acc.: 93.75%] [Generator loss: 7.963241]\n",
      "12311 [Discriminator loss: 0.192566, acc.: 87.50%] [Generator loss: 8.724157]\n",
      "12312 [Discriminator loss: 0.087382, acc.: 96.88%] [Generator loss: 7.225636]\n",
      "12313 [Discriminator loss: 0.085651, acc.: 96.88%] [Generator loss: 6.803239]\n",
      "12314 [Discriminator loss: 0.042779, acc.: 98.44%] [Generator loss: 7.224284]\n",
      "12315 [Discriminator loss: 0.074624, acc.: 96.88%] [Generator loss: 7.969413]\n",
      "12316 [Discriminator loss: 0.058146, acc.: 98.44%] [Generator loss: 6.615869]\n",
      "12317 [Discriminator loss: 0.079791, acc.: 96.88%] [Generator loss: 6.749753]\n",
      "12318 [Discriminator loss: 0.069311, acc.: 96.88%] [Generator loss: 8.003983]\n",
      "12319 [Discriminator loss: 0.083455, acc.: 96.88%] [Generator loss: 7.429417]\n",
      "12320 [Discriminator loss: 0.038342, acc.: 98.44%] [Generator loss: 7.392259]\n",
      "12321 [Discriminator loss: 0.187364, acc.: 90.62%] [Generator loss: 7.887983]\n",
      "12322 [Discriminator loss: 0.087610, acc.: 96.88%] [Generator loss: 7.086594]\n",
      "12323 [Discriminator loss: 0.077335, acc.: 95.31%] [Generator loss: 6.036808]\n",
      "12324 [Discriminator loss: 0.086643, acc.: 95.31%] [Generator loss: 5.763086]\n",
      "12325 [Discriminator loss: 0.084015, acc.: 96.88%] [Generator loss: 5.393777]\n",
      "12326 [Discriminator loss: 0.039341, acc.: 100.00%] [Generator loss: 6.886990]\n",
      "12327 [Discriminator loss: 0.203668, acc.: 90.62%] [Generator loss: 6.334133]\n",
      "12328 [Discriminator loss: 0.042880, acc.: 100.00%] [Generator loss: 6.727768]\n",
      "12329 [Discriminator loss: 0.077893, acc.: 93.75%] [Generator loss: 7.936471]\n",
      "12330 [Discriminator loss: 0.060660, acc.: 98.44%] [Generator loss: 7.554389]\n",
      "12331 [Discriminator loss: 0.092115, acc.: 96.88%] [Generator loss: 6.044176]\n",
      "12332 [Discriminator loss: 0.084178, acc.: 96.88%] [Generator loss: 7.505348]\n",
      "12333 [Discriminator loss: 0.079299, acc.: 98.44%] [Generator loss: 7.583763]\n",
      "12334 [Discriminator loss: 0.062078, acc.: 100.00%] [Generator loss: 7.422983]\n",
      "12335 [Discriminator loss: 0.205122, acc.: 92.19%] [Generator loss: 7.913327]\n",
      "12336 [Discriminator loss: 0.059146, acc.: 96.88%] [Generator loss: 8.467148]\n",
      "12337 [Discriminator loss: 0.105593, acc.: 95.31%] [Generator loss: 5.707090]\n",
      "12338 [Discriminator loss: 0.072837, acc.: 98.44%] [Generator loss: 5.491156]\n",
      "12339 [Discriminator loss: 0.092173, acc.: 96.88%] [Generator loss: 6.787319]\n",
      "12340 [Discriminator loss: 0.053721, acc.: 96.88%] [Generator loss: 7.427567]\n",
      "12341 [Discriminator loss: 0.053489, acc.: 96.88%] [Generator loss: 6.154090]\n",
      "12342 [Discriminator loss: 0.139656, acc.: 95.31%] [Generator loss: 7.292059]\n",
      "12343 [Discriminator loss: 0.045122, acc.: 100.00%] [Generator loss: 7.440558]\n",
      "12344 [Discriminator loss: 0.056645, acc.: 98.44%] [Generator loss: 6.534860]\n",
      "12345 [Discriminator loss: 0.034294, acc.: 100.00%] [Generator loss: 6.376101]\n",
      "12346 [Discriminator loss: 0.125893, acc.: 96.88%] [Generator loss: 5.859703]\n",
      "12347 [Discriminator loss: 0.051916, acc.: 98.44%] [Generator loss: 5.728751]\n",
      "12348 [Discriminator loss: 0.106035, acc.: 93.75%] [Generator loss: 6.856652]\n",
      "12349 [Discriminator loss: 0.151790, acc.: 95.31%] [Generator loss: 6.079046]\n",
      "12350 [Discriminator loss: 0.176722, acc.: 92.19%] [Generator loss: 6.006412]\n",
      "12351 [Discriminator loss: 0.072048, acc.: 96.88%] [Generator loss: 7.219923]\n",
      "12352 [Discriminator loss: 0.038996, acc.: 100.00%] [Generator loss: 6.095682]\n",
      "12353 [Discriminator loss: 0.192542, acc.: 95.31%] [Generator loss: 6.331655]\n",
      "12354 [Discriminator loss: 0.080436, acc.: 98.44%] [Generator loss: 7.781653]\n",
      "12355 [Discriminator loss: 0.104318, acc.: 95.31%] [Generator loss: 7.880553]\n",
      "12356 [Discriminator loss: 0.045323, acc.: 98.44%] [Generator loss: 6.918034]\n",
      "12357 [Discriminator loss: 0.215484, acc.: 89.06%] [Generator loss: 7.312705]\n",
      "12358 [Discriminator loss: 0.148080, acc.: 96.88%] [Generator loss: 8.384744]\n",
      "12359 [Discriminator loss: 0.033505, acc.: 100.00%] [Generator loss: 8.946387]\n",
      "12360 [Discriminator loss: 0.060687, acc.: 96.88%] [Generator loss: 6.869138]\n",
      "12361 [Discriminator loss: 0.040139, acc.: 98.44%] [Generator loss: 7.138629]\n",
      "12362 [Discriminator loss: 0.093389, acc.: 98.44%] [Generator loss: 6.419339]\n",
      "12363 [Discriminator loss: 0.066793, acc.: 98.44%] [Generator loss: 6.891620]\n",
      "12364 [Discriminator loss: 0.064924, acc.: 98.44%] [Generator loss: 6.306283]\n",
      "12365 [Discriminator loss: 0.039801, acc.: 98.44%] [Generator loss: 6.925588]\n",
      "12366 [Discriminator loss: 0.097032, acc.: 95.31%] [Generator loss: 7.768774]\n",
      "12367 [Discriminator loss: 0.062352, acc.: 98.44%] [Generator loss: 5.743991]\n",
      "12368 [Discriminator loss: 0.224918, acc.: 93.75%] [Generator loss: 4.796149]\n",
      "12369 [Discriminator loss: 0.122864, acc.: 95.31%] [Generator loss: 6.011267]\n",
      "12370 [Discriminator loss: 0.141939, acc.: 93.75%] [Generator loss: 7.711133]\n",
      "12371 [Discriminator loss: 0.042107, acc.: 100.00%] [Generator loss: 7.268591]\n",
      "12372 [Discriminator loss: 0.112698, acc.: 95.31%] [Generator loss: 7.786349]\n",
      "12373 [Discriminator loss: 0.137613, acc.: 93.75%] [Generator loss: 6.094461]\n",
      "12374 [Discriminator loss: 0.137534, acc.: 95.31%] [Generator loss: 7.086927]\n",
      "12375 [Discriminator loss: 0.171785, acc.: 92.19%] [Generator loss: 9.191436]\n",
      "12376 [Discriminator loss: 0.037892, acc.: 98.44%] [Generator loss: 7.644446]\n",
      "12377 [Discriminator loss: 0.077626, acc.: 96.88%] [Generator loss: 6.613143]\n",
      "12378 [Discriminator loss: 0.088208, acc.: 96.88%] [Generator loss: 7.610501]\n",
      "12379 [Discriminator loss: 0.155384, acc.: 95.31%] [Generator loss: 7.355529]\n",
      "12380 [Discriminator loss: 0.112819, acc.: 95.31%] [Generator loss: 7.957601]\n",
      "12381 [Discriminator loss: 0.048890, acc.: 98.44%] [Generator loss: 7.298544]\n",
      "12382 [Discriminator loss: 0.124416, acc.: 98.44%] [Generator loss: 5.390371]\n",
      "12383 [Discriminator loss: 0.069338, acc.: 96.88%] [Generator loss: 6.427414]\n",
      "12384 [Discriminator loss: 0.031818, acc.: 100.00%] [Generator loss: 7.812471]\n",
      "12385 [Discriminator loss: 0.064600, acc.: 96.88%] [Generator loss: 5.882116]\n",
      "12386 [Discriminator loss: 0.162966, acc.: 95.31%] [Generator loss: 8.101491]\n",
      "12387 [Discriminator loss: 0.191032, acc.: 93.75%] [Generator loss: 6.201469]\n",
      "12388 [Discriminator loss: 0.017315, acc.: 100.00%] [Generator loss: 7.143600]\n",
      "12389 [Discriminator loss: 0.064636, acc.: 98.44%] [Generator loss: 6.315035]\n",
      "12390 [Discriminator loss: 0.040554, acc.: 100.00%] [Generator loss: 6.542232]\n",
      "12391 [Discriminator loss: 0.375305, acc.: 84.38%] [Generator loss: 6.396953]\n",
      "12392 [Discriminator loss: 0.053686, acc.: 98.44%] [Generator loss: 8.054165]\n",
      "12393 [Discriminator loss: 0.052234, acc.: 98.44%] [Generator loss: 8.191292]\n",
      "12394 [Discriminator loss: 0.110174, acc.: 93.75%] [Generator loss: 6.974056]\n",
      "12395 [Discriminator loss: 0.077364, acc.: 98.44%] [Generator loss: 8.236042]\n",
      "12396 [Discriminator loss: 0.127104, acc.: 95.31%] [Generator loss: 8.331440]\n",
      "12397 [Discriminator loss: 0.135580, acc.: 95.31%] [Generator loss: 8.035378]\n",
      "12398 [Discriminator loss: 0.316519, acc.: 82.81%] [Generator loss: 5.434237]\n",
      "12399 [Discriminator loss: 0.020435, acc.: 100.00%] [Generator loss: 8.478202]\n",
      "12400 [Discriminator loss: 0.120705, acc.: 95.31%] [Generator loss: 7.472665]\n",
      "12401 [Discriminator loss: 0.088686, acc.: 96.88%] [Generator loss: 7.234308]\n",
      "12402 [Discriminator loss: 0.050714, acc.: 98.44%] [Generator loss: 6.976192]\n",
      "12403 [Discriminator loss: 0.100580, acc.: 95.31%] [Generator loss: 6.397588]\n",
      "12404 [Discriminator loss: 0.053160, acc.: 98.44%] [Generator loss: 8.026816]\n",
      "12405 [Discriminator loss: 0.196965, acc.: 93.75%] [Generator loss: 7.966886]\n",
      "12406 [Discriminator loss: 0.096390, acc.: 93.75%] [Generator loss: 6.973466]\n",
      "12407 [Discriminator loss: 0.170819, acc.: 93.75%] [Generator loss: 5.757044]\n",
      "12408 [Discriminator loss: 0.230491, acc.: 89.06%] [Generator loss: 9.358088]\n",
      "12409 [Discriminator loss: 0.069129, acc.: 96.88%] [Generator loss: 9.374827]\n",
      "12410 [Discriminator loss: 0.372411, acc.: 87.50%] [Generator loss: 7.129618]\n",
      "12411 [Discriminator loss: 0.067306, acc.: 98.44%] [Generator loss: 6.705929]\n",
      "12412 [Discriminator loss: 0.081833, acc.: 95.31%] [Generator loss: 7.692348]\n",
      "12413 [Discriminator loss: 0.118586, acc.: 95.31%] [Generator loss: 6.595566]\n",
      "12414 [Discriminator loss: 0.094160, acc.: 93.75%] [Generator loss: 6.271108]\n",
      "12415 [Discriminator loss: 0.071078, acc.: 96.88%] [Generator loss: 5.797939]\n",
      "12416 [Discriminator loss: 0.096411, acc.: 96.88%] [Generator loss: 7.771020]\n",
      "12417 [Discriminator loss: 0.063221, acc.: 98.44%] [Generator loss: 6.594251]\n",
      "12418 [Discriminator loss: 0.017018, acc.: 100.00%] [Generator loss: 6.193028]\n",
      "12419 [Discriminator loss: 0.084004, acc.: 95.31%] [Generator loss: 5.564745]\n",
      "12420 [Discriminator loss: 0.111380, acc.: 95.31%] [Generator loss: 6.475066]\n",
      "12421 [Discriminator loss: 0.088515, acc.: 98.44%] [Generator loss: 8.656647]\n",
      "12422 [Discriminator loss: 0.169656, acc.: 87.50%] [Generator loss: 6.838206]\n",
      "12423 [Discriminator loss: 0.162405, acc.: 93.75%] [Generator loss: 7.604543]\n",
      "12424 [Discriminator loss: 0.092817, acc.: 96.88%] [Generator loss: 6.688868]\n",
      "12425 [Discriminator loss: 0.255930, acc.: 89.06%] [Generator loss: 5.750267]\n",
      "12426 [Discriminator loss: 0.060471, acc.: 98.44%] [Generator loss: 7.502438]\n",
      "12427 [Discriminator loss: 0.311843, acc.: 93.75%] [Generator loss: 7.886453]\n",
      "12428 [Discriminator loss: 0.046385, acc.: 98.44%] [Generator loss: 9.457433]\n",
      "12429 [Discriminator loss: 0.098137, acc.: 96.88%] [Generator loss: 7.078889]\n",
      "12430 [Discriminator loss: 0.203242, acc.: 92.19%] [Generator loss: 6.263398]\n",
      "12431 [Discriminator loss: 0.059872, acc.: 98.44%] [Generator loss: 8.972836]\n",
      "12432 [Discriminator loss: 0.041333, acc.: 100.00%] [Generator loss: 7.134377]\n",
      "12433 [Discriminator loss: 0.066579, acc.: 96.88%] [Generator loss: 7.849040]\n",
      "12434 [Discriminator loss: 0.056632, acc.: 98.44%] [Generator loss: 7.811374]\n",
      "12435 [Discriminator loss: 0.057909, acc.: 98.44%] [Generator loss: 5.886291]\n",
      "12436 [Discriminator loss: 0.052824, acc.: 98.44%] [Generator loss: 6.559260]\n",
      "12437 [Discriminator loss: 0.082828, acc.: 95.31%] [Generator loss: 6.980000]\n",
      "12438 [Discriminator loss: 0.061039, acc.: 98.44%] [Generator loss: 6.091326]\n",
      "12439 [Discriminator loss: 0.109814, acc.: 96.88%] [Generator loss: 6.790878]\n",
      "12440 [Discriminator loss: 0.123258, acc.: 95.31%] [Generator loss: 6.984941]\n",
      "12441 [Discriminator loss: 0.239167, acc.: 90.62%] [Generator loss: 5.984522]\n",
      "12442 [Discriminator loss: 0.137761, acc.: 93.75%] [Generator loss: 6.075974]\n",
      "12443 [Discriminator loss: 0.063379, acc.: 98.44%] [Generator loss: 8.284952]\n",
      "12444 [Discriminator loss: 0.040502, acc.: 98.44%] [Generator loss: 7.895321]\n",
      "12445 [Discriminator loss: 0.095787, acc.: 96.88%] [Generator loss: 7.240885]\n",
      "12446 [Discriminator loss: 0.031603, acc.: 100.00%] [Generator loss: 7.666660]\n",
      "12447 [Discriminator loss: 0.108615, acc.: 93.75%] [Generator loss: 5.828255]\n",
      "12448 [Discriminator loss: 0.128102, acc.: 93.75%] [Generator loss: 7.154776]\n",
      "12449 [Discriminator loss: 0.116119, acc.: 93.75%] [Generator loss: 6.659476]\n",
      "12450 [Discriminator loss: 0.071378, acc.: 96.88%] [Generator loss: 5.638930]\n",
      "12451 [Discriminator loss: 0.055010, acc.: 98.44%] [Generator loss: 6.081699]\n",
      "12452 [Discriminator loss: 0.130628, acc.: 93.75%] [Generator loss: 7.915537]\n",
      "12453 [Discriminator loss: 0.042259, acc.: 100.00%] [Generator loss: 7.501396]\n",
      "12454 [Discriminator loss: 0.102854, acc.: 93.75%] [Generator loss: 6.996555]\n",
      "12455 [Discriminator loss: 0.172616, acc.: 92.19%] [Generator loss: 8.103974]\n",
      "12456 [Discriminator loss: 0.229270, acc.: 93.75%] [Generator loss: 7.959862]\n",
      "12457 [Discriminator loss: 0.111756, acc.: 93.75%] [Generator loss: 7.439472]\n",
      "12458 [Discriminator loss: 0.066385, acc.: 96.88%] [Generator loss: 6.106863]\n",
      "12459 [Discriminator loss: 0.069435, acc.: 98.44%] [Generator loss: 7.336875]\n",
      "12460 [Discriminator loss: 0.069990, acc.: 96.88%] [Generator loss: 5.927188]\n",
      "12461 [Discriminator loss: 0.052214, acc.: 98.44%] [Generator loss: 6.302833]\n",
      "12462 [Discriminator loss: 0.028148, acc.: 100.00%] [Generator loss: 6.618279]\n",
      "12463 [Discriminator loss: 0.092755, acc.: 98.44%] [Generator loss: 7.637964]\n",
      "12464 [Discriminator loss: 0.031509, acc.: 100.00%] [Generator loss: 7.675054]\n",
      "12465 [Discriminator loss: 0.027976, acc.: 100.00%] [Generator loss: 7.559666]\n",
      "12466 [Discriminator loss: 0.136129, acc.: 95.31%] [Generator loss: 6.797979]\n",
      "12467 [Discriminator loss: 0.083960, acc.: 98.44%] [Generator loss: 7.493502]\n",
      "12468 [Discriminator loss: 0.141389, acc.: 93.75%] [Generator loss: 7.456882]\n",
      "12469 [Discriminator loss: 0.030094, acc.: 100.00%] [Generator loss: 6.745580]\n",
      "12470 [Discriminator loss: 0.074111, acc.: 98.44%] [Generator loss: 7.125187]\n",
      "12471 [Discriminator loss: 0.029192, acc.: 100.00%] [Generator loss: 6.144138]\n",
      "12472 [Discriminator loss: 0.063022, acc.: 96.88%] [Generator loss: 7.199043]\n",
      "12473 [Discriminator loss: 0.332728, acc.: 90.62%] [Generator loss: 7.817188]\n",
      "12474 [Discriminator loss: 0.082313, acc.: 96.88%] [Generator loss: 5.475111]\n",
      "12475 [Discriminator loss: 0.023704, acc.: 100.00%] [Generator loss: 6.132539]\n",
      "12476 [Discriminator loss: 0.141466, acc.: 95.31%] [Generator loss: 7.132703]\n",
      "12477 [Discriminator loss: 0.087368, acc.: 96.88%] [Generator loss: 8.482496]\n",
      "12478 [Discriminator loss: 0.078434, acc.: 96.88%] [Generator loss: 8.383768]\n",
      "12479 [Discriminator loss: 0.100440, acc.: 95.31%] [Generator loss: 6.001203]\n",
      "12480 [Discriminator loss: 0.094075, acc.: 96.88%] [Generator loss: 5.644589]\n",
      "12481 [Discriminator loss: 0.127728, acc.: 93.75%] [Generator loss: 7.041821]\n",
      "12482 [Discriminator loss: 0.042386, acc.: 98.44%] [Generator loss: 7.074502]\n",
      "12483 [Discriminator loss: 0.150180, acc.: 96.88%] [Generator loss: 7.582153]\n",
      "12484 [Discriminator loss: 0.076272, acc.: 95.31%] [Generator loss: 5.776882]\n",
      "12485 [Discriminator loss: 0.232754, acc.: 89.06%] [Generator loss: 7.467567]\n",
      "12486 [Discriminator loss: 0.025322, acc.: 100.00%] [Generator loss: 7.025966]\n",
      "12487 [Discriminator loss: 0.211523, acc.: 92.19%] [Generator loss: 6.305122]\n",
      "12488 [Discriminator loss: 0.060029, acc.: 96.88%] [Generator loss: 7.016345]\n",
      "12489 [Discriminator loss: 0.069492, acc.: 98.44%] [Generator loss: 6.909368]\n",
      "12490 [Discriminator loss: 0.085514, acc.: 95.31%] [Generator loss: 6.944717]\n",
      "12491 [Discriminator loss: 0.127384, acc.: 93.75%] [Generator loss: 6.938820]\n",
      "12492 [Discriminator loss: 0.070430, acc.: 96.88%] [Generator loss: 7.214660]\n",
      "12493 [Discriminator loss: 0.076699, acc.: 98.44%] [Generator loss: 6.678345]\n",
      "12494 [Discriminator loss: 0.027929, acc.: 98.44%] [Generator loss: 7.644637]\n",
      "12495 [Discriminator loss: 0.114779, acc.: 96.88%] [Generator loss: 6.689908]\n",
      "12496 [Discriminator loss: 0.090625, acc.: 96.88%] [Generator loss: 7.295012]\n",
      "12497 [Discriminator loss: 0.160305, acc.: 93.75%] [Generator loss: 6.808774]\n",
      "12498 [Discriminator loss: 0.041282, acc.: 100.00%] [Generator loss: 6.456811]\n",
      "12499 [Discriminator loss: 0.127949, acc.: 95.31%] [Generator loss: 7.479103]\n",
      "12500 [Discriminator loss: 0.109720, acc.: 95.31%] [Generator loss: 7.412505]\n",
      "12501 [Discriminator loss: 0.234472, acc.: 90.62%] [Generator loss: 7.441406]\n",
      "12502 [Discriminator loss: 0.229876, acc.: 92.19%] [Generator loss: 8.877562]\n",
      "12503 [Discriminator loss: 0.124889, acc.: 96.88%] [Generator loss: 7.640455]\n",
      "12504 [Discriminator loss: 0.384373, acc.: 89.06%] [Generator loss: 7.291897]\n",
      "12505 [Discriminator loss: 0.135343, acc.: 95.31%] [Generator loss: 6.243757]\n",
      "12506 [Discriminator loss: 0.145231, acc.: 92.19%] [Generator loss: 9.007564]\n",
      "12507 [Discriminator loss: 0.236772, acc.: 93.75%] [Generator loss: 7.894185]\n",
      "12508 [Discriminator loss: 0.112439, acc.: 93.75%] [Generator loss: 7.276297]\n",
      "12509 [Discriminator loss: 0.085813, acc.: 98.44%] [Generator loss: 7.495196]\n",
      "12510 [Discriminator loss: 0.115725, acc.: 95.31%] [Generator loss: 6.472215]\n",
      "12511 [Discriminator loss: 0.106544, acc.: 95.31%] [Generator loss: 8.150726]\n",
      "12512 [Discriminator loss: 0.287295, acc.: 90.62%] [Generator loss: 7.009452]\n",
      "12513 [Discriminator loss: 0.081789, acc.: 96.88%] [Generator loss: 7.918967]\n",
      "12514 [Discriminator loss: 0.050971, acc.: 98.44%] [Generator loss: 7.201968]\n",
      "12515 [Discriminator loss: 0.125631, acc.: 92.19%] [Generator loss: 7.032217]\n",
      "12516 [Discriminator loss: 0.285490, acc.: 89.06%] [Generator loss: 8.770322]\n",
      "12517 [Discriminator loss: 0.141989, acc.: 92.19%] [Generator loss: 7.088443]\n",
      "12518 [Discriminator loss: 0.116272, acc.: 95.31%] [Generator loss: 6.064273]\n",
      "12519 [Discriminator loss: 0.150364, acc.: 96.88%] [Generator loss: 7.521749]\n",
      "12520 [Discriminator loss: 0.122323, acc.: 96.88%] [Generator loss: 7.344581]\n",
      "12521 [Discriminator loss: 0.070676, acc.: 96.88%] [Generator loss: 6.144462]\n",
      "12522 [Discriminator loss: 0.260797, acc.: 84.38%] [Generator loss: 6.983290]\n",
      "12523 [Discriminator loss: 0.050878, acc.: 98.44%] [Generator loss: 8.194147]\n",
      "12524 [Discriminator loss: 0.107017, acc.: 96.88%] [Generator loss: 6.730244]\n",
      "12525 [Discriminator loss: 0.276898, acc.: 92.19%] [Generator loss: 7.097568]\n",
      "12526 [Discriminator loss: 0.049003, acc.: 96.88%] [Generator loss: 5.654325]\n",
      "12527 [Discriminator loss: 0.077825, acc.: 96.88%] [Generator loss: 6.381823]\n",
      "12528 [Discriminator loss: 0.049241, acc.: 100.00%] [Generator loss: 6.471072]\n",
      "12529 [Discriminator loss: 0.082450, acc.: 98.44%] [Generator loss: 7.935615]\n",
      "12530 [Discriminator loss: 0.054014, acc.: 98.44%] [Generator loss: 7.781769]\n",
      "12531 [Discriminator loss: 0.093838, acc.: 95.31%] [Generator loss: 7.201408]\n",
      "12532 [Discriminator loss: 0.081704, acc.: 98.44%] [Generator loss: 7.960704]\n",
      "12533 [Discriminator loss: 0.082526, acc.: 95.31%] [Generator loss: 5.968268]\n",
      "12534 [Discriminator loss: 0.187772, acc.: 90.62%] [Generator loss: 8.711676]\n",
      "12535 [Discriminator loss: 0.045850, acc.: 98.44%] [Generator loss: 9.713765]\n",
      "12536 [Discriminator loss: 0.276219, acc.: 87.50%] [Generator loss: 6.966579]\n",
      "12537 [Discriminator loss: 0.036285, acc.: 100.00%] [Generator loss: 6.474298]\n",
      "12538 [Discriminator loss: 0.194277, acc.: 93.75%] [Generator loss: 8.262486]\n",
      "12539 [Discriminator loss: 0.180087, acc.: 92.19%] [Generator loss: 7.125803]\n",
      "12540 [Discriminator loss: 0.050875, acc.: 98.44%] [Generator loss: 6.577021]\n",
      "12541 [Discriminator loss: 0.204951, acc.: 92.19%] [Generator loss: 7.807305]\n",
      "12542 [Discriminator loss: 0.161785, acc.: 93.75%] [Generator loss: 7.855519]\n",
      "12543 [Discriminator loss: 0.115941, acc.: 95.31%] [Generator loss: 6.492757]\n",
      "12544 [Discriminator loss: 0.127212, acc.: 95.31%] [Generator loss: 6.683313]\n",
      "12545 [Discriminator loss: 0.076998, acc.: 98.44%] [Generator loss: 7.664101]\n",
      "12546 [Discriminator loss: 0.130541, acc.: 95.31%] [Generator loss: 8.185013]\n",
      "12547 [Discriminator loss: 0.034015, acc.: 100.00%] [Generator loss: 8.190647]\n",
      "12548 [Discriminator loss: 0.102376, acc.: 98.44%] [Generator loss: 7.533757]\n",
      "12549 [Discriminator loss: 0.043302, acc.: 98.44%] [Generator loss: 6.932924]\n",
      "12550 [Discriminator loss: 0.071096, acc.: 96.88%] [Generator loss: 6.574506]\n",
      "12551 [Discriminator loss: 0.079802, acc.: 95.31%] [Generator loss: 7.243262]\n",
      "12552 [Discriminator loss: 0.167789, acc.: 92.19%] [Generator loss: 7.091292]\n",
      "12553 [Discriminator loss: 0.084077, acc.: 98.44%] [Generator loss: 7.234088]\n",
      "12554 [Discriminator loss: 0.110958, acc.: 95.31%] [Generator loss: 6.375019]\n",
      "12555 [Discriminator loss: 0.104442, acc.: 95.31%] [Generator loss: 8.388175]\n",
      "12556 [Discriminator loss: 0.181875, acc.: 92.19%] [Generator loss: 7.233532]\n",
      "12557 [Discriminator loss: 0.019974, acc.: 100.00%] [Generator loss: 7.642625]\n",
      "12558 [Discriminator loss: 0.074921, acc.: 96.88%] [Generator loss: 5.617040]\n",
      "12559 [Discriminator loss: 0.206059, acc.: 90.62%] [Generator loss: 8.485271]\n",
      "12560 [Discriminator loss: 0.124162, acc.: 93.75%] [Generator loss: 7.800125]\n",
      "12561 [Discriminator loss: 0.089318, acc.: 96.88%] [Generator loss: 8.036280]\n",
      "12562 [Discriminator loss: 0.068484, acc.: 95.31%] [Generator loss: 6.557517]\n",
      "12563 [Discriminator loss: 0.035383, acc.: 100.00%] [Generator loss: 6.362632]\n",
      "12564 [Discriminator loss: 0.106228, acc.: 96.88%] [Generator loss: 8.383310]\n",
      "12565 [Discriminator loss: 0.122024, acc.: 96.88%] [Generator loss: 5.984368]\n",
      "12566 [Discriminator loss: 0.072040, acc.: 96.88%] [Generator loss: 6.027869]\n",
      "12567 [Discriminator loss: 0.196212, acc.: 95.31%] [Generator loss: 8.415488]\n",
      "12568 [Discriminator loss: 0.132773, acc.: 96.88%] [Generator loss: 6.805503]\n",
      "12569 [Discriminator loss: 0.153814, acc.: 95.31%] [Generator loss: 8.710950]\n",
      "12570 [Discriminator loss: 0.228593, acc.: 90.62%] [Generator loss: 7.622799]\n",
      "12571 [Discriminator loss: 0.062689, acc.: 96.88%] [Generator loss: 6.513247]\n",
      "12572 [Discriminator loss: 0.043632, acc.: 98.44%] [Generator loss: 7.589333]\n",
      "12573 [Discriminator loss: 0.221649, acc.: 93.75%] [Generator loss: 6.833347]\n",
      "12574 [Discriminator loss: 0.170483, acc.: 93.75%] [Generator loss: 7.349220]\n",
      "12575 [Discriminator loss: 0.128236, acc.: 95.31%] [Generator loss: 8.412497]\n",
      "12576 [Discriminator loss: 0.127880, acc.: 93.75%] [Generator loss: 7.239225]\n",
      "12577 [Discriminator loss: 0.172940, acc.: 95.31%] [Generator loss: 7.416471]\n",
      "12578 [Discriminator loss: 0.084365, acc.: 98.44%] [Generator loss: 6.774841]\n",
      "12579 [Discriminator loss: 0.066899, acc.: 98.44%] [Generator loss: 6.941467]\n",
      "12580 [Discriminator loss: 0.119313, acc.: 98.44%] [Generator loss: 6.720389]\n",
      "12581 [Discriminator loss: 0.071370, acc.: 98.44%] [Generator loss: 6.994872]\n",
      "12582 [Discriminator loss: 0.072312, acc.: 96.88%] [Generator loss: 6.936891]\n",
      "12583 [Discriminator loss: 0.043346, acc.: 100.00%] [Generator loss: 7.895925]\n",
      "12584 [Discriminator loss: 0.107716, acc.: 95.31%] [Generator loss: 6.360703]\n",
      "12585 [Discriminator loss: 0.078200, acc.: 96.88%] [Generator loss: 5.288615]\n",
      "12586 [Discriminator loss: 0.165650, acc.: 95.31%] [Generator loss: 7.069019]\n",
      "12587 [Discriminator loss: 0.136188, acc.: 93.75%] [Generator loss: 8.364326]\n",
      "12588 [Discriminator loss: 0.059783, acc.: 98.44%] [Generator loss: 7.686454]\n",
      "12589 [Discriminator loss: 0.178106, acc.: 87.50%] [Generator loss: 7.075089]\n",
      "12590 [Discriminator loss: 0.196860, acc.: 92.19%] [Generator loss: 7.196033]\n",
      "12591 [Discriminator loss: 0.027635, acc.: 100.00%] [Generator loss: 8.002058]\n",
      "12592 [Discriminator loss: 0.049700, acc.: 98.44%] [Generator loss: 7.580080]\n",
      "12593 [Discriminator loss: 0.171807, acc.: 92.19%] [Generator loss: 7.442048]\n",
      "12594 [Discriminator loss: 0.071449, acc.: 98.44%] [Generator loss: 7.388343]\n",
      "12595 [Discriminator loss: 0.075717, acc.: 95.31%] [Generator loss: 6.877056]\n",
      "12596 [Discriminator loss: 0.056731, acc.: 98.44%] [Generator loss: 8.708797]\n",
      "12597 [Discriminator loss: 0.148290, acc.: 95.31%] [Generator loss: 6.814617]\n",
      "12598 [Discriminator loss: 0.099120, acc.: 96.88%] [Generator loss: 8.454184]\n",
      "12599 [Discriminator loss: 0.014849, acc.: 100.00%] [Generator loss: 8.047929]\n",
      "12600 [Discriminator loss: 0.061661, acc.: 95.31%] [Generator loss: 7.202055]\n",
      "12601 [Discriminator loss: 0.041453, acc.: 100.00%] [Generator loss: 6.508480]\n",
      "12602 [Discriminator loss: 0.092676, acc.: 95.31%] [Generator loss: 6.385367]\n",
      "12603 [Discriminator loss: 0.077960, acc.: 96.88%] [Generator loss: 5.009318]\n",
      "12604 [Discriminator loss: 0.151042, acc.: 93.75%] [Generator loss: 7.714095]\n",
      "12605 [Discriminator loss: 0.078943, acc.: 98.44%] [Generator loss: 8.210196]\n",
      "12606 [Discriminator loss: 0.134936, acc.: 95.31%] [Generator loss: 7.335890]\n",
      "12607 [Discriminator loss: 0.158139, acc.: 92.19%] [Generator loss: 6.628078]\n",
      "12608 [Discriminator loss: 0.112547, acc.: 95.31%] [Generator loss: 7.352558]\n",
      "12609 [Discriminator loss: 0.096079, acc.: 96.88%] [Generator loss: 7.001840]\n",
      "12610 [Discriminator loss: 0.115531, acc.: 95.31%] [Generator loss: 7.171088]\n",
      "12611 [Discriminator loss: 0.104155, acc.: 95.31%] [Generator loss: 8.215210]\n",
      "12612 [Discriminator loss: 0.147399, acc.: 93.75%] [Generator loss: 7.667761]\n",
      "12613 [Discriminator loss: 0.081713, acc.: 96.88%] [Generator loss: 8.120493]\n",
      "12614 [Discriminator loss: 0.071357, acc.: 96.88%] [Generator loss: 7.016623]\n",
      "12615 [Discriminator loss: 0.214708, acc.: 93.75%] [Generator loss: 6.051687]\n",
      "12616 [Discriminator loss: 0.023794, acc.: 100.00%] [Generator loss: 7.072105]\n",
      "12617 [Discriminator loss: 0.030827, acc.: 98.44%] [Generator loss: 6.828021]\n",
      "12618 [Discriminator loss: 0.046228, acc.: 100.00%] [Generator loss: 6.651147]\n",
      "12619 [Discriminator loss: 0.018885, acc.: 100.00%] [Generator loss: 7.563475]\n",
      "12620 [Discriminator loss: 0.022675, acc.: 100.00%] [Generator loss: 7.193137]\n",
      "12621 [Discriminator loss: 0.019832, acc.: 100.00%] [Generator loss: 5.939671]\n",
      "12622 [Discriminator loss: 0.180940, acc.: 90.62%] [Generator loss: 7.404209]\n",
      "12623 [Discriminator loss: 0.054110, acc.: 98.44%] [Generator loss: 7.290495]\n",
      "12624 [Discriminator loss: 0.102559, acc.: 98.44%] [Generator loss: 5.674114]\n",
      "12625 [Discriminator loss: 0.056563, acc.: 98.44%] [Generator loss: 7.394849]\n",
      "12626 [Discriminator loss: 0.045160, acc.: 98.44%] [Generator loss: 5.931845]\n",
      "12627 [Discriminator loss: 0.154638, acc.: 93.75%] [Generator loss: 7.646325]\n",
      "12628 [Discriminator loss: 0.042655, acc.: 98.44%] [Generator loss: 6.287419]\n",
      "12629 [Discriminator loss: 0.139378, acc.: 93.75%] [Generator loss: 6.993466]\n",
      "12630 [Discriminator loss: 0.049380, acc.: 98.44%] [Generator loss: 7.369749]\n",
      "12631 [Discriminator loss: 0.034824, acc.: 98.44%] [Generator loss: 8.340519]\n",
      "12632 [Discriminator loss: 0.018649, acc.: 100.00%] [Generator loss: 7.028544]\n",
      "12633 [Discriminator loss: 0.251202, acc.: 90.62%] [Generator loss: 6.196846]\n",
      "12634 [Discriminator loss: 0.198581, acc.: 95.31%] [Generator loss: 7.842572]\n",
      "12635 [Discriminator loss: 0.045447, acc.: 100.00%] [Generator loss: 8.652901]\n",
      "12636 [Discriminator loss: 0.042445, acc.: 98.44%] [Generator loss: 7.574381]\n",
      "12637 [Discriminator loss: 0.083080, acc.: 98.44%] [Generator loss: 8.149187]\n",
      "12638 [Discriminator loss: 0.134968, acc.: 93.75%] [Generator loss: 6.819506]\n",
      "12639 [Discriminator loss: 0.088692, acc.: 96.88%] [Generator loss: 7.661532]\n",
      "12640 [Discriminator loss: 0.134861, acc.: 96.88%] [Generator loss: 7.322305]\n",
      "12641 [Discriminator loss: 0.120770, acc.: 92.19%] [Generator loss: 7.658912]\n",
      "12642 [Discriminator loss: 0.090492, acc.: 96.88%] [Generator loss: 8.735743]\n",
      "12643 [Discriminator loss: 0.048541, acc.: 98.44%] [Generator loss: 6.578990]\n",
      "12644 [Discriminator loss: 0.025847, acc.: 98.44%] [Generator loss: 7.856226]\n",
      "12645 [Discriminator loss: 0.081860, acc.: 96.88%] [Generator loss: 7.188625]\n",
      "12646 [Discriminator loss: 0.073485, acc.: 96.88%] [Generator loss: 7.482141]\n",
      "12647 [Discriminator loss: 0.140610, acc.: 96.88%] [Generator loss: 7.754189]\n",
      "12648 [Discriminator loss: 0.070092, acc.: 96.88%] [Generator loss: 7.282413]\n",
      "12649 [Discriminator loss: 0.057318, acc.: 96.88%] [Generator loss: 7.762412]\n",
      "12650 [Discriminator loss: 0.054054, acc.: 98.44%] [Generator loss: 6.953449]\n",
      "12651 [Discriminator loss: 0.060836, acc.: 96.88%] [Generator loss: 4.971082]\n",
      "12652 [Discriminator loss: 0.053476, acc.: 98.44%] [Generator loss: 5.156110]\n",
      "12653 [Discriminator loss: 0.129645, acc.: 93.75%] [Generator loss: 6.101186]\n",
      "12654 [Discriminator loss: 0.022150, acc.: 100.00%] [Generator loss: 7.833970]\n",
      "12655 [Discriminator loss: 0.097022, acc.: 96.88%] [Generator loss: 8.567143]\n",
      "12656 [Discriminator loss: 0.166772, acc.: 95.31%] [Generator loss: 7.330336]\n",
      "12657 [Discriminator loss: 0.025804, acc.: 100.00%] [Generator loss: 9.291428]\n",
      "12658 [Discriminator loss: 0.096827, acc.: 98.44%] [Generator loss: 8.107153]\n",
      "12659 [Discriminator loss: 0.050962, acc.: 98.44%] [Generator loss: 8.361778]\n",
      "12660 [Discriminator loss: 0.043084, acc.: 96.88%] [Generator loss: 6.939379]\n",
      "12661 [Discriminator loss: 0.277709, acc.: 89.06%] [Generator loss: 6.348508]\n",
      "12662 [Discriminator loss: 0.033278, acc.: 100.00%] [Generator loss: 8.224735]\n",
      "12663 [Discriminator loss: 0.201775, acc.: 92.19%] [Generator loss: 7.409165]\n",
      "12664 [Discriminator loss: 0.074741, acc.: 96.88%] [Generator loss: 8.932058]\n",
      "12665 [Discriminator loss: 0.086291, acc.: 96.88%] [Generator loss: 7.216420]\n",
      "12666 [Discriminator loss: 0.063322, acc.: 96.88%] [Generator loss: 7.738646]\n",
      "12667 [Discriminator loss: 0.052109, acc.: 98.44%] [Generator loss: 6.461929]\n",
      "12668 [Discriminator loss: 0.146716, acc.: 93.75%] [Generator loss: 8.362331]\n",
      "12669 [Discriminator loss: 0.057534, acc.: 96.88%] [Generator loss: 8.552107]\n",
      "12670 [Discriminator loss: 0.119099, acc.: 95.31%] [Generator loss: 7.029010]\n",
      "12671 [Discriminator loss: 0.143801, acc.: 95.31%] [Generator loss: 7.975504]\n",
      "12672 [Discriminator loss: 0.062889, acc.: 98.44%] [Generator loss: 8.299829]\n",
      "12673 [Discriminator loss: 0.110985, acc.: 96.88%] [Generator loss: 6.947030]\n",
      "12674 [Discriminator loss: 0.089587, acc.: 95.31%] [Generator loss: 6.516088]\n",
      "12675 [Discriminator loss: 0.029991, acc.: 98.44%] [Generator loss: 7.714365]\n",
      "12676 [Discriminator loss: 0.067588, acc.: 96.88%] [Generator loss: 6.623467]\n",
      "12677 [Discriminator loss: 0.084838, acc.: 98.44%] [Generator loss: 6.496997]\n",
      "12678 [Discriminator loss: 0.101692, acc.: 98.44%] [Generator loss: 7.060722]\n",
      "12679 [Discriminator loss: 0.142704, acc.: 95.31%] [Generator loss: 5.558340]\n",
      "12680 [Discriminator loss: 0.033672, acc.: 98.44%] [Generator loss: 7.025655]\n",
      "12681 [Discriminator loss: 0.035105, acc.: 98.44%] [Generator loss: 6.945484]\n",
      "12682 [Discriminator loss: 0.110580, acc.: 96.88%] [Generator loss: 6.717093]\n",
      "12683 [Discriminator loss: 0.071598, acc.: 96.88%] [Generator loss: 7.711570]\n",
      "12684 [Discriminator loss: 0.014759, acc.: 100.00%] [Generator loss: 10.048093]\n",
      "12685 [Discriminator loss: 0.054958, acc.: 96.88%] [Generator loss: 7.409562]\n",
      "12686 [Discriminator loss: 0.166606, acc.: 92.19%] [Generator loss: 7.175138]\n",
      "12687 [Discriminator loss: 0.104246, acc.: 98.44%] [Generator loss: 7.407722]\n",
      "12688 [Discriminator loss: 0.217346, acc.: 95.31%] [Generator loss: 7.485297]\n",
      "12689 [Discriminator loss: 0.117761, acc.: 96.88%] [Generator loss: 8.841195]\n",
      "12690 [Discriminator loss: 0.166854, acc.: 92.19%] [Generator loss: 8.076128]\n",
      "12691 [Discriminator loss: 0.067619, acc.: 98.44%] [Generator loss: 6.873327]\n",
      "12692 [Discriminator loss: 0.029742, acc.: 98.44%] [Generator loss: 6.287389]\n",
      "12693 [Discriminator loss: 0.218767, acc.: 93.75%] [Generator loss: 6.882900]\n",
      "12694 [Discriminator loss: 0.050541, acc.: 98.44%] [Generator loss: 8.530502]\n",
      "12695 [Discriminator loss: 0.558743, acc.: 73.44%] [Generator loss: 7.166967]\n",
      "12696 [Discriminator loss: 0.040935, acc.: 98.44%] [Generator loss: 9.314765]\n",
      "12697 [Discriminator loss: 0.020037, acc.: 100.00%] [Generator loss: 7.422483]\n",
      "12698 [Discriminator loss: 0.253568, acc.: 89.06%] [Generator loss: 6.747681]\n",
      "12699 [Discriminator loss: 0.163418, acc.: 95.31%] [Generator loss: 6.179235]\n",
      "12700 [Discriminator loss: 0.081496, acc.: 95.31%] [Generator loss: 6.841882]\n",
      "12701 [Discriminator loss: 0.060958, acc.: 100.00%] [Generator loss: 8.288336]\n",
      "12702 [Discriminator loss: 0.167524, acc.: 93.75%] [Generator loss: 7.674670]\n",
      "12703 [Discriminator loss: 0.212433, acc.: 95.31%] [Generator loss: 7.182011]\n",
      "12704 [Discriminator loss: 0.249144, acc.: 87.50%] [Generator loss: 8.389251]\n",
      "12705 [Discriminator loss: 0.158199, acc.: 93.75%] [Generator loss: 8.759773]\n",
      "12706 [Discriminator loss: 0.071232, acc.: 95.31%] [Generator loss: 7.135818]\n",
      "12707 [Discriminator loss: 0.020606, acc.: 100.00%] [Generator loss: 7.019985]\n",
      "12708 [Discriminator loss: 0.107396, acc.: 93.75%] [Generator loss: 7.368435]\n",
      "12709 [Discriminator loss: 0.081259, acc.: 96.88%] [Generator loss: 7.550787]\n",
      "12710 [Discriminator loss: 0.093842, acc.: 96.88%] [Generator loss: 7.154945]\n",
      "12711 [Discriminator loss: 0.125113, acc.: 96.88%] [Generator loss: 7.931199]\n",
      "12712 [Discriminator loss: 0.113907, acc.: 96.88%] [Generator loss: 9.390955]\n",
      "12713 [Discriminator loss: 0.102816, acc.: 96.88%] [Generator loss: 6.847673]\n",
      "12714 [Discriminator loss: 0.096405, acc.: 95.31%] [Generator loss: 6.260701]\n",
      "12715 [Discriminator loss: 0.334931, acc.: 89.06%] [Generator loss: 10.572807]\n",
      "12716 [Discriminator loss: 0.127361, acc.: 93.75%] [Generator loss: 9.496202]\n",
      "12717 [Discriminator loss: 0.070387, acc.: 96.88%] [Generator loss: 10.783633]\n",
      "12718 [Discriminator loss: 0.147130, acc.: 93.75%] [Generator loss: 6.839088]\n",
      "12719 [Discriminator loss: 0.111852, acc.: 96.88%] [Generator loss: 7.573896]\n",
      "12720 [Discriminator loss: 0.059041, acc.: 96.88%] [Generator loss: 7.240367]\n",
      "12721 [Discriminator loss: 0.040230, acc.: 98.44%] [Generator loss: 7.247847]\n",
      "12722 [Discriminator loss: 0.097830, acc.: 95.31%] [Generator loss: 7.075362]\n",
      "12723 [Discriminator loss: 0.104289, acc.: 95.31%] [Generator loss: 6.508630]\n",
      "12724 [Discriminator loss: 0.252539, acc.: 85.94%] [Generator loss: 8.617375]\n",
      "12725 [Discriminator loss: 0.099012, acc.: 96.88%] [Generator loss: 7.501612]\n",
      "12726 [Discriminator loss: 0.135679, acc.: 92.19%] [Generator loss: 6.864150]\n",
      "12727 [Discriminator loss: 0.292394, acc.: 90.62%] [Generator loss: 8.465078]\n",
      "12728 [Discriminator loss: 0.115627, acc.: 95.31%] [Generator loss: 8.695461]\n",
      "12729 [Discriminator loss: 0.020474, acc.: 100.00%] [Generator loss: 8.593384]\n",
      "12730 [Discriminator loss: 0.204979, acc.: 93.75%] [Generator loss: 6.700780]\n",
      "12731 [Discriminator loss: 0.070677, acc.: 95.31%] [Generator loss: 7.015938]\n",
      "12732 [Discriminator loss: 0.208354, acc.: 92.19%] [Generator loss: 8.006344]\n",
      "12733 [Discriminator loss: 0.135392, acc.: 93.75%] [Generator loss: 7.599685]\n",
      "12734 [Discriminator loss: 0.047690, acc.: 96.88%] [Generator loss: 6.902649]\n",
      "12735 [Discriminator loss: 0.034748, acc.: 98.44%] [Generator loss: 8.046064]\n",
      "12736 [Discriminator loss: 0.121250, acc.: 96.88%] [Generator loss: 7.272184]\n",
      "12737 [Discriminator loss: 0.081652, acc.: 96.88%] [Generator loss: 6.112346]\n",
      "12738 [Discriminator loss: 0.087616, acc.: 98.44%] [Generator loss: 7.471895]\n",
      "12739 [Discriminator loss: 0.166832, acc.: 90.62%] [Generator loss: 7.672275]\n",
      "12740 [Discriminator loss: 0.112037, acc.: 95.31%] [Generator loss: 9.067378]\n",
      "12741 [Discriminator loss: 0.090564, acc.: 98.44%] [Generator loss: 7.532695]\n",
      "12742 [Discriminator loss: 0.072916, acc.: 96.88%] [Generator loss: 7.541998]\n",
      "12743 [Discriminator loss: 0.144609, acc.: 96.88%] [Generator loss: 5.299633]\n",
      "12744 [Discriminator loss: 0.207408, acc.: 93.75%] [Generator loss: 6.509193]\n",
      "12745 [Discriminator loss: 0.050911, acc.: 98.44%] [Generator loss: 6.909076]\n",
      "12746 [Discriminator loss: 0.090056, acc.: 95.31%] [Generator loss: 8.221664]\n",
      "12747 [Discriminator loss: 0.094246, acc.: 95.31%] [Generator loss: 7.515936]\n",
      "12748 [Discriminator loss: 0.180352, acc.: 96.88%] [Generator loss: 6.544567]\n",
      "12749 [Discriminator loss: 0.028771, acc.: 100.00%] [Generator loss: 7.989430]\n",
      "12750 [Discriminator loss: 0.188429, acc.: 90.62%] [Generator loss: 7.695733]\n",
      "12751 [Discriminator loss: 0.135669, acc.: 92.19%] [Generator loss: 7.197682]\n",
      "12752 [Discriminator loss: 0.089498, acc.: 96.88%] [Generator loss: 7.338111]\n",
      "12753 [Discriminator loss: 0.154768, acc.: 95.31%] [Generator loss: 7.416101]\n",
      "12754 [Discriminator loss: 0.031473, acc.: 100.00%] [Generator loss: 7.241522]\n",
      "12755 [Discriminator loss: 0.130327, acc.: 93.75%] [Generator loss: 5.769941]\n",
      "12756 [Discriminator loss: 0.152703, acc.: 95.31%] [Generator loss: 7.371231]\n",
      "12757 [Discriminator loss: 0.079424, acc.: 95.31%] [Generator loss: 7.777665]\n",
      "12758 [Discriminator loss: 0.121017, acc.: 93.75%] [Generator loss: 7.531271]\n",
      "12759 [Discriminator loss: 0.157696, acc.: 93.75%] [Generator loss: 5.495287]\n",
      "12760 [Discriminator loss: 0.085263, acc.: 96.88%] [Generator loss: 7.187850]\n",
      "12761 [Discriminator loss: 0.058575, acc.: 98.44%] [Generator loss: 7.499750]\n",
      "12762 [Discriminator loss: 0.106588, acc.: 95.31%] [Generator loss: 7.941759]\n",
      "12763 [Discriminator loss: 0.021012, acc.: 98.44%] [Generator loss: 8.206037]\n",
      "12764 [Discriminator loss: 0.287960, acc.: 82.81%] [Generator loss: 8.825140]\n",
      "12765 [Discriminator loss: 0.085111, acc.: 96.88%] [Generator loss: 8.238010]\n",
      "12766 [Discriminator loss: 0.103708, acc.: 96.88%] [Generator loss: 6.283799]\n",
      "12767 [Discriminator loss: 0.128125, acc.: 93.75%] [Generator loss: 7.260583]\n",
      "12768 [Discriminator loss: 0.122183, acc.: 93.75%] [Generator loss: 6.071074]\n",
      "12769 [Discriminator loss: 0.020297, acc.: 100.00%] [Generator loss: 8.127352]\n",
      "12770 [Discriminator loss: 0.104928, acc.: 93.75%] [Generator loss: 7.736462]\n",
      "12771 [Discriminator loss: 0.045790, acc.: 96.88%] [Generator loss: 7.309158]\n",
      "12772 [Discriminator loss: 0.049841, acc.: 98.44%] [Generator loss: 6.285040]\n",
      "12773 [Discriminator loss: 0.061155, acc.: 98.44%] [Generator loss: 7.228768]\n",
      "12774 [Discriminator loss: 0.043195, acc.: 96.88%] [Generator loss: 7.596457]\n",
      "12775 [Discriminator loss: 0.235642, acc.: 92.19%] [Generator loss: 5.797578]\n",
      "12776 [Discriminator loss: 0.041046, acc.: 98.44%] [Generator loss: 7.152529]\n",
      "12777 [Discriminator loss: 0.115169, acc.: 93.75%] [Generator loss: 6.146074]\n",
      "12778 [Discriminator loss: 0.148194, acc.: 92.19%] [Generator loss: 8.491498]\n",
      "12779 [Discriminator loss: 0.021579, acc.: 100.00%] [Generator loss: 7.928303]\n",
      "12780 [Discriminator loss: 0.080989, acc.: 95.31%] [Generator loss: 8.126026]\n",
      "12781 [Discriminator loss: 0.041686, acc.: 98.44%] [Generator loss: 8.231119]\n",
      "12782 [Discriminator loss: 0.040169, acc.: 98.44%] [Generator loss: 8.265684]\n",
      "12783 [Discriminator loss: 0.090138, acc.: 95.31%] [Generator loss: 5.999637]\n",
      "12784 [Discriminator loss: 0.192631, acc.: 92.19%] [Generator loss: 8.959017]\n",
      "12785 [Discriminator loss: 0.038939, acc.: 100.00%] [Generator loss: 9.081505]\n",
      "12786 [Discriminator loss: 0.232646, acc.: 93.75%] [Generator loss: 6.510438]\n",
      "12787 [Discriminator loss: 0.063275, acc.: 98.44%] [Generator loss: 7.031953]\n",
      "12788 [Discriminator loss: 0.095761, acc.: 96.88%] [Generator loss: 7.415562]\n",
      "12789 [Discriminator loss: 0.096353, acc.: 95.31%] [Generator loss: 8.209391]\n",
      "12790 [Discriminator loss: 0.145053, acc.: 95.31%] [Generator loss: 7.664041]\n",
      "12791 [Discriminator loss: 0.122217, acc.: 95.31%] [Generator loss: 7.465009]\n",
      "12792 [Discriminator loss: 0.124015, acc.: 93.75%] [Generator loss: 6.686724]\n",
      "12793 [Discriminator loss: 0.167840, acc.: 92.19%] [Generator loss: 8.566425]\n",
      "12794 [Discriminator loss: 0.082192, acc.: 98.44%] [Generator loss: 8.398083]\n",
      "12795 [Discriminator loss: 0.034069, acc.: 100.00%] [Generator loss: 7.806798]\n",
      "12796 [Discriminator loss: 0.116646, acc.: 95.31%] [Generator loss: 7.717117]\n",
      "12797 [Discriminator loss: 0.101717, acc.: 92.19%] [Generator loss: 8.971008]\n",
      "12798 [Discriminator loss: 0.051001, acc.: 98.44%] [Generator loss: 7.499684]\n",
      "12799 [Discriminator loss: 0.075291, acc.: 96.88%] [Generator loss: 7.596653]\n",
      "12800 [Discriminator loss: 0.134375, acc.: 95.31%] [Generator loss: 8.360837]\n",
      "12801 [Discriminator loss: 0.119583, acc.: 93.75%] [Generator loss: 7.309264]\n",
      "12802 [Discriminator loss: 0.068810, acc.: 96.88%] [Generator loss: 7.907112]\n",
      "12803 [Discriminator loss: 0.061727, acc.: 98.44%] [Generator loss: 6.706606]\n",
      "12804 [Discriminator loss: 0.220930, acc.: 92.19%] [Generator loss: 6.650477]\n",
      "12805 [Discriminator loss: 0.093516, acc.: 96.88%] [Generator loss: 5.862143]\n",
      "12806 [Discriminator loss: 0.055653, acc.: 98.44%] [Generator loss: 5.955240]\n",
      "12807 [Discriminator loss: 0.132019, acc.: 93.75%] [Generator loss: 8.617071]\n",
      "12808 [Discriminator loss: 0.051235, acc.: 98.44%] [Generator loss: 7.468872]\n",
      "12809 [Discriminator loss: 0.178289, acc.: 93.75%] [Generator loss: 7.363544]\n",
      "12810 [Discriminator loss: 0.132476, acc.: 92.19%] [Generator loss: 7.513727]\n",
      "12811 [Discriminator loss: 0.040155, acc.: 100.00%] [Generator loss: 7.768925]\n",
      "12812 [Discriminator loss: 0.205499, acc.: 92.19%] [Generator loss: 7.741801]\n",
      "12813 [Discriminator loss: 0.068137, acc.: 98.44%] [Generator loss: 7.343304]\n",
      "12814 [Discriminator loss: 0.081918, acc.: 96.88%] [Generator loss: 6.943206]\n",
      "12815 [Discriminator loss: 0.071708, acc.: 96.88%] [Generator loss: 6.671063]\n",
      "12816 [Discriminator loss: 0.068953, acc.: 96.88%] [Generator loss: 7.668355]\n",
      "12817 [Discriminator loss: 0.041956, acc.: 96.88%] [Generator loss: 5.758261]\n",
      "12818 [Discriminator loss: 0.096449, acc.: 93.75%] [Generator loss: 7.281096]\n",
      "12819 [Discriminator loss: 0.389721, acc.: 81.25%] [Generator loss: 8.447533]\n",
      "12820 [Discriminator loss: 0.134669, acc.: 90.62%] [Generator loss: 6.811090]\n",
      "12821 [Discriminator loss: 0.104586, acc.: 95.31%] [Generator loss: 8.023918]\n",
      "12822 [Discriminator loss: 0.132112, acc.: 92.19%] [Generator loss: 5.949216]\n",
      "12823 [Discriminator loss: 0.082366, acc.: 95.31%] [Generator loss: 8.135602]\n",
      "12824 [Discriminator loss: 0.172438, acc.: 93.75%] [Generator loss: 6.633822]\n",
      "12825 [Discriminator loss: 0.066122, acc.: 98.44%] [Generator loss: 7.303443]\n",
      "12826 [Discriminator loss: 0.064580, acc.: 95.31%] [Generator loss: 6.416559]\n",
      "12827 [Discriminator loss: 0.102190, acc.: 95.31%] [Generator loss: 6.934703]\n",
      "12828 [Discriminator loss: 0.136503, acc.: 92.19%] [Generator loss: 5.268114]\n",
      "12829 [Discriminator loss: 0.169980, acc.: 90.62%] [Generator loss: 9.336211]\n",
      "12830 [Discriminator loss: 0.104345, acc.: 96.88%] [Generator loss: 9.056728]\n",
      "12831 [Discriminator loss: 0.125445, acc.: 95.31%] [Generator loss: 7.780926]\n",
      "12832 [Discriminator loss: 0.091431, acc.: 95.31%] [Generator loss: 7.961963]\n",
      "12833 [Discriminator loss: 0.104589, acc.: 95.31%] [Generator loss: 8.451742]\n",
      "12834 [Discriminator loss: 0.038479, acc.: 100.00%] [Generator loss: 6.894314]\n",
      "12835 [Discriminator loss: 0.036848, acc.: 100.00%] [Generator loss: 6.990644]\n",
      "12836 [Discriminator loss: 0.081272, acc.: 95.31%] [Generator loss: 6.792819]\n",
      "12837 [Discriminator loss: 0.150999, acc.: 93.75%] [Generator loss: 7.879547]\n",
      "12838 [Discriminator loss: 0.020502, acc.: 100.00%] [Generator loss: 9.689154]\n",
      "12839 [Discriminator loss: 0.208497, acc.: 93.75%] [Generator loss: 7.627860]\n",
      "12840 [Discriminator loss: 0.095734, acc.: 95.31%] [Generator loss: 7.034022]\n",
      "12841 [Discriminator loss: 0.221549, acc.: 92.19%] [Generator loss: 6.250324]\n",
      "12842 [Discriminator loss: 0.093131, acc.: 96.88%] [Generator loss: 6.715196]\n",
      "12843 [Discriminator loss: 0.052142, acc.: 98.44%] [Generator loss: 6.196949]\n",
      "12844 [Discriminator loss: 0.116072, acc.: 93.75%] [Generator loss: 6.577106]\n",
      "12845 [Discriminator loss: 0.190709, acc.: 90.62%] [Generator loss: 8.493036]\n",
      "12846 [Discriminator loss: 0.095000, acc.: 95.31%] [Generator loss: 7.603245]\n",
      "12847 [Discriminator loss: 0.129569, acc.: 96.88%] [Generator loss: 7.031932]\n",
      "12848 [Discriminator loss: 0.046853, acc.: 100.00%] [Generator loss: 7.813128]\n",
      "12849 [Discriminator loss: 0.106600, acc.: 95.31%] [Generator loss: 6.969746]\n",
      "12850 [Discriminator loss: 0.066195, acc.: 98.44%] [Generator loss: 6.686779]\n",
      "12851 [Discriminator loss: 0.035908, acc.: 98.44%] [Generator loss: 7.967230]\n",
      "12852 [Discriminator loss: 0.091777, acc.: 96.88%] [Generator loss: 7.752126]\n",
      "12853 [Discriminator loss: 0.093455, acc.: 98.44%] [Generator loss: 7.799535]\n",
      "12854 [Discriminator loss: 0.029004, acc.: 100.00%] [Generator loss: 7.578100]\n",
      "12855 [Discriminator loss: 0.061053, acc.: 98.44%] [Generator loss: 6.192966]\n",
      "12856 [Discriminator loss: 0.122290, acc.: 96.88%] [Generator loss: 6.485962]\n",
      "12857 [Discriminator loss: 0.186589, acc.: 89.06%] [Generator loss: 8.171114]\n",
      "12858 [Discriminator loss: 0.098308, acc.: 96.88%] [Generator loss: 6.287691]\n",
      "12859 [Discriminator loss: 0.253458, acc.: 89.06%] [Generator loss: 6.274366]\n",
      "12860 [Discriminator loss: 0.082935, acc.: 96.88%] [Generator loss: 8.013746]\n",
      "12861 [Discriminator loss: 0.203705, acc.: 92.19%] [Generator loss: 7.292339]\n",
      "12862 [Discriminator loss: 0.171714, acc.: 95.31%] [Generator loss: 8.872128]\n",
      "12863 [Discriminator loss: 0.082539, acc.: 96.88%] [Generator loss: 7.549320]\n",
      "12864 [Discriminator loss: 0.034038, acc.: 100.00%] [Generator loss: 6.582871]\n",
      "12865 [Discriminator loss: 0.312656, acc.: 90.62%] [Generator loss: 8.336744]\n",
      "12866 [Discriminator loss: 0.070188, acc.: 98.44%] [Generator loss: 8.165321]\n",
      "12867 [Discriminator loss: 0.071388, acc.: 98.44%] [Generator loss: 7.308452]\n",
      "12868 [Discriminator loss: 0.064690, acc.: 98.44%] [Generator loss: 6.049150]\n",
      "12869 [Discriminator loss: 0.050868, acc.: 98.44%] [Generator loss: 6.555489]\n",
      "12870 [Discriminator loss: 0.114440, acc.: 95.31%] [Generator loss: 6.811481]\n",
      "12871 [Discriminator loss: 0.102801, acc.: 93.75%] [Generator loss: 5.929093]\n",
      "12872 [Discriminator loss: 0.062481, acc.: 96.88%] [Generator loss: 8.406389]\n",
      "12873 [Discriminator loss: 0.230743, acc.: 90.62%] [Generator loss: 5.328733]\n",
      "12874 [Discriminator loss: 0.110449, acc.: 93.75%] [Generator loss: 7.489303]\n",
      "12875 [Discriminator loss: 0.050401, acc.: 98.44%] [Generator loss: 8.597612]\n",
      "12876 [Discriminator loss: 0.033068, acc.: 100.00%] [Generator loss: 6.815916]\n",
      "12877 [Discriminator loss: 0.108104, acc.: 93.75%] [Generator loss: 6.191062]\n",
      "12878 [Discriminator loss: 0.039758, acc.: 98.44%] [Generator loss: 7.925231]\n",
      "12879 [Discriminator loss: 0.052141, acc.: 98.44%] [Generator loss: 7.579992]\n",
      "12880 [Discriminator loss: 0.068750, acc.: 98.44%] [Generator loss: 6.070364]\n",
      "12881 [Discriminator loss: 0.050641, acc.: 96.88%] [Generator loss: 5.380198]\n",
      "12882 [Discriminator loss: 0.101853, acc.: 96.88%] [Generator loss: 7.702219]\n",
      "12883 [Discriminator loss: 0.093342, acc.: 95.31%] [Generator loss: 7.388969]\n",
      "12884 [Discriminator loss: 0.080842, acc.: 95.31%] [Generator loss: 6.999859]\n",
      "12885 [Discriminator loss: 0.117926, acc.: 98.44%] [Generator loss: 6.100002]\n",
      "12886 [Discriminator loss: 0.249910, acc.: 89.06%] [Generator loss: 6.908916]\n",
      "12887 [Discriminator loss: 0.043035, acc.: 98.44%] [Generator loss: 7.442906]\n",
      "12888 [Discriminator loss: 0.281567, acc.: 89.06%] [Generator loss: 7.370034]\n",
      "12889 [Discriminator loss: 0.136292, acc.: 96.88%] [Generator loss: 6.452518]\n",
      "12890 [Discriminator loss: 0.106596, acc.: 93.75%] [Generator loss: 6.455841]\n",
      "12891 [Discriminator loss: 0.028814, acc.: 100.00%] [Generator loss: 6.069730]\n",
      "12892 [Discriminator loss: 0.032951, acc.: 98.44%] [Generator loss: 7.701195]\n",
      "12893 [Discriminator loss: 0.073128, acc.: 98.44%] [Generator loss: 6.005264]\n",
      "12894 [Discriminator loss: 0.041013, acc.: 98.44%] [Generator loss: 7.098872]\n",
      "12895 [Discriminator loss: 0.099728, acc.: 95.31%] [Generator loss: 7.312251]\n",
      "12896 [Discriminator loss: 0.144087, acc.: 93.75%] [Generator loss: 7.988872]\n",
      "12897 [Discriminator loss: 0.089018, acc.: 98.44%] [Generator loss: 7.398884]\n",
      "12898 [Discriminator loss: 0.081318, acc.: 95.31%] [Generator loss: 8.037264]\n",
      "12899 [Discriminator loss: 0.071635, acc.: 96.88%] [Generator loss: 7.847199]\n",
      "12900 [Discriminator loss: 0.037640, acc.: 100.00%] [Generator loss: 7.355791]\n",
      "12901 [Discriminator loss: 0.032260, acc.: 100.00%] [Generator loss: 7.353902]\n",
      "12902 [Discriminator loss: 0.081855, acc.: 96.88%] [Generator loss: 7.852122]\n",
      "12903 [Discriminator loss: 0.134883, acc.: 93.75%] [Generator loss: 8.529448]\n",
      "12904 [Discriminator loss: 0.179702, acc.: 93.75%] [Generator loss: 7.188184]\n",
      "12905 [Discriminator loss: 0.191027, acc.: 89.06%] [Generator loss: 6.806043]\n",
      "12906 [Discriminator loss: 0.230368, acc.: 92.19%] [Generator loss: 7.640579]\n",
      "12907 [Discriminator loss: 0.033176, acc.: 100.00%] [Generator loss: 8.234104]\n",
      "12908 [Discriminator loss: 0.108621, acc.: 96.88%] [Generator loss: 5.906353]\n",
      "12909 [Discriminator loss: 0.051608, acc.: 98.44%] [Generator loss: 7.677351]\n",
      "12910 [Discriminator loss: 0.059570, acc.: 96.88%] [Generator loss: 7.824212]\n",
      "12911 [Discriminator loss: 0.030996, acc.: 100.00%] [Generator loss: 8.071251]\n",
      "12912 [Discriminator loss: 0.096843, acc.: 95.31%] [Generator loss: 7.237886]\n",
      "12913 [Discriminator loss: 0.083614, acc.: 96.88%] [Generator loss: 7.335174]\n",
      "12914 [Discriminator loss: 0.058975, acc.: 98.44%] [Generator loss: 7.051297]\n",
      "12915 [Discriminator loss: 0.218075, acc.: 92.19%] [Generator loss: 9.218829]\n",
      "12916 [Discriminator loss: 0.068636, acc.: 96.88%] [Generator loss: 6.455405]\n",
      "12917 [Discriminator loss: 0.122732, acc.: 93.75%] [Generator loss: 4.844006]\n",
      "12918 [Discriminator loss: 0.038702, acc.: 100.00%] [Generator loss: 7.274136]\n",
      "12919 [Discriminator loss: 0.120853, acc.: 93.75%] [Generator loss: 9.051941]\n",
      "12920 [Discriminator loss: 0.168705, acc.: 92.19%] [Generator loss: 6.217137]\n",
      "12921 [Discriminator loss: 0.056400, acc.: 96.88%] [Generator loss: 6.287861]\n",
      "12922 [Discriminator loss: 0.124333, acc.: 95.31%] [Generator loss: 9.786003]\n",
      "12923 [Discriminator loss: 0.059057, acc.: 98.44%] [Generator loss: 7.977150]\n",
      "12924 [Discriminator loss: 0.056157, acc.: 98.44%] [Generator loss: 6.082934]\n",
      "12925 [Discriminator loss: 0.077286, acc.: 98.44%] [Generator loss: 7.460107]\n",
      "12926 [Discriminator loss: 0.090030, acc.: 96.88%] [Generator loss: 6.517478]\n",
      "12927 [Discriminator loss: 0.069250, acc.: 96.88%] [Generator loss: 7.245020]\n",
      "12928 [Discriminator loss: 0.149278, acc.: 93.75%] [Generator loss: 7.371148]\n",
      "12929 [Discriminator loss: 0.011550, acc.: 100.00%] [Generator loss: 6.993406]\n",
      "12930 [Discriminator loss: 0.103266, acc.: 96.88%] [Generator loss: 8.129862]\n",
      "12931 [Discriminator loss: 0.027835, acc.: 98.44%] [Generator loss: 6.521771]\n",
      "12932 [Discriminator loss: 0.104860, acc.: 93.75%] [Generator loss: 6.334981]\n",
      "12933 [Discriminator loss: 0.034658, acc.: 100.00%] [Generator loss: 8.554089]\n",
      "12934 [Discriminator loss: 0.039768, acc.: 100.00%] [Generator loss: 7.086834]\n",
      "12935 [Discriminator loss: 0.104162, acc.: 95.31%] [Generator loss: 6.972319]\n",
      "12936 [Discriminator loss: 0.113559, acc.: 95.31%] [Generator loss: 7.285855]\n",
      "12937 [Discriminator loss: 0.146152, acc.: 96.88%] [Generator loss: 8.504725]\n",
      "12938 [Discriminator loss: 0.032924, acc.: 100.00%] [Generator loss: 6.919893]\n",
      "12939 [Discriminator loss: 0.162231, acc.: 92.19%] [Generator loss: 8.384204]\n",
      "12940 [Discriminator loss: 0.089735, acc.: 95.31%] [Generator loss: 7.604949]\n",
      "12941 [Discriminator loss: 0.028470, acc.: 100.00%] [Generator loss: 7.395328]\n",
      "12942 [Discriminator loss: 0.043658, acc.: 98.44%] [Generator loss: 7.536129]\n",
      "12943 [Discriminator loss: 0.048698, acc.: 98.44%] [Generator loss: 5.923485]\n",
      "12944 [Discriminator loss: 0.124923, acc.: 95.31%] [Generator loss: 6.770204]\n",
      "12945 [Discriminator loss: 0.151904, acc.: 93.75%] [Generator loss: 7.237373]\n",
      "12946 [Discriminator loss: 0.091234, acc.: 96.88%] [Generator loss: 7.645674]\n",
      "12947 [Discriminator loss: 0.030828, acc.: 100.00%] [Generator loss: 5.581806]\n",
      "12948 [Discriminator loss: 0.086346, acc.: 95.31%] [Generator loss: 6.623215]\n",
      "12949 [Discriminator loss: 0.096321, acc.: 96.88%] [Generator loss: 6.304297]\n",
      "12950 [Discriminator loss: 0.225228, acc.: 95.31%] [Generator loss: 7.525919]\n",
      "12951 [Discriminator loss: 0.100526, acc.: 95.31%] [Generator loss: 8.875723]\n",
      "12952 [Discriminator loss: 0.134627, acc.: 92.19%] [Generator loss: 5.898483]\n",
      "12953 [Discriminator loss: 0.231760, acc.: 92.19%] [Generator loss: 8.492967]\n",
      "12954 [Discriminator loss: 0.090420, acc.: 95.31%] [Generator loss: 10.400543]\n",
      "12955 [Discriminator loss: 0.110262, acc.: 95.31%] [Generator loss: 8.002812]\n",
      "12956 [Discriminator loss: 0.330954, acc.: 90.62%] [Generator loss: 8.603826]\n",
      "12957 [Discriminator loss: 0.152730, acc.: 93.75%] [Generator loss: 7.547992]\n",
      "12958 [Discriminator loss: 0.057849, acc.: 96.88%] [Generator loss: 6.819714]\n",
      "12959 [Discriminator loss: 0.047019, acc.: 100.00%] [Generator loss: 8.555300]\n",
      "12960 [Discriminator loss: 0.150335, acc.: 93.75%] [Generator loss: 8.126040]\n",
      "12961 [Discriminator loss: 0.128017, acc.: 93.75%] [Generator loss: 7.832283]\n",
      "12962 [Discriminator loss: 0.159890, acc.: 92.19%] [Generator loss: 6.922673]\n",
      "12963 [Discriminator loss: 0.146537, acc.: 93.75%] [Generator loss: 7.927679]\n",
      "12964 [Discriminator loss: 0.085700, acc.: 95.31%] [Generator loss: 7.507686]\n",
      "12965 [Discriminator loss: 0.059612, acc.: 98.44%] [Generator loss: 8.552236]\n",
      "12966 [Discriminator loss: 0.125601, acc.: 95.31%] [Generator loss: 8.639557]\n",
      "12967 [Discriminator loss: 0.125752, acc.: 95.31%] [Generator loss: 7.921932]\n",
      "12968 [Discriminator loss: 0.062991, acc.: 98.44%] [Generator loss: 7.554448]\n",
      "12969 [Discriminator loss: 0.048688, acc.: 100.00%] [Generator loss: 6.974621]\n",
      "12970 [Discriminator loss: 0.109015, acc.: 95.31%] [Generator loss: 7.855392]\n",
      "12971 [Discriminator loss: 0.045705, acc.: 98.44%] [Generator loss: 7.457699]\n",
      "12972 [Discriminator loss: 0.221246, acc.: 89.06%] [Generator loss: 7.017677]\n",
      "12973 [Discriminator loss: 0.167985, acc.: 95.31%] [Generator loss: 6.916902]\n",
      "12974 [Discriminator loss: 0.050182, acc.: 98.44%] [Generator loss: 7.430063]\n",
      "12975 [Discriminator loss: 0.069768, acc.: 96.88%] [Generator loss: 7.183101]\n",
      "12976 [Discriminator loss: 0.087015, acc.: 96.88%] [Generator loss: 9.529383]\n",
      "12977 [Discriminator loss: 0.117685, acc.: 96.88%] [Generator loss: 6.489450]\n",
      "12978 [Discriminator loss: 0.081968, acc.: 96.88%] [Generator loss: 8.422943]\n",
      "12979 [Discriminator loss: 0.321502, acc.: 89.06%] [Generator loss: 8.675722]\n",
      "12980 [Discriminator loss: 0.093594, acc.: 93.75%] [Generator loss: 8.360990]\n",
      "12981 [Discriminator loss: 0.066232, acc.: 96.88%] [Generator loss: 8.903203]\n",
      "12982 [Discriminator loss: 0.136647, acc.: 93.75%] [Generator loss: 6.364120]\n",
      "12983 [Discriminator loss: 0.206984, acc.: 92.19%] [Generator loss: 6.873935]\n",
      "12984 [Discriminator loss: 0.080411, acc.: 98.44%] [Generator loss: 7.879965]\n",
      "12985 [Discriminator loss: 0.031356, acc.: 100.00%] [Generator loss: 7.420081]\n",
      "12986 [Discriminator loss: 0.178082, acc.: 95.31%] [Generator loss: 7.434355]\n",
      "12987 [Discriminator loss: 0.086263, acc.: 96.88%] [Generator loss: 7.440417]\n",
      "12988 [Discriminator loss: 0.027787, acc.: 100.00%] [Generator loss: 7.379127]\n",
      "12989 [Discriminator loss: 0.097955, acc.: 95.31%] [Generator loss: 7.247963]\n",
      "12990 [Discriminator loss: 0.074979, acc.: 96.88%] [Generator loss: 8.174410]\n",
      "12991 [Discriminator loss: 0.115136, acc.: 95.31%] [Generator loss: 7.950876]\n",
      "12992 [Discriminator loss: 0.307915, acc.: 89.06%] [Generator loss: 6.008615]\n",
      "12993 [Discriminator loss: 0.099486, acc.: 95.31%] [Generator loss: 7.974688]\n",
      "12994 [Discriminator loss: 0.027193, acc.: 100.00%] [Generator loss: 8.459071]\n",
      "12995 [Discriminator loss: 0.154443, acc.: 92.19%] [Generator loss: 6.889452]\n",
      "12996 [Discriminator loss: 0.116534, acc.: 93.75%] [Generator loss: 7.145017]\n",
      "12997 [Discriminator loss: 0.062624, acc.: 96.88%] [Generator loss: 7.695256]\n",
      "12998 [Discriminator loss: 0.099117, acc.: 95.31%] [Generator loss: 7.690297]\n",
      "12999 [Discriminator loss: 0.052587, acc.: 98.44%] [Generator loss: 6.934975]\n",
      "13000 [Discriminator loss: 0.114291, acc.: 96.88%] [Generator loss: 8.676431]\n",
      "13001 [Discriminator loss: 0.049417, acc.: 98.44%] [Generator loss: 9.220899]\n",
      "13002 [Discriminator loss: 0.192435, acc.: 89.06%] [Generator loss: 8.987333]\n",
      "13003 [Discriminator loss: 0.264946, acc.: 85.94%] [Generator loss: 5.656485]\n",
      "13004 [Discriminator loss: 0.170634, acc.: 93.75%] [Generator loss: 8.784285]\n",
      "13005 [Discriminator loss: 0.018954, acc.: 98.44%] [Generator loss: 8.510031]\n",
      "13006 [Discriminator loss: 0.179385, acc.: 87.50%] [Generator loss: 7.410916]\n",
      "13007 [Discriminator loss: 0.016341, acc.: 100.00%] [Generator loss: 7.458107]\n",
      "13008 [Discriminator loss: 0.223372, acc.: 95.31%] [Generator loss: 6.990758]\n",
      "13009 [Discriminator loss: 0.150655, acc.: 93.75%] [Generator loss: 8.264544]\n",
      "13010 [Discriminator loss: 0.050866, acc.: 96.88%] [Generator loss: 6.968389]\n",
      "13011 [Discriminator loss: 0.063021, acc.: 98.44%] [Generator loss: 6.966900]\n",
      "13012 [Discriminator loss: 0.021403, acc.: 100.00%] [Generator loss: 7.425210]\n",
      "13013 [Discriminator loss: 0.143859, acc.: 93.75%] [Generator loss: 6.506362]\n",
      "13014 [Discriminator loss: 0.013850, acc.: 100.00%] [Generator loss: 8.050938]\n",
      "13015 [Discriminator loss: 0.073738, acc.: 96.88%] [Generator loss: 6.960469]\n",
      "13016 [Discriminator loss: 0.084931, acc.: 95.31%] [Generator loss: 5.096818]\n",
      "13017 [Discriminator loss: 0.357716, acc.: 87.50%] [Generator loss: 10.278645]\n",
      "13018 [Discriminator loss: 0.134047, acc.: 96.88%] [Generator loss: 10.324366]\n",
      "13019 [Discriminator loss: 0.239288, acc.: 90.62%] [Generator loss: 8.236221]\n",
      "13020 [Discriminator loss: 0.060387, acc.: 98.44%] [Generator loss: 7.349276]\n",
      "13021 [Discriminator loss: 0.053668, acc.: 100.00%] [Generator loss: 8.215872]\n",
      "13022 [Discriminator loss: 0.118892, acc.: 95.31%] [Generator loss: 7.088716]\n",
      "13023 [Discriminator loss: 0.077094, acc.: 95.31%] [Generator loss: 8.092703]\n",
      "13024 [Discriminator loss: 0.121111, acc.: 96.88%] [Generator loss: 7.054534]\n",
      "13025 [Discriminator loss: 0.060152, acc.: 98.44%] [Generator loss: 7.904568]\n",
      "13026 [Discriminator loss: 0.131034, acc.: 95.31%] [Generator loss: 6.385135]\n",
      "13027 [Discriminator loss: 0.109383, acc.: 95.31%] [Generator loss: 6.715986]\n",
      "13028 [Discriminator loss: 0.153824, acc.: 92.19%] [Generator loss: 7.998782]\n",
      "13029 [Discriminator loss: 0.052819, acc.: 98.44%] [Generator loss: 7.481633]\n",
      "13030 [Discriminator loss: 0.091883, acc.: 98.44%] [Generator loss: 6.768252]\n",
      "13031 [Discriminator loss: 0.040387, acc.: 100.00%] [Generator loss: 7.015410]\n",
      "13032 [Discriminator loss: 0.132967, acc.: 90.62%] [Generator loss: 4.684493]\n",
      "13033 [Discriminator loss: 0.053563, acc.: 96.88%] [Generator loss: 6.790006]\n",
      "13034 [Discriminator loss: 0.083569, acc.: 96.88%] [Generator loss: 6.808967]\n",
      "13035 [Discriminator loss: 0.027162, acc.: 100.00%] [Generator loss: 6.593259]\n",
      "13036 [Discriminator loss: 0.038130, acc.: 98.44%] [Generator loss: 6.838789]\n",
      "13037 [Discriminator loss: 0.075466, acc.: 96.88%] [Generator loss: 7.404569]\n",
      "13038 [Discriminator loss: 0.086762, acc.: 96.88%] [Generator loss: 6.980593]\n",
      "13039 [Discriminator loss: 0.069249, acc.: 96.88%] [Generator loss: 6.229326]\n",
      "13040 [Discriminator loss: 0.124801, acc.: 95.31%] [Generator loss: 6.431816]\n",
      "13041 [Discriminator loss: 0.098073, acc.: 95.31%] [Generator loss: 8.371150]\n",
      "13042 [Discriminator loss: 0.102834, acc.: 96.88%] [Generator loss: 8.434745]\n",
      "13043 [Discriminator loss: 0.075001, acc.: 98.44%] [Generator loss: 7.739758]\n",
      "13044 [Discriminator loss: 0.153907, acc.: 96.88%] [Generator loss: 7.125970]\n",
      "13045 [Discriminator loss: 0.047143, acc.: 98.44%] [Generator loss: 5.908392]\n",
      "13046 [Discriminator loss: 0.074727, acc.: 95.31%] [Generator loss: 7.365489]\n",
      "13047 [Discriminator loss: 0.148631, acc.: 92.19%] [Generator loss: 5.649018]\n",
      "13048 [Discriminator loss: 0.179112, acc.: 92.19%] [Generator loss: 7.251282]\n",
      "13049 [Discriminator loss: 0.068253, acc.: 95.31%] [Generator loss: 7.589152]\n",
      "13050 [Discriminator loss: 0.057964, acc.: 98.44%] [Generator loss: 6.877760]\n",
      "13051 [Discriminator loss: 0.115768, acc.: 95.31%] [Generator loss: 8.352486]\n",
      "13052 [Discriminator loss: 0.060833, acc.: 98.44%] [Generator loss: 7.146159]\n",
      "13053 [Discriminator loss: 0.061035, acc.: 98.44%] [Generator loss: 6.338349]\n",
      "13054 [Discriminator loss: 0.056643, acc.: 98.44%] [Generator loss: 7.536407]\n",
      "13055 [Discriminator loss: 0.080910, acc.: 96.88%] [Generator loss: 5.627149]\n",
      "13056 [Discriminator loss: 0.058546, acc.: 98.44%] [Generator loss: 5.685844]\n",
      "13057 [Discriminator loss: 0.026618, acc.: 100.00%] [Generator loss: 6.216833]\n",
      "13058 [Discriminator loss: 0.062826, acc.: 96.88%] [Generator loss: 7.332385]\n",
      "13059 [Discriminator loss: 0.262421, acc.: 89.06%] [Generator loss: 6.280644]\n",
      "13060 [Discriminator loss: 0.074984, acc.: 96.88%] [Generator loss: 6.976000]\n",
      "13061 [Discriminator loss: 0.061948, acc.: 96.88%] [Generator loss: 6.062438]\n",
      "13062 [Discriminator loss: 0.062045, acc.: 100.00%] [Generator loss: 6.756332]\n",
      "13063 [Discriminator loss: 0.093264, acc.: 95.31%] [Generator loss: 8.544619]\n",
      "13064 [Discriminator loss: 0.261847, acc.: 93.75%] [Generator loss: 6.298386]\n",
      "13065 [Discriminator loss: 0.171863, acc.: 93.75%] [Generator loss: 5.965011]\n",
      "13066 [Discriminator loss: 0.141422, acc.: 93.75%] [Generator loss: 6.222287]\n",
      "13067 [Discriminator loss: 0.125648, acc.: 95.31%] [Generator loss: 7.321253]\n",
      "13068 [Discriminator loss: 0.084477, acc.: 95.31%] [Generator loss: 8.231476]\n",
      "13069 [Discriminator loss: 0.215591, acc.: 92.19%] [Generator loss: 9.004502]\n",
      "13070 [Discriminator loss: 0.179980, acc.: 95.31%] [Generator loss: 7.292063]\n",
      "13071 [Discriminator loss: 0.149281, acc.: 95.31%] [Generator loss: 8.924088]\n",
      "13072 [Discriminator loss: 0.023227, acc.: 100.00%] [Generator loss: 7.846750]\n",
      "13073 [Discriminator loss: 0.249018, acc.: 90.62%] [Generator loss: 6.290171]\n",
      "13074 [Discriminator loss: 0.113422, acc.: 93.75%] [Generator loss: 5.300507]\n",
      "13075 [Discriminator loss: 0.022343, acc.: 100.00%] [Generator loss: 6.236195]\n",
      "13076 [Discriminator loss: 0.093624, acc.: 98.44%] [Generator loss: 5.653795]\n",
      "13077 [Discriminator loss: 0.075511, acc.: 98.44%] [Generator loss: 7.849249]\n",
      "13078 [Discriminator loss: 0.071282, acc.: 96.88%] [Generator loss: 7.063734]\n",
      "13079 [Discriminator loss: 0.069999, acc.: 98.44%] [Generator loss: 7.429333]\n",
      "13080 [Discriminator loss: 0.161233, acc.: 92.19%] [Generator loss: 8.133616]\n",
      "13081 [Discriminator loss: 0.156610, acc.: 93.75%] [Generator loss: 6.768635]\n",
      "13082 [Discriminator loss: 0.066407, acc.: 96.88%] [Generator loss: 7.335010]\n",
      "13083 [Discriminator loss: 0.077914, acc.: 98.44%] [Generator loss: 9.059477]\n",
      "13084 [Discriminator loss: 0.223376, acc.: 90.62%] [Generator loss: 9.163654]\n",
      "13085 [Discriminator loss: 0.177375, acc.: 90.62%] [Generator loss: 6.725092]\n",
      "13086 [Discriminator loss: 0.094361, acc.: 96.88%] [Generator loss: 6.618630]\n",
      "13087 [Discriminator loss: 0.055672, acc.: 98.44%] [Generator loss: 7.691871]\n",
      "13088 [Discriminator loss: 0.032400, acc.: 98.44%] [Generator loss: 8.528462]\n",
      "13089 [Discriminator loss: 0.191550, acc.: 92.19%] [Generator loss: 6.689540]\n",
      "13090 [Discriminator loss: 0.114897, acc.: 95.31%] [Generator loss: 7.815143]\n",
      "13091 [Discriminator loss: 0.069818, acc.: 95.31%] [Generator loss: 8.725005]\n",
      "13092 [Discriminator loss: 0.108032, acc.: 93.75%] [Generator loss: 6.554794]\n",
      "13093 [Discriminator loss: 0.049291, acc.: 96.88%] [Generator loss: 7.330820]\n",
      "13094 [Discriminator loss: 0.103883, acc.: 95.31%] [Generator loss: 6.482995]\n",
      "13095 [Discriminator loss: 0.116683, acc.: 96.88%] [Generator loss: 8.058174]\n",
      "13096 [Discriminator loss: 0.097529, acc.: 96.88%] [Generator loss: 7.231133]\n",
      "13097 [Discriminator loss: 0.122449, acc.: 95.31%] [Generator loss: 9.282008]\n",
      "13098 [Discriminator loss: 0.045345, acc.: 100.00%] [Generator loss: 8.353262]\n",
      "13099 [Discriminator loss: 0.166750, acc.: 95.31%] [Generator loss: 5.745905]\n",
      "13100 [Discriminator loss: 0.163073, acc.: 92.19%] [Generator loss: 7.306299]\n",
      "13101 [Discriminator loss: 0.040504, acc.: 100.00%] [Generator loss: 8.867928]\n",
      "13102 [Discriminator loss: 0.174039, acc.: 89.06%] [Generator loss: 7.909389]\n",
      "13103 [Discriminator loss: 0.149206, acc.: 96.88%] [Generator loss: 6.673992]\n",
      "13104 [Discriminator loss: 0.038810, acc.: 96.88%] [Generator loss: 6.620449]\n",
      "13105 [Discriminator loss: 0.031829, acc.: 98.44%] [Generator loss: 6.092832]\n",
      "13106 [Discriminator loss: 0.088967, acc.: 96.88%] [Generator loss: 7.905876]\n",
      "13107 [Discriminator loss: 0.121642, acc.: 96.88%] [Generator loss: 8.482276]\n",
      "13108 [Discriminator loss: 0.214057, acc.: 90.62%] [Generator loss: 6.877489]\n",
      "13109 [Discriminator loss: 0.063677, acc.: 98.44%] [Generator loss: 7.113583]\n",
      "13110 [Discriminator loss: 0.306102, acc.: 90.62%] [Generator loss: 6.262914]\n",
      "13111 [Discriminator loss: 0.057232, acc.: 96.88%] [Generator loss: 7.486099]\n",
      "13112 [Discriminator loss: 0.190911, acc.: 92.19%] [Generator loss: 8.119720]\n",
      "13113 [Discriminator loss: 0.048327, acc.: 98.44%] [Generator loss: 7.695306]\n",
      "13114 [Discriminator loss: 0.066323, acc.: 98.44%] [Generator loss: 7.218924]\n",
      "13115 [Discriminator loss: 0.025006, acc.: 100.00%] [Generator loss: 8.367746]\n",
      "13116 [Discriminator loss: 0.231009, acc.: 89.06%] [Generator loss: 6.768041]\n",
      "13117 [Discriminator loss: 0.084392, acc.: 96.88%] [Generator loss: 7.710266]\n",
      "13118 [Discriminator loss: 0.115406, acc.: 93.75%] [Generator loss: 6.757387]\n",
      "13119 [Discriminator loss: 0.094057, acc.: 95.31%] [Generator loss: 6.144234]\n",
      "13120 [Discriminator loss: 0.058791, acc.: 98.44%] [Generator loss: 7.253693]\n",
      "13121 [Discriminator loss: 0.123329, acc.: 95.31%] [Generator loss: 5.146407]\n",
      "13122 [Discriminator loss: 0.117971, acc.: 93.75%] [Generator loss: 8.217165]\n",
      "13123 [Discriminator loss: 0.034932, acc.: 98.44%] [Generator loss: 9.540001]\n",
      "13124 [Discriminator loss: 0.061121, acc.: 98.44%] [Generator loss: 7.935477]\n",
      "13125 [Discriminator loss: 0.071874, acc.: 98.44%] [Generator loss: 7.733375]\n",
      "13126 [Discriminator loss: 0.145840, acc.: 93.75%] [Generator loss: 8.698946]\n",
      "13127 [Discriminator loss: 0.117861, acc.: 96.88%] [Generator loss: 8.066463]\n",
      "13128 [Discriminator loss: 0.213080, acc.: 93.75%] [Generator loss: 6.007503]\n",
      "13129 [Discriminator loss: 0.070243, acc.: 98.44%] [Generator loss: 6.585204]\n",
      "13130 [Discriminator loss: 0.054887, acc.: 98.44%] [Generator loss: 8.170725]\n",
      "13131 [Discriminator loss: 0.086126, acc.: 95.31%] [Generator loss: 7.160460]\n",
      "13132 [Discriminator loss: 0.066491, acc.: 98.44%] [Generator loss: 7.541750]\n",
      "13133 [Discriminator loss: 0.074258, acc.: 96.88%] [Generator loss: 7.801841]\n",
      "13134 [Discriminator loss: 0.092784, acc.: 95.31%] [Generator loss: 6.082438]\n",
      "13135 [Discriminator loss: 0.067964, acc.: 98.44%] [Generator loss: 6.526283]\n",
      "13136 [Discriminator loss: 0.057599, acc.: 95.31%] [Generator loss: 5.573910]\n",
      "13137 [Discriminator loss: 0.097676, acc.: 95.31%] [Generator loss: 6.881036]\n",
      "13138 [Discriminator loss: 0.155461, acc.: 93.75%] [Generator loss: 7.482059]\n",
      "13139 [Discriminator loss: 0.034603, acc.: 98.44%] [Generator loss: 7.485780]\n",
      "13140 [Discriminator loss: 0.139617, acc.: 92.19%] [Generator loss: 4.843302]\n",
      "13141 [Discriminator loss: 0.313553, acc.: 87.50%] [Generator loss: 10.205700]\n",
      "13142 [Discriminator loss: 0.189860, acc.: 95.31%] [Generator loss: 9.915018]\n",
      "13143 [Discriminator loss: 0.400429, acc.: 87.50%] [Generator loss: 5.681561]\n",
      "13144 [Discriminator loss: 0.115604, acc.: 93.75%] [Generator loss: 5.493025]\n",
      "13145 [Discriminator loss: 0.060228, acc.: 98.44%] [Generator loss: 6.937069]\n",
      "13146 [Discriminator loss: 0.067425, acc.: 96.88%] [Generator loss: 8.250856]\n",
      "13147 [Discriminator loss: 0.102213, acc.: 95.31%] [Generator loss: 6.855082]\n",
      "13148 [Discriminator loss: 0.041744, acc.: 98.44%] [Generator loss: 6.560972]\n",
      "13149 [Discriminator loss: 0.103874, acc.: 95.31%] [Generator loss: 6.937675]\n",
      "13150 [Discriminator loss: 0.048380, acc.: 98.44%] [Generator loss: 8.651655]\n",
      "13151 [Discriminator loss: 0.118856, acc.: 98.44%] [Generator loss: 6.421565]\n",
      "13152 [Discriminator loss: 0.054253, acc.: 98.44%] [Generator loss: 6.269533]\n",
      "13153 [Discriminator loss: 0.083793, acc.: 95.31%] [Generator loss: 7.350840]\n",
      "13154 [Discriminator loss: 0.076845, acc.: 98.44%] [Generator loss: 7.044715]\n",
      "13155 [Discriminator loss: 0.038768, acc.: 98.44%] [Generator loss: 7.519875]\n",
      "13156 [Discriminator loss: 0.196253, acc.: 93.75%] [Generator loss: 7.632714]\n",
      "13157 [Discriminator loss: 0.175830, acc.: 93.75%] [Generator loss: 7.004162]\n",
      "13158 [Discriminator loss: 0.035847, acc.: 100.00%] [Generator loss: 6.482906]\n",
      "13159 [Discriminator loss: 0.075206, acc.: 98.44%] [Generator loss: 7.887884]\n",
      "13160 [Discriminator loss: 0.054446, acc.: 96.88%] [Generator loss: 7.728552]\n",
      "13161 [Discriminator loss: 0.194553, acc.: 92.19%] [Generator loss: 7.460737]\n",
      "13162 [Discriminator loss: 0.168052, acc.: 92.19%] [Generator loss: 7.886143]\n",
      "13163 [Discriminator loss: 0.021405, acc.: 100.00%] [Generator loss: 8.031441]\n",
      "13164 [Discriminator loss: 0.189401, acc.: 89.06%] [Generator loss: 6.967716]\n",
      "13165 [Discriminator loss: 0.106684, acc.: 96.88%] [Generator loss: 7.113395]\n",
      "13166 [Discriminator loss: 0.160143, acc.: 92.19%] [Generator loss: 6.963608]\n",
      "13167 [Discriminator loss: 0.087314, acc.: 95.31%] [Generator loss: 6.099260]\n",
      "13168 [Discriminator loss: 0.069028, acc.: 96.88%] [Generator loss: 6.253823]\n",
      "13169 [Discriminator loss: 0.052899, acc.: 98.44%] [Generator loss: 7.674656]\n",
      "13170 [Discriminator loss: 0.092635, acc.: 96.88%] [Generator loss: 7.062709]\n",
      "13171 [Discriminator loss: 0.238058, acc.: 90.62%] [Generator loss: 5.838913]\n",
      "13172 [Discriminator loss: 0.229643, acc.: 93.75%] [Generator loss: 7.142317]\n",
      "13173 [Discriminator loss: 0.032543, acc.: 98.44%] [Generator loss: 8.685900]\n",
      "13174 [Discriminator loss: 0.116481, acc.: 93.75%] [Generator loss: 8.368749]\n",
      "13175 [Discriminator loss: 0.213209, acc.: 92.19%] [Generator loss: 6.405221]\n",
      "13176 [Discriminator loss: 0.109689, acc.: 96.88%] [Generator loss: 7.540232]\n",
      "13177 [Discriminator loss: 0.082020, acc.: 95.31%] [Generator loss: 5.960737]\n",
      "13178 [Discriminator loss: 0.079842, acc.: 96.88%] [Generator loss: 5.752645]\n",
      "13179 [Discriminator loss: 0.184776, acc.: 95.31%] [Generator loss: 6.282978]\n",
      "13180 [Discriminator loss: 0.019175, acc.: 100.00%] [Generator loss: 7.102898]\n",
      "13181 [Discriminator loss: 0.155572, acc.: 90.62%] [Generator loss: 5.347814]\n",
      "13182 [Discriminator loss: 0.233327, acc.: 87.50%] [Generator loss: 7.911234]\n",
      "13183 [Discriminator loss: 0.101052, acc.: 95.31%] [Generator loss: 9.477368]\n",
      "13184 [Discriminator loss: 0.456995, acc.: 82.81%] [Generator loss: 5.709667]\n",
      "13185 [Discriminator loss: 0.178184, acc.: 95.31%] [Generator loss: 7.371160]\n",
      "13186 [Discriminator loss: 0.081723, acc.: 95.31%] [Generator loss: 8.181164]\n",
      "13187 [Discriminator loss: 0.106298, acc.: 95.31%] [Generator loss: 7.043414]\n",
      "13188 [Discriminator loss: 0.078114, acc.: 96.88%] [Generator loss: 8.396225]\n",
      "13189 [Discriminator loss: 0.327387, acc.: 89.06%] [Generator loss: 5.059127]\n",
      "13190 [Discriminator loss: 0.227108, acc.: 90.62%] [Generator loss: 7.440789]\n",
      "13191 [Discriminator loss: 0.050955, acc.: 98.44%] [Generator loss: 8.106906]\n",
      "13192 [Discriminator loss: 0.107376, acc.: 93.75%] [Generator loss: 8.376957]\n",
      "13193 [Discriminator loss: 0.109364, acc.: 95.31%] [Generator loss: 7.666656]\n",
      "13194 [Discriminator loss: 0.092652, acc.: 98.44%] [Generator loss: 6.969026]\n",
      "13195 [Discriminator loss: 0.072782, acc.: 96.88%] [Generator loss: 6.733165]\n",
      "13196 [Discriminator loss: 0.082036, acc.: 95.31%] [Generator loss: 8.123322]\n",
      "13197 [Discriminator loss: 0.061295, acc.: 98.44%] [Generator loss: 7.711594]\n",
      "13198 [Discriminator loss: 0.141956, acc.: 95.31%] [Generator loss: 8.512434]\n",
      "13199 [Discriminator loss: 0.169914, acc.: 92.19%] [Generator loss: 7.444083]\n",
      "13200 [Discriminator loss: 0.072054, acc.: 95.31%] [Generator loss: 6.364037]\n",
      "13201 [Discriminator loss: 0.079545, acc.: 96.88%] [Generator loss: 7.990823]\n",
      "13202 [Discriminator loss: 0.046898, acc.: 100.00%] [Generator loss: 6.670846]\n",
      "13203 [Discriminator loss: 0.102988, acc.: 96.88%] [Generator loss: 6.742408]\n",
      "13204 [Discriminator loss: 0.174203, acc.: 93.75%] [Generator loss: 5.188836]\n",
      "13205 [Discriminator loss: 0.312721, acc.: 84.38%] [Generator loss: 9.925697]\n",
      "13206 [Discriminator loss: 0.293029, acc.: 84.38%] [Generator loss: 6.561012]\n",
      "13207 [Discriminator loss: 0.048798, acc.: 98.44%] [Generator loss: 8.237789]\n",
      "13208 [Discriminator loss: 0.044987, acc.: 98.44%] [Generator loss: 8.513246]\n",
      "13209 [Discriminator loss: 0.071268, acc.: 98.44%] [Generator loss: 6.394304]\n",
      "13210 [Discriminator loss: 0.091332, acc.: 96.88%] [Generator loss: 6.635891]\n",
      "13211 [Discriminator loss: 0.040612, acc.: 98.44%] [Generator loss: 8.208589]\n",
      "13212 [Discriminator loss: 0.092867, acc.: 96.88%] [Generator loss: 7.893067]\n",
      "13213 [Discriminator loss: 0.159133, acc.: 92.19%] [Generator loss: 7.913870]\n",
      "13214 [Discriminator loss: 0.108989, acc.: 98.44%] [Generator loss: 7.150470]\n",
      "13215 [Discriminator loss: 0.045224, acc.: 100.00%] [Generator loss: 7.918363]\n",
      "13216 [Discriminator loss: 0.151391, acc.: 96.88%] [Generator loss: 7.755917]\n",
      "13217 [Discriminator loss: 0.148577, acc.: 92.19%] [Generator loss: 7.992861]\n",
      "13218 [Discriminator loss: 0.017817, acc.: 100.00%] [Generator loss: 8.444151]\n",
      "13219 [Discriminator loss: 0.030176, acc.: 100.00%] [Generator loss: 5.605639]\n",
      "13220 [Discriminator loss: 0.054077, acc.: 96.88%] [Generator loss: 7.051638]\n",
      "13221 [Discriminator loss: 0.299446, acc.: 85.94%] [Generator loss: 6.434190]\n",
      "13222 [Discriminator loss: 0.097866, acc.: 95.31%] [Generator loss: 7.791482]\n",
      "13223 [Discriminator loss: 0.036381, acc.: 98.44%] [Generator loss: 7.069873]\n",
      "13224 [Discriminator loss: 0.182493, acc.: 93.75%] [Generator loss: 7.174422]\n",
      "13225 [Discriminator loss: 0.053917, acc.: 100.00%] [Generator loss: 6.669759]\n",
      "13226 [Discriminator loss: 0.059616, acc.: 96.88%] [Generator loss: 6.992578]\n",
      "13227 [Discriminator loss: 0.080155, acc.: 96.88%] [Generator loss: 7.085143]\n",
      "13228 [Discriminator loss: 0.041986, acc.: 100.00%] [Generator loss: 8.230898]\n",
      "13229 [Discriminator loss: 0.054331, acc.: 98.44%] [Generator loss: 7.584537]\n",
      "13230 [Discriminator loss: 0.110888, acc.: 93.75%] [Generator loss: 7.394738]\n",
      "13231 [Discriminator loss: 0.027006, acc.: 100.00%] [Generator loss: 6.173954]\n",
      "13232 [Discriminator loss: 0.143725, acc.: 93.75%] [Generator loss: 6.652707]\n",
      "13233 [Discriminator loss: 0.186039, acc.: 92.19%] [Generator loss: 5.551298]\n",
      "13234 [Discriminator loss: 0.039055, acc.: 100.00%] [Generator loss: 5.461871]\n",
      "13235 [Discriminator loss: 0.241850, acc.: 92.19%] [Generator loss: 9.495432]\n",
      "13236 [Discriminator loss: 0.057317, acc.: 98.44%] [Generator loss: 9.964432]\n",
      "13237 [Discriminator loss: 0.049542, acc.: 98.44%] [Generator loss: 8.237118]\n",
      "13238 [Discriminator loss: 0.083890, acc.: 96.88%] [Generator loss: 6.442486]\n",
      "13239 [Discriminator loss: 0.059979, acc.: 96.88%] [Generator loss: 7.270664]\n",
      "13240 [Discriminator loss: 0.046674, acc.: 98.44%] [Generator loss: 6.347483]\n",
      "13241 [Discriminator loss: 0.107627, acc.: 92.19%] [Generator loss: 6.269917]\n",
      "13242 [Discriminator loss: 0.231081, acc.: 90.62%] [Generator loss: 6.581901]\n",
      "13243 [Discriminator loss: 0.068201, acc.: 98.44%] [Generator loss: 7.544339]\n",
      "13244 [Discriminator loss: 0.062896, acc.: 96.88%] [Generator loss: 6.620520]\n",
      "13245 [Discriminator loss: 0.104297, acc.: 95.31%] [Generator loss: 6.451726]\n",
      "13246 [Discriminator loss: 0.136181, acc.: 95.31%] [Generator loss: 7.411480]\n",
      "13247 [Discriminator loss: 0.077612, acc.: 96.88%] [Generator loss: 7.016422]\n",
      "13248 [Discriminator loss: 0.052977, acc.: 98.44%] [Generator loss: 8.738352]\n",
      "13249 [Discriminator loss: 0.176388, acc.: 92.19%] [Generator loss: 6.771903]\n",
      "13250 [Discriminator loss: 0.048058, acc.: 96.88%] [Generator loss: 7.099583]\n",
      "13251 [Discriminator loss: 0.082526, acc.: 96.88%] [Generator loss: 5.694101]\n",
      "13252 [Discriminator loss: 0.133252, acc.: 92.19%] [Generator loss: 7.973197]\n",
      "13253 [Discriminator loss: 0.071821, acc.: 98.44%] [Generator loss: 7.454492]\n",
      "13254 [Discriminator loss: 0.202878, acc.: 90.62%] [Generator loss: 7.873969]\n",
      "13255 [Discriminator loss: 0.078943, acc.: 96.88%] [Generator loss: 7.593445]\n",
      "13256 [Discriminator loss: 0.073874, acc.: 98.44%] [Generator loss: 6.844595]\n",
      "13257 [Discriminator loss: 0.215004, acc.: 95.31%] [Generator loss: 7.526601]\n",
      "13258 [Discriminator loss: 0.113657, acc.: 95.31%] [Generator loss: 7.878788]\n",
      "13259 [Discriminator loss: 0.246703, acc.: 90.62%] [Generator loss: 8.389122]\n",
      "13260 [Discriminator loss: 0.153539, acc.: 90.62%] [Generator loss: 6.636791]\n",
      "13261 [Discriminator loss: 0.208221, acc.: 93.75%] [Generator loss: 6.999130]\n",
      "13262 [Discriminator loss: 0.123468, acc.: 93.75%] [Generator loss: 6.779194]\n",
      "13263 [Discriminator loss: 0.064441, acc.: 96.88%] [Generator loss: 7.360256]\n",
      "13264 [Discriminator loss: 0.150578, acc.: 93.75%] [Generator loss: 6.780571]\n",
      "13265 [Discriminator loss: 0.044361, acc.: 98.44%] [Generator loss: 8.284028]\n",
      "13266 [Discriminator loss: 0.050608, acc.: 96.88%] [Generator loss: 6.744069]\n",
      "13267 [Discriminator loss: 0.024695, acc.: 98.44%] [Generator loss: 7.721789]\n",
      "13268 [Discriminator loss: 0.193885, acc.: 93.75%] [Generator loss: 7.731218]\n",
      "13269 [Discriminator loss: 0.078555, acc.: 96.88%] [Generator loss: 6.644020]\n",
      "13270 [Discriminator loss: 0.021336, acc.: 100.00%] [Generator loss: 7.503671]\n",
      "13271 [Discriminator loss: 0.067659, acc.: 96.88%] [Generator loss: 6.476021]\n",
      "13272 [Discriminator loss: 0.181939, acc.: 92.19%] [Generator loss: 7.195123]\n",
      "13273 [Discriminator loss: 0.072946, acc.: 98.44%] [Generator loss: 7.594093]\n",
      "13274 [Discriminator loss: 0.044975, acc.: 98.44%] [Generator loss: 8.610970]\n",
      "13275 [Discriminator loss: 0.086726, acc.: 98.44%] [Generator loss: 7.848510]\n",
      "13276 [Discriminator loss: 0.163287, acc.: 93.75%] [Generator loss: 5.688651]\n",
      "13277 [Discriminator loss: 0.113670, acc.: 93.75%] [Generator loss: 7.589634]\n",
      "13278 [Discriminator loss: 0.030318, acc.: 100.00%] [Generator loss: 8.985337]\n",
      "13279 [Discriminator loss: 0.102226, acc.: 95.31%] [Generator loss: 7.504042]\n",
      "13280 [Discriminator loss: 0.166518, acc.: 93.75%] [Generator loss: 7.127665]\n",
      "13281 [Discriminator loss: 0.130006, acc.: 95.31%] [Generator loss: 8.548905]\n",
      "13282 [Discriminator loss: 0.104267, acc.: 96.88%] [Generator loss: 6.720873]\n",
      "13283 [Discriminator loss: 0.017961, acc.: 100.00%] [Generator loss: 6.705022]\n",
      "13284 [Discriminator loss: 0.122304, acc.: 95.31%] [Generator loss: 7.804379]\n",
      "13285 [Discriminator loss: 0.056453, acc.: 98.44%] [Generator loss: 9.020729]\n",
      "13286 [Discriminator loss: 0.013724, acc.: 100.00%] [Generator loss: 7.418497]\n",
      "13287 [Discriminator loss: 0.108752, acc.: 95.31%] [Generator loss: 6.633364]\n",
      "13288 [Discriminator loss: 0.024778, acc.: 100.00%] [Generator loss: 6.404814]\n",
      "13289 [Discriminator loss: 0.049490, acc.: 98.44%] [Generator loss: 7.536750]\n",
      "13290 [Discriminator loss: 0.222048, acc.: 89.06%] [Generator loss: 5.463930]\n",
      "13291 [Discriminator loss: 0.158678, acc.: 93.75%] [Generator loss: 8.166626]\n",
      "13292 [Discriminator loss: 0.097645, acc.: 95.31%] [Generator loss: 7.848556]\n",
      "13293 [Discriminator loss: 0.087062, acc.: 95.31%] [Generator loss: 7.704991]\n",
      "13294 [Discriminator loss: 0.236527, acc.: 92.19%] [Generator loss: 7.402370]\n",
      "13295 [Discriminator loss: 0.102480, acc.: 95.31%] [Generator loss: 8.766151]\n",
      "13296 [Discriminator loss: 0.076085, acc.: 95.31%] [Generator loss: 6.506880]\n",
      "13297 [Discriminator loss: 0.119346, acc.: 92.19%] [Generator loss: 7.713406]\n",
      "13298 [Discriminator loss: 0.071264, acc.: 96.88%] [Generator loss: 8.656228]\n",
      "13299 [Discriminator loss: 0.096438, acc.: 95.31%] [Generator loss: 7.243777]\n",
      "13300 [Discriminator loss: 0.082661, acc.: 96.88%] [Generator loss: 7.312672]\n",
      "13301 [Discriminator loss: 0.055637, acc.: 96.88%] [Generator loss: 8.907738]\n",
      "13302 [Discriminator loss: 0.068221, acc.: 96.88%] [Generator loss: 6.635812]\n",
      "13303 [Discriminator loss: 0.078044, acc.: 96.88%] [Generator loss: 7.328991]\n",
      "13304 [Discriminator loss: 0.025890, acc.: 100.00%] [Generator loss: 7.690753]\n",
      "13305 [Discriminator loss: 0.018625, acc.: 100.00%] [Generator loss: 7.250758]\n",
      "13306 [Discriminator loss: 0.140025, acc.: 95.31%] [Generator loss: 8.123831]\n",
      "13307 [Discriminator loss: 0.151629, acc.: 92.19%] [Generator loss: 7.285477]\n",
      "13308 [Discriminator loss: 0.360313, acc.: 90.62%] [Generator loss: 7.822442]\n",
      "13309 [Discriminator loss: 0.059051, acc.: 98.44%] [Generator loss: 9.339569]\n",
      "13310 [Discriminator loss: 0.015870, acc.: 100.00%] [Generator loss: 8.739602]\n",
      "13311 [Discriminator loss: 0.099422, acc.: 96.88%] [Generator loss: 6.814912]\n",
      "13312 [Discriminator loss: 0.154610, acc.: 95.31%] [Generator loss: 7.450521]\n",
      "13313 [Discriminator loss: 0.045587, acc.: 98.44%] [Generator loss: 8.206031]\n",
      "13314 [Discriminator loss: 0.073858, acc.: 96.88%] [Generator loss: 6.895138]\n",
      "13315 [Discriminator loss: 0.066050, acc.: 96.88%] [Generator loss: 5.827138]\n",
      "13316 [Discriminator loss: 0.167332, acc.: 92.19%] [Generator loss: 9.118153]\n",
      "13317 [Discriminator loss: 0.093007, acc.: 95.31%] [Generator loss: 7.691852]\n",
      "13318 [Discriminator loss: 0.062863, acc.: 98.44%] [Generator loss: 6.370353]\n",
      "13319 [Discriminator loss: 0.089163, acc.: 95.31%] [Generator loss: 6.364235]\n",
      "13320 [Discriminator loss: 0.067942, acc.: 96.88%] [Generator loss: 7.795353]\n",
      "13321 [Discriminator loss: 0.137407, acc.: 95.31%] [Generator loss: 7.539834]\n",
      "13322 [Discriminator loss: 0.053017, acc.: 96.88%] [Generator loss: 8.540731]\n",
      "13323 [Discriminator loss: 0.243302, acc.: 90.62%] [Generator loss: 7.478036]\n",
      "13324 [Discriminator loss: 0.092097, acc.: 96.88%] [Generator loss: 6.817091]\n",
      "13325 [Discriminator loss: 0.198901, acc.: 95.31%] [Generator loss: 6.033402]\n",
      "13326 [Discriminator loss: 0.127189, acc.: 96.88%] [Generator loss: 7.867608]\n",
      "13327 [Discriminator loss: 0.036990, acc.: 98.44%] [Generator loss: 8.163935]\n",
      "13328 [Discriminator loss: 0.013868, acc.: 100.00%] [Generator loss: 7.118714]\n",
      "13329 [Discriminator loss: 0.050316, acc.: 98.44%] [Generator loss: 6.661714]\n",
      "13330 [Discriminator loss: 0.188011, acc.: 93.75%] [Generator loss: 8.640953]\n",
      "13331 [Discriminator loss: 0.024167, acc.: 100.00%] [Generator loss: 8.359404]\n",
      "13332 [Discriminator loss: 0.039694, acc.: 98.44%] [Generator loss: 7.876708]\n",
      "13333 [Discriminator loss: 0.193066, acc.: 92.19%] [Generator loss: 4.919615]\n",
      "13334 [Discriminator loss: 0.148185, acc.: 93.75%] [Generator loss: 7.148459]\n",
      "13335 [Discriminator loss: 0.190642, acc.: 90.62%] [Generator loss: 7.299876]\n",
      "13336 [Discriminator loss: 0.077322, acc.: 98.44%] [Generator loss: 9.203808]\n",
      "13337 [Discriminator loss: 0.180303, acc.: 89.06%] [Generator loss: 9.176458]\n",
      "13338 [Discriminator loss: 0.212604, acc.: 90.62%] [Generator loss: 6.924117]\n",
      "13339 [Discriminator loss: 0.051163, acc.: 96.88%] [Generator loss: 6.807201]\n",
      "13340 [Discriminator loss: 0.073615, acc.: 98.44%] [Generator loss: 6.752600]\n",
      "13341 [Discriminator loss: 0.171108, acc.: 90.62%] [Generator loss: 5.647872]\n",
      "13342 [Discriminator loss: 0.237423, acc.: 87.50%] [Generator loss: 7.717023]\n",
      "13343 [Discriminator loss: 0.130385, acc.: 93.75%] [Generator loss: 6.883700]\n",
      "13344 [Discriminator loss: 0.051258, acc.: 98.44%] [Generator loss: 7.762826]\n",
      "13345 [Discriminator loss: 0.106096, acc.: 96.88%] [Generator loss: 7.768301]\n",
      "13346 [Discriminator loss: 0.025362, acc.: 100.00%] [Generator loss: 7.721889]\n",
      "13347 [Discriminator loss: 0.085023, acc.: 96.88%] [Generator loss: 8.234303]\n",
      "13348 [Discriminator loss: 0.060068, acc.: 98.44%] [Generator loss: 7.682167]\n",
      "13349 [Discriminator loss: 0.212137, acc.: 89.06%] [Generator loss: 5.899890]\n",
      "13350 [Discriminator loss: 0.036504, acc.: 100.00%] [Generator loss: 6.098018]\n",
      "13351 [Discriminator loss: 0.184017, acc.: 90.62%] [Generator loss: 7.644565]\n",
      "13352 [Discriminator loss: 0.065733, acc.: 96.88%] [Generator loss: 6.815150]\n",
      "13353 [Discriminator loss: 0.270623, acc.: 84.38%] [Generator loss: 8.072506]\n",
      "13354 [Discriminator loss: 0.024715, acc.: 100.00%] [Generator loss: 7.707567]\n",
      "13355 [Discriminator loss: 0.082633, acc.: 96.88%] [Generator loss: 7.570300]\n",
      "13356 [Discriminator loss: 0.069672, acc.: 96.88%] [Generator loss: 7.516869]\n",
      "13357 [Discriminator loss: 0.038914, acc.: 98.44%] [Generator loss: 8.048831]\n",
      "13358 [Discriminator loss: 0.089469, acc.: 96.88%] [Generator loss: 7.180654]\n",
      "13359 [Discriminator loss: 0.162960, acc.: 90.62%] [Generator loss: 7.359366]\n",
      "13360 [Discriminator loss: 0.043031, acc.: 100.00%] [Generator loss: 6.728434]\n",
      "13361 [Discriminator loss: 0.136746, acc.: 92.19%] [Generator loss: 6.838415]\n",
      "13362 [Discriminator loss: 0.111486, acc.: 96.88%] [Generator loss: 8.079366]\n",
      "13363 [Discriminator loss: 0.085041, acc.: 96.88%] [Generator loss: 6.704547]\n",
      "13364 [Discriminator loss: 0.040262, acc.: 98.44%] [Generator loss: 9.433033]\n",
      "13365 [Discriminator loss: 0.052904, acc.: 95.31%] [Generator loss: 6.669233]\n",
      "13366 [Discriminator loss: 0.084467, acc.: 95.31%] [Generator loss: 6.584786]\n",
      "13367 [Discriminator loss: 0.020870, acc.: 100.00%] [Generator loss: 7.353168]\n",
      "13368 [Discriminator loss: 0.212688, acc.: 93.75%] [Generator loss: 4.731354]\n",
      "13369 [Discriminator loss: 0.087245, acc.: 95.31%] [Generator loss: 6.034287]\n",
      "13370 [Discriminator loss: 0.118128, acc.: 96.88%] [Generator loss: 7.317266]\n",
      "13371 [Discriminator loss: 0.063356, acc.: 96.88%] [Generator loss: 5.960811]\n",
      "13372 [Discriminator loss: 0.101977, acc.: 95.31%] [Generator loss: 8.189591]\n",
      "13373 [Discriminator loss: 0.092829, acc.: 96.88%] [Generator loss: 6.867701]\n",
      "13374 [Discriminator loss: 0.017487, acc.: 100.00%] [Generator loss: 7.278286]\n",
      "13375 [Discriminator loss: 0.239610, acc.: 92.19%] [Generator loss: 7.628517]\n",
      "13376 [Discriminator loss: 0.096247, acc.: 95.31%] [Generator loss: 6.731116]\n",
      "13377 [Discriminator loss: 0.067651, acc.: 96.88%] [Generator loss: 7.462288]\n",
      "13378 [Discriminator loss: 0.006395, acc.: 100.00%] [Generator loss: 8.329107]\n",
      "13379 [Discriminator loss: 0.140160, acc.: 95.31%] [Generator loss: 7.956247]\n",
      "13380 [Discriminator loss: 0.057652, acc.: 98.44%] [Generator loss: 8.170296]\n",
      "13381 [Discriminator loss: 0.021709, acc.: 100.00%] [Generator loss: 7.409611]\n",
      "13382 [Discriminator loss: 0.106905, acc.: 95.31%] [Generator loss: 7.815412]\n",
      "13383 [Discriminator loss: 0.066884, acc.: 95.31%] [Generator loss: 6.957660]\n",
      "13384 [Discriminator loss: 0.073017, acc.: 95.31%] [Generator loss: 5.192952]\n",
      "13385 [Discriminator loss: 0.254877, acc.: 87.50%] [Generator loss: 8.303226]\n",
      "13386 [Discriminator loss: 0.041247, acc.: 100.00%] [Generator loss: 8.136999]\n",
      "13387 [Discriminator loss: 0.063052, acc.: 96.88%] [Generator loss: 7.276560]\n",
      "13388 [Discriminator loss: 0.073985, acc.: 95.31%] [Generator loss: 6.052388]\n",
      "13389 [Discriminator loss: 0.062671, acc.: 95.31%] [Generator loss: 9.337168]\n",
      "13390 [Discriminator loss: 0.189132, acc.: 95.31%] [Generator loss: 7.596010]\n",
      "13391 [Discriminator loss: 0.195727, acc.: 92.19%] [Generator loss: 7.761442]\n",
      "13392 [Discriminator loss: 0.098746, acc.: 95.31%] [Generator loss: 7.448318]\n",
      "13393 [Discriminator loss: 0.053883, acc.: 98.44%] [Generator loss: 6.282033]\n",
      "13394 [Discriminator loss: 0.034649, acc.: 98.44%] [Generator loss: 7.036415]\n",
      "13395 [Discriminator loss: 0.122468, acc.: 96.88%] [Generator loss: 6.561935]\n",
      "13396 [Discriminator loss: 0.018897, acc.: 100.00%] [Generator loss: 6.385712]\n",
      "13397 [Discriminator loss: 0.087648, acc.: 96.88%] [Generator loss: 7.188417]\n",
      "13398 [Discriminator loss: 0.026457, acc.: 100.00%] [Generator loss: 7.709481]\n",
      "13399 [Discriminator loss: 0.117272, acc.: 93.75%] [Generator loss: 7.916557]\n",
      "13400 [Discriminator loss: 0.081741, acc.: 95.31%] [Generator loss: 7.284648]\n",
      "13401 [Discriminator loss: 0.040918, acc.: 96.88%] [Generator loss: 8.108547]\n",
      "13402 [Discriminator loss: 0.069473, acc.: 98.44%] [Generator loss: 7.844827]\n",
      "13403 [Discriminator loss: 0.078114, acc.: 96.88%] [Generator loss: 8.281704]\n",
      "13404 [Discriminator loss: 0.147784, acc.: 92.19%] [Generator loss: 7.304000]\n",
      "13405 [Discriminator loss: 0.146410, acc.: 96.88%] [Generator loss: 7.243591]\n",
      "13406 [Discriminator loss: 0.076604, acc.: 98.44%] [Generator loss: 6.546166]\n",
      "13407 [Discriminator loss: 0.059597, acc.: 98.44%] [Generator loss: 7.781504]\n",
      "13408 [Discriminator loss: 0.089627, acc.: 96.88%] [Generator loss: 8.013307]\n",
      "13409 [Discriminator loss: 0.061016, acc.: 98.44%] [Generator loss: 7.914910]\n",
      "13410 [Discriminator loss: 0.121514, acc.: 96.88%] [Generator loss: 8.385050]\n",
      "13411 [Discriminator loss: 0.174020, acc.: 92.19%] [Generator loss: 8.041324]\n",
      "13412 [Discriminator loss: 0.033672, acc.: 100.00%] [Generator loss: 7.120384]\n",
      "13413 [Discriminator loss: 0.029706, acc.: 100.00%] [Generator loss: 7.141304]\n",
      "13414 [Discriminator loss: 0.071689, acc.: 96.88%] [Generator loss: 6.785460]\n",
      "13415 [Discriminator loss: 0.126309, acc.: 95.31%] [Generator loss: 6.891131]\n",
      "13416 [Discriminator loss: 0.081457, acc.: 95.31%] [Generator loss: 6.081867]\n",
      "13417 [Discriminator loss: 0.138511, acc.: 92.19%] [Generator loss: 8.814714]\n",
      "13418 [Discriminator loss: 0.133145, acc.: 98.44%] [Generator loss: 7.290353]\n",
      "13419 [Discriminator loss: 0.053506, acc.: 98.44%] [Generator loss: 7.689980]\n",
      "13420 [Discriminator loss: 0.052105, acc.: 98.44%] [Generator loss: 7.906041]\n",
      "13421 [Discriminator loss: 0.053796, acc.: 96.88%] [Generator loss: 6.670726]\n",
      "13422 [Discriminator loss: 0.116206, acc.: 93.75%] [Generator loss: 6.951284]\n",
      "13423 [Discriminator loss: 0.118814, acc.: 96.88%] [Generator loss: 6.616943]\n",
      "13424 [Discriminator loss: 0.123560, acc.: 92.19%] [Generator loss: 7.581262]\n",
      "13425 [Discriminator loss: 0.026013, acc.: 100.00%] [Generator loss: 8.412572]\n",
      "13426 [Discriminator loss: 0.054948, acc.: 96.88%] [Generator loss: 6.891396]\n",
      "13427 [Discriminator loss: 0.225126, acc.: 89.06%] [Generator loss: 7.030676]\n",
      "13428 [Discriminator loss: 0.047670, acc.: 98.44%] [Generator loss: 7.828122]\n",
      "13429 [Discriminator loss: 0.049434, acc.: 98.44%] [Generator loss: 7.489058]\n",
      "13430 [Discriminator loss: 0.159155, acc.: 92.19%] [Generator loss: 5.772995]\n",
      "13431 [Discriminator loss: 0.050072, acc.: 98.44%] [Generator loss: 6.205742]\n",
      "13432 [Discriminator loss: 0.024219, acc.: 100.00%] [Generator loss: 6.556566]\n",
      "13433 [Discriminator loss: 0.114331, acc.: 93.75%] [Generator loss: 7.161385]\n",
      "13434 [Discriminator loss: 0.037879, acc.: 100.00%] [Generator loss: 8.995726]\n",
      "13435 [Discriminator loss: 0.035589, acc.: 100.00%] [Generator loss: 7.234157]\n",
      "13436 [Discriminator loss: 0.088619, acc.: 96.88%] [Generator loss: 7.784197]\n",
      "13437 [Discriminator loss: 0.077811, acc.: 95.31%] [Generator loss: 7.251381]\n",
      "13438 [Discriminator loss: 0.072332, acc.: 96.88%] [Generator loss: 7.136172]\n",
      "13439 [Discriminator loss: 0.149771, acc.: 92.19%] [Generator loss: 6.331235]\n",
      "13440 [Discriminator loss: 0.074643, acc.: 98.44%] [Generator loss: 6.015832]\n",
      "13441 [Discriminator loss: 0.093507, acc.: 95.31%] [Generator loss: 8.068395]\n",
      "13442 [Discriminator loss: 0.229588, acc.: 92.19%] [Generator loss: 7.547441]\n",
      "13443 [Discriminator loss: 0.125413, acc.: 95.31%] [Generator loss: 9.315327]\n",
      "13444 [Discriminator loss: 0.184636, acc.: 92.19%] [Generator loss: 6.510501]\n",
      "13445 [Discriminator loss: 0.104528, acc.: 98.44%] [Generator loss: 7.326258]\n",
      "13446 [Discriminator loss: 0.069433, acc.: 98.44%] [Generator loss: 6.134467]\n",
      "13447 [Discriminator loss: 0.106286, acc.: 95.31%] [Generator loss: 8.752304]\n",
      "13448 [Discriminator loss: 0.013479, acc.: 100.00%] [Generator loss: 8.218954]\n",
      "13449 [Discriminator loss: 0.129314, acc.: 93.75%] [Generator loss: 6.534171]\n",
      "13450 [Discriminator loss: 0.087768, acc.: 95.31%] [Generator loss: 6.792787]\n",
      "13451 [Discriminator loss: 0.161900, acc.: 95.31%] [Generator loss: 7.446953]\n",
      "13452 [Discriminator loss: 0.073632, acc.: 96.88%] [Generator loss: 7.290659]\n",
      "13453 [Discriminator loss: 0.186698, acc.: 92.19%] [Generator loss: 6.412493]\n",
      "13454 [Discriminator loss: 0.070249, acc.: 98.44%] [Generator loss: 7.742537]\n",
      "13455 [Discriminator loss: 0.068978, acc.: 98.44%] [Generator loss: 7.290407]\n",
      "13456 [Discriminator loss: 0.134658, acc.: 95.31%] [Generator loss: 7.850198]\n",
      "13457 [Discriminator loss: 0.041139, acc.: 100.00%] [Generator loss: 7.342761]\n",
      "13458 [Discriminator loss: 0.082989, acc.: 95.31%] [Generator loss: 5.961658]\n",
      "13459 [Discriminator loss: 0.059978, acc.: 98.44%] [Generator loss: 6.563437]\n",
      "13460 [Discriminator loss: 0.127063, acc.: 95.31%] [Generator loss: 8.402197]\n",
      "13461 [Discriminator loss: 0.158119, acc.: 95.31%] [Generator loss: 6.379082]\n",
      "13462 [Discriminator loss: 0.140028, acc.: 95.31%] [Generator loss: 6.774744]\n",
      "13463 [Discriminator loss: 0.079674, acc.: 96.88%] [Generator loss: 7.153989]\n",
      "13464 [Discriminator loss: 0.052822, acc.: 98.44%] [Generator loss: 8.591928]\n",
      "13465 [Discriminator loss: 0.141440, acc.: 93.75%] [Generator loss: 8.542039]\n",
      "13466 [Discriminator loss: 0.213989, acc.: 95.31%] [Generator loss: 7.871184]\n",
      "13467 [Discriminator loss: 0.051482, acc.: 100.00%] [Generator loss: 7.405940]\n",
      "13468 [Discriminator loss: 0.083193, acc.: 93.75%] [Generator loss: 6.542905]\n",
      "13469 [Discriminator loss: 0.050742, acc.: 98.44%] [Generator loss: 6.059515]\n",
      "13470 [Discriminator loss: 0.049276, acc.: 100.00%] [Generator loss: 7.758728]\n",
      "13471 [Discriminator loss: 0.043090, acc.: 98.44%] [Generator loss: 7.870778]\n",
      "13472 [Discriminator loss: 0.521125, acc.: 78.12%] [Generator loss: 8.312914]\n",
      "13473 [Discriminator loss: 0.058932, acc.: 98.44%] [Generator loss: 8.015962]\n",
      "13474 [Discriminator loss: 0.224414, acc.: 92.19%] [Generator loss: 8.002916]\n",
      "13475 [Discriminator loss: 0.156481, acc.: 92.19%] [Generator loss: 5.649809]\n",
      "13476 [Discriminator loss: 0.013482, acc.: 100.00%] [Generator loss: 6.756585]\n",
      "13477 [Discriminator loss: 0.208810, acc.: 92.19%] [Generator loss: 8.280295]\n",
      "13478 [Discriminator loss: 0.054287, acc.: 98.44%] [Generator loss: 8.906378]\n",
      "13479 [Discriminator loss: 0.071692, acc.: 96.88%] [Generator loss: 8.107829]\n",
      "13480 [Discriminator loss: 0.023302, acc.: 100.00%] [Generator loss: 6.770989]\n",
      "13481 [Discriminator loss: 0.055060, acc.: 96.88%] [Generator loss: 7.134247]\n",
      "13482 [Discriminator loss: 0.196172, acc.: 89.06%] [Generator loss: 6.524570]\n",
      "13483 [Discriminator loss: 0.020126, acc.: 100.00%] [Generator loss: 6.592486]\n",
      "13484 [Discriminator loss: 0.196704, acc.: 90.62%] [Generator loss: 6.785227]\n",
      "13485 [Discriminator loss: 0.201684, acc.: 95.31%] [Generator loss: 7.420044]\n",
      "13486 [Discriminator loss: 0.036441, acc.: 98.44%] [Generator loss: 8.159315]\n",
      "13487 [Discriminator loss: 0.047532, acc.: 100.00%] [Generator loss: 7.117982]\n",
      "13488 [Discriminator loss: 0.075843, acc.: 98.44%] [Generator loss: 7.311816]\n",
      "13489 [Discriminator loss: 0.105372, acc.: 95.31%] [Generator loss: 7.807251]\n",
      "13490 [Discriminator loss: 0.103389, acc.: 95.31%] [Generator loss: 9.002229]\n",
      "13491 [Discriminator loss: 0.039769, acc.: 98.44%] [Generator loss: 7.137733]\n",
      "13492 [Discriminator loss: 0.079822, acc.: 96.88%] [Generator loss: 9.278458]\n",
      "13493 [Discriminator loss: 0.154107, acc.: 93.75%] [Generator loss: 6.035303]\n",
      "13494 [Discriminator loss: 0.153572, acc.: 95.31%] [Generator loss: 6.605429]\n",
      "13495 [Discriminator loss: 0.140558, acc.: 90.62%] [Generator loss: 8.840260]\n",
      "13496 [Discriminator loss: 0.104167, acc.: 96.88%] [Generator loss: 8.224201]\n",
      "13497 [Discriminator loss: 0.207430, acc.: 90.62%] [Generator loss: 6.205954]\n",
      "13498 [Discriminator loss: 0.043910, acc.: 98.44%] [Generator loss: 8.141028]\n",
      "13499 [Discriminator loss: 0.025081, acc.: 100.00%] [Generator loss: 9.075184]\n",
      "13500 [Discriminator loss: 0.207987, acc.: 98.44%] [Generator loss: 7.266056]\n",
      "13501 [Discriminator loss: 0.062031, acc.: 96.88%] [Generator loss: 5.439455]\n",
      "13502 [Discriminator loss: 0.135029, acc.: 98.44%] [Generator loss: 6.129554]\n",
      "13503 [Discriminator loss: 0.063465, acc.: 96.88%] [Generator loss: 6.581421]\n",
      "13504 [Discriminator loss: 0.071475, acc.: 96.88%] [Generator loss: 6.854571]\n",
      "13505 [Discriminator loss: 0.082679, acc.: 96.88%] [Generator loss: 6.716465]\n",
      "13506 [Discriminator loss: 0.178316, acc.: 90.62%] [Generator loss: 6.397615]\n",
      "13507 [Discriminator loss: 0.020820, acc.: 100.00%] [Generator loss: 7.051373]\n",
      "13508 [Discriminator loss: 0.118875, acc.: 93.75%] [Generator loss: 6.185649]\n",
      "13509 [Discriminator loss: 0.043396, acc.: 98.44%] [Generator loss: 7.439054]\n",
      "13510 [Discriminator loss: 0.093894, acc.: 96.88%] [Generator loss: 6.718671]\n",
      "13511 [Discriminator loss: 0.259794, acc.: 93.75%] [Generator loss: 6.013555]\n",
      "13512 [Discriminator loss: 0.098132, acc.: 95.31%] [Generator loss: 6.770459]\n",
      "13513 [Discriminator loss: 0.057787, acc.: 98.44%] [Generator loss: 4.653068]\n",
      "13514 [Discriminator loss: 0.037976, acc.: 100.00%] [Generator loss: 7.076958]\n",
      "13515 [Discriminator loss: 0.119978, acc.: 95.31%] [Generator loss: 6.757095]\n",
      "13516 [Discriminator loss: 0.099944, acc.: 96.88%] [Generator loss: 6.055338]\n",
      "13517 [Discriminator loss: 0.192722, acc.: 90.62%] [Generator loss: 8.617946]\n",
      "13518 [Discriminator loss: 0.085826, acc.: 96.88%] [Generator loss: 7.672520]\n",
      "13519 [Discriminator loss: 0.027708, acc.: 100.00%] [Generator loss: 7.710705]\n",
      "13520 [Discriminator loss: 0.152365, acc.: 93.75%] [Generator loss: 8.343145]\n",
      "13521 [Discriminator loss: 0.174938, acc.: 92.19%] [Generator loss: 8.566299]\n",
      "13522 [Discriminator loss: 0.049971, acc.: 96.88%] [Generator loss: 9.632042]\n",
      "13523 [Discriminator loss: 0.071414, acc.: 96.88%] [Generator loss: 7.921350]\n",
      "13524 [Discriminator loss: 0.210698, acc.: 90.62%] [Generator loss: 6.041579]\n",
      "13525 [Discriminator loss: 0.113423, acc.: 95.31%] [Generator loss: 8.537537]\n",
      "13526 [Discriminator loss: 0.094934, acc.: 95.31%] [Generator loss: 7.456196]\n",
      "13527 [Discriminator loss: 0.128139, acc.: 95.31%] [Generator loss: 8.185927]\n",
      "13528 [Discriminator loss: 0.125005, acc.: 95.31%] [Generator loss: 7.741575]\n",
      "13529 [Discriminator loss: 0.052109, acc.: 98.44%] [Generator loss: 7.012615]\n",
      "13530 [Discriminator loss: 0.060507, acc.: 98.44%] [Generator loss: 8.262867]\n",
      "13531 [Discriminator loss: 0.036394, acc.: 100.00%] [Generator loss: 7.199894]\n",
      "13532 [Discriminator loss: 0.255321, acc.: 90.62%] [Generator loss: 6.184426]\n",
      "13533 [Discriminator loss: 0.059945, acc.: 96.88%] [Generator loss: 5.322606]\n",
      "13534 [Discriminator loss: 0.137341, acc.: 95.31%] [Generator loss: 6.546957]\n",
      "13535 [Discriminator loss: 0.171993, acc.: 90.62%] [Generator loss: 7.241682]\n",
      "13536 [Discriminator loss: 0.086415, acc.: 96.88%] [Generator loss: 7.652308]\n",
      "13537 [Discriminator loss: 0.040044, acc.: 96.88%] [Generator loss: 6.106840]\n",
      "13538 [Discriminator loss: 0.072765, acc.: 95.31%] [Generator loss: 5.668931]\n",
      "13539 [Discriminator loss: 0.208785, acc.: 92.19%] [Generator loss: 6.371486]\n",
      "13540 [Discriminator loss: 0.055260, acc.: 96.88%] [Generator loss: 8.210344]\n",
      "13541 [Discriminator loss: 0.133227, acc.: 95.31%] [Generator loss: 6.956918]\n",
      "13542 [Discriminator loss: 0.223117, acc.: 95.31%] [Generator loss: 7.443961]\n",
      "13543 [Discriminator loss: 0.135450, acc.: 95.31%] [Generator loss: 8.625268]\n",
      "13544 [Discriminator loss: 0.155643, acc.: 90.62%] [Generator loss: 7.893638]\n",
      "13545 [Discriminator loss: 0.029939, acc.: 98.44%] [Generator loss: 8.109730]\n",
      "13546 [Discriminator loss: 0.129531, acc.: 92.19%] [Generator loss: 7.815437]\n",
      "13547 [Discriminator loss: 0.060234, acc.: 98.44%] [Generator loss: 7.581081]\n",
      "13548 [Discriminator loss: 0.056647, acc.: 96.88%] [Generator loss: 8.079636]\n",
      "13549 [Discriminator loss: 0.048241, acc.: 100.00%] [Generator loss: 8.005123]\n",
      "13550 [Discriminator loss: 0.101940, acc.: 96.88%] [Generator loss: 8.167486]\n",
      "13551 [Discriminator loss: 0.174768, acc.: 93.75%] [Generator loss: 6.186898]\n",
      "13552 [Discriminator loss: 0.255076, acc.: 92.19%] [Generator loss: 8.560849]\n",
      "13553 [Discriminator loss: 0.035089, acc.: 100.00%] [Generator loss: 10.030803]\n",
      "13554 [Discriminator loss: 0.063374, acc.: 96.88%] [Generator loss: 7.867859]\n",
      "13555 [Discriminator loss: 0.129575, acc.: 92.19%] [Generator loss: 7.261643]\n",
      "13556 [Discriminator loss: 0.056552, acc.: 96.88%] [Generator loss: 9.218224]\n",
      "13557 [Discriminator loss: 0.523994, acc.: 84.38%] [Generator loss: 4.653133]\n",
      "13558 [Discriminator loss: 0.054865, acc.: 98.44%] [Generator loss: 6.724580]\n",
      "13559 [Discriminator loss: 0.131408, acc.: 98.44%] [Generator loss: 6.107181]\n",
      "13560 [Discriminator loss: 0.075605, acc.: 96.88%] [Generator loss: 7.877741]\n",
      "13561 [Discriminator loss: 0.023632, acc.: 100.00%] [Generator loss: 9.538291]\n",
      "13562 [Discriminator loss: 0.398530, acc.: 84.38%] [Generator loss: 6.350000]\n",
      "13563 [Discriminator loss: 0.020258, acc.: 100.00%] [Generator loss: 8.717331]\n",
      "13564 [Discriminator loss: 0.143639, acc.: 93.75%] [Generator loss: 7.497427]\n",
      "13565 [Discriminator loss: 0.058101, acc.: 96.88%] [Generator loss: 7.623491]\n",
      "13566 [Discriminator loss: 0.065748, acc.: 95.31%] [Generator loss: 6.724248]\n",
      "13567 [Discriminator loss: 0.060722, acc.: 98.44%] [Generator loss: 7.665159]\n",
      "13568 [Discriminator loss: 0.149390, acc.: 90.62%] [Generator loss: 7.453844]\n",
      "13569 [Discriminator loss: 0.048725, acc.: 98.44%] [Generator loss: 7.696737]\n",
      "13570 [Discriminator loss: 0.149905, acc.: 90.62%] [Generator loss: 6.979418]\n",
      "13571 [Discriminator loss: 0.069225, acc.: 96.88%] [Generator loss: 7.577966]\n",
      "13572 [Discriminator loss: 0.064205, acc.: 96.88%] [Generator loss: 6.773795]\n",
      "13573 [Discriminator loss: 0.029072, acc.: 98.44%] [Generator loss: 6.789268]\n",
      "13574 [Discriminator loss: 0.087273, acc.: 95.31%] [Generator loss: 7.122005]\n",
      "13575 [Discriminator loss: 0.187369, acc.: 89.06%] [Generator loss: 5.848410]\n",
      "13576 [Discriminator loss: 0.234871, acc.: 93.75%] [Generator loss: 8.198132]\n",
      "13577 [Discriminator loss: 0.194964, acc.: 90.62%] [Generator loss: 7.111751]\n",
      "13578 [Discriminator loss: 0.083907, acc.: 95.31%] [Generator loss: 8.352036]\n",
      "13579 [Discriminator loss: 0.103693, acc.: 93.75%] [Generator loss: 7.510841]\n",
      "13580 [Discriminator loss: 0.053674, acc.: 96.88%] [Generator loss: 7.123285]\n",
      "13581 [Discriminator loss: 0.094209, acc.: 96.88%] [Generator loss: 8.160948]\n",
      "13582 [Discriminator loss: 0.066605, acc.: 96.88%] [Generator loss: 9.106947]\n",
      "13583 [Discriminator loss: 0.094535, acc.: 95.31%] [Generator loss: 8.737195]\n",
      "13584 [Discriminator loss: 0.126945, acc.: 93.75%] [Generator loss: 8.692541]\n",
      "13585 [Discriminator loss: 0.059045, acc.: 96.88%] [Generator loss: 7.799302]\n",
      "13586 [Discriminator loss: 0.022568, acc.: 100.00%] [Generator loss: 7.396926]\n",
      "13587 [Discriminator loss: 0.109374, acc.: 96.88%] [Generator loss: 7.731569]\n",
      "13588 [Discriminator loss: 0.148586, acc.: 93.75%] [Generator loss: 8.173293]\n",
      "13589 [Discriminator loss: 0.330644, acc.: 89.06%] [Generator loss: 7.252048]\n",
      "13590 [Discriminator loss: 0.121516, acc.: 93.75%] [Generator loss: 8.243752]\n",
      "13591 [Discriminator loss: 0.120732, acc.: 92.19%] [Generator loss: 8.158684]\n",
      "13592 [Discriminator loss: 0.046587, acc.: 98.44%] [Generator loss: 7.855806]\n",
      "13593 [Discriminator loss: 0.126081, acc.: 96.88%] [Generator loss: 6.365438]\n",
      "13594 [Discriminator loss: 0.091531, acc.: 96.88%] [Generator loss: 7.967188]\n",
      "13595 [Discriminator loss: 0.070931, acc.: 98.44%] [Generator loss: 9.179422]\n",
      "13596 [Discriminator loss: 0.223654, acc.: 93.75%] [Generator loss: 7.092192]\n",
      "13597 [Discriminator loss: 0.270635, acc.: 90.62%] [Generator loss: 6.360682]\n",
      "13598 [Discriminator loss: 0.060061, acc.: 98.44%] [Generator loss: 8.753944]\n",
      "13599 [Discriminator loss: 0.093770, acc.: 96.88%] [Generator loss: 6.939370]\n",
      "13600 [Discriminator loss: 0.023398, acc.: 98.44%] [Generator loss: 6.765637]\n",
      "13601 [Discriminator loss: 0.126838, acc.: 96.88%] [Generator loss: 7.311773]\n",
      "13602 [Discriminator loss: 0.071040, acc.: 95.31%] [Generator loss: 7.442484]\n",
      "13603 [Discriminator loss: 0.070390, acc.: 96.88%] [Generator loss: 7.524768]\n",
      "13604 [Discriminator loss: 0.116183, acc.: 95.31%] [Generator loss: 9.229963]\n",
      "13605 [Discriminator loss: 0.016526, acc.: 100.00%] [Generator loss: 8.529586]\n",
      "13606 [Discriminator loss: 0.239184, acc.: 90.62%] [Generator loss: 7.260183]\n",
      "13607 [Discriminator loss: 0.124892, acc.: 95.31%] [Generator loss: 6.982276]\n",
      "13608 [Discriminator loss: 0.109016, acc.: 95.31%] [Generator loss: 8.041923]\n",
      "13609 [Discriminator loss: 0.038749, acc.: 96.88%] [Generator loss: 7.612602]\n",
      "13610 [Discriminator loss: 0.196088, acc.: 93.75%] [Generator loss: 9.237530]\n",
      "13611 [Discriminator loss: 0.040831, acc.: 98.44%] [Generator loss: 9.247116]\n",
      "13612 [Discriminator loss: 0.130088, acc.: 98.44%] [Generator loss: 6.907775]\n",
      "13613 [Discriminator loss: 0.021717, acc.: 100.00%] [Generator loss: 6.399548]\n",
      "13614 [Discriminator loss: 0.071424, acc.: 96.88%] [Generator loss: 8.038669]\n",
      "13615 [Discriminator loss: 0.070639, acc.: 98.44%] [Generator loss: 6.123620]\n",
      "13616 [Discriminator loss: 0.032822, acc.: 98.44%] [Generator loss: 7.376479]\n",
      "13617 [Discriminator loss: 0.085272, acc.: 96.88%] [Generator loss: 7.217879]\n",
      "13618 [Discriminator loss: 0.045013, acc.: 98.44%] [Generator loss: 7.654066]\n",
      "13619 [Discriminator loss: 0.161732, acc.: 92.19%] [Generator loss: 7.447306]\n",
      "13620 [Discriminator loss: 0.128186, acc.: 93.75%] [Generator loss: 8.740409]\n",
      "13621 [Discriminator loss: 0.163542, acc.: 93.75%] [Generator loss: 7.545450]\n",
      "13622 [Discriminator loss: 0.050721, acc.: 100.00%] [Generator loss: 6.741023]\n",
      "13623 [Discriminator loss: 0.043199, acc.: 98.44%] [Generator loss: 7.567230]\n",
      "13624 [Discriminator loss: 0.126526, acc.: 95.31%] [Generator loss: 8.414855]\n",
      "13625 [Discriminator loss: 0.116076, acc.: 95.31%] [Generator loss: 8.844177]\n",
      "13626 [Discriminator loss: 0.051576, acc.: 96.88%] [Generator loss: 6.647493]\n",
      "13627 [Discriminator loss: 0.067053, acc.: 98.44%] [Generator loss: 7.822231]\n",
      "13628 [Discriminator loss: 0.151432, acc.: 95.31%] [Generator loss: 7.714161]\n",
      "13629 [Discriminator loss: 0.103518, acc.: 93.75%] [Generator loss: 7.585174]\n",
      "13630 [Discriminator loss: 0.102670, acc.: 95.31%] [Generator loss: 6.515067]\n",
      "13631 [Discriminator loss: 0.106618, acc.: 93.75%] [Generator loss: 8.643455]\n",
      "13632 [Discriminator loss: 0.045052, acc.: 98.44%] [Generator loss: 7.443396]\n",
      "13633 [Discriminator loss: 0.264994, acc.: 84.38%] [Generator loss: 6.514495]\n",
      "13634 [Discriminator loss: 0.169093, acc.: 89.06%] [Generator loss: 8.373808]\n",
      "13635 [Discriminator loss: 0.044395, acc.: 98.44%] [Generator loss: 8.833369]\n",
      "13636 [Discriminator loss: 0.274382, acc.: 90.62%] [Generator loss: 7.462937]\n",
      "13637 [Discriminator loss: 0.118618, acc.: 92.19%] [Generator loss: 7.032703]\n",
      "13638 [Discriminator loss: 0.070528, acc.: 98.44%] [Generator loss: 7.147738]\n",
      "13639 [Discriminator loss: 0.056972, acc.: 98.44%] [Generator loss: 6.407205]\n",
      "13640 [Discriminator loss: 0.115025, acc.: 96.88%] [Generator loss: 7.329416]\n",
      "13641 [Discriminator loss: 0.025855, acc.: 100.00%] [Generator loss: 8.541885]\n",
      "13642 [Discriminator loss: 0.038140, acc.: 98.44%] [Generator loss: 7.815365]\n",
      "13643 [Discriminator loss: 0.046700, acc.: 98.44%] [Generator loss: 7.572057]\n",
      "13644 [Discriminator loss: 0.244409, acc.: 90.62%] [Generator loss: 9.204504]\n",
      "13645 [Discriminator loss: 0.157096, acc.: 92.19%] [Generator loss: 7.991735]\n",
      "13646 [Discriminator loss: 0.059857, acc.: 98.44%] [Generator loss: 7.473294]\n",
      "13647 [Discriminator loss: 0.247200, acc.: 92.19%] [Generator loss: 6.794655]\n",
      "13648 [Discriminator loss: 0.060153, acc.: 98.44%] [Generator loss: 6.720499]\n",
      "13649 [Discriminator loss: 0.106585, acc.: 95.31%] [Generator loss: 7.290958]\n",
      "13650 [Discriminator loss: 0.153839, acc.: 95.31%] [Generator loss: 7.204648]\n",
      "13651 [Discriminator loss: 0.106453, acc.: 96.88%] [Generator loss: 7.192765]\n",
      "13652 [Discriminator loss: 0.055373, acc.: 98.44%] [Generator loss: 6.722843]\n",
      "13653 [Discriminator loss: 0.043160, acc.: 98.44%] [Generator loss: 7.295558]\n",
      "13654 [Discriminator loss: 0.085296, acc.: 98.44%] [Generator loss: 7.355219]\n",
      "13655 [Discriminator loss: 0.027448, acc.: 100.00%] [Generator loss: 7.964552]\n",
      "13656 [Discriminator loss: 0.115779, acc.: 93.75%] [Generator loss: 7.014183]\n",
      "13657 [Discriminator loss: 0.080158, acc.: 96.88%] [Generator loss: 7.215503]\n",
      "13658 [Discriminator loss: 0.088819, acc.: 96.88%] [Generator loss: 7.211628]\n",
      "13659 [Discriminator loss: 0.034847, acc.: 98.44%] [Generator loss: 7.461495]\n",
      "13660 [Discriminator loss: 0.119866, acc.: 96.88%] [Generator loss: 7.205717]\n",
      "13661 [Discriminator loss: 0.123502, acc.: 95.31%] [Generator loss: 7.985164]\n",
      "13662 [Discriminator loss: 0.043354, acc.: 100.00%] [Generator loss: 7.010061]\n",
      "13663 [Discriminator loss: 0.105193, acc.: 96.88%] [Generator loss: 4.895172]\n",
      "13664 [Discriminator loss: 0.082666, acc.: 96.88%] [Generator loss: 7.068002]\n",
      "13665 [Discriminator loss: 0.058331, acc.: 98.44%] [Generator loss: 7.559791]\n",
      "13666 [Discriminator loss: 0.108031, acc.: 96.88%] [Generator loss: 7.135098]\n",
      "13667 [Discriminator loss: 0.043666, acc.: 96.88%] [Generator loss: 6.867718]\n",
      "13668 [Discriminator loss: 0.033436, acc.: 98.44%] [Generator loss: 7.317742]\n",
      "13669 [Discriminator loss: 0.127188, acc.: 95.31%] [Generator loss: 6.567630]\n",
      "13670 [Discriminator loss: 0.075563, acc.: 98.44%] [Generator loss: 7.573200]\n",
      "13671 [Discriminator loss: 0.016265, acc.: 100.00%] [Generator loss: 7.641722]\n",
      "13672 [Discriminator loss: 0.038553, acc.: 98.44%] [Generator loss: 7.683959]\n",
      "13673 [Discriminator loss: 0.062333, acc.: 96.88%] [Generator loss: 7.525407]\n",
      "13674 [Discriminator loss: 0.050702, acc.: 96.88%] [Generator loss: 7.756019]\n",
      "13675 [Discriminator loss: 0.214734, acc.: 96.88%] [Generator loss: 6.292716]\n",
      "13676 [Discriminator loss: 0.065495, acc.: 96.88%] [Generator loss: 6.498792]\n",
      "13677 [Discriminator loss: 0.145822, acc.: 95.31%] [Generator loss: 7.749075]\n",
      "13678 [Discriminator loss: 0.062790, acc.: 100.00%] [Generator loss: 8.069896]\n",
      "13679 [Discriminator loss: 0.102452, acc.: 96.88%] [Generator loss: 6.770522]\n",
      "13680 [Discriminator loss: 0.423083, acc.: 87.50%] [Generator loss: 8.592117]\n",
      "13681 [Discriminator loss: 0.032074, acc.: 98.44%] [Generator loss: 9.172665]\n",
      "13682 [Discriminator loss: 0.039115, acc.: 98.44%] [Generator loss: 8.123091]\n",
      "13683 [Discriminator loss: 0.140831, acc.: 93.75%] [Generator loss: 5.750102]\n",
      "13684 [Discriminator loss: 0.157693, acc.: 93.75%] [Generator loss: 7.605841]\n",
      "13685 [Discriminator loss: 0.036151, acc.: 98.44%] [Generator loss: 8.401545]\n",
      "13686 [Discriminator loss: 0.095032, acc.: 95.31%] [Generator loss: 6.667057]\n",
      "13687 [Discriminator loss: 0.118230, acc.: 93.75%] [Generator loss: 8.637007]\n",
      "13688 [Discriminator loss: 0.202405, acc.: 92.19%] [Generator loss: 7.367178]\n",
      "13689 [Discriminator loss: 0.187492, acc.: 92.19%] [Generator loss: 4.946432]\n",
      "13690 [Discriminator loss: 0.114021, acc.: 98.44%] [Generator loss: 7.446820]\n",
      "13691 [Discriminator loss: 0.038203, acc.: 98.44%] [Generator loss: 8.477477]\n",
      "13692 [Discriminator loss: 0.022428, acc.: 100.00%] [Generator loss: 7.248549]\n",
      "13693 [Discriminator loss: 0.059115, acc.: 96.88%] [Generator loss: 7.565917]\n",
      "13694 [Discriminator loss: 0.057672, acc.: 95.31%] [Generator loss: 10.043303]\n",
      "13695 [Discriminator loss: 0.392292, acc.: 84.38%] [Generator loss: 7.520097]\n",
      "13696 [Discriminator loss: 0.060237, acc.: 98.44%] [Generator loss: 8.156090]\n",
      "13697 [Discriminator loss: 0.152726, acc.: 95.31%] [Generator loss: 7.090041]\n",
      "13698 [Discriminator loss: 0.124222, acc.: 95.31%] [Generator loss: 8.984241]\n",
      "13699 [Discriminator loss: 0.051549, acc.: 98.44%] [Generator loss: 8.855282]\n",
      "13700 [Discriminator loss: 0.096424, acc.: 93.75%] [Generator loss: 6.784082]\n",
      "13701 [Discriminator loss: 0.100132, acc.: 95.31%] [Generator loss: 7.830102]\n",
      "13702 [Discriminator loss: 0.035727, acc.: 100.00%] [Generator loss: 8.433554]\n",
      "13703 [Discriminator loss: 0.034022, acc.: 98.44%] [Generator loss: 8.615780]\n",
      "13704 [Discriminator loss: 0.184124, acc.: 93.75%] [Generator loss: 8.462395]\n",
      "13705 [Discriminator loss: 0.118594, acc.: 95.31%] [Generator loss: 6.768805]\n",
      "13706 [Discriminator loss: 0.089402, acc.: 96.88%] [Generator loss: 7.699966]\n",
      "13707 [Discriminator loss: 0.140824, acc.: 96.88%] [Generator loss: 6.908223]\n",
      "13708 [Discriminator loss: 0.219034, acc.: 92.19%] [Generator loss: 7.974589]\n",
      "13709 [Discriminator loss: 0.020847, acc.: 100.00%] [Generator loss: 8.141282]\n",
      "13710 [Discriminator loss: 0.043641, acc.: 98.44%] [Generator loss: 7.289664]\n",
      "13711 [Discriminator loss: 0.253710, acc.: 93.75%] [Generator loss: 8.016617]\n",
      "13712 [Discriminator loss: 0.106223, acc.: 95.31%] [Generator loss: 6.011221]\n",
      "13713 [Discriminator loss: 0.077535, acc.: 96.88%] [Generator loss: 5.926382]\n",
      "13714 [Discriminator loss: 0.186270, acc.: 93.75%] [Generator loss: 7.693079]\n",
      "13715 [Discriminator loss: 0.121124, acc.: 95.31%] [Generator loss: 7.747927]\n",
      "13716 [Discriminator loss: 0.098634, acc.: 96.88%] [Generator loss: 5.921350]\n",
      "13717 [Discriminator loss: 0.434453, acc.: 79.69%] [Generator loss: 8.367439]\n",
      "13718 [Discriminator loss: 0.025051, acc.: 98.44%] [Generator loss: 11.260532]\n",
      "13719 [Discriminator loss: 0.282840, acc.: 89.06%] [Generator loss: 7.280278]\n",
      "13720 [Discriminator loss: 0.098068, acc.: 95.31%] [Generator loss: 6.355115]\n",
      "13721 [Discriminator loss: 0.035688, acc.: 96.88%] [Generator loss: 7.555246]\n",
      "13722 [Discriminator loss: 0.194486, acc.: 92.19%] [Generator loss: 6.538724]\n",
      "13723 [Discriminator loss: 0.065241, acc.: 96.88%] [Generator loss: 7.508532]\n",
      "13724 [Discriminator loss: 0.072232, acc.: 95.31%] [Generator loss: 7.378616]\n",
      "13725 [Discriminator loss: 0.118661, acc.: 95.31%] [Generator loss: 5.940880]\n",
      "13726 [Discriminator loss: 0.097531, acc.: 98.44%] [Generator loss: 6.398639]\n",
      "13727 [Discriminator loss: 0.060620, acc.: 95.31%] [Generator loss: 8.635448]\n",
      "13728 [Discriminator loss: 0.029221, acc.: 100.00%] [Generator loss: 8.708370]\n",
      "13729 [Discriminator loss: 0.156441, acc.: 92.19%] [Generator loss: 7.389097]\n",
      "13730 [Discriminator loss: 0.054108, acc.: 98.44%] [Generator loss: 7.468238]\n",
      "13731 [Discriminator loss: 0.016592, acc.: 100.00%] [Generator loss: 7.404375]\n",
      "13732 [Discriminator loss: 0.153000, acc.: 90.62%] [Generator loss: 7.701509]\n",
      "13733 [Discriminator loss: 0.024650, acc.: 100.00%] [Generator loss: 7.858508]\n",
      "13734 [Discriminator loss: 0.128477, acc.: 95.31%] [Generator loss: 6.493783]\n",
      "13735 [Discriminator loss: 0.111457, acc.: 95.31%] [Generator loss: 6.883253]\n",
      "13736 [Discriminator loss: 0.071004, acc.: 96.88%] [Generator loss: 6.611319]\n",
      "13737 [Discriminator loss: 0.058507, acc.: 98.44%] [Generator loss: 7.277647]\n",
      "13738 [Discriminator loss: 0.077511, acc.: 96.88%] [Generator loss: 7.914048]\n",
      "13739 [Discriminator loss: 0.070258, acc.: 95.31%] [Generator loss: 7.521124]\n",
      "13740 [Discriminator loss: 0.162294, acc.: 92.19%] [Generator loss: 7.033408]\n",
      "13741 [Discriminator loss: 0.013817, acc.: 100.00%] [Generator loss: 6.391427]\n",
      "13742 [Discriminator loss: 0.037309, acc.: 98.44%] [Generator loss: 6.073844]\n",
      "13743 [Discriminator loss: 0.161770, acc.: 95.31%] [Generator loss: 7.467587]\n",
      "13744 [Discriminator loss: 0.109145, acc.: 96.88%] [Generator loss: 9.117577]\n",
      "13745 [Discriminator loss: 0.118576, acc.: 95.31%] [Generator loss: 6.662384]\n",
      "13746 [Discriminator loss: 0.179555, acc.: 93.75%] [Generator loss: 6.618253]\n",
      "13747 [Discriminator loss: 0.059875, acc.: 95.31%] [Generator loss: 7.802463]\n",
      "13748 [Discriminator loss: 0.064495, acc.: 95.31%] [Generator loss: 8.611030]\n",
      "13749 [Discriminator loss: 0.054042, acc.: 98.44%] [Generator loss: 7.950198]\n",
      "13750 [Discriminator loss: 0.022087, acc.: 100.00%] [Generator loss: 6.795014]\n",
      "13751 [Discriminator loss: 0.110367, acc.: 95.31%] [Generator loss: 8.340111]\n",
      "13752 [Discriminator loss: 0.065199, acc.: 96.88%] [Generator loss: 7.242461]\n",
      "13753 [Discriminator loss: 0.210410, acc.: 95.31%] [Generator loss: 8.066063]\n",
      "13754 [Discriminator loss: 0.026024, acc.: 100.00%] [Generator loss: 8.075377]\n",
      "13755 [Discriminator loss: 0.120274, acc.: 95.31%] [Generator loss: 6.445179]\n",
      "13756 [Discriminator loss: 0.088036, acc.: 95.31%] [Generator loss: 7.835664]\n",
      "13757 [Discriminator loss: 0.036554, acc.: 98.44%] [Generator loss: 7.437163]\n",
      "13758 [Discriminator loss: 0.119091, acc.: 95.31%] [Generator loss: 7.547695]\n",
      "13759 [Discriminator loss: 0.100991, acc.: 95.31%] [Generator loss: 6.701343]\n",
      "13760 [Discriminator loss: 0.088661, acc.: 96.88%] [Generator loss: 7.783599]\n",
      "13761 [Discriminator loss: 0.099872, acc.: 96.88%] [Generator loss: 6.916093]\n",
      "13762 [Discriminator loss: 0.172596, acc.: 92.19%] [Generator loss: 6.623883]\n",
      "13763 [Discriminator loss: 0.126277, acc.: 93.75%] [Generator loss: 9.099358]\n",
      "13764 [Discriminator loss: 0.022661, acc.: 100.00%] [Generator loss: 7.681178]\n",
      "13765 [Discriminator loss: 0.070111, acc.: 96.88%] [Generator loss: 7.322098]\n",
      "13766 [Discriminator loss: 0.040232, acc.: 100.00%] [Generator loss: 6.428604]\n",
      "13767 [Discriminator loss: 0.140154, acc.: 95.31%] [Generator loss: 6.966033]\n",
      "13768 [Discriminator loss: 0.154402, acc.: 90.62%] [Generator loss: 7.343239]\n",
      "13769 [Discriminator loss: 0.063122, acc.: 98.44%] [Generator loss: 7.832909]\n",
      "13770 [Discriminator loss: 0.042296, acc.: 98.44%] [Generator loss: 7.643091]\n",
      "13771 [Discriminator loss: 0.074094, acc.: 98.44%] [Generator loss: 8.370974]\n",
      "13772 [Discriminator loss: 0.053839, acc.: 96.88%] [Generator loss: 8.032122]\n",
      "13773 [Discriminator loss: 0.126209, acc.: 95.31%] [Generator loss: 6.200298]\n",
      "13774 [Discriminator loss: 0.081918, acc.: 96.88%] [Generator loss: 8.135998]\n",
      "13775 [Discriminator loss: 0.026833, acc.: 100.00%] [Generator loss: 8.631193]\n",
      "13776 [Discriminator loss: 0.036471, acc.: 98.44%] [Generator loss: 7.238731]\n",
      "13777 [Discriminator loss: 0.025855, acc.: 100.00%] [Generator loss: 7.112740]\n",
      "13778 [Discriminator loss: 0.171109, acc.: 93.75%] [Generator loss: 6.432118]\n",
      "13779 [Discriminator loss: 0.036878, acc.: 100.00%] [Generator loss: 7.076270]\n",
      "13780 [Discriminator loss: 0.083472, acc.: 96.88%] [Generator loss: 7.228108]\n",
      "13781 [Discriminator loss: 0.047627, acc.: 98.44%] [Generator loss: 7.816629]\n",
      "13782 [Discriminator loss: 0.045141, acc.: 98.44%] [Generator loss: 6.919541]\n",
      "13783 [Discriminator loss: 0.088190, acc.: 95.31%] [Generator loss: 8.213207]\n",
      "13784 [Discriminator loss: 0.069824, acc.: 98.44%] [Generator loss: 6.739295]\n",
      "13785 [Discriminator loss: 0.115067, acc.: 96.88%] [Generator loss: 8.633848]\n",
      "13786 [Discriminator loss: 0.121871, acc.: 93.75%] [Generator loss: 6.983289]\n",
      "13787 [Discriminator loss: 0.098118, acc.: 93.75%] [Generator loss: 7.834697]\n",
      "13788 [Discriminator loss: 0.112255, acc.: 96.88%] [Generator loss: 7.813920]\n",
      "13789 [Discriminator loss: 0.083752, acc.: 95.31%] [Generator loss: 6.186704]\n",
      "13790 [Discriminator loss: 0.099343, acc.: 96.88%] [Generator loss: 6.833143]\n",
      "13791 [Discriminator loss: 0.153860, acc.: 95.31%] [Generator loss: 8.412561]\n",
      "13792 [Discriminator loss: 0.157140, acc.: 90.62%] [Generator loss: 7.215583]\n",
      "13793 [Discriminator loss: 0.024015, acc.: 100.00%] [Generator loss: 8.956570]\n",
      "13794 [Discriminator loss: 0.069260, acc.: 98.44%] [Generator loss: 8.470716]\n",
      "13795 [Discriminator loss: 0.040082, acc.: 96.88%] [Generator loss: 8.085299]\n",
      "13796 [Discriminator loss: 0.032507, acc.: 98.44%] [Generator loss: 8.266297]\n",
      "13797 [Discriminator loss: 0.203919, acc.: 92.19%] [Generator loss: 6.196704]\n",
      "13798 [Discriminator loss: 0.195985, acc.: 93.75%] [Generator loss: 6.942082]\n",
      "13799 [Discriminator loss: 0.016577, acc.: 100.00%] [Generator loss: 8.559267]\n",
      "13800 [Discriminator loss: 0.098905, acc.: 95.31%] [Generator loss: 6.751753]\n",
      "13801 [Discriminator loss: 0.178961, acc.: 90.62%] [Generator loss: 7.549476]\n",
      "13802 [Discriminator loss: 0.180910, acc.: 92.19%] [Generator loss: 7.954641]\n",
      "13803 [Discriminator loss: 0.154899, acc.: 92.19%] [Generator loss: 6.053360]\n",
      "13804 [Discriminator loss: 0.083169, acc.: 96.88%] [Generator loss: 7.115196]\n",
      "13805 [Discriminator loss: 0.140078, acc.: 93.75%] [Generator loss: 7.784319]\n",
      "13806 [Discriminator loss: 0.017861, acc.: 100.00%] [Generator loss: 9.754661]\n",
      "13807 [Discriminator loss: 0.091077, acc.: 95.31%] [Generator loss: 7.003779]\n",
      "13808 [Discriminator loss: 0.069225, acc.: 95.31%] [Generator loss: 6.042960]\n",
      "13809 [Discriminator loss: 0.095042, acc.: 95.31%] [Generator loss: 6.000667]\n",
      "13810 [Discriminator loss: 0.107689, acc.: 96.88%] [Generator loss: 6.695626]\n",
      "13811 [Discriminator loss: 0.176017, acc.: 96.88%] [Generator loss: 5.723195]\n",
      "13812 [Discriminator loss: 0.020108, acc.: 100.00%] [Generator loss: 7.718455]\n",
      "13813 [Discriminator loss: 0.067128, acc.: 96.88%] [Generator loss: 6.716876]\n",
      "13814 [Discriminator loss: 0.086172, acc.: 95.31%] [Generator loss: 8.087192]\n",
      "13815 [Discriminator loss: 0.239079, acc.: 92.19%] [Generator loss: 5.879635]\n",
      "13816 [Discriminator loss: 0.248718, acc.: 89.06%] [Generator loss: 9.106582]\n",
      "13817 [Discriminator loss: 0.062623, acc.: 98.44%] [Generator loss: 8.820704]\n",
      "13818 [Discriminator loss: 0.101991, acc.: 96.88%] [Generator loss: 8.368780]\n",
      "13819 [Discriminator loss: 0.059124, acc.: 98.44%] [Generator loss: 7.584671]\n",
      "13820 [Discriminator loss: 0.021791, acc.: 100.00%] [Generator loss: 6.656284]\n",
      "13821 [Discriminator loss: 0.087564, acc.: 95.31%] [Generator loss: 9.133831]\n",
      "13822 [Discriminator loss: 0.031656, acc.: 100.00%] [Generator loss: 6.006154]\n",
      "13823 [Discriminator loss: 0.122963, acc.: 95.31%] [Generator loss: 7.040936]\n",
      "13824 [Discriminator loss: 0.022007, acc.: 100.00%] [Generator loss: 7.647321]\n",
      "13825 [Discriminator loss: 0.050272, acc.: 96.88%] [Generator loss: 6.988882]\n",
      "13826 [Discriminator loss: 0.087614, acc.: 95.31%] [Generator loss: 5.561471]\n",
      "13827 [Discriminator loss: 0.263880, acc.: 90.62%] [Generator loss: 8.572479]\n",
      "13828 [Discriminator loss: 0.137581, acc.: 95.31%] [Generator loss: 7.771219]\n",
      "13829 [Discriminator loss: 0.037278, acc.: 98.44%] [Generator loss: 8.168186]\n",
      "13830 [Discriminator loss: 0.076011, acc.: 96.88%] [Generator loss: 7.966772]\n",
      "13831 [Discriminator loss: 0.055335, acc.: 98.44%] [Generator loss: 6.237598]\n",
      "13832 [Discriminator loss: 0.088414, acc.: 96.88%] [Generator loss: 6.656979]\n",
      "13833 [Discriminator loss: 0.072593, acc.: 96.88%] [Generator loss: 7.610764]\n",
      "13834 [Discriminator loss: 0.050698, acc.: 98.44%] [Generator loss: 7.868481]\n",
      "13835 [Discriminator loss: 0.290607, acc.: 87.50%] [Generator loss: 8.357206]\n",
      "13836 [Discriminator loss: 0.031190, acc.: 100.00%] [Generator loss: 7.974304]\n",
      "13837 [Discriminator loss: 0.136760, acc.: 93.75%] [Generator loss: 5.947652]\n",
      "13838 [Discriminator loss: 0.118574, acc.: 96.88%] [Generator loss: 6.672102]\n",
      "13839 [Discriminator loss: 0.032727, acc.: 100.00%] [Generator loss: 7.981965]\n",
      "13840 [Discriminator loss: 0.146709, acc.: 92.19%] [Generator loss: 7.480968]\n",
      "13841 [Discriminator loss: 0.041631, acc.: 98.44%] [Generator loss: 7.942733]\n",
      "13842 [Discriminator loss: 0.208208, acc.: 92.19%] [Generator loss: 6.899661]\n",
      "13843 [Discriminator loss: 0.078543, acc.: 95.31%] [Generator loss: 7.707282]\n",
      "13844 [Discriminator loss: 0.036357, acc.: 100.00%] [Generator loss: 8.381505]\n",
      "13845 [Discriminator loss: 0.041122, acc.: 98.44%] [Generator loss: 8.028970]\n",
      "13846 [Discriminator loss: 0.101158, acc.: 95.31%] [Generator loss: 7.569016]\n",
      "13847 [Discriminator loss: 0.017486, acc.: 100.00%] [Generator loss: 7.477355]\n",
      "13848 [Discriminator loss: 0.073310, acc.: 96.88%] [Generator loss: 7.470633]\n",
      "13849 [Discriminator loss: 0.064166, acc.: 96.88%] [Generator loss: 7.240530]\n",
      "13850 [Discriminator loss: 0.397881, acc.: 82.81%] [Generator loss: 8.483845]\n",
      "13851 [Discriminator loss: 0.125274, acc.: 96.88%] [Generator loss: 8.097675]\n",
      "13852 [Discriminator loss: 0.081198, acc.: 95.31%] [Generator loss: 9.836700]\n",
      "13853 [Discriminator loss: 0.271878, acc.: 92.19%] [Generator loss: 6.355906]\n",
      "13854 [Discriminator loss: 0.047496, acc.: 96.88%] [Generator loss: 7.365243]\n",
      "13855 [Discriminator loss: 0.065576, acc.: 96.88%] [Generator loss: 6.912414]\n",
      "13856 [Discriminator loss: 0.068309, acc.: 98.44%] [Generator loss: 8.377535]\n",
      "13857 [Discriminator loss: 0.028569, acc.: 100.00%] [Generator loss: 7.137154]\n",
      "13858 [Discriminator loss: 0.073126, acc.: 98.44%] [Generator loss: 7.817671]\n",
      "13859 [Discriminator loss: 0.108927, acc.: 95.31%] [Generator loss: 7.741000]\n",
      "13860 [Discriminator loss: 0.189531, acc.: 92.19%] [Generator loss: 6.821446]\n",
      "13861 [Discriminator loss: 0.041944, acc.: 100.00%] [Generator loss: 6.391496]\n",
      "13862 [Discriminator loss: 0.157861, acc.: 93.75%] [Generator loss: 6.481810]\n",
      "13863 [Discriminator loss: 0.118399, acc.: 96.88%] [Generator loss: 9.275076]\n",
      "13864 [Discriminator loss: 0.200419, acc.: 95.31%] [Generator loss: 8.683338]\n",
      "13865 [Discriminator loss: 0.080684, acc.: 96.88%] [Generator loss: 7.754591]\n",
      "13866 [Discriminator loss: 0.036455, acc.: 98.44%] [Generator loss: 8.186658]\n",
      "13867 [Discriminator loss: 0.045427, acc.: 98.44%] [Generator loss: 6.065215]\n",
      "13868 [Discriminator loss: 0.122842, acc.: 96.88%] [Generator loss: 7.554318]\n",
      "13869 [Discriminator loss: 0.025859, acc.: 100.00%] [Generator loss: 8.042293]\n",
      "13870 [Discriminator loss: 0.021200, acc.: 100.00%] [Generator loss: 7.549959]\n",
      "13871 [Discriminator loss: 0.039997, acc.: 98.44%] [Generator loss: 6.641469]\n",
      "13872 [Discriminator loss: 0.176500, acc.: 92.19%] [Generator loss: 7.662147]\n",
      "13873 [Discriminator loss: 0.105590, acc.: 96.88%] [Generator loss: 7.687852]\n",
      "13874 [Discriminator loss: 0.027819, acc.: 100.00%] [Generator loss: 6.735791]\n",
      "13875 [Discriminator loss: 0.118397, acc.: 93.75%] [Generator loss: 7.138247]\n",
      "13876 [Discriminator loss: 0.160787, acc.: 95.31%] [Generator loss: 8.366710]\n",
      "13877 [Discriminator loss: 0.210913, acc.: 89.06%] [Generator loss: 7.528697]\n",
      "13878 [Discriminator loss: 0.087262, acc.: 95.31%] [Generator loss: 7.963917]\n",
      "13879 [Discriminator loss: 0.055042, acc.: 96.88%] [Generator loss: 8.147215]\n",
      "13880 [Discriminator loss: 0.041086, acc.: 100.00%] [Generator loss: 7.619255]\n",
      "13881 [Discriminator loss: 0.067234, acc.: 96.88%] [Generator loss: 7.115511]\n",
      "13882 [Discriminator loss: 0.047903, acc.: 98.44%] [Generator loss: 7.074499]\n",
      "13883 [Discriminator loss: 0.098969, acc.: 96.88%] [Generator loss: 6.000003]\n",
      "13884 [Discriminator loss: 0.160414, acc.: 89.06%] [Generator loss: 7.836320]\n",
      "13885 [Discriminator loss: 0.101864, acc.: 95.31%] [Generator loss: 9.419527]\n",
      "13886 [Discriminator loss: 0.162754, acc.: 92.19%] [Generator loss: 5.907206]\n",
      "13887 [Discriminator loss: 0.155535, acc.: 95.31%] [Generator loss: 7.249070]\n",
      "13888 [Discriminator loss: 0.044910, acc.: 100.00%] [Generator loss: 8.058852]\n",
      "13889 [Discriminator loss: 0.106506, acc.: 93.75%] [Generator loss: 6.761935]\n",
      "13890 [Discriminator loss: 0.044662, acc.: 100.00%] [Generator loss: 6.443381]\n",
      "13891 [Discriminator loss: 0.099910, acc.: 95.31%] [Generator loss: 8.815758]\n",
      "13892 [Discriminator loss: 0.031201, acc.: 98.44%] [Generator loss: 8.554611]\n",
      "13893 [Discriminator loss: 0.059596, acc.: 96.88%] [Generator loss: 7.519727]\n",
      "13894 [Discriminator loss: 0.075014, acc.: 96.88%] [Generator loss: 7.989451]\n",
      "13895 [Discriminator loss: 0.130848, acc.: 96.88%] [Generator loss: 7.128927]\n",
      "13896 [Discriminator loss: 0.137532, acc.: 93.75%] [Generator loss: 7.332243]\n",
      "13897 [Discriminator loss: 0.020952, acc.: 100.00%] [Generator loss: 7.313338]\n",
      "13898 [Discriminator loss: 0.094884, acc.: 96.88%] [Generator loss: 8.967573]\n",
      "13899 [Discriminator loss: 0.093135, acc.: 96.88%] [Generator loss: 7.081382]\n",
      "13900 [Discriminator loss: 0.071241, acc.: 96.88%] [Generator loss: 6.754198]\n",
      "13901 [Discriminator loss: 0.082194, acc.: 95.31%] [Generator loss: 6.336084]\n",
      "13902 [Discriminator loss: 0.100743, acc.: 96.88%] [Generator loss: 6.520253]\n",
      "13903 [Discriminator loss: 0.059645, acc.: 98.44%] [Generator loss: 7.108916]\n",
      "13904 [Discriminator loss: 0.039714, acc.: 96.88%] [Generator loss: 7.837135]\n",
      "13905 [Discriminator loss: 0.060950, acc.: 96.88%] [Generator loss: 7.784941]\n",
      "13906 [Discriminator loss: 0.352934, acc.: 87.50%] [Generator loss: 8.541889]\n",
      "13907 [Discriminator loss: 0.141655, acc.: 95.31%] [Generator loss: 9.317720]\n",
      "13908 [Discriminator loss: 0.061245, acc.: 96.88%] [Generator loss: 8.969709]\n",
      "13909 [Discriminator loss: 0.099762, acc.: 96.88%] [Generator loss: 8.335093]\n",
      "13910 [Discriminator loss: 0.086206, acc.: 95.31%] [Generator loss: 6.461001]\n",
      "13911 [Discriminator loss: 0.121298, acc.: 95.31%] [Generator loss: 7.631411]\n",
      "13912 [Discriminator loss: 0.209692, acc.: 95.31%] [Generator loss: 8.670015]\n",
      "13913 [Discriminator loss: 0.018026, acc.: 100.00%] [Generator loss: 8.855996]\n",
      "13914 [Discriminator loss: 0.119642, acc.: 93.75%] [Generator loss: 7.741737]\n",
      "13915 [Discriminator loss: 0.103695, acc.: 95.31%] [Generator loss: 6.110574]\n",
      "13916 [Discriminator loss: 0.098775, acc.: 93.75%] [Generator loss: 8.153878]\n",
      "13917 [Discriminator loss: 0.014739, acc.: 100.00%] [Generator loss: 8.339439]\n",
      "13918 [Discriminator loss: 0.074647, acc.: 98.44%] [Generator loss: 8.372999]\n",
      "13919 [Discriminator loss: 0.200486, acc.: 89.06%] [Generator loss: 6.638255]\n",
      "13920 [Discriminator loss: 0.093699, acc.: 93.75%] [Generator loss: 7.519085]\n",
      "13921 [Discriminator loss: 0.114241, acc.: 95.31%] [Generator loss: 6.724054]\n",
      "13922 [Discriminator loss: 0.065541, acc.: 96.88%] [Generator loss: 8.253361]\n",
      "13923 [Discriminator loss: 0.230528, acc.: 92.19%] [Generator loss: 8.132111]\n",
      "13924 [Discriminator loss: 0.054068, acc.: 98.44%] [Generator loss: 7.251974]\n",
      "13925 [Discriminator loss: 0.080930, acc.: 98.44%] [Generator loss: 8.529251]\n",
      "13926 [Discriminator loss: 0.030227, acc.: 100.00%] [Generator loss: 6.481355]\n",
      "13927 [Discriminator loss: 0.135379, acc.: 93.75%] [Generator loss: 7.437583]\n",
      "13928 [Discriminator loss: 0.133630, acc.: 93.75%] [Generator loss: 7.792089]\n",
      "13929 [Discriminator loss: 0.131012, acc.: 93.75%] [Generator loss: 8.193921]\n",
      "13930 [Discriminator loss: 0.035979, acc.: 100.00%] [Generator loss: 7.311188]\n",
      "13931 [Discriminator loss: 0.038424, acc.: 98.44%] [Generator loss: 7.238999]\n",
      "13932 [Discriminator loss: 0.137850, acc.: 96.88%] [Generator loss: 6.644091]\n",
      "13933 [Discriminator loss: 0.119860, acc.: 96.88%] [Generator loss: 9.028784]\n",
      "13934 [Discriminator loss: 0.049022, acc.: 96.88%] [Generator loss: 8.000587]\n",
      "13935 [Discriminator loss: 0.065528, acc.: 96.88%] [Generator loss: 6.483937]\n",
      "13936 [Discriminator loss: 0.039439, acc.: 100.00%] [Generator loss: 5.852983]\n",
      "13937 [Discriminator loss: 0.135098, acc.: 93.75%] [Generator loss: 8.135537]\n",
      "13938 [Discriminator loss: 0.054962, acc.: 96.88%] [Generator loss: 8.825853]\n",
      "13939 [Discriminator loss: 0.109320, acc.: 93.75%] [Generator loss: 8.368684]\n",
      "13940 [Discriminator loss: 0.035271, acc.: 100.00%] [Generator loss: 7.468386]\n",
      "13941 [Discriminator loss: 0.123291, acc.: 93.75%] [Generator loss: 7.662365]\n",
      "13942 [Discriminator loss: 0.230910, acc.: 87.50%] [Generator loss: 6.505283]\n",
      "13943 [Discriminator loss: 0.032864, acc.: 98.44%] [Generator loss: 7.629802]\n",
      "13944 [Discriminator loss: 0.144993, acc.: 92.19%] [Generator loss: 7.135303]\n",
      "13945 [Discriminator loss: 0.015320, acc.: 100.00%] [Generator loss: 6.393102]\n",
      "13946 [Discriminator loss: 0.094180, acc.: 96.88%] [Generator loss: 6.959115]\n",
      "13947 [Discriminator loss: 0.072952, acc.: 96.88%] [Generator loss: 7.648519]\n",
      "13948 [Discriminator loss: 0.161045, acc.: 92.19%] [Generator loss: 9.807457]\n",
      "13949 [Discriminator loss: 0.105365, acc.: 96.88%] [Generator loss: 8.480034]\n",
      "13950 [Discriminator loss: 0.058285, acc.: 96.88%] [Generator loss: 7.047729]\n",
      "13951 [Discriminator loss: 0.061520, acc.: 98.44%] [Generator loss: 5.754800]\n",
      "13952 [Discriminator loss: 0.038647, acc.: 100.00%] [Generator loss: 6.865708]\n",
      "13953 [Discriminator loss: 0.076440, acc.: 95.31%] [Generator loss: 6.288981]\n",
      "13954 [Discriminator loss: 0.031716, acc.: 100.00%] [Generator loss: 6.449211]\n",
      "13955 [Discriminator loss: 0.115996, acc.: 95.31%] [Generator loss: 7.197505]\n",
      "13956 [Discriminator loss: 0.025707, acc.: 100.00%] [Generator loss: 9.224344]\n",
      "13957 [Discriminator loss: 0.068496, acc.: 96.88%] [Generator loss: 7.530970]\n",
      "13958 [Discriminator loss: 0.065661, acc.: 96.88%] [Generator loss: 7.773176]\n",
      "13959 [Discriminator loss: 0.033978, acc.: 98.44%] [Generator loss: 7.906159]\n",
      "13960 [Discriminator loss: 0.110551, acc.: 96.88%] [Generator loss: 7.495293]\n",
      "13961 [Discriminator loss: 0.015128, acc.: 100.00%] [Generator loss: 7.943865]\n",
      "13962 [Discriminator loss: 0.123495, acc.: 93.75%] [Generator loss: 7.068226]\n",
      "13963 [Discriminator loss: 0.070377, acc.: 98.44%] [Generator loss: 7.898481]\n",
      "13964 [Discriminator loss: 0.092266, acc.: 98.44%] [Generator loss: 5.686587]\n",
      "13965 [Discriminator loss: 0.025864, acc.: 98.44%] [Generator loss: 6.633502]\n",
      "13966 [Discriminator loss: 0.097730, acc.: 96.88%] [Generator loss: 7.323673]\n",
      "13967 [Discriminator loss: 0.043848, acc.: 98.44%] [Generator loss: 7.229194]\n",
      "13968 [Discriminator loss: 0.204032, acc.: 90.62%] [Generator loss: 6.440872]\n",
      "13969 [Discriminator loss: 0.064712, acc.: 96.88%] [Generator loss: 6.708691]\n",
      "13970 [Discriminator loss: 0.079131, acc.: 95.31%] [Generator loss: 8.580019]\n",
      "13971 [Discriminator loss: 0.084071, acc.: 95.31%] [Generator loss: 8.358595]\n",
      "13972 [Discriminator loss: 0.078891, acc.: 96.88%] [Generator loss: 7.843963]\n",
      "13973 [Discriminator loss: 0.138968, acc.: 93.75%] [Generator loss: 7.553294]\n",
      "13974 [Discriminator loss: 0.098449, acc.: 96.88%] [Generator loss: 9.026172]\n",
      "13975 [Discriminator loss: 0.231963, acc.: 89.06%] [Generator loss: 6.226803]\n",
      "13976 [Discriminator loss: 0.024965, acc.: 100.00%] [Generator loss: 7.449596]\n",
      "13977 [Discriminator loss: 0.063321, acc.: 96.88%] [Generator loss: 8.027361]\n",
      "13978 [Discriminator loss: 0.028481, acc.: 100.00%] [Generator loss: 9.402916]\n",
      "13979 [Discriminator loss: 0.119016, acc.: 95.31%] [Generator loss: 7.600616]\n",
      "13980 [Discriminator loss: 0.053393, acc.: 98.44%] [Generator loss: 6.374253]\n",
      "13981 [Discriminator loss: 0.122059, acc.: 93.75%] [Generator loss: 8.161344]\n",
      "13982 [Discriminator loss: 0.056768, acc.: 96.88%] [Generator loss: 8.077919]\n",
      "13983 [Discriminator loss: 0.043614, acc.: 98.44%] [Generator loss: 8.102441]\n",
      "13984 [Discriminator loss: 0.100483, acc.: 96.88%] [Generator loss: 9.360199]\n",
      "13985 [Discriminator loss: 0.068134, acc.: 96.88%] [Generator loss: 7.014747]\n",
      "13986 [Discriminator loss: 0.068472, acc.: 96.88%] [Generator loss: 8.215765]\n",
      "13987 [Discriminator loss: 0.151401, acc.: 96.88%] [Generator loss: 8.055063]\n",
      "13988 [Discriminator loss: 0.047645, acc.: 96.88%] [Generator loss: 6.620167]\n",
      "13989 [Discriminator loss: 0.230692, acc.: 92.19%] [Generator loss: 8.703077]\n",
      "13990 [Discriminator loss: 0.043048, acc.: 98.44%] [Generator loss: 8.041063]\n",
      "13991 [Discriminator loss: 0.112307, acc.: 93.75%] [Generator loss: 8.336840]\n",
      "13992 [Discriminator loss: 0.077507, acc.: 96.88%] [Generator loss: 8.230200]\n",
      "13993 [Discriminator loss: 0.097977, acc.: 95.31%] [Generator loss: 7.902404]\n",
      "13994 [Discriminator loss: 0.028173, acc.: 100.00%] [Generator loss: 8.172985]\n",
      "13995 [Discriminator loss: 0.130670, acc.: 95.31%] [Generator loss: 7.067973]\n",
      "13996 [Discriminator loss: 0.071539, acc.: 98.44%] [Generator loss: 6.159768]\n",
      "13997 [Discriminator loss: 0.085874, acc.: 96.88%] [Generator loss: 6.329970]\n",
      "13998 [Discriminator loss: 0.061473, acc.: 96.88%] [Generator loss: 6.555511]\n",
      "13999 [Discriminator loss: 0.173802, acc.: 93.75%] [Generator loss: 6.578804]\n",
      "14000 [Discriminator loss: 0.044558, acc.: 98.44%] [Generator loss: 8.398294]\n",
      "14001 [Discriminator loss: 0.149223, acc.: 93.75%] [Generator loss: 6.316021]\n",
      "14002 [Discriminator loss: 0.104656, acc.: 93.75%] [Generator loss: 7.457153]\n",
      "14003 [Discriminator loss: 0.074735, acc.: 96.88%] [Generator loss: 7.643473]\n",
      "14004 [Discriminator loss: 0.059788, acc.: 98.44%] [Generator loss: 8.567493]\n",
      "14005 [Discriminator loss: 0.139226, acc.: 95.31%] [Generator loss: 8.016594]\n",
      "14006 [Discriminator loss: 0.091726, acc.: 95.31%] [Generator loss: 7.767470]\n",
      "14007 [Discriminator loss: 0.139056, acc.: 95.31%] [Generator loss: 8.354284]\n",
      "14008 [Discriminator loss: 0.078632, acc.: 95.31%] [Generator loss: 8.885542]\n",
      "14009 [Discriminator loss: 0.145637, acc.: 93.75%] [Generator loss: 8.295620]\n",
      "14010 [Discriminator loss: 0.244848, acc.: 90.62%] [Generator loss: 6.619012]\n",
      "14011 [Discriminator loss: 0.091321, acc.: 93.75%] [Generator loss: 7.903240]\n",
      "14012 [Discriminator loss: 0.056954, acc.: 96.88%] [Generator loss: 8.834774]\n",
      "14013 [Discriminator loss: 0.068137, acc.: 96.88%] [Generator loss: 6.531888]\n",
      "14014 [Discriminator loss: 0.021485, acc.: 100.00%] [Generator loss: 8.741220]\n",
      "14015 [Discriminator loss: 0.139796, acc.: 95.31%] [Generator loss: 6.099380]\n",
      "14016 [Discriminator loss: 0.129163, acc.: 93.75%] [Generator loss: 6.700466]\n",
      "14017 [Discriminator loss: 0.061024, acc.: 96.88%] [Generator loss: 7.024892]\n",
      "14018 [Discriminator loss: 0.094824, acc.: 96.88%] [Generator loss: 6.856293]\n",
      "14019 [Discriminator loss: 0.038602, acc.: 98.44%] [Generator loss: 7.656316]\n",
      "14020 [Discriminator loss: 0.134043, acc.: 93.75%] [Generator loss: 8.110849]\n",
      "14021 [Discriminator loss: 0.053884, acc.: 96.88%] [Generator loss: 7.769287]\n",
      "14022 [Discriminator loss: 0.028986, acc.: 100.00%] [Generator loss: 7.192054]\n",
      "14023 [Discriminator loss: 0.217719, acc.: 90.62%] [Generator loss: 6.020711]\n",
      "14024 [Discriminator loss: 0.162091, acc.: 95.31%] [Generator loss: 8.396448]\n",
      "14025 [Discriminator loss: 0.015933, acc.: 100.00%] [Generator loss: 9.293010]\n",
      "14026 [Discriminator loss: 0.078476, acc.: 96.88%] [Generator loss: 7.933918]\n",
      "14027 [Discriminator loss: 0.215878, acc.: 90.62%] [Generator loss: 10.035290]\n",
      "14028 [Discriminator loss: 0.031071, acc.: 96.88%] [Generator loss: 10.197521]\n",
      "14029 [Discriminator loss: 0.192331, acc.: 89.06%] [Generator loss: 7.390618]\n",
      "14030 [Discriminator loss: 0.092161, acc.: 98.44%] [Generator loss: 8.163141]\n",
      "14031 [Discriminator loss: 0.119104, acc.: 95.31%] [Generator loss: 6.045570]\n",
      "14032 [Discriminator loss: 0.168401, acc.: 93.75%] [Generator loss: 8.817342]\n",
      "14033 [Discriminator loss: 0.113732, acc.: 98.44%] [Generator loss: 7.973068]\n",
      "14034 [Discriminator loss: 0.240132, acc.: 92.19%] [Generator loss: 7.820432]\n",
      "14035 [Discriminator loss: 0.088585, acc.: 98.44%] [Generator loss: 6.431977]\n",
      "14036 [Discriminator loss: 0.114572, acc.: 96.88%] [Generator loss: 7.327522]\n",
      "14037 [Discriminator loss: 0.111825, acc.: 95.31%] [Generator loss: 6.551880]\n",
      "14038 [Discriminator loss: 0.136479, acc.: 89.06%] [Generator loss: 7.623950]\n",
      "14039 [Discriminator loss: 0.052120, acc.: 98.44%] [Generator loss: 7.436761]\n",
      "14040 [Discriminator loss: 0.133201, acc.: 96.88%] [Generator loss: 6.331367]\n",
      "14041 [Discriminator loss: 0.135490, acc.: 93.75%] [Generator loss: 7.800290]\n",
      "14042 [Discriminator loss: 0.017522, acc.: 100.00%] [Generator loss: 7.981564]\n",
      "14043 [Discriminator loss: 0.069407, acc.: 96.88%] [Generator loss: 7.528928]\n",
      "14044 [Discriminator loss: 0.024472, acc.: 100.00%] [Generator loss: 7.202080]\n",
      "14045 [Discriminator loss: 0.067058, acc.: 96.88%] [Generator loss: 6.994588]\n",
      "14046 [Discriminator loss: 0.157677, acc.: 90.62%] [Generator loss: 6.760430]\n",
      "14047 [Discriminator loss: 0.089896, acc.: 96.88%] [Generator loss: 7.948946]\n",
      "14048 [Discriminator loss: 0.081777, acc.: 95.31%] [Generator loss: 7.931503]\n",
      "14049 [Discriminator loss: 0.062137, acc.: 98.44%] [Generator loss: 8.450690]\n",
      "14050 [Discriminator loss: 0.124655, acc.: 93.75%] [Generator loss: 7.659083]\n",
      "14051 [Discriminator loss: 0.044076, acc.: 98.44%] [Generator loss: 7.716107]\n",
      "14052 [Discriminator loss: 0.115665, acc.: 93.75%] [Generator loss: 8.127931]\n",
      "14053 [Discriminator loss: 0.101963, acc.: 95.31%] [Generator loss: 8.225394]\n",
      "14054 [Discriminator loss: 0.108997, acc.: 96.88%] [Generator loss: 9.110609]\n",
      "14055 [Discriminator loss: 0.124215, acc.: 93.75%] [Generator loss: 8.492757]\n",
      "14056 [Discriminator loss: 0.074103, acc.: 96.88%] [Generator loss: 6.584654]\n",
      "14057 [Discriminator loss: 0.078659, acc.: 96.88%] [Generator loss: 6.581248]\n",
      "14058 [Discriminator loss: 0.069489, acc.: 98.44%] [Generator loss: 7.269787]\n",
      "14059 [Discriminator loss: 0.275919, acc.: 92.19%] [Generator loss: 6.452520]\n",
      "14060 [Discriminator loss: 0.052044, acc.: 98.44%] [Generator loss: 7.168491]\n",
      "14061 [Discriminator loss: 0.024539, acc.: 100.00%] [Generator loss: 8.751947]\n",
      "14062 [Discriminator loss: 0.174166, acc.: 95.31%] [Generator loss: 8.560860]\n",
      "14063 [Discriminator loss: 0.135311, acc.: 95.31%] [Generator loss: 8.804230]\n",
      "14064 [Discriminator loss: 0.058293, acc.: 96.88%] [Generator loss: 8.226522]\n",
      "14065 [Discriminator loss: 0.053715, acc.: 98.44%] [Generator loss: 7.952617]\n",
      "14066 [Discriminator loss: 0.073000, acc.: 96.88%] [Generator loss: 7.276036]\n",
      "14067 [Discriminator loss: 0.018154, acc.: 100.00%] [Generator loss: 7.217234]\n",
      "14068 [Discriminator loss: 0.150703, acc.: 93.75%] [Generator loss: 6.841893]\n",
      "14069 [Discriminator loss: 0.162852, acc.: 92.19%] [Generator loss: 8.763247]\n",
      "14070 [Discriminator loss: 0.050899, acc.: 98.44%] [Generator loss: 8.268465]\n",
      "14071 [Discriminator loss: 0.251405, acc.: 90.62%] [Generator loss: 6.769215]\n",
      "14072 [Discriminator loss: 0.097011, acc.: 93.75%] [Generator loss: 6.630440]\n",
      "14073 [Discriminator loss: 0.094291, acc.: 98.44%] [Generator loss: 7.190555]\n",
      "14074 [Discriminator loss: 0.051094, acc.: 98.44%] [Generator loss: 7.439762]\n",
      "14075 [Discriminator loss: 0.122697, acc.: 92.19%] [Generator loss: 8.417733]\n",
      "14076 [Discriminator loss: 0.218449, acc.: 89.06%] [Generator loss: 6.285831]\n",
      "14077 [Discriminator loss: 0.132232, acc.: 93.75%] [Generator loss: 7.578829]\n",
      "14078 [Discriminator loss: 0.074082, acc.: 95.31%] [Generator loss: 8.033439]\n",
      "14079 [Discriminator loss: 0.039593, acc.: 98.44%] [Generator loss: 6.925018]\n",
      "14080 [Discriminator loss: 0.025982, acc.: 100.00%] [Generator loss: 6.244321]\n",
      "14081 [Discriminator loss: 0.043036, acc.: 98.44%] [Generator loss: 7.574114]\n",
      "14082 [Discriminator loss: 0.085487, acc.: 98.44%] [Generator loss: 7.225213]\n",
      "14083 [Discriminator loss: 0.118662, acc.: 93.75%] [Generator loss: 7.088999]\n",
      "14084 [Discriminator loss: 0.120998, acc.: 96.88%] [Generator loss: 6.746614]\n",
      "14085 [Discriminator loss: 0.185877, acc.: 93.75%] [Generator loss: 6.967996]\n",
      "14086 [Discriminator loss: 0.033161, acc.: 98.44%] [Generator loss: 9.351002]\n",
      "14087 [Discriminator loss: 0.049396, acc.: 98.44%] [Generator loss: 6.912193]\n",
      "14088 [Discriminator loss: 0.085762, acc.: 96.88%] [Generator loss: 8.314985]\n",
      "14089 [Discriminator loss: 0.078324, acc.: 96.88%] [Generator loss: 7.897777]\n",
      "14090 [Discriminator loss: 0.112248, acc.: 96.88%] [Generator loss: 6.637465]\n",
      "14091 [Discriminator loss: 0.225767, acc.: 92.19%] [Generator loss: 6.413434]\n",
      "14092 [Discriminator loss: 0.028384, acc.: 98.44%] [Generator loss: 8.528444]\n",
      "14093 [Discriminator loss: 0.041756, acc.: 98.44%] [Generator loss: 7.666834]\n",
      "14094 [Discriminator loss: 0.140875, acc.: 93.75%] [Generator loss: 7.394434]\n",
      "14095 [Discriminator loss: 0.075526, acc.: 96.88%] [Generator loss: 6.521601]\n",
      "14096 [Discriminator loss: 0.029243, acc.: 100.00%] [Generator loss: 7.277801]\n",
      "14097 [Discriminator loss: 0.064055, acc.: 96.88%] [Generator loss: 8.130388]\n",
      "14098 [Discriminator loss: 0.112249, acc.: 95.31%] [Generator loss: 7.103378]\n",
      "14099 [Discriminator loss: 0.147482, acc.: 93.75%] [Generator loss: 7.147885]\n",
      "14100 [Discriminator loss: 0.044024, acc.: 100.00%] [Generator loss: 6.941301]\n",
      "14101 [Discriminator loss: 0.157805, acc.: 92.19%] [Generator loss: 6.542600]\n",
      "14102 [Discriminator loss: 0.021395, acc.: 100.00%] [Generator loss: 6.290414]\n",
      "14103 [Discriminator loss: 0.040796, acc.: 98.44%] [Generator loss: 6.865358]\n",
      "14104 [Discriminator loss: 0.250558, acc.: 92.19%] [Generator loss: 7.242539]\n",
      "14105 [Discriminator loss: 0.021850, acc.: 100.00%] [Generator loss: 7.612675]\n",
      "14106 [Discriminator loss: 0.130876, acc.: 92.19%] [Generator loss: 7.314020]\n",
      "14107 [Discriminator loss: 0.248176, acc.: 92.19%] [Generator loss: 8.889208]\n",
      "14108 [Discriminator loss: 0.125616, acc.: 95.31%] [Generator loss: 9.428371]\n",
      "14109 [Discriminator loss: 0.034961, acc.: 98.44%] [Generator loss: 8.378345]\n",
      "14110 [Discriminator loss: 0.136202, acc.: 93.75%] [Generator loss: 5.645956]\n",
      "14111 [Discriminator loss: 0.226884, acc.: 92.19%] [Generator loss: 8.165936]\n",
      "14112 [Discriminator loss: 0.016718, acc.: 100.00%] [Generator loss: 9.292993]\n",
      "14113 [Discriminator loss: 0.203453, acc.: 93.75%] [Generator loss: 8.089699]\n",
      "14114 [Discriminator loss: 0.051020, acc.: 100.00%] [Generator loss: 6.572749]\n",
      "14115 [Discriminator loss: 0.054959, acc.: 98.44%] [Generator loss: 7.884106]\n",
      "14116 [Discriminator loss: 0.103259, acc.: 95.31%] [Generator loss: 7.944006]\n",
      "14117 [Discriminator loss: 0.027895, acc.: 98.44%] [Generator loss: 8.314420]\n",
      "14118 [Discriminator loss: 0.218815, acc.: 93.75%] [Generator loss: 6.560818]\n",
      "14119 [Discriminator loss: 0.063419, acc.: 95.31%] [Generator loss: 8.264566]\n",
      "14120 [Discriminator loss: 0.077764, acc.: 95.31%] [Generator loss: 8.041110]\n",
      "14121 [Discriminator loss: 0.044157, acc.: 96.88%] [Generator loss: 7.691405]\n",
      "14122 [Discriminator loss: 0.206797, acc.: 92.19%] [Generator loss: 5.524227]\n",
      "14123 [Discriminator loss: 0.247273, acc.: 87.50%] [Generator loss: 8.568159]\n",
      "14124 [Discriminator loss: 0.019232, acc.: 100.00%] [Generator loss: 10.582289]\n",
      "14125 [Discriminator loss: 0.273563, acc.: 90.62%] [Generator loss: 6.160982]\n",
      "14126 [Discriminator loss: 0.066402, acc.: 98.44%] [Generator loss: 7.522851]\n",
      "14127 [Discriminator loss: 0.034442, acc.: 100.00%] [Generator loss: 7.809072]\n",
      "14128 [Discriminator loss: 0.062462, acc.: 98.44%] [Generator loss: 5.250556]\n",
      "14129 [Discriminator loss: 0.038990, acc.: 100.00%] [Generator loss: 6.007169]\n",
      "14130 [Discriminator loss: 0.091984, acc.: 98.44%] [Generator loss: 7.349213]\n",
      "14131 [Discriminator loss: 0.139907, acc.: 92.19%] [Generator loss: 7.566720]\n",
      "14132 [Discriminator loss: 0.067858, acc.: 96.88%] [Generator loss: 7.851459]\n",
      "14133 [Discriminator loss: 0.249178, acc.: 92.19%] [Generator loss: 9.348776]\n",
      "14134 [Discriminator loss: 0.139144, acc.: 95.31%] [Generator loss: 8.667662]\n",
      "14135 [Discriminator loss: 0.139535, acc.: 90.62%] [Generator loss: 7.502028]\n",
      "14136 [Discriminator loss: 0.108810, acc.: 96.88%] [Generator loss: 8.448215]\n",
      "14137 [Discriminator loss: 0.162102, acc.: 93.75%] [Generator loss: 7.183291]\n",
      "14138 [Discriminator loss: 0.055155, acc.: 100.00%] [Generator loss: 7.187349]\n",
      "14139 [Discriminator loss: 0.029325, acc.: 100.00%] [Generator loss: 6.918781]\n",
      "14140 [Discriminator loss: 0.055556, acc.: 98.44%] [Generator loss: 5.688284]\n",
      "14141 [Discriminator loss: 0.107742, acc.: 95.31%] [Generator loss: 6.180963]\n",
      "14142 [Discriminator loss: 0.156500, acc.: 95.31%] [Generator loss: 7.924133]\n",
      "14143 [Discriminator loss: 0.100044, acc.: 96.88%] [Generator loss: 8.468702]\n",
      "14144 [Discriminator loss: 0.075097, acc.: 95.31%] [Generator loss: 7.307076]\n",
      "14145 [Discriminator loss: 0.106230, acc.: 96.88%] [Generator loss: 8.347612]\n",
      "14146 [Discriminator loss: 0.134450, acc.: 96.88%] [Generator loss: 7.287036]\n",
      "14147 [Discriminator loss: 0.041825, acc.: 98.44%] [Generator loss: 8.281986]\n",
      "14148 [Discriminator loss: 0.044132, acc.: 96.88%] [Generator loss: 7.396975]\n",
      "14149 [Discriminator loss: 0.167546, acc.: 93.75%] [Generator loss: 6.116735]\n",
      "14150 [Discriminator loss: 0.069083, acc.: 98.44%] [Generator loss: 6.835768]\n",
      "14151 [Discriminator loss: 0.086488, acc.: 95.31%] [Generator loss: 9.226957]\n",
      "14152 [Discriminator loss: 0.122946, acc.: 95.31%] [Generator loss: 8.631783]\n",
      "14153 [Discriminator loss: 0.045337, acc.: 100.00%] [Generator loss: 7.463786]\n",
      "14154 [Discriminator loss: 0.103934, acc.: 96.88%] [Generator loss: 7.279867]\n",
      "14155 [Discriminator loss: 0.128016, acc.: 95.31%] [Generator loss: 7.495982]\n",
      "14156 [Discriminator loss: 0.087826, acc.: 95.31%] [Generator loss: 8.619515]\n",
      "14157 [Discriminator loss: 0.027683, acc.: 100.00%] [Generator loss: 9.138101]\n",
      "14158 [Discriminator loss: 0.089980, acc.: 96.88%] [Generator loss: 8.349037]\n",
      "14159 [Discriminator loss: 0.088749, acc.: 96.88%] [Generator loss: 6.676184]\n",
      "14160 [Discriminator loss: 0.026476, acc.: 100.00%] [Generator loss: 8.011729]\n",
      "14161 [Discriminator loss: 0.064025, acc.: 96.88%] [Generator loss: 7.949038]\n",
      "14162 [Discriminator loss: 0.157197, acc.: 95.31%] [Generator loss: 6.256230]\n",
      "14163 [Discriminator loss: 0.046806, acc.: 98.44%] [Generator loss: 6.359244]\n",
      "14164 [Discriminator loss: 0.067291, acc.: 96.88%] [Generator loss: 8.058163]\n",
      "14165 [Discriminator loss: 0.063299, acc.: 98.44%] [Generator loss: 7.217649]\n",
      "14166 [Discriminator loss: 0.127477, acc.: 98.44%] [Generator loss: 8.642503]\n",
      "14167 [Discriminator loss: 0.072434, acc.: 96.88%] [Generator loss: 6.485433]\n",
      "14168 [Discriminator loss: 0.183311, acc.: 93.75%] [Generator loss: 6.360580]\n",
      "14169 [Discriminator loss: 0.240763, acc.: 93.75%] [Generator loss: 8.104683]\n",
      "14170 [Discriminator loss: 0.161923, acc.: 93.75%] [Generator loss: 7.372022]\n",
      "14171 [Discriminator loss: 0.035157, acc.: 98.44%] [Generator loss: 7.601819]\n",
      "14172 [Discriminator loss: 0.102660, acc.: 98.44%] [Generator loss: 8.302723]\n",
      "14173 [Discriminator loss: 0.060104, acc.: 96.88%] [Generator loss: 8.278798]\n",
      "14174 [Discriminator loss: 0.041514, acc.: 100.00%] [Generator loss: 7.207072]\n",
      "14175 [Discriminator loss: 0.067493, acc.: 96.88%] [Generator loss: 7.285789]\n",
      "14176 [Discriminator loss: 0.066351, acc.: 96.88%] [Generator loss: 8.137531]\n",
      "14177 [Discriminator loss: 0.072789, acc.: 96.88%] [Generator loss: 5.973434]\n",
      "14178 [Discriminator loss: 0.033520, acc.: 98.44%] [Generator loss: 6.522522]\n",
      "14179 [Discriminator loss: 0.043016, acc.: 96.88%] [Generator loss: 6.998343]\n",
      "14180 [Discriminator loss: 0.043048, acc.: 98.44%] [Generator loss: 9.223280]\n",
      "14181 [Discriminator loss: 0.078804, acc.: 96.88%] [Generator loss: 7.341636]\n",
      "14182 [Discriminator loss: 0.062742, acc.: 98.44%] [Generator loss: 9.621346]\n",
      "14183 [Discriminator loss: 0.018732, acc.: 100.00%] [Generator loss: 8.157565]\n",
      "14184 [Discriminator loss: 0.262185, acc.: 90.62%] [Generator loss: 7.339184]\n",
      "14185 [Discriminator loss: 0.125850, acc.: 93.75%] [Generator loss: 6.640690]\n",
      "14186 [Discriminator loss: 0.044170, acc.: 98.44%] [Generator loss: 7.743328]\n",
      "14187 [Discriminator loss: 0.062029, acc.: 96.88%] [Generator loss: 8.025471]\n",
      "14188 [Discriminator loss: 0.092796, acc.: 95.31%] [Generator loss: 8.878454]\n",
      "14189 [Discriminator loss: 0.089861, acc.: 96.88%] [Generator loss: 6.247006]\n",
      "14190 [Discriminator loss: 0.101784, acc.: 96.88%] [Generator loss: 6.777908]\n",
      "14191 [Discriminator loss: 0.151810, acc.: 93.75%] [Generator loss: 6.712638]\n",
      "14192 [Discriminator loss: 0.059656, acc.: 98.44%] [Generator loss: 6.690348]\n",
      "14193 [Discriminator loss: 0.121388, acc.: 96.88%] [Generator loss: 7.522523]\n",
      "14194 [Discriminator loss: 0.066638, acc.: 96.88%] [Generator loss: 7.848978]\n",
      "14195 [Discriminator loss: 0.152780, acc.: 93.75%] [Generator loss: 8.422106]\n",
      "14196 [Discriminator loss: 0.038575, acc.: 100.00%] [Generator loss: 8.166045]\n",
      "14197 [Discriminator loss: 0.107184, acc.: 96.88%] [Generator loss: 6.884100]\n",
      "14198 [Discriminator loss: 0.037842, acc.: 98.44%] [Generator loss: 8.745254]\n",
      "14199 [Discriminator loss: 0.041106, acc.: 98.44%] [Generator loss: 7.263113]\n",
      "14200 [Discriminator loss: 0.178823, acc.: 90.62%] [Generator loss: 6.527030]\n",
      "14201 [Discriminator loss: 0.077382, acc.: 93.75%] [Generator loss: 7.636511]\n",
      "14202 [Discriminator loss: 0.026433, acc.: 100.00%] [Generator loss: 9.727882]\n",
      "14203 [Discriminator loss: 0.066755, acc.: 98.44%] [Generator loss: 6.204316]\n",
      "14204 [Discriminator loss: 0.091694, acc.: 96.88%] [Generator loss: 6.459699]\n",
      "14205 [Discriminator loss: 0.088632, acc.: 95.31%] [Generator loss: 9.302843]\n",
      "14206 [Discriminator loss: 0.111645, acc.: 96.88%] [Generator loss: 9.064445]\n",
      "14207 [Discriminator loss: 0.131959, acc.: 92.19%] [Generator loss: 6.927003]\n",
      "14208 [Discriminator loss: 0.013277, acc.: 100.00%] [Generator loss: 7.748384]\n",
      "14209 [Discriminator loss: 0.187949, acc.: 95.31%] [Generator loss: 7.005819]\n",
      "14210 [Discriminator loss: 0.073257, acc.: 96.88%] [Generator loss: 7.817146]\n",
      "14211 [Discriminator loss: 0.038216, acc.: 98.44%] [Generator loss: 8.249048]\n",
      "14212 [Discriminator loss: 0.347445, acc.: 89.06%] [Generator loss: 8.275234]\n",
      "14213 [Discriminator loss: 0.126406, acc.: 93.75%] [Generator loss: 7.902507]\n",
      "14214 [Discriminator loss: 0.387702, acc.: 87.50%] [Generator loss: 7.135459]\n",
      "14215 [Discriminator loss: 0.033666, acc.: 98.44%] [Generator loss: 6.716671]\n",
      "14216 [Discriminator loss: 0.040674, acc.: 98.44%] [Generator loss: 6.547451]\n",
      "14217 [Discriminator loss: 0.058435, acc.: 96.88%] [Generator loss: 7.672588]\n",
      "14218 [Discriminator loss: 0.036001, acc.: 98.44%] [Generator loss: 7.185763]\n",
      "14219 [Discriminator loss: 0.181697, acc.: 95.31%] [Generator loss: 7.385504]\n",
      "14220 [Discriminator loss: 0.071224, acc.: 96.88%] [Generator loss: 8.812880]\n",
      "14221 [Discriminator loss: 0.122583, acc.: 95.31%] [Generator loss: 7.027966]\n",
      "14222 [Discriminator loss: 0.106208, acc.: 92.19%] [Generator loss: 7.021010]\n",
      "14223 [Discriminator loss: 0.079665, acc.: 96.88%] [Generator loss: 6.128165]\n",
      "14224 [Discriminator loss: 0.180840, acc.: 93.75%] [Generator loss: 5.995866]\n",
      "14225 [Discriminator loss: 0.055352, acc.: 98.44%] [Generator loss: 7.599871]\n",
      "14226 [Discriminator loss: 0.038209, acc.: 100.00%] [Generator loss: 6.925598]\n",
      "14227 [Discriminator loss: 0.106654, acc.: 98.44%] [Generator loss: 8.604919]\n",
      "14228 [Discriminator loss: 0.278796, acc.: 92.19%] [Generator loss: 8.692617]\n",
      "14229 [Discriminator loss: 0.111502, acc.: 96.88%] [Generator loss: 8.517894]\n",
      "14230 [Discriminator loss: 0.071502, acc.: 96.88%] [Generator loss: 6.833597]\n",
      "14231 [Discriminator loss: 0.225204, acc.: 95.31%] [Generator loss: 7.598180]\n",
      "14232 [Discriminator loss: 0.049056, acc.: 98.44%] [Generator loss: 7.880036]\n",
      "14233 [Discriminator loss: 0.098810, acc.: 96.88%] [Generator loss: 5.920374]\n",
      "14234 [Discriminator loss: 0.197319, acc.: 90.62%] [Generator loss: 8.133963]\n",
      "14235 [Discriminator loss: 0.218948, acc.: 93.75%] [Generator loss: 8.127417]\n",
      "14236 [Discriminator loss: 0.043721, acc.: 98.44%] [Generator loss: 6.814095]\n",
      "14237 [Discriminator loss: 0.107748, acc.: 93.75%] [Generator loss: 7.537405]\n",
      "14238 [Discriminator loss: 0.146085, acc.: 95.31%] [Generator loss: 6.903299]\n",
      "14239 [Discriminator loss: 0.018140, acc.: 100.00%] [Generator loss: 6.120623]\n",
      "14240 [Discriminator loss: 0.057974, acc.: 96.88%] [Generator loss: 7.140580]\n",
      "14241 [Discriminator loss: 0.031783, acc.: 100.00%] [Generator loss: 7.000597]\n",
      "14242 [Discriminator loss: 0.078112, acc.: 96.88%] [Generator loss: 7.137112]\n",
      "14243 [Discriminator loss: 0.073721, acc.: 95.31%] [Generator loss: 6.774862]\n",
      "14244 [Discriminator loss: 0.273139, acc.: 90.62%] [Generator loss: 7.080572]\n",
      "14245 [Discriminator loss: 0.110761, acc.: 96.88%] [Generator loss: 6.968579]\n",
      "14246 [Discriminator loss: 0.131922, acc.: 93.75%] [Generator loss: 8.275515]\n",
      "14247 [Discriminator loss: 0.068438, acc.: 96.88%] [Generator loss: 8.467012]\n",
      "14248 [Discriminator loss: 0.097409, acc.: 96.88%] [Generator loss: 7.439876]\n",
      "14249 [Discriminator loss: 0.066144, acc.: 98.44%] [Generator loss: 7.847440]\n",
      "14250 [Discriminator loss: 0.088007, acc.: 95.31%] [Generator loss: 7.485821]\n",
      "14251 [Discriminator loss: 0.059232, acc.: 98.44%] [Generator loss: 7.400919]\n",
      "14252 [Discriminator loss: 0.094564, acc.: 96.88%] [Generator loss: 7.108448]\n",
      "14253 [Discriminator loss: 0.100594, acc.: 95.31%] [Generator loss: 9.032972]\n",
      "14254 [Discriminator loss: 0.124087, acc.: 95.31%] [Generator loss: 7.799277]\n",
      "14255 [Discriminator loss: 0.206216, acc.: 93.75%] [Generator loss: 6.579365]\n",
      "14256 [Discriminator loss: 0.068185, acc.: 98.44%] [Generator loss: 7.476308]\n",
      "14257 [Discriminator loss: 0.034760, acc.: 100.00%] [Generator loss: 8.390177]\n",
      "14258 [Discriminator loss: 0.091267, acc.: 96.88%] [Generator loss: 7.715802]\n",
      "14259 [Discriminator loss: 0.070398, acc.: 96.88%] [Generator loss: 6.835489]\n",
      "14260 [Discriminator loss: 0.129277, acc.: 96.88%] [Generator loss: 8.437101]\n",
      "14261 [Discriminator loss: 0.058314, acc.: 98.44%] [Generator loss: 8.287392]\n",
      "14262 [Discriminator loss: 0.224041, acc.: 95.31%] [Generator loss: 6.312151]\n",
      "14263 [Discriminator loss: 0.132053, acc.: 92.19%] [Generator loss: 7.528947]\n",
      "14264 [Discriminator loss: 0.030594, acc.: 100.00%] [Generator loss: 8.903685]\n",
      "14265 [Discriminator loss: 0.037551, acc.: 98.44%] [Generator loss: 7.231988]\n",
      "14266 [Discriminator loss: 0.136230, acc.: 95.31%] [Generator loss: 6.856881]\n",
      "14267 [Discriminator loss: 0.098626, acc.: 93.75%] [Generator loss: 8.286772]\n",
      "14268 [Discriminator loss: 0.117256, acc.: 95.31%] [Generator loss: 7.480276]\n",
      "14269 [Discriminator loss: 0.028117, acc.: 98.44%] [Generator loss: 6.877676]\n",
      "14270 [Discriminator loss: 0.136871, acc.: 93.75%] [Generator loss: 6.226423]\n",
      "14271 [Discriminator loss: 0.058445, acc.: 98.44%] [Generator loss: 7.612631]\n",
      "14272 [Discriminator loss: 0.015322, acc.: 100.00%] [Generator loss: 7.983705]\n",
      "14273 [Discriminator loss: 0.062332, acc.: 98.44%] [Generator loss: 8.235195]\n",
      "14274 [Discriminator loss: 0.017314, acc.: 100.00%] [Generator loss: 8.400669]\n",
      "14275 [Discriminator loss: 0.018584, acc.: 100.00%] [Generator loss: 6.743481]\n",
      "14276 [Discriminator loss: 0.064811, acc.: 98.44%] [Generator loss: 6.977349]\n",
      "14277 [Discriminator loss: 0.123400, acc.: 93.75%] [Generator loss: 7.453938]\n",
      "14278 [Discriminator loss: 0.026576, acc.: 100.00%] [Generator loss: 8.835111]\n",
      "14279 [Discriminator loss: 0.060476, acc.: 98.44%] [Generator loss: 6.659188]\n",
      "14280 [Discriminator loss: 0.074494, acc.: 96.88%] [Generator loss: 7.310244]\n",
      "14281 [Discriminator loss: 0.042495, acc.: 98.44%] [Generator loss: 6.865131]\n",
      "14282 [Discriminator loss: 0.201251, acc.: 92.19%] [Generator loss: 8.736685]\n",
      "14283 [Discriminator loss: 0.040449, acc.: 100.00%] [Generator loss: 8.970000]\n",
      "14284 [Discriminator loss: 0.170445, acc.: 95.31%] [Generator loss: 6.343277]\n",
      "14285 [Discriminator loss: 0.134377, acc.: 95.31%] [Generator loss: 7.318801]\n",
      "14286 [Discriminator loss: 0.086402, acc.: 98.44%] [Generator loss: 7.840669]\n",
      "14287 [Discriminator loss: 0.049122, acc.: 98.44%] [Generator loss: 8.912247]\n",
      "14288 [Discriminator loss: 0.070743, acc.: 96.88%] [Generator loss: 8.103249]\n",
      "14289 [Discriminator loss: 0.046419, acc.: 98.44%] [Generator loss: 7.905344]\n",
      "14290 [Discriminator loss: 0.046614, acc.: 98.44%] [Generator loss: 7.538409]\n",
      "14291 [Discriminator loss: 0.099569, acc.: 96.88%] [Generator loss: 7.437251]\n",
      "14292 [Discriminator loss: 0.107531, acc.: 96.88%] [Generator loss: 8.508488]\n",
      "14293 [Discriminator loss: 0.294740, acc.: 90.62%] [Generator loss: 7.886445]\n",
      "14294 [Discriminator loss: 0.069816, acc.: 98.44%] [Generator loss: 8.265182]\n",
      "14295 [Discriminator loss: 0.048756, acc.: 98.44%] [Generator loss: 8.075953]\n",
      "14296 [Discriminator loss: 0.108942, acc.: 95.31%] [Generator loss: 8.580362]\n",
      "14297 [Discriminator loss: 0.111782, acc.: 95.31%] [Generator loss: 7.862477]\n",
      "14298 [Discriminator loss: 0.129853, acc.: 96.88%] [Generator loss: 6.402273]\n",
      "14299 [Discriminator loss: 0.132810, acc.: 95.31%] [Generator loss: 8.112967]\n",
      "14300 [Discriminator loss: 0.050471, acc.: 98.44%] [Generator loss: 7.488557]\n",
      "14301 [Discriminator loss: 0.060954, acc.: 98.44%] [Generator loss: 7.917031]\n",
      "14302 [Discriminator loss: 0.163539, acc.: 95.31%] [Generator loss: 7.156820]\n",
      "14303 [Discriminator loss: 0.048731, acc.: 98.44%] [Generator loss: 7.184697]\n",
      "14304 [Discriminator loss: 0.045368, acc.: 98.44%] [Generator loss: 8.791788]\n",
      "14305 [Discriminator loss: 0.050942, acc.: 96.88%] [Generator loss: 6.907846]\n",
      "14306 [Discriminator loss: 0.033811, acc.: 98.44%] [Generator loss: 6.761195]\n",
      "14307 [Discriminator loss: 0.199873, acc.: 90.62%] [Generator loss: 6.297237]\n",
      "14308 [Discriminator loss: 0.084110, acc.: 96.88%] [Generator loss: 7.174396]\n",
      "14309 [Discriminator loss: 0.033993, acc.: 100.00%] [Generator loss: 8.101686]\n",
      "14310 [Discriminator loss: 0.173709, acc.: 93.75%] [Generator loss: 7.337711]\n",
      "14311 [Discriminator loss: 0.122583, acc.: 93.75%] [Generator loss: 7.039736]\n",
      "14312 [Discriminator loss: 0.143367, acc.: 93.75%] [Generator loss: 7.272909]\n",
      "14313 [Discriminator loss: 0.154455, acc.: 93.75%] [Generator loss: 8.869957]\n",
      "14314 [Discriminator loss: 0.319044, acc.: 90.62%] [Generator loss: 5.162468]\n",
      "14315 [Discriminator loss: 0.133925, acc.: 90.62%] [Generator loss: 7.049128]\n",
      "14316 [Discriminator loss: 0.065098, acc.: 98.44%] [Generator loss: 8.733458]\n",
      "14317 [Discriminator loss: 0.180159, acc.: 95.31%] [Generator loss: 7.505118]\n",
      "14318 [Discriminator loss: 0.077581, acc.: 95.31%] [Generator loss: 7.094944]\n",
      "14319 [Discriminator loss: 0.083346, acc.: 98.44%] [Generator loss: 8.266031]\n",
      "14320 [Discriminator loss: 0.107654, acc.: 95.31%] [Generator loss: 7.302892]\n",
      "14321 [Discriminator loss: 0.062705, acc.: 98.44%] [Generator loss: 7.038397]\n",
      "14322 [Discriminator loss: 0.050639, acc.: 98.44%] [Generator loss: 6.592892]\n",
      "14323 [Discriminator loss: 0.116752, acc.: 93.75%] [Generator loss: 5.860845]\n",
      "14324 [Discriminator loss: 0.107646, acc.: 95.31%] [Generator loss: 7.093837]\n",
      "14325 [Discriminator loss: 0.086810, acc.: 96.88%] [Generator loss: 7.393878]\n",
      "14326 [Discriminator loss: 0.115644, acc.: 95.31%] [Generator loss: 6.027889]\n",
      "14327 [Discriminator loss: 0.037574, acc.: 100.00%] [Generator loss: 6.279309]\n",
      "14328 [Discriminator loss: 0.043189, acc.: 98.44%] [Generator loss: 7.697839]\n",
      "14329 [Discriminator loss: 0.010839, acc.: 100.00%] [Generator loss: 7.717623]\n",
      "14330 [Discriminator loss: 0.181453, acc.: 93.75%] [Generator loss: 5.539307]\n",
      "14331 [Discriminator loss: 0.031510, acc.: 100.00%] [Generator loss: 7.573666]\n",
      "14332 [Discriminator loss: 0.027944, acc.: 100.00%] [Generator loss: 6.959146]\n",
      "14333 [Discriminator loss: 0.073200, acc.: 98.44%] [Generator loss: 5.897490]\n",
      "14334 [Discriminator loss: 0.110250, acc.: 96.88%] [Generator loss: 8.075017]\n",
      "14335 [Discriminator loss: 0.124341, acc.: 93.75%] [Generator loss: 5.303352]\n",
      "14336 [Discriminator loss: 0.048933, acc.: 98.44%] [Generator loss: 7.175721]\n",
      "14337 [Discriminator loss: 0.116224, acc.: 95.31%] [Generator loss: 7.171178]\n",
      "14338 [Discriminator loss: 0.069433, acc.: 98.44%] [Generator loss: 8.159578]\n",
      "14339 [Discriminator loss: 0.020947, acc.: 100.00%] [Generator loss: 7.143626]\n",
      "14340 [Discriminator loss: 0.137833, acc.: 93.75%] [Generator loss: 8.469348]\n",
      "14341 [Discriminator loss: 0.084248, acc.: 96.88%] [Generator loss: 8.841055]\n",
      "14342 [Discriminator loss: 0.132908, acc.: 92.19%] [Generator loss: 7.186472]\n",
      "14343 [Discriminator loss: 0.066625, acc.: 96.88%] [Generator loss: 9.168983]\n",
      "14344 [Discriminator loss: 0.069298, acc.: 96.88%] [Generator loss: 7.747667]\n",
      "14345 [Discriminator loss: 0.069166, acc.: 98.44%] [Generator loss: 7.370460]\n",
      "14346 [Discriminator loss: 0.029036, acc.: 98.44%] [Generator loss: 8.441212]\n",
      "14347 [Discriminator loss: 0.023760, acc.: 100.00%] [Generator loss: 7.180017]\n",
      "14348 [Discriminator loss: 0.038348, acc.: 98.44%] [Generator loss: 8.669586]\n",
      "14349 [Discriminator loss: 0.039572, acc.: 100.00%] [Generator loss: 5.993856]\n",
      "14350 [Discriminator loss: 0.061473, acc.: 96.88%] [Generator loss: 6.946222]\n",
      "14351 [Discriminator loss: 0.160244, acc.: 92.19%] [Generator loss: 8.671434]\n",
      "14352 [Discriminator loss: 0.113572, acc.: 96.88%] [Generator loss: 9.168732]\n",
      "14353 [Discriminator loss: 0.036123, acc.: 100.00%] [Generator loss: 7.998527]\n",
      "14354 [Discriminator loss: 0.137883, acc.: 95.31%] [Generator loss: 8.275856]\n",
      "14355 [Discriminator loss: 0.051117, acc.: 98.44%] [Generator loss: 8.048193]\n",
      "14356 [Discriminator loss: 0.191005, acc.: 95.31%] [Generator loss: 8.042403]\n",
      "14357 [Discriminator loss: 0.053309, acc.: 96.88%] [Generator loss: 7.682366]\n",
      "14358 [Discriminator loss: 0.058725, acc.: 98.44%] [Generator loss: 8.261073]\n",
      "14359 [Discriminator loss: 0.049320, acc.: 96.88%] [Generator loss: 9.214484]\n",
      "14360 [Discriminator loss: 0.066028, acc.: 98.44%] [Generator loss: 7.733934]\n",
      "14361 [Discriminator loss: 0.016623, acc.: 100.00%] [Generator loss: 9.733719]\n",
      "14362 [Discriminator loss: 0.071388, acc.: 96.88%] [Generator loss: 6.275092]\n",
      "14363 [Discriminator loss: 0.097760, acc.: 98.44%] [Generator loss: 8.160452]\n",
      "14364 [Discriminator loss: 0.123486, acc.: 93.75%] [Generator loss: 7.390639]\n",
      "14365 [Discriminator loss: 0.065815, acc.: 96.88%] [Generator loss: 7.149300]\n",
      "14366 [Discriminator loss: 0.067301, acc.: 98.44%] [Generator loss: 5.446514]\n",
      "14367 [Discriminator loss: 0.022418, acc.: 100.00%] [Generator loss: 7.114007]\n",
      "14368 [Discriminator loss: 0.113706, acc.: 93.75%] [Generator loss: 7.464571]\n",
      "14369 [Discriminator loss: 0.077109, acc.: 96.88%] [Generator loss: 7.869921]\n",
      "14370 [Discriminator loss: 0.126160, acc.: 95.31%] [Generator loss: 7.381059]\n",
      "14371 [Discriminator loss: 0.048336, acc.: 98.44%] [Generator loss: 8.288057]\n",
      "14372 [Discriminator loss: 0.121247, acc.: 93.75%] [Generator loss: 9.902719]\n",
      "14373 [Discriminator loss: 0.057530, acc.: 96.88%] [Generator loss: 8.972024]\n",
      "14374 [Discriminator loss: 0.087730, acc.: 95.31%] [Generator loss: 8.726644]\n",
      "14375 [Discriminator loss: 0.156257, acc.: 93.75%] [Generator loss: 5.963569]\n",
      "14376 [Discriminator loss: 0.122159, acc.: 95.31%] [Generator loss: 8.307028]\n",
      "14377 [Discriminator loss: 0.041615, acc.: 98.44%] [Generator loss: 8.345378]\n",
      "14378 [Discriminator loss: 0.084767, acc.: 95.31%] [Generator loss: 6.635134]\n",
      "14379 [Discriminator loss: 0.035320, acc.: 100.00%] [Generator loss: 6.633818]\n",
      "14380 [Discriminator loss: 0.229357, acc.: 92.19%] [Generator loss: 8.882313]\n",
      "14381 [Discriminator loss: 0.150322, acc.: 90.62%] [Generator loss: 8.024662]\n",
      "14382 [Discriminator loss: 0.060120, acc.: 96.88%] [Generator loss: 8.401722]\n",
      "14383 [Discriminator loss: 0.060391, acc.: 98.44%] [Generator loss: 7.277517]\n",
      "14384 [Discriminator loss: 0.036828, acc.: 98.44%] [Generator loss: 7.508982]\n",
      "14385 [Discriminator loss: 0.090432, acc.: 95.31%] [Generator loss: 7.947241]\n",
      "14386 [Discriminator loss: 0.072245, acc.: 98.44%] [Generator loss: 8.767630]\n",
      "14387 [Discriminator loss: 0.131851, acc.: 95.31%] [Generator loss: 7.597690]\n",
      "14388 [Discriminator loss: 0.029536, acc.: 98.44%] [Generator loss: 6.814920]\n",
      "14389 [Discriminator loss: 0.067839, acc.: 98.44%] [Generator loss: 5.432302]\n",
      "14390 [Discriminator loss: 0.156135, acc.: 92.19%] [Generator loss: 8.238283]\n",
      "14391 [Discriminator loss: 0.075376, acc.: 96.88%] [Generator loss: 7.391996]\n",
      "14392 [Discriminator loss: 0.081326, acc.: 96.88%] [Generator loss: 6.381335]\n",
      "14393 [Discriminator loss: 0.110123, acc.: 95.31%] [Generator loss: 8.771626]\n",
      "14394 [Discriminator loss: 0.036181, acc.: 98.44%] [Generator loss: 7.122354]\n",
      "14395 [Discriminator loss: 0.144116, acc.: 93.75%] [Generator loss: 5.696366]\n",
      "14396 [Discriminator loss: 0.067930, acc.: 96.88%] [Generator loss: 6.377636]\n",
      "14397 [Discriminator loss: 0.142643, acc.: 95.31%] [Generator loss: 8.117300]\n",
      "14398 [Discriminator loss: 0.117631, acc.: 93.75%] [Generator loss: 8.959583]\n",
      "14399 [Discriminator loss: 0.056357, acc.: 98.44%] [Generator loss: 9.422764]\n",
      "14400 [Discriminator loss: 0.063548, acc.: 98.44%] [Generator loss: 8.000382]\n",
      "14401 [Discriminator loss: 0.365517, acc.: 84.38%] [Generator loss: 9.021894]\n",
      "14402 [Discriminator loss: 0.185174, acc.: 95.31%] [Generator loss: 8.117621]\n",
      "14403 [Discriminator loss: 0.097369, acc.: 95.31%] [Generator loss: 4.802734]\n",
      "14404 [Discriminator loss: 0.105768, acc.: 96.88%] [Generator loss: 6.065937]\n",
      "14405 [Discriminator loss: 0.072318, acc.: 96.88%] [Generator loss: 8.082690]\n",
      "14406 [Discriminator loss: 0.010682, acc.: 100.00%] [Generator loss: 8.163231]\n",
      "14407 [Discriminator loss: 0.065598, acc.: 98.44%] [Generator loss: 7.417034]\n",
      "14408 [Discriminator loss: 0.022225, acc.: 98.44%] [Generator loss: 6.766230]\n",
      "14409 [Discriminator loss: 0.110956, acc.: 96.88%] [Generator loss: 7.215240]\n",
      "14410 [Discriminator loss: 0.141010, acc.: 93.75%] [Generator loss: 9.609501]\n",
      "14411 [Discriminator loss: 0.090242, acc.: 98.44%] [Generator loss: 7.801334]\n",
      "14412 [Discriminator loss: 0.085140, acc.: 96.88%] [Generator loss: 9.052486]\n",
      "14413 [Discriminator loss: 0.156393, acc.: 96.88%] [Generator loss: 7.616229]\n",
      "14414 [Discriminator loss: 0.094204, acc.: 95.31%] [Generator loss: 6.881693]\n",
      "14415 [Discriminator loss: 0.059530, acc.: 98.44%] [Generator loss: 7.595003]\n",
      "14416 [Discriminator loss: 0.297414, acc.: 87.50%] [Generator loss: 5.232883]\n",
      "14417 [Discriminator loss: 0.125987, acc.: 95.31%] [Generator loss: 7.321284]\n",
      "14418 [Discriminator loss: 0.088600, acc.: 96.88%] [Generator loss: 7.732634]\n",
      "14419 [Discriminator loss: 0.024609, acc.: 100.00%] [Generator loss: 7.428950]\n",
      "14420 [Discriminator loss: 0.170724, acc.: 93.75%] [Generator loss: 7.834870]\n",
      "14421 [Discriminator loss: 0.119110, acc.: 95.31%] [Generator loss: 7.787129]\n",
      "14422 [Discriminator loss: 0.027832, acc.: 98.44%] [Generator loss: 7.994085]\n",
      "14423 [Discriminator loss: 0.130972, acc.: 93.75%] [Generator loss: 8.035825]\n",
      "14424 [Discriminator loss: 0.019342, acc.: 100.00%] [Generator loss: 7.934834]\n",
      "14425 [Discriminator loss: 0.076905, acc.: 98.44%] [Generator loss: 7.285182]\n",
      "14426 [Discriminator loss: 0.177197, acc.: 96.88%] [Generator loss: 6.084814]\n",
      "14427 [Discriminator loss: 0.219303, acc.: 90.62%] [Generator loss: 9.178366]\n",
      "14428 [Discriminator loss: 0.084671, acc.: 98.44%] [Generator loss: 8.404491]\n",
      "14429 [Discriminator loss: 0.106786, acc.: 95.31%] [Generator loss: 8.111400]\n",
      "14430 [Discriminator loss: 0.078669, acc.: 96.88%] [Generator loss: 9.317702]\n",
      "14431 [Discriminator loss: 0.052401, acc.: 98.44%] [Generator loss: 7.841488]\n",
      "14432 [Discriminator loss: 0.179115, acc.: 92.19%] [Generator loss: 8.686449]\n",
      "14433 [Discriminator loss: 0.096604, acc.: 95.31%] [Generator loss: 6.798681]\n",
      "14434 [Discriminator loss: 0.150308, acc.: 93.75%] [Generator loss: 8.621224]\n",
      "14435 [Discriminator loss: 0.091723, acc.: 95.31%] [Generator loss: 9.045063]\n",
      "14436 [Discriminator loss: 0.039713, acc.: 98.44%] [Generator loss: 7.265949]\n",
      "14437 [Discriminator loss: 0.060342, acc.: 96.88%] [Generator loss: 6.746264]\n",
      "14438 [Discriminator loss: 0.056863, acc.: 96.88%] [Generator loss: 6.737756]\n",
      "14439 [Discriminator loss: 0.147181, acc.: 93.75%] [Generator loss: 7.067079]\n",
      "14440 [Discriminator loss: 0.027137, acc.: 100.00%] [Generator loss: 7.601460]\n",
      "14441 [Discriminator loss: 0.050212, acc.: 96.88%] [Generator loss: 7.323946]\n",
      "14442 [Discriminator loss: 0.099633, acc.: 95.31%] [Generator loss: 9.376461]\n",
      "14443 [Discriminator loss: 0.076789, acc.: 98.44%] [Generator loss: 7.313985]\n",
      "14444 [Discriminator loss: 0.328782, acc.: 87.50%] [Generator loss: 7.845881]\n",
      "14445 [Discriminator loss: 0.200762, acc.: 90.62%] [Generator loss: 9.307922]\n",
      "14446 [Discriminator loss: 0.112397, acc.: 96.88%] [Generator loss: 7.996458]\n",
      "14447 [Discriminator loss: 0.135089, acc.: 93.75%] [Generator loss: 7.638731]\n",
      "14448 [Discriminator loss: 0.196609, acc.: 92.19%] [Generator loss: 6.513473]\n",
      "14449 [Discriminator loss: 0.029556, acc.: 98.44%] [Generator loss: 8.219755]\n",
      "14450 [Discriminator loss: 0.027607, acc.: 98.44%] [Generator loss: 7.809730]\n",
      "14451 [Discriminator loss: 0.265534, acc.: 87.50%] [Generator loss: 8.971601]\n",
      "14452 [Discriminator loss: 0.062127, acc.: 98.44%] [Generator loss: 9.430470]\n",
      "14453 [Discriminator loss: 0.069860, acc.: 98.44%] [Generator loss: 7.754628]\n",
      "14454 [Discriminator loss: 0.140099, acc.: 96.88%] [Generator loss: 8.119909]\n",
      "14455 [Discriminator loss: 0.110416, acc.: 95.31%] [Generator loss: 8.420230]\n",
      "14456 [Discriminator loss: 0.080157, acc.: 96.88%] [Generator loss: 6.409542]\n",
      "14457 [Discriminator loss: 0.063064, acc.: 95.31%] [Generator loss: 6.651792]\n",
      "14458 [Discriminator loss: 0.054777, acc.: 100.00%] [Generator loss: 7.373061]\n",
      "14459 [Discriminator loss: 0.033482, acc.: 98.44%] [Generator loss: 6.991143]\n",
      "14460 [Discriminator loss: 0.152557, acc.: 95.31%] [Generator loss: 8.169985]\n",
      "14461 [Discriminator loss: 0.104051, acc.: 95.31%] [Generator loss: 9.329512]\n",
      "14462 [Discriminator loss: 0.158730, acc.: 92.19%] [Generator loss: 8.306468]\n",
      "14463 [Discriminator loss: 0.057216, acc.: 98.44%] [Generator loss: 8.452490]\n",
      "14464 [Discriminator loss: 0.108134, acc.: 98.44%] [Generator loss: 7.118323]\n",
      "14465 [Discriminator loss: 0.118557, acc.: 95.31%] [Generator loss: 6.771988]\n",
      "14466 [Discriminator loss: 0.070771, acc.: 95.31%] [Generator loss: 6.727198]\n",
      "14467 [Discriminator loss: 0.079717, acc.: 93.75%] [Generator loss: 8.089115]\n",
      "14468 [Discriminator loss: 0.144175, acc.: 92.19%] [Generator loss: 8.693691]\n",
      "14469 [Discriminator loss: 0.178176, acc.: 92.19%] [Generator loss: 6.516722]\n",
      "14470 [Discriminator loss: 0.123896, acc.: 96.88%] [Generator loss: 7.766474]\n",
      "14471 [Discriminator loss: 0.085831, acc.: 95.31%] [Generator loss: 7.549858]\n",
      "14472 [Discriminator loss: 0.054002, acc.: 98.44%] [Generator loss: 9.332932]\n",
      "14473 [Discriminator loss: 0.055224, acc.: 98.44%] [Generator loss: 8.436281]\n",
      "14474 [Discriminator loss: 0.069473, acc.: 98.44%] [Generator loss: 7.233125]\n",
      "14475 [Discriminator loss: 0.104240, acc.: 96.88%] [Generator loss: 7.058538]\n",
      "14476 [Discriminator loss: 0.117946, acc.: 95.31%] [Generator loss: 6.331254]\n",
      "14477 [Discriminator loss: 0.117781, acc.: 95.31%] [Generator loss: 7.686436]\n",
      "14478 [Discriminator loss: 0.118581, acc.: 96.88%] [Generator loss: 7.955198]\n",
      "14479 [Discriminator loss: 0.122145, acc.: 95.31%] [Generator loss: 7.355721]\n",
      "14480 [Discriminator loss: 0.086074, acc.: 95.31%] [Generator loss: 6.886675]\n",
      "14481 [Discriminator loss: 0.119305, acc.: 95.31%] [Generator loss: 6.899416]\n",
      "14482 [Discriminator loss: 0.076389, acc.: 96.88%] [Generator loss: 7.947969]\n",
      "14483 [Discriminator loss: 0.030438, acc.: 98.44%] [Generator loss: 8.254452]\n",
      "14484 [Discriminator loss: 0.058062, acc.: 96.88%] [Generator loss: 6.740272]\n",
      "14485 [Discriminator loss: 0.098987, acc.: 98.44%] [Generator loss: 5.676153]\n",
      "14486 [Discriminator loss: 0.092329, acc.: 93.75%] [Generator loss: 7.510762]\n",
      "14487 [Discriminator loss: 0.068833, acc.: 98.44%] [Generator loss: 7.925724]\n",
      "14488 [Discriminator loss: 0.051515, acc.: 96.88%] [Generator loss: 7.901654]\n",
      "14489 [Discriminator loss: 0.132797, acc.: 95.31%] [Generator loss: 7.092317]\n",
      "14490 [Discriminator loss: 0.126357, acc.: 93.75%] [Generator loss: 7.276616]\n",
      "14491 [Discriminator loss: 0.125906, acc.: 93.75%] [Generator loss: 5.987680]\n",
      "14492 [Discriminator loss: 0.074719, acc.: 95.31%] [Generator loss: 7.034727]\n",
      "14493 [Discriminator loss: 0.024497, acc.: 100.00%] [Generator loss: 6.705109]\n",
      "14494 [Discriminator loss: 0.161413, acc.: 93.75%] [Generator loss: 7.188068]\n",
      "14495 [Discriminator loss: 0.076244, acc.: 98.44%] [Generator loss: 7.521160]\n",
      "14496 [Discriminator loss: 0.133159, acc.: 93.75%] [Generator loss: 7.967598]\n",
      "14497 [Discriminator loss: 0.040140, acc.: 98.44%] [Generator loss: 8.031742]\n",
      "14498 [Discriminator loss: 0.069845, acc.: 98.44%] [Generator loss: 7.587945]\n",
      "14499 [Discriminator loss: 0.063169, acc.: 95.31%] [Generator loss: 8.409077]\n",
      "14500 [Discriminator loss: 0.030115, acc.: 100.00%] [Generator loss: 7.170374]\n",
      "14501 [Discriminator loss: 0.117612, acc.: 96.88%] [Generator loss: 6.407006]\n",
      "14502 [Discriminator loss: 0.213294, acc.: 92.19%] [Generator loss: 6.440418]\n",
      "14503 [Discriminator loss: 0.059133, acc.: 98.44%] [Generator loss: 9.125610]\n",
      "14504 [Discriminator loss: 0.060409, acc.: 96.88%] [Generator loss: 9.062905]\n",
      "14505 [Discriminator loss: 0.026730, acc.: 100.00%] [Generator loss: 8.432787]\n",
      "14506 [Discriminator loss: 0.021107, acc.: 100.00%] [Generator loss: 8.349111]\n",
      "14507 [Discriminator loss: 0.188188, acc.: 90.62%] [Generator loss: 7.695135]\n",
      "14508 [Discriminator loss: 0.041157, acc.: 98.44%] [Generator loss: 7.176129]\n",
      "14509 [Discriminator loss: 0.117454, acc.: 93.75%] [Generator loss: 6.411322]\n",
      "14510 [Discriminator loss: 0.026299, acc.: 100.00%] [Generator loss: 8.387448]\n",
      "14511 [Discriminator loss: 0.067279, acc.: 98.44%] [Generator loss: 6.813972]\n",
      "14512 [Discriminator loss: 0.089430, acc.: 96.88%] [Generator loss: 9.443320]\n",
      "14513 [Discriminator loss: 0.067052, acc.: 95.31%] [Generator loss: 7.622644]\n",
      "14514 [Discriminator loss: 0.197711, acc.: 95.31%] [Generator loss: 8.116455]\n",
      "14515 [Discriminator loss: 0.069199, acc.: 96.88%] [Generator loss: 7.239347]\n",
      "14516 [Discriminator loss: 0.087335, acc.: 96.88%] [Generator loss: 8.503671]\n",
      "14517 [Discriminator loss: 0.052777, acc.: 98.44%] [Generator loss: 8.181761]\n",
      "14518 [Discriminator loss: 0.162931, acc.: 93.75%] [Generator loss: 7.191751]\n",
      "14519 [Discriminator loss: 0.166342, acc.: 96.88%] [Generator loss: 6.589077]\n",
      "14520 [Discriminator loss: 0.145466, acc.: 95.31%] [Generator loss: 9.735191]\n",
      "14521 [Discriminator loss: 0.118376, acc.: 95.31%] [Generator loss: 7.678777]\n",
      "14522 [Discriminator loss: 0.131366, acc.: 93.75%] [Generator loss: 9.988353]\n",
      "14523 [Discriminator loss: 0.153650, acc.: 92.19%] [Generator loss: 6.784612]\n",
      "14524 [Discriminator loss: 0.052617, acc.: 96.88%] [Generator loss: 6.795557]\n",
      "14525 [Discriminator loss: 0.043513, acc.: 100.00%] [Generator loss: 7.383512]\n",
      "14526 [Discriminator loss: 0.141959, acc.: 93.75%] [Generator loss: 8.682281]\n",
      "14527 [Discriminator loss: 0.169797, acc.: 93.75%] [Generator loss: 6.822686]\n",
      "14528 [Discriminator loss: 0.054794, acc.: 96.88%] [Generator loss: 7.079960]\n",
      "14529 [Discriminator loss: 0.120174, acc.: 95.31%] [Generator loss: 9.529411]\n",
      "14530 [Discriminator loss: 0.096670, acc.: 96.88%] [Generator loss: 9.338571]\n",
      "14531 [Discriminator loss: 0.282781, acc.: 85.94%] [Generator loss: 5.280844]\n",
      "14532 [Discriminator loss: 0.047933, acc.: 96.88%] [Generator loss: 6.887503]\n",
      "14533 [Discriminator loss: 0.145664, acc.: 96.88%] [Generator loss: 8.876931]\n",
      "14534 [Discriminator loss: 0.040815, acc.: 96.88%] [Generator loss: 9.108240]\n",
      "14535 [Discriminator loss: 0.220705, acc.: 85.94%] [Generator loss: 7.256606]\n",
      "14536 [Discriminator loss: 0.171813, acc.: 95.31%] [Generator loss: 8.358781]\n",
      "14537 [Discriminator loss: 0.128942, acc.: 92.19%] [Generator loss: 7.943993]\n",
      "14538 [Discriminator loss: 0.218902, acc.: 93.75%] [Generator loss: 6.963942]\n",
      "14539 [Discriminator loss: 0.046271, acc.: 98.44%] [Generator loss: 6.937576]\n",
      "14540 [Discriminator loss: 0.196085, acc.: 93.75%] [Generator loss: 8.507017]\n",
      "14541 [Discriminator loss: 0.049696, acc.: 98.44%] [Generator loss: 9.274511]\n",
      "14542 [Discriminator loss: 0.064095, acc.: 96.88%] [Generator loss: 9.180309]\n",
      "14543 [Discriminator loss: 0.063651, acc.: 98.44%] [Generator loss: 6.661955]\n",
      "14544 [Discriminator loss: 0.125447, acc.: 95.31%] [Generator loss: 7.916371]\n",
      "14545 [Discriminator loss: 0.081113, acc.: 96.88%] [Generator loss: 8.594191]\n",
      "14546 [Discriminator loss: 0.287227, acc.: 95.31%] [Generator loss: 7.771650]\n",
      "14547 [Discriminator loss: 0.138702, acc.: 95.31%] [Generator loss: 7.571882]\n",
      "14548 [Discriminator loss: 0.061463, acc.: 98.44%] [Generator loss: 6.404758]\n",
      "14549 [Discriminator loss: 0.090721, acc.: 96.88%] [Generator loss: 7.541544]\n",
      "14550 [Discriminator loss: 0.126226, acc.: 93.75%] [Generator loss: 8.272797]\n",
      "14551 [Discriminator loss: 0.056564, acc.: 98.44%] [Generator loss: 7.614725]\n",
      "14552 [Discriminator loss: 0.265063, acc.: 90.62%] [Generator loss: 7.087566]\n",
      "14553 [Discriminator loss: 0.032942, acc.: 100.00%] [Generator loss: 7.775283]\n",
      "14554 [Discriminator loss: 0.023755, acc.: 100.00%] [Generator loss: 8.055552]\n",
      "14555 [Discriminator loss: 0.111188, acc.: 93.75%] [Generator loss: 9.118143]\n",
      "14556 [Discriminator loss: 0.063668, acc.: 96.88%] [Generator loss: 7.089952]\n",
      "14557 [Discriminator loss: 0.103174, acc.: 95.31%] [Generator loss: 8.086161]\n",
      "14558 [Discriminator loss: 0.049264, acc.: 98.44%] [Generator loss: 9.153861]\n",
      "14559 [Discriminator loss: 0.112421, acc.: 98.44%] [Generator loss: 7.622821]\n",
      "14560 [Discriminator loss: 0.075122, acc.: 96.88%] [Generator loss: 7.179412]\n",
      "14561 [Discriminator loss: 0.130037, acc.: 92.19%] [Generator loss: 8.220146]\n",
      "14562 [Discriminator loss: 0.109232, acc.: 95.31%] [Generator loss: 9.745342]\n",
      "14563 [Discriminator loss: 0.027024, acc.: 100.00%] [Generator loss: 10.151689]\n",
      "14564 [Discriminator loss: 0.117897, acc.: 95.31%] [Generator loss: 6.984257]\n",
      "14565 [Discriminator loss: 0.113588, acc.: 95.31%] [Generator loss: 6.679837]\n",
      "14566 [Discriminator loss: 0.199148, acc.: 90.62%] [Generator loss: 8.241674]\n",
      "14567 [Discriminator loss: 0.172865, acc.: 92.19%] [Generator loss: 7.977462]\n",
      "14568 [Discriminator loss: 0.130172, acc.: 95.31%] [Generator loss: 6.811660]\n",
      "14569 [Discriminator loss: 0.202675, acc.: 92.19%] [Generator loss: 9.449666]\n",
      "14570 [Discriminator loss: 0.104290, acc.: 95.31%] [Generator loss: 9.095404]\n",
      "14571 [Discriminator loss: 0.033643, acc.: 98.44%] [Generator loss: 8.535576]\n",
      "14572 [Discriminator loss: 0.042769, acc.: 98.44%] [Generator loss: 7.166336]\n",
      "14573 [Discriminator loss: 0.054329, acc.: 98.44%] [Generator loss: 6.900795]\n",
      "14574 [Discriminator loss: 0.088339, acc.: 96.88%] [Generator loss: 6.549082]\n",
      "14575 [Discriminator loss: 0.060273, acc.: 96.88%] [Generator loss: 7.809617]\n",
      "14576 [Discriminator loss: 0.046940, acc.: 96.88%] [Generator loss: 7.659345]\n",
      "14577 [Discriminator loss: 0.101075, acc.: 93.75%] [Generator loss: 6.598851]\n",
      "14578 [Discriminator loss: 0.124683, acc.: 95.31%] [Generator loss: 8.812977]\n",
      "14579 [Discriminator loss: 0.107397, acc.: 95.31%] [Generator loss: 7.848332]\n",
      "14580 [Discriminator loss: 0.212021, acc.: 92.19%] [Generator loss: 7.858784]\n",
      "14581 [Discriminator loss: 0.043322, acc.: 96.88%] [Generator loss: 7.338552]\n",
      "14582 [Discriminator loss: 0.068629, acc.: 96.88%] [Generator loss: 6.345753]\n",
      "14583 [Discriminator loss: 0.164195, acc.: 93.75%] [Generator loss: 7.977010]\n",
      "14584 [Discriminator loss: 0.153979, acc.: 93.75%] [Generator loss: 8.287216]\n",
      "14585 [Discriminator loss: 0.082703, acc.: 98.44%] [Generator loss: 7.863208]\n",
      "14586 [Discriminator loss: 0.036551, acc.: 98.44%] [Generator loss: 8.898868]\n",
      "14587 [Discriminator loss: 0.048110, acc.: 96.88%] [Generator loss: 8.159247]\n",
      "14588 [Discriminator loss: 0.146026, acc.: 93.75%] [Generator loss: 7.529004]\n",
      "14589 [Discriminator loss: 0.036503, acc.: 100.00%] [Generator loss: 9.169165]\n",
      "14590 [Discriminator loss: 0.095287, acc.: 93.75%] [Generator loss: 7.089317]\n",
      "14591 [Discriminator loss: 0.168942, acc.: 92.19%] [Generator loss: 8.457100]\n",
      "14592 [Discriminator loss: 0.064479, acc.: 98.44%] [Generator loss: 7.877001]\n",
      "14593 [Discriminator loss: 0.280744, acc.: 87.50%] [Generator loss: 7.165494]\n",
      "14594 [Discriminator loss: 0.035445, acc.: 98.44%] [Generator loss: 7.894857]\n",
      "14595 [Discriminator loss: 0.010511, acc.: 100.00%] [Generator loss: 8.526085]\n",
      "14596 [Discriminator loss: 0.047071, acc.: 98.44%] [Generator loss: 7.405696]\n",
      "14597 [Discriminator loss: 0.079590, acc.: 96.88%] [Generator loss: 8.302224]\n",
      "14598 [Discriminator loss: 0.107982, acc.: 89.06%] [Generator loss: 7.797204]\n",
      "14599 [Discriminator loss: 0.056561, acc.: 96.88%] [Generator loss: 8.553673]\n",
      "14600 [Discriminator loss: 0.053600, acc.: 98.44%] [Generator loss: 9.477101]\n",
      "14601 [Discriminator loss: 0.093969, acc.: 95.31%] [Generator loss: 7.366117]\n",
      "14602 [Discriminator loss: 0.104100, acc.: 96.88%] [Generator loss: 8.255905]\n",
      "14603 [Discriminator loss: 0.032206, acc.: 100.00%] [Generator loss: 7.670581]\n",
      "14604 [Discriminator loss: 0.047564, acc.: 98.44%] [Generator loss: 8.478981]\n",
      "14605 [Discriminator loss: 0.137084, acc.: 95.31%] [Generator loss: 6.709224]\n",
      "14606 [Discriminator loss: 0.066203, acc.: 96.88%] [Generator loss: 6.149281]\n",
      "14607 [Discriminator loss: 0.071574, acc.: 98.44%] [Generator loss: 7.252230]\n",
      "14608 [Discriminator loss: 0.142717, acc.: 96.88%] [Generator loss: 7.857455]\n",
      "14609 [Discriminator loss: 0.097619, acc.: 93.75%] [Generator loss: 7.729633]\n",
      "14610 [Discriminator loss: 0.057671, acc.: 98.44%] [Generator loss: 7.327149]\n",
      "14611 [Discriminator loss: 0.115360, acc.: 96.88%] [Generator loss: 7.189590]\n",
      "14612 [Discriminator loss: 0.115993, acc.: 95.31%] [Generator loss: 8.241650]\n",
      "14613 [Discriminator loss: 0.071223, acc.: 96.88%] [Generator loss: 8.723667]\n",
      "14614 [Discriminator loss: 0.090397, acc.: 96.88%] [Generator loss: 7.115026]\n",
      "14615 [Discriminator loss: 0.053266, acc.: 96.88%] [Generator loss: 6.826550]\n",
      "14616 [Discriminator loss: 0.085511, acc.: 95.31%] [Generator loss: 8.558210]\n",
      "14617 [Discriminator loss: 0.062716, acc.: 96.88%] [Generator loss: 7.498200]\n",
      "14618 [Discriminator loss: 0.083043, acc.: 96.88%] [Generator loss: 7.011998]\n",
      "14619 [Discriminator loss: 0.058202, acc.: 96.88%] [Generator loss: 7.578907]\n",
      "14620 [Discriminator loss: 0.072324, acc.: 96.88%] [Generator loss: 8.799973]\n",
      "14621 [Discriminator loss: 0.086715, acc.: 96.88%] [Generator loss: 7.100349]\n",
      "14622 [Discriminator loss: 0.076740, acc.: 96.88%] [Generator loss: 7.627987]\n",
      "14623 [Discriminator loss: 0.228748, acc.: 93.75%] [Generator loss: 7.117230]\n",
      "14624 [Discriminator loss: 0.062029, acc.: 96.88%] [Generator loss: 6.861844]\n",
      "14625 [Discriminator loss: 0.079581, acc.: 98.44%] [Generator loss: 7.924480]\n",
      "14626 [Discriminator loss: 0.076041, acc.: 98.44%] [Generator loss: 7.720574]\n",
      "14627 [Discriminator loss: 0.113639, acc.: 95.31%] [Generator loss: 9.869833]\n",
      "14628 [Discriminator loss: 0.029855, acc.: 98.44%] [Generator loss: 9.697763]\n",
      "14629 [Discriminator loss: 0.154239, acc.: 96.88%] [Generator loss: 7.161712]\n",
      "14630 [Discriminator loss: 0.067879, acc.: 98.44%] [Generator loss: 7.324251]\n",
      "14631 [Discriminator loss: 0.067311, acc.: 98.44%] [Generator loss: 7.063128]\n",
      "14632 [Discriminator loss: 0.088227, acc.: 96.88%] [Generator loss: 8.100567]\n",
      "14633 [Discriminator loss: 0.065900, acc.: 98.44%] [Generator loss: 7.877198]\n",
      "14634 [Discriminator loss: 0.048637, acc.: 98.44%] [Generator loss: 6.570222]\n",
      "14635 [Discriminator loss: 0.105608, acc.: 95.31%] [Generator loss: 6.628561]\n",
      "14636 [Discriminator loss: 0.078386, acc.: 98.44%] [Generator loss: 6.480249]\n",
      "14637 [Discriminator loss: 0.106782, acc.: 95.31%] [Generator loss: 8.410044]\n",
      "14638 [Discriminator loss: 0.014085, acc.: 100.00%] [Generator loss: 9.131783]\n",
      "14639 [Discriminator loss: 0.102902, acc.: 96.88%] [Generator loss: 9.432896]\n",
      "14640 [Discriminator loss: 0.435125, acc.: 82.81%] [Generator loss: 7.389455]\n",
      "14641 [Discriminator loss: 0.131882, acc.: 93.75%] [Generator loss: 6.873981]\n",
      "14642 [Discriminator loss: 0.025023, acc.: 100.00%] [Generator loss: 8.602885]\n",
      "14643 [Discriminator loss: 0.120670, acc.: 95.31%] [Generator loss: 6.980697]\n",
      "14644 [Discriminator loss: 0.179452, acc.: 93.75%] [Generator loss: 8.087884]\n",
      "14645 [Discriminator loss: 0.185212, acc.: 95.31%] [Generator loss: 7.986357]\n",
      "14646 [Discriminator loss: 0.088080, acc.: 98.44%] [Generator loss: 7.782098]\n",
      "14647 [Discriminator loss: 0.040691, acc.: 96.88%] [Generator loss: 7.147324]\n",
      "14648 [Discriminator loss: 0.138475, acc.: 95.31%] [Generator loss: 8.660873]\n",
      "14649 [Discriminator loss: 0.085283, acc.: 96.88%] [Generator loss: 8.695156]\n",
      "14650 [Discriminator loss: 0.072808, acc.: 96.88%] [Generator loss: 8.506687]\n",
      "14651 [Discriminator loss: 0.085636, acc.: 96.88%] [Generator loss: 7.879490]\n",
      "14652 [Discriminator loss: 0.044378, acc.: 98.44%] [Generator loss: 8.909460]\n",
      "14653 [Discriminator loss: 0.095143, acc.: 96.88%] [Generator loss: 6.906917]\n",
      "14654 [Discriminator loss: 0.248011, acc.: 87.50%] [Generator loss: 8.119197]\n",
      "14655 [Discriminator loss: 0.022083, acc.: 100.00%] [Generator loss: 9.216660]\n",
      "14656 [Discriminator loss: 0.053656, acc.: 98.44%] [Generator loss: 7.537910]\n",
      "14657 [Discriminator loss: 0.055611, acc.: 98.44%] [Generator loss: 8.559408]\n",
      "14658 [Discriminator loss: 0.119369, acc.: 95.31%] [Generator loss: 6.281808]\n",
      "14659 [Discriminator loss: 0.113050, acc.: 96.88%] [Generator loss: 6.199954]\n",
      "14660 [Discriminator loss: 0.056974, acc.: 96.88%] [Generator loss: 5.858991]\n",
      "14661 [Discriminator loss: 0.129184, acc.: 93.75%] [Generator loss: 6.921507]\n",
      "14662 [Discriminator loss: 0.104334, acc.: 95.31%] [Generator loss: 8.396422]\n",
      "14663 [Discriminator loss: 0.169027, acc.: 92.19%] [Generator loss: 7.496889]\n",
      "14664 [Discriminator loss: 0.049484, acc.: 96.88%] [Generator loss: 7.217771]\n",
      "14665 [Discriminator loss: 0.061585, acc.: 98.44%] [Generator loss: 7.707718]\n",
      "14666 [Discriminator loss: 0.060350, acc.: 98.44%] [Generator loss: 5.846191]\n",
      "14667 [Discriminator loss: 0.150155, acc.: 92.19%] [Generator loss: 7.534622]\n",
      "14668 [Discriminator loss: 0.047317, acc.: 100.00%] [Generator loss: 8.487246]\n",
      "14669 [Discriminator loss: 0.017015, acc.: 100.00%] [Generator loss: 6.796057]\n",
      "14670 [Discriminator loss: 0.075625, acc.: 96.88%] [Generator loss: 7.410604]\n",
      "14671 [Discriminator loss: 0.019362, acc.: 100.00%] [Generator loss: 7.802715]\n",
      "14672 [Discriminator loss: 0.088831, acc.: 98.44%] [Generator loss: 8.265993]\n",
      "14673 [Discriminator loss: 0.077241, acc.: 96.88%] [Generator loss: 7.470796]\n",
      "14674 [Discriminator loss: 0.062156, acc.: 98.44%] [Generator loss: 7.547705]\n",
      "14675 [Discriminator loss: 0.073050, acc.: 96.88%] [Generator loss: 6.944236]\n",
      "14676 [Discriminator loss: 0.156632, acc.: 93.75%] [Generator loss: 8.643405]\n",
      "14677 [Discriminator loss: 0.044059, acc.: 96.88%] [Generator loss: 8.515018]\n",
      "14678 [Discriminator loss: 0.584719, acc.: 82.81%] [Generator loss: 6.884668]\n",
      "14679 [Discriminator loss: 0.030723, acc.: 98.44%] [Generator loss: 8.124701]\n",
      "14680 [Discriminator loss: 0.014686, acc.: 100.00%] [Generator loss: 7.741239]\n",
      "14681 [Discriminator loss: 0.090124, acc.: 95.31%] [Generator loss: 6.606098]\n",
      "14682 [Discriminator loss: 0.068085, acc.: 98.44%] [Generator loss: 7.468796]\n",
      "14683 [Discriminator loss: 0.078168, acc.: 98.44%] [Generator loss: 7.111112]\n",
      "14684 [Discriminator loss: 0.031142, acc.: 100.00%] [Generator loss: 8.228572]\n",
      "14685 [Discriminator loss: 0.097808, acc.: 95.31%] [Generator loss: 7.780625]\n",
      "14686 [Discriminator loss: 0.061874, acc.: 98.44%] [Generator loss: 7.015339]\n",
      "14687 [Discriminator loss: 0.091309, acc.: 95.31%] [Generator loss: 6.580874]\n",
      "14688 [Discriminator loss: 0.043095, acc.: 98.44%] [Generator loss: 6.727015]\n",
      "14689 [Discriminator loss: 0.069044, acc.: 96.88%] [Generator loss: 6.684344]\n",
      "14690 [Discriminator loss: 0.090836, acc.: 96.88%] [Generator loss: 7.684105]\n",
      "14691 [Discriminator loss: 0.115864, acc.: 98.44%] [Generator loss: 7.300707]\n",
      "14692 [Discriminator loss: 0.020348, acc.: 100.00%] [Generator loss: 9.059143]\n",
      "14693 [Discriminator loss: 0.192192, acc.: 90.62%] [Generator loss: 7.958947]\n",
      "14694 [Discriminator loss: 0.131591, acc.: 95.31%] [Generator loss: 7.462590]\n",
      "14695 [Discriminator loss: 0.102341, acc.: 95.31%] [Generator loss: 7.900412]\n",
      "14696 [Discriminator loss: 0.075583, acc.: 96.88%] [Generator loss: 8.627228]\n",
      "14697 [Discriminator loss: 0.041281, acc.: 100.00%] [Generator loss: 8.704554]\n",
      "14698 [Discriminator loss: 0.332245, acc.: 89.06%] [Generator loss: 7.359484]\n",
      "14699 [Discriminator loss: 0.075497, acc.: 96.88%] [Generator loss: 8.321828]\n",
      "14700 [Discriminator loss: 0.132999, acc.: 95.31%] [Generator loss: 8.867860]\n",
      "14701 [Discriminator loss: 0.233837, acc.: 89.06%] [Generator loss: 8.869947]\n",
      "14702 [Discriminator loss: 0.027281, acc.: 100.00%] [Generator loss: 7.537937]\n",
      "14703 [Discriminator loss: 0.076310, acc.: 96.88%] [Generator loss: 7.424721]\n",
      "14704 [Discriminator loss: 0.080003, acc.: 95.31%] [Generator loss: 6.570550]\n",
      "14705 [Discriminator loss: 0.044474, acc.: 96.88%] [Generator loss: 6.828321]\n",
      "14706 [Discriminator loss: 0.038422, acc.: 98.44%] [Generator loss: 7.878392]\n",
      "14707 [Discriminator loss: 0.060729, acc.: 96.88%] [Generator loss: 7.774466]\n",
      "14708 [Discriminator loss: 0.080694, acc.: 95.31%] [Generator loss: 8.313339]\n",
      "14709 [Discriminator loss: 0.027221, acc.: 98.44%] [Generator loss: 7.752166]\n",
      "14710 [Discriminator loss: 0.124974, acc.: 93.75%] [Generator loss: 6.419813]\n",
      "14711 [Discriminator loss: 0.063394, acc.: 96.88%] [Generator loss: 7.562119]\n",
      "14712 [Discriminator loss: 0.009476, acc.: 100.00%] [Generator loss: 9.234600]\n",
      "14713 [Discriminator loss: 0.172033, acc.: 92.19%] [Generator loss: 6.106278]\n",
      "14714 [Discriminator loss: 0.076382, acc.: 98.44%] [Generator loss: 7.950076]\n",
      "14715 [Discriminator loss: 0.019581, acc.: 100.00%] [Generator loss: 8.615698]\n",
      "14716 [Discriminator loss: 0.040581, acc.: 98.44%] [Generator loss: 7.071943]\n",
      "14717 [Discriminator loss: 0.051810, acc.: 96.88%] [Generator loss: 6.193453]\n",
      "14718 [Discriminator loss: 0.060064, acc.: 96.88%] [Generator loss: 6.618875]\n",
      "14719 [Discriminator loss: 0.029741, acc.: 100.00%] [Generator loss: 7.504379]\n",
      "14720 [Discriminator loss: 0.122534, acc.: 95.31%] [Generator loss: 7.518104]\n",
      "14721 [Discriminator loss: 0.185098, acc.: 92.19%] [Generator loss: 7.381384]\n",
      "14722 [Discriminator loss: 0.148300, acc.: 95.31%] [Generator loss: 8.021729]\n",
      "14723 [Discriminator loss: 0.027075, acc.: 100.00%] [Generator loss: 8.168565]\n",
      "14724 [Discriminator loss: 0.039712, acc.: 98.44%] [Generator loss: 7.160309]\n",
      "14725 [Discriminator loss: 0.252999, acc.: 89.06%] [Generator loss: 7.318239]\n",
      "14726 [Discriminator loss: 0.127130, acc.: 95.31%] [Generator loss: 7.679888]\n",
      "14727 [Discriminator loss: 0.076799, acc.: 96.88%] [Generator loss: 6.658789]\n",
      "14728 [Discriminator loss: 0.209883, acc.: 92.19%] [Generator loss: 8.742056]\n",
      "14729 [Discriminator loss: 0.022493, acc.: 100.00%] [Generator loss: 10.613575]\n",
      "14730 [Discriminator loss: 0.103103, acc.: 95.31%] [Generator loss: 7.520136]\n",
      "14731 [Discriminator loss: 0.134103, acc.: 92.19%] [Generator loss: 6.923150]\n",
      "14732 [Discriminator loss: 0.079929, acc.: 95.31%] [Generator loss: 8.085509]\n",
      "14733 [Discriminator loss: 0.041380, acc.: 96.88%] [Generator loss: 7.406190]\n",
      "14734 [Discriminator loss: 0.053760, acc.: 98.44%] [Generator loss: 6.499415]\n",
      "14735 [Discriminator loss: 0.130373, acc.: 95.31%] [Generator loss: 6.062325]\n",
      "14736 [Discriminator loss: 0.356464, acc.: 85.94%] [Generator loss: 8.092346]\n",
      "14737 [Discriminator loss: 0.086392, acc.: 96.88%] [Generator loss: 8.234617]\n",
      "14738 [Discriminator loss: 0.250546, acc.: 92.19%] [Generator loss: 7.365609]\n",
      "14739 [Discriminator loss: 0.085093, acc.: 95.31%] [Generator loss: 9.146840]\n",
      "14740 [Discriminator loss: 0.075057, acc.: 98.44%] [Generator loss: 8.605818]\n",
      "14741 [Discriminator loss: 0.074058, acc.: 96.88%] [Generator loss: 7.521431]\n",
      "14742 [Discriminator loss: 0.020873, acc.: 100.00%] [Generator loss: 8.090925]\n",
      "14743 [Discriminator loss: 0.120037, acc.: 95.31%] [Generator loss: 8.937752]\n",
      "14744 [Discriminator loss: 0.174575, acc.: 93.75%] [Generator loss: 8.658899]\n",
      "14745 [Discriminator loss: 0.052563, acc.: 96.88%] [Generator loss: 7.908502]\n",
      "14746 [Discriminator loss: 0.062148, acc.: 98.44%] [Generator loss: 7.493846]\n",
      "14747 [Discriminator loss: 0.020563, acc.: 100.00%] [Generator loss: 7.122813]\n",
      "14748 [Discriminator loss: 0.201520, acc.: 92.19%] [Generator loss: 7.371569]\n",
      "14749 [Discriminator loss: 0.162835, acc.: 96.88%] [Generator loss: 7.248550]\n",
      "14750 [Discriminator loss: 0.042881, acc.: 98.44%] [Generator loss: 8.256602]\n",
      "14751 [Discriminator loss: 0.020185, acc.: 100.00%] [Generator loss: 9.583803]\n",
      "14752 [Discriminator loss: 0.199436, acc.: 95.31%] [Generator loss: 6.948045]\n",
      "14753 [Discriminator loss: 0.072160, acc.: 96.88%] [Generator loss: 6.204320]\n",
      "14754 [Discriminator loss: 0.034384, acc.: 98.44%] [Generator loss: 7.614618]\n",
      "14755 [Discriminator loss: 0.097629, acc.: 93.75%] [Generator loss: 9.237139]\n",
      "14756 [Discriminator loss: 0.114349, acc.: 96.88%] [Generator loss: 9.053673]\n",
      "14757 [Discriminator loss: 0.171705, acc.: 90.62%] [Generator loss: 6.727339]\n",
      "14758 [Discriminator loss: 0.116918, acc.: 96.88%] [Generator loss: 7.939635]\n",
      "14759 [Discriminator loss: 0.023210, acc.: 100.00%] [Generator loss: 9.380205]\n",
      "14760 [Discriminator loss: 0.110024, acc.: 95.31%] [Generator loss: 8.760975]\n",
      "14761 [Discriminator loss: 0.042049, acc.: 96.88%] [Generator loss: 6.662126]\n",
      "14762 [Discriminator loss: 0.118080, acc.: 95.31%] [Generator loss: 6.809067]\n",
      "14763 [Discriminator loss: 0.076277, acc.: 96.88%] [Generator loss: 8.196327]\n",
      "14764 [Discriminator loss: 0.103812, acc.: 93.75%] [Generator loss: 8.373493]\n",
      "14765 [Discriminator loss: 0.033636, acc.: 100.00%] [Generator loss: 7.734808]\n",
      "14766 [Discriminator loss: 0.019370, acc.: 100.00%] [Generator loss: 7.307285]\n",
      "14767 [Discriminator loss: 0.085365, acc.: 95.31%] [Generator loss: 6.526672]\n",
      "14768 [Discriminator loss: 0.054869, acc.: 96.88%] [Generator loss: 8.309351]\n",
      "14769 [Discriminator loss: 0.097306, acc.: 98.44%] [Generator loss: 8.552441]\n",
      "14770 [Discriminator loss: 0.086834, acc.: 95.31%] [Generator loss: 7.952614]\n",
      "14771 [Discriminator loss: 0.099761, acc.: 96.88%] [Generator loss: 7.432126]\n",
      "14772 [Discriminator loss: 0.056199, acc.: 96.88%] [Generator loss: 8.357770]\n",
      "14773 [Discriminator loss: 0.046044, acc.: 98.44%] [Generator loss: 9.279131]\n",
      "14774 [Discriminator loss: 0.035609, acc.: 98.44%] [Generator loss: 7.461518]\n",
      "14775 [Discriminator loss: 0.013143, acc.: 100.00%] [Generator loss: 7.295583]\n",
      "14776 [Discriminator loss: 0.101597, acc.: 96.88%] [Generator loss: 8.329110]\n",
      "14777 [Discriminator loss: 0.088025, acc.: 95.31%] [Generator loss: 6.766232]\n",
      "14778 [Discriminator loss: 0.010610, acc.: 100.00%] [Generator loss: 8.570983]\n",
      "14779 [Discriminator loss: 0.140610, acc.: 95.31%] [Generator loss: 6.861621]\n",
      "14780 [Discriminator loss: 0.060595, acc.: 100.00%] [Generator loss: 6.405075]\n",
      "14781 [Discriminator loss: 0.039965, acc.: 100.00%] [Generator loss: 7.108295]\n",
      "14782 [Discriminator loss: 0.019200, acc.: 100.00%] [Generator loss: 6.727740]\n",
      "14783 [Discriminator loss: 0.116012, acc.: 93.75%] [Generator loss: 7.913206]\n",
      "14784 [Discriminator loss: 0.029544, acc.: 100.00%] [Generator loss: 7.991305]\n",
      "14785 [Discriminator loss: 0.052536, acc.: 96.88%] [Generator loss: 7.503340]\n",
      "14786 [Discriminator loss: 0.089180, acc.: 95.31%] [Generator loss: 6.853027]\n",
      "14787 [Discriminator loss: 0.041241, acc.: 100.00%] [Generator loss: 6.957139]\n",
      "14788 [Discriminator loss: 0.103364, acc.: 92.19%] [Generator loss: 8.563011]\n",
      "14789 [Discriminator loss: 0.029788, acc.: 100.00%] [Generator loss: 8.937229]\n",
      "14790 [Discriminator loss: 0.140843, acc.: 92.19%] [Generator loss: 6.994308]\n",
      "14791 [Discriminator loss: 0.199306, acc.: 93.75%] [Generator loss: 8.777480]\n",
      "14792 [Discriminator loss: 0.123984, acc.: 95.31%] [Generator loss: 9.079170]\n",
      "14793 [Discriminator loss: 0.196323, acc.: 93.75%] [Generator loss: 9.629398]\n",
      "14794 [Discriminator loss: 0.096970, acc.: 95.31%] [Generator loss: 7.062389]\n",
      "14795 [Discriminator loss: 0.086901, acc.: 95.31%] [Generator loss: 9.502817]\n",
      "14796 [Discriminator loss: 0.049784, acc.: 98.44%] [Generator loss: 7.692578]\n",
      "14797 [Discriminator loss: 0.081629, acc.: 96.88%] [Generator loss: 7.308698]\n",
      "14798 [Discriminator loss: 0.082969, acc.: 96.88%] [Generator loss: 7.102022]\n",
      "14799 [Discriminator loss: 0.065782, acc.: 98.44%] [Generator loss: 8.964443]\n",
      "14800 [Discriminator loss: 0.073477, acc.: 96.88%] [Generator loss: 7.423368]\n",
      "14801 [Discriminator loss: 0.103114, acc.: 95.31%] [Generator loss: 6.912543]\n",
      "14802 [Discriminator loss: 0.081151, acc.: 96.88%] [Generator loss: 6.490411]\n",
      "14803 [Discriminator loss: 0.041217, acc.: 98.44%] [Generator loss: 7.245557]\n",
      "14804 [Discriminator loss: 0.029982, acc.: 100.00%] [Generator loss: 7.485606]\n",
      "14805 [Discriminator loss: 0.081705, acc.: 98.44%] [Generator loss: 7.919262]\n",
      "14806 [Discriminator loss: 0.094472, acc.: 96.88%] [Generator loss: 7.169699]\n",
      "14807 [Discriminator loss: 0.119134, acc.: 93.75%] [Generator loss: 7.831421]\n",
      "14808 [Discriminator loss: 0.148359, acc.: 96.88%] [Generator loss: 8.035508]\n",
      "14809 [Discriminator loss: 0.214302, acc.: 92.19%] [Generator loss: 7.021674]\n",
      "14810 [Discriminator loss: 0.053716, acc.: 98.44%] [Generator loss: 8.569180]\n",
      "14811 [Discriminator loss: 0.062772, acc.: 96.88%] [Generator loss: 8.697868]\n",
      "14812 [Discriminator loss: 0.077476, acc.: 96.88%] [Generator loss: 8.919363]\n",
      "14813 [Discriminator loss: 0.056995, acc.: 96.88%] [Generator loss: 9.026989]\n",
      "14814 [Discriminator loss: 0.125683, acc.: 95.31%] [Generator loss: 8.101559]\n",
      "14815 [Discriminator loss: 0.016030, acc.: 100.00%] [Generator loss: 6.873004]\n",
      "14816 [Discriminator loss: 0.129768, acc.: 95.31%] [Generator loss: 6.779337]\n",
      "14817 [Discriminator loss: 0.076054, acc.: 95.31%] [Generator loss: 7.047739]\n",
      "14818 [Discriminator loss: 0.043793, acc.: 98.44%] [Generator loss: 7.937991]\n",
      "14819 [Discriminator loss: 0.114759, acc.: 96.88%] [Generator loss: 8.436979]\n",
      "14820 [Discriminator loss: 0.152393, acc.: 95.31%] [Generator loss: 7.775048]\n",
      "14821 [Discriminator loss: 0.183387, acc.: 90.62%] [Generator loss: 5.906809]\n",
      "14822 [Discriminator loss: 0.089676, acc.: 93.75%] [Generator loss: 8.479924]\n",
      "14823 [Discriminator loss: 0.162904, acc.: 90.62%] [Generator loss: 9.061356]\n",
      "14824 [Discriminator loss: 0.032027, acc.: 100.00%] [Generator loss: 9.439421]\n",
      "14825 [Discriminator loss: 0.058960, acc.: 98.44%] [Generator loss: 7.917246]\n",
      "14826 [Discriminator loss: 0.372443, acc.: 85.94%] [Generator loss: 10.662721]\n",
      "14827 [Discriminator loss: 0.080133, acc.: 95.31%] [Generator loss: 10.260073]\n",
      "14828 [Discriminator loss: 0.129429, acc.: 93.75%] [Generator loss: 7.644808]\n",
      "14829 [Discriminator loss: 0.234598, acc.: 93.75%] [Generator loss: 9.075272]\n",
      "14830 [Discriminator loss: 0.055883, acc.: 95.31%] [Generator loss: 9.187876]\n",
      "14831 [Discriminator loss: 0.043440, acc.: 96.88%] [Generator loss: 7.978045]\n",
      "14832 [Discriminator loss: 0.218475, acc.: 90.62%] [Generator loss: 7.137906]\n",
      "14833 [Discriminator loss: 0.037948, acc.: 98.44%] [Generator loss: 6.511304]\n",
      "14834 [Discriminator loss: 0.164427, acc.: 93.75%] [Generator loss: 9.131308]\n",
      "14835 [Discriminator loss: 0.109354, acc.: 96.88%] [Generator loss: 9.094777]\n",
      "14836 [Discriminator loss: 0.150461, acc.: 95.31%] [Generator loss: 9.821395]\n",
      "14837 [Discriminator loss: 0.236095, acc.: 89.06%] [Generator loss: 8.271953]\n",
      "14838 [Discriminator loss: 0.088253, acc.: 95.31%] [Generator loss: 8.497652]\n",
      "14839 [Discriminator loss: 0.066252, acc.: 100.00%] [Generator loss: 8.926431]\n",
      "14840 [Discriminator loss: 0.104853, acc.: 95.31%] [Generator loss: 8.828867]\n",
      "14841 [Discriminator loss: 0.034547, acc.: 100.00%] [Generator loss: 8.318947]\n",
      "14842 [Discriminator loss: 0.053400, acc.: 98.44%] [Generator loss: 6.767448]\n",
      "14843 [Discriminator loss: 0.129180, acc.: 95.31%] [Generator loss: 7.084494]\n",
      "14844 [Discriminator loss: 0.154346, acc.: 92.19%] [Generator loss: 8.009460]\n",
      "14845 [Discriminator loss: 0.181305, acc.: 93.75%] [Generator loss: 7.549630]\n",
      "14846 [Discriminator loss: 0.116321, acc.: 95.31%] [Generator loss: 9.008266]\n",
      "14847 [Discriminator loss: 0.026860, acc.: 100.00%] [Generator loss: 7.560871]\n",
      "14848 [Discriminator loss: 0.101547, acc.: 96.88%] [Generator loss: 8.682669]\n",
      "14849 [Discriminator loss: 0.114053, acc.: 98.44%] [Generator loss: 7.990793]\n",
      "14850 [Discriminator loss: 0.052196, acc.: 96.88%] [Generator loss: 8.144260]\n",
      "14851 [Discriminator loss: 0.035844, acc.: 98.44%] [Generator loss: 6.804050]\n",
      "14852 [Discriminator loss: 0.150084, acc.: 95.31%] [Generator loss: 6.622125]\n",
      "14853 [Discriminator loss: 0.104408, acc.: 96.88%] [Generator loss: 8.645781]\n",
      "14854 [Discriminator loss: 0.091340, acc.: 96.88%] [Generator loss: 6.508737]\n",
      "14855 [Discriminator loss: 0.045260, acc.: 98.44%] [Generator loss: 6.563107]\n",
      "14856 [Discriminator loss: 0.093766, acc.: 95.31%] [Generator loss: 7.435273]\n",
      "14857 [Discriminator loss: 0.082725, acc.: 98.44%] [Generator loss: 6.609169]\n",
      "14858 [Discriminator loss: 0.077278, acc.: 96.88%] [Generator loss: 5.891713]\n",
      "14859 [Discriminator loss: 0.166690, acc.: 93.75%] [Generator loss: 8.147693]\n",
      "14860 [Discriminator loss: 0.153777, acc.: 96.88%] [Generator loss: 7.520755]\n",
      "14861 [Discriminator loss: 0.087830, acc.: 98.44%] [Generator loss: 6.570723]\n",
      "14862 [Discriminator loss: 0.168738, acc.: 92.19%] [Generator loss: 9.109866]\n",
      "14863 [Discriminator loss: 0.202405, acc.: 93.75%] [Generator loss: 8.269564]\n",
      "14864 [Discriminator loss: 0.117295, acc.: 93.75%] [Generator loss: 8.352077]\n",
      "14865 [Discriminator loss: 0.084017, acc.: 98.44%] [Generator loss: 8.990425]\n",
      "14866 [Discriminator loss: 0.202832, acc.: 92.19%] [Generator loss: 6.545281]\n",
      "14867 [Discriminator loss: 0.163707, acc.: 95.31%] [Generator loss: 9.145239]\n",
      "14868 [Discriminator loss: 0.029223, acc.: 100.00%] [Generator loss: 7.834224]\n",
      "14869 [Discriminator loss: 0.062667, acc.: 98.44%] [Generator loss: 6.679228]\n",
      "14870 [Discriminator loss: 0.023290, acc.: 100.00%] [Generator loss: 8.168880]\n",
      "14871 [Discriminator loss: 0.054070, acc.: 98.44%] [Generator loss: 6.009604]\n",
      "14872 [Discriminator loss: 0.190109, acc.: 92.19%] [Generator loss: 9.016291]\n",
      "14873 [Discriminator loss: 0.213515, acc.: 95.31%] [Generator loss: 8.308479]\n",
      "14874 [Discriminator loss: 0.035118, acc.: 98.44%] [Generator loss: 8.293476]\n",
      "14875 [Discriminator loss: 0.235788, acc.: 92.19%] [Generator loss: 6.191182]\n",
      "14876 [Discriminator loss: 0.187807, acc.: 93.75%] [Generator loss: 9.159620]\n",
      "14877 [Discriminator loss: 0.211123, acc.: 95.31%] [Generator loss: 7.590827]\n",
      "14878 [Discriminator loss: 0.076888, acc.: 95.31%] [Generator loss: 7.335783]\n",
      "14879 [Discriminator loss: 0.102151, acc.: 95.31%] [Generator loss: 8.946831]\n",
      "14880 [Discriminator loss: 0.167140, acc.: 95.31%] [Generator loss: 8.328526]\n",
      "14881 [Discriminator loss: 0.020998, acc.: 100.00%] [Generator loss: 9.076635]\n",
      "14882 [Discriminator loss: 0.141077, acc.: 95.31%] [Generator loss: 7.644608]\n",
      "14883 [Discriminator loss: 0.026825, acc.: 100.00%] [Generator loss: 8.426179]\n",
      "14884 [Discriminator loss: 0.057169, acc.: 98.44%] [Generator loss: 7.044161]\n",
      "14885 [Discriminator loss: 0.054501, acc.: 98.44%] [Generator loss: 8.942290]\n",
      "14886 [Discriminator loss: 0.059616, acc.: 95.31%] [Generator loss: 8.729650]\n",
      "14887 [Discriminator loss: 0.078001, acc.: 96.88%] [Generator loss: 7.518963]\n",
      "14888 [Discriminator loss: 0.013916, acc.: 100.00%] [Generator loss: 8.745108]\n",
      "14889 [Discriminator loss: 0.179846, acc.: 95.31%] [Generator loss: 5.544237]\n",
      "14890 [Discriminator loss: 0.321100, acc.: 87.50%] [Generator loss: 9.145214]\n",
      "14891 [Discriminator loss: 0.134511, acc.: 95.31%] [Generator loss: 9.284846]\n",
      "14892 [Discriminator loss: 0.019894, acc.: 100.00%] [Generator loss: 8.456538]\n",
      "14893 [Discriminator loss: 0.174460, acc.: 93.75%] [Generator loss: 7.864005]\n",
      "14894 [Discriminator loss: 0.090493, acc.: 95.31%] [Generator loss: 7.837429]\n",
      "14895 [Discriminator loss: 0.103367, acc.: 96.88%] [Generator loss: 7.375741]\n",
      "14896 [Discriminator loss: 0.058049, acc.: 98.44%] [Generator loss: 7.469178]\n",
      "14897 [Discriminator loss: 0.070223, acc.: 96.88%] [Generator loss: 6.936755]\n",
      "14898 [Discriminator loss: 0.148214, acc.: 93.75%] [Generator loss: 7.431704]\n",
      "14899 [Discriminator loss: 0.081676, acc.: 96.88%] [Generator loss: 8.129324]\n",
      "14900 [Discriminator loss: 0.026428, acc.: 98.44%] [Generator loss: 8.622101]\n",
      "14901 [Discriminator loss: 0.063900, acc.: 96.88%] [Generator loss: 9.250166]\n",
      "14902 [Discriminator loss: 0.025190, acc.: 100.00%] [Generator loss: 6.730794]\n",
      "14903 [Discriminator loss: 0.141409, acc.: 96.88%] [Generator loss: 6.838539]\n",
      "14904 [Discriminator loss: 0.047416, acc.: 98.44%] [Generator loss: 7.378633]\n",
      "14905 [Discriminator loss: 0.118455, acc.: 96.88%] [Generator loss: 9.840151]\n",
      "14906 [Discriminator loss: 0.079334, acc.: 96.88%] [Generator loss: 7.708851]\n",
      "14907 [Discriminator loss: 0.027741, acc.: 100.00%] [Generator loss: 7.372730]\n",
      "14908 [Discriminator loss: 0.477973, acc.: 79.69%] [Generator loss: 6.116531]\n",
      "14909 [Discriminator loss: 0.015064, acc.: 100.00%] [Generator loss: 7.056066]\n",
      "14910 [Discriminator loss: 0.080396, acc.: 98.44%] [Generator loss: 7.712877]\n",
      "14911 [Discriminator loss: 0.116465, acc.: 96.88%] [Generator loss: 6.207359]\n",
      "14912 [Discriminator loss: 0.076581, acc.: 96.88%] [Generator loss: 6.023637]\n",
      "14913 [Discriminator loss: 0.152719, acc.: 90.62%] [Generator loss: 7.674273]\n",
      "14914 [Discriminator loss: 0.070074, acc.: 96.88%] [Generator loss: 8.548792]\n",
      "14915 [Discriminator loss: 0.095980, acc.: 95.31%] [Generator loss: 6.714629]\n",
      "14916 [Discriminator loss: 0.136917, acc.: 95.31%] [Generator loss: 6.084925]\n",
      "14917 [Discriminator loss: 0.190892, acc.: 93.75%] [Generator loss: 8.866258]\n",
      "14918 [Discriminator loss: 0.143178, acc.: 93.75%] [Generator loss: 8.261860]\n",
      "14919 [Discriminator loss: 0.086937, acc.: 98.44%] [Generator loss: 9.051923]\n",
      "14920 [Discriminator loss: 0.027236, acc.: 100.00%] [Generator loss: 10.166606]\n",
      "14921 [Discriminator loss: 0.101592, acc.: 98.44%] [Generator loss: 6.660190]\n",
      "14922 [Discriminator loss: 0.046430, acc.: 98.44%] [Generator loss: 6.998120]\n",
      "14923 [Discriminator loss: 0.060855, acc.: 98.44%] [Generator loss: 6.225699]\n",
      "14924 [Discriminator loss: 0.056997, acc.: 96.88%] [Generator loss: 9.216408]\n",
      "14925 [Discriminator loss: 0.238828, acc.: 90.62%] [Generator loss: 8.694903]\n",
      "14926 [Discriminator loss: 0.008149, acc.: 100.00%] [Generator loss: 8.322729]\n",
      "14927 [Discriminator loss: 0.044411, acc.: 96.88%] [Generator loss: 8.069049]\n",
      "14928 [Discriminator loss: 0.076141, acc.: 95.31%] [Generator loss: 7.973011]\n",
      "14929 [Discriminator loss: 0.040850, acc.: 98.44%] [Generator loss: 7.712829]\n",
      "14930 [Discriminator loss: 0.070225, acc.: 95.31%] [Generator loss: 8.258518]\n",
      "14931 [Discriminator loss: 0.335556, acc.: 89.06%] [Generator loss: 8.913336]\n",
      "14932 [Discriminator loss: 0.012969, acc.: 100.00%] [Generator loss: 9.898539]\n",
      "14933 [Discriminator loss: 0.148808, acc.: 93.75%] [Generator loss: 7.193781]\n",
      "14934 [Discriminator loss: 0.073300, acc.: 96.88%] [Generator loss: 8.025333]\n",
      "14935 [Discriminator loss: 0.023101, acc.: 100.00%] [Generator loss: 8.143965]\n",
      "14936 [Discriminator loss: 0.130108, acc.: 93.75%] [Generator loss: 6.351966]\n",
      "14937 [Discriminator loss: 0.069349, acc.: 95.31%] [Generator loss: 7.486985]\n",
      "14938 [Discriminator loss: 0.087900, acc.: 96.88%] [Generator loss: 8.552761]\n",
      "14939 [Discriminator loss: 0.088210, acc.: 96.88%] [Generator loss: 8.197669]\n",
      "14940 [Discriminator loss: 0.035938, acc.: 100.00%] [Generator loss: 9.548182]\n",
      "14941 [Discriminator loss: 0.091231, acc.: 95.31%] [Generator loss: 7.228674]\n",
      "14942 [Discriminator loss: 0.150597, acc.: 92.19%] [Generator loss: 7.315396]\n",
      "14943 [Discriminator loss: 0.060515, acc.: 96.88%] [Generator loss: 8.128584]\n",
      "14944 [Discriminator loss: 0.155126, acc.: 92.19%] [Generator loss: 8.528492]\n",
      "14945 [Discriminator loss: 0.057810, acc.: 96.88%] [Generator loss: 8.729846]\n",
      "14946 [Discriminator loss: 0.079091, acc.: 96.88%] [Generator loss: 4.910238]\n",
      "14947 [Discriminator loss: 0.096285, acc.: 95.31%] [Generator loss: 8.231157]\n",
      "14948 [Discriminator loss: 0.042472, acc.: 100.00%] [Generator loss: 8.290195]\n",
      "14949 [Discriminator loss: 0.055910, acc.: 96.88%] [Generator loss: 8.446768]\n",
      "14950 [Discriminator loss: 0.102512, acc.: 95.31%] [Generator loss: 7.512963]\n",
      "14951 [Discriminator loss: 0.105543, acc.: 95.31%] [Generator loss: 6.174516]\n",
      "14952 [Discriminator loss: 0.106238, acc.: 95.31%] [Generator loss: 8.487778]\n",
      "14953 [Discriminator loss: 0.075661, acc.: 95.31%] [Generator loss: 8.817469]\n",
      "14954 [Discriminator loss: 0.014231, acc.: 100.00%] [Generator loss: 8.971712]\n",
      "14955 [Discriminator loss: 0.111800, acc.: 95.31%] [Generator loss: 6.326281]\n",
      "14956 [Discriminator loss: 0.151918, acc.: 92.19%] [Generator loss: 8.646988]\n",
      "14957 [Discriminator loss: 0.062147, acc.: 96.88%] [Generator loss: 7.066204]\n",
      "14958 [Discriminator loss: 0.230626, acc.: 90.62%] [Generator loss: 8.186856]\n",
      "14959 [Discriminator loss: 0.068548, acc.: 95.31%] [Generator loss: 7.892660]\n",
      "14960 [Discriminator loss: 0.211254, acc.: 92.19%] [Generator loss: 7.427504]\n",
      "14961 [Discriminator loss: 0.133592, acc.: 96.88%] [Generator loss: 7.734566]\n",
      "14962 [Discriminator loss: 0.018481, acc.: 100.00%] [Generator loss: 8.665592]\n",
      "14963 [Discriminator loss: 0.094502, acc.: 98.44%] [Generator loss: 7.268326]\n",
      "14964 [Discriminator loss: 0.129223, acc.: 96.88%] [Generator loss: 7.986939]\n",
      "14965 [Discriminator loss: 0.101245, acc.: 96.88%] [Generator loss: 7.468625]\n",
      "14966 [Discriminator loss: 0.024124, acc.: 100.00%] [Generator loss: 6.996369]\n",
      "14967 [Discriminator loss: 0.077615, acc.: 96.88%] [Generator loss: 6.721342]\n",
      "14968 [Discriminator loss: 0.274007, acc.: 92.19%] [Generator loss: 8.426293]\n",
      "14969 [Discriminator loss: 0.132498, acc.: 93.75%] [Generator loss: 8.503658]\n",
      "14970 [Discriminator loss: 0.056379, acc.: 96.88%] [Generator loss: 7.411328]\n",
      "14971 [Discriminator loss: 0.127279, acc.: 93.75%] [Generator loss: 9.429258]\n",
      "14972 [Discriminator loss: 0.055610, acc.: 96.88%] [Generator loss: 8.212214]\n",
      "14973 [Discriminator loss: 0.012539, acc.: 100.00%] [Generator loss: 7.327419]\n",
      "14974 [Discriminator loss: 0.392934, acc.: 81.25%] [Generator loss: 8.515984]\n",
      "14975 [Discriminator loss: 0.045567, acc.: 96.88%] [Generator loss: 9.985039]\n",
      "14976 [Discriminator loss: 0.070914, acc.: 96.88%] [Generator loss: 8.366293]\n",
      "14977 [Discriminator loss: 0.039980, acc.: 96.88%] [Generator loss: 8.845119]\n",
      "14978 [Discriminator loss: 0.044790, acc.: 96.88%] [Generator loss: 9.061682]\n",
      "14979 [Discriminator loss: 0.059211, acc.: 95.31%] [Generator loss: 8.526083]\n",
      "14980 [Discriminator loss: 0.050283, acc.: 98.44%] [Generator loss: 7.892046]\n",
      "14981 [Discriminator loss: 0.027706, acc.: 98.44%] [Generator loss: 6.180928]\n",
      "14982 [Discriminator loss: 0.261112, acc.: 90.62%] [Generator loss: 7.830767]\n",
      "14983 [Discriminator loss: 0.073645, acc.: 95.31%] [Generator loss: 7.719708]\n",
      "14984 [Discriminator loss: 0.147250, acc.: 93.75%] [Generator loss: 7.505888]\n",
      "14985 [Discriminator loss: 0.135693, acc.: 96.88%] [Generator loss: 7.909384]\n",
      "14986 [Discriminator loss: 0.016472, acc.: 100.00%] [Generator loss: 7.615966]\n",
      "14987 [Discriminator loss: 0.087135, acc.: 96.88%] [Generator loss: 7.488589]\n",
      "14988 [Discriminator loss: 0.050483, acc.: 98.44%] [Generator loss: 7.420849]\n",
      "14989 [Discriminator loss: 0.039321, acc.: 98.44%] [Generator loss: 6.548793]\n",
      "14990 [Discriminator loss: 0.122135, acc.: 96.88%] [Generator loss: 7.282756]\n",
      "14991 [Discriminator loss: 0.105534, acc.: 93.75%] [Generator loss: 6.325260]\n",
      "14992 [Discriminator loss: 0.209552, acc.: 93.75%] [Generator loss: 8.820820]\n",
      "14993 [Discriminator loss: 0.044751, acc.: 98.44%] [Generator loss: 8.711395]\n",
      "14994 [Discriminator loss: 0.146649, acc.: 95.31%] [Generator loss: 6.557697]\n",
      "14995 [Discriminator loss: 0.030285, acc.: 100.00%] [Generator loss: 6.338480]\n",
      "14996 [Discriminator loss: 0.028568, acc.: 98.44%] [Generator loss: 7.357236]\n",
      "14997 [Discriminator loss: 0.043958, acc.: 98.44%] [Generator loss: 7.780299]\n",
      "14998 [Discriminator loss: 0.022363, acc.: 100.00%] [Generator loss: 8.098605]\n",
      "14999 [Discriminator loss: 0.151548, acc.: 92.19%] [Generator loss: 6.937288]\n",
      "15000 [Discriminator loss: 0.020206, acc.: 100.00%] [Generator loss: 7.162015]\n",
      "15001 [Discriminator loss: 0.250593, acc.: 93.75%] [Generator loss: 9.233667]\n",
      "15002 [Discriminator loss: 0.114642, acc.: 95.31%] [Generator loss: 8.243597]\n",
      "15003 [Discriminator loss: 0.262715, acc.: 93.75%] [Generator loss: 8.960551]\n",
      "15004 [Discriminator loss: 0.144711, acc.: 92.19%] [Generator loss: 8.158455]\n",
      "15005 [Discriminator loss: 0.110794, acc.: 95.31%] [Generator loss: 7.359280]\n",
      "15006 [Discriminator loss: 0.029056, acc.: 100.00%] [Generator loss: 7.720227]\n",
      "15007 [Discriminator loss: 0.164927, acc.: 95.31%] [Generator loss: 7.653101]\n",
      "15008 [Discriminator loss: 0.171598, acc.: 93.75%] [Generator loss: 8.015070]\n",
      "15009 [Discriminator loss: 0.039845, acc.: 96.88%] [Generator loss: 9.175124]\n",
      "15010 [Discriminator loss: 0.054368, acc.: 98.44%] [Generator loss: 6.773836]\n",
      "15011 [Discriminator loss: 0.087569, acc.: 95.31%] [Generator loss: 7.997921]\n",
      "15012 [Discriminator loss: 0.057917, acc.: 96.88%] [Generator loss: 9.515299]\n",
      "15013 [Discriminator loss: 0.134357, acc.: 93.75%] [Generator loss: 6.674995]\n",
      "15014 [Discriminator loss: 0.137546, acc.: 93.75%] [Generator loss: 9.003755]\n",
      "15015 [Discriminator loss: 0.038167, acc.: 98.44%] [Generator loss: 8.739725]\n",
      "15016 [Discriminator loss: 0.080365, acc.: 96.88%] [Generator loss: 6.542503]\n",
      "15017 [Discriminator loss: 0.091141, acc.: 95.31%] [Generator loss: 7.969524]\n",
      "15018 [Discriminator loss: 0.050359, acc.: 96.88%] [Generator loss: 9.265290]\n",
      "15019 [Discriminator loss: 0.261476, acc.: 92.19%] [Generator loss: 5.356146]\n",
      "15020 [Discriminator loss: 0.033569, acc.: 100.00%] [Generator loss: 7.399968]\n",
      "15021 [Discriminator loss: 0.145060, acc.: 93.75%] [Generator loss: 8.430523]\n",
      "15022 [Discriminator loss: 0.025057, acc.: 98.44%] [Generator loss: 9.394579]\n",
      "15023 [Discriminator loss: 0.183054, acc.: 95.31%] [Generator loss: 6.297022]\n",
      "15024 [Discriminator loss: 0.076621, acc.: 98.44%] [Generator loss: 5.396708]\n",
      "15025 [Discriminator loss: 0.132514, acc.: 95.31%] [Generator loss: 6.774195]\n",
      "15026 [Discriminator loss: 0.047366, acc.: 98.44%] [Generator loss: 10.020715]\n",
      "15027 [Discriminator loss: 0.190706, acc.: 93.75%] [Generator loss: 7.592194]\n",
      "15028 [Discriminator loss: 0.069355, acc.: 95.31%] [Generator loss: 7.792219]\n",
      "15029 [Discriminator loss: 0.030833, acc.: 98.44%] [Generator loss: 9.223099]\n",
      "15030 [Discriminator loss: 0.039930, acc.: 100.00%] [Generator loss: 8.494104]\n",
      "15031 [Discriminator loss: 0.092483, acc.: 96.88%] [Generator loss: 9.001896]\n",
      "15032 [Discriminator loss: 0.079844, acc.: 98.44%] [Generator loss: 8.429930]\n",
      "15033 [Discriminator loss: 0.122942, acc.: 95.31%] [Generator loss: 7.501038]\n",
      "15034 [Discriminator loss: 0.042423, acc.: 100.00%] [Generator loss: 6.371729]\n",
      "15035 [Discriminator loss: 0.087099, acc.: 98.44%] [Generator loss: 7.974154]\n",
      "15036 [Discriminator loss: 0.021329, acc.: 100.00%] [Generator loss: 8.494038]\n",
      "15037 [Discriminator loss: 0.035483, acc.: 100.00%] [Generator loss: 8.565551]\n",
      "15038 [Discriminator loss: 0.151599, acc.: 95.31%] [Generator loss: 7.032638]\n",
      "15039 [Discriminator loss: 0.059958, acc.: 95.31%] [Generator loss: 6.472765]\n",
      "15040 [Discriminator loss: 0.065618, acc.: 98.44%] [Generator loss: 8.416072]\n",
      "15041 [Discriminator loss: 0.038394, acc.: 100.00%] [Generator loss: 8.276011]\n",
      "15042 [Discriminator loss: 0.032808, acc.: 100.00%] [Generator loss: 7.466457]\n",
      "15043 [Discriminator loss: 0.049459, acc.: 98.44%] [Generator loss: 7.487509]\n",
      "15044 [Discriminator loss: 0.095087, acc.: 98.44%] [Generator loss: 7.962663]\n",
      "15045 [Discriminator loss: 0.070754, acc.: 95.31%] [Generator loss: 8.323450]\n",
      "15046 [Discriminator loss: 0.109956, acc.: 93.75%] [Generator loss: 7.787627]\n",
      "15047 [Discriminator loss: 0.172383, acc.: 92.19%] [Generator loss: 7.642126]\n",
      "15048 [Discriminator loss: 0.150202, acc.: 92.19%] [Generator loss: 7.588849]\n",
      "15049 [Discriminator loss: 0.014249, acc.: 100.00%] [Generator loss: 10.150984]\n",
      "15050 [Discriminator loss: 0.167871, acc.: 90.62%] [Generator loss: 7.543768]\n",
      "15051 [Discriminator loss: 0.040795, acc.: 98.44%] [Generator loss: 8.513848]\n",
      "15052 [Discriminator loss: 0.038562, acc.: 98.44%] [Generator loss: 9.840904]\n",
      "15053 [Discriminator loss: 0.056168, acc.: 98.44%] [Generator loss: 8.591487]\n",
      "15054 [Discriminator loss: 0.069295, acc.: 98.44%] [Generator loss: 6.899238]\n",
      "15055 [Discriminator loss: 0.054332, acc.: 96.88%] [Generator loss: 8.953038]\n",
      "15056 [Discriminator loss: 0.041766, acc.: 98.44%] [Generator loss: 7.902729]\n",
      "15057 [Discriminator loss: 0.083667, acc.: 98.44%] [Generator loss: 7.148264]\n",
      "15058 [Discriminator loss: 0.059154, acc.: 98.44%] [Generator loss: 7.405042]\n",
      "15059 [Discriminator loss: 0.183646, acc.: 92.19%] [Generator loss: 6.705414]\n",
      "15060 [Discriminator loss: 0.061110, acc.: 98.44%] [Generator loss: 7.040613]\n",
      "15061 [Discriminator loss: 0.156138, acc.: 95.31%] [Generator loss: 8.941359]\n",
      "15062 [Discriminator loss: 0.023170, acc.: 100.00%] [Generator loss: 9.009530]\n",
      "15063 [Discriminator loss: 0.060450, acc.: 98.44%] [Generator loss: 9.064493]\n",
      "15064 [Discriminator loss: 0.049847, acc.: 98.44%] [Generator loss: 6.957204]\n",
      "15065 [Discriminator loss: 0.149648, acc.: 92.19%] [Generator loss: 8.057179]\n",
      "15066 [Discriminator loss: 0.232758, acc.: 87.50%] [Generator loss: 10.155924]\n",
      "15067 [Discriminator loss: 0.204990, acc.: 95.31%] [Generator loss: 8.261801]\n",
      "15068 [Discriminator loss: 0.056221, acc.: 96.88%] [Generator loss: 7.316249]\n",
      "15069 [Discriminator loss: 0.060599, acc.: 100.00%] [Generator loss: 7.809761]\n",
      "15070 [Discriminator loss: 0.024169, acc.: 98.44%] [Generator loss: 7.500888]\n",
      "15071 [Discriminator loss: 0.047981, acc.: 98.44%] [Generator loss: 5.935215]\n",
      "15072 [Discriminator loss: 0.055943, acc.: 98.44%] [Generator loss: 7.945113]\n",
      "15073 [Discriminator loss: 0.129603, acc.: 93.75%] [Generator loss: 7.048627]\n",
      "15074 [Discriminator loss: 0.344696, acc.: 84.38%] [Generator loss: 9.402917]\n",
      "15075 [Discriminator loss: 0.043416, acc.: 98.44%] [Generator loss: 11.048284]\n",
      "15076 [Discriminator loss: 0.108206, acc.: 96.88%] [Generator loss: 9.138915]\n",
      "15077 [Discriminator loss: 0.065072, acc.: 95.31%] [Generator loss: 8.809027]\n",
      "15078 [Discriminator loss: 0.068359, acc.: 95.31%] [Generator loss: 6.822904]\n",
      "15079 [Discriminator loss: 0.042220, acc.: 98.44%] [Generator loss: 8.628895]\n",
      "15080 [Discriminator loss: 0.086092, acc.: 95.31%] [Generator loss: 8.127590]\n",
      "15081 [Discriminator loss: 0.136680, acc.: 93.75%] [Generator loss: 7.464715]\n",
      "15082 [Discriminator loss: 0.129689, acc.: 95.31%] [Generator loss: 7.276038]\n",
      "15083 [Discriminator loss: 0.063670, acc.: 98.44%] [Generator loss: 7.351916]\n",
      "15084 [Discriminator loss: 0.151016, acc.: 95.31%] [Generator loss: 6.814025]\n",
      "15085 [Discriminator loss: 0.035290, acc.: 98.44%] [Generator loss: 7.731710]\n",
      "15086 [Discriminator loss: 0.060861, acc.: 96.88%] [Generator loss: 8.467768]\n",
      "15087 [Discriminator loss: 0.075262, acc.: 96.88%] [Generator loss: 8.065103]\n",
      "15088 [Discriminator loss: 0.136557, acc.: 92.19%] [Generator loss: 8.782521]\n",
      "15089 [Discriminator loss: 0.078545, acc.: 98.44%] [Generator loss: 7.708382]\n",
      "15090 [Discriminator loss: 0.157404, acc.: 92.19%] [Generator loss: 6.220634]\n",
      "15091 [Discriminator loss: 0.154770, acc.: 93.75%] [Generator loss: 8.713563]\n",
      "15092 [Discriminator loss: 0.101095, acc.: 93.75%] [Generator loss: 7.466820]\n",
      "15093 [Discriminator loss: 0.083061, acc.: 98.44%] [Generator loss: 7.294864]\n",
      "15094 [Discriminator loss: 0.047972, acc.: 98.44%] [Generator loss: 6.983507]\n",
      "15095 [Discriminator loss: 0.083924, acc.: 95.31%] [Generator loss: 9.700115]\n",
      "15096 [Discriminator loss: 0.049458, acc.: 98.44%] [Generator loss: 8.721780]\n",
      "15097 [Discriminator loss: 0.012248, acc.: 100.00%] [Generator loss: 9.812833]\n",
      "15098 [Discriminator loss: 0.024730, acc.: 100.00%] [Generator loss: 7.318897]\n",
      "15099 [Discriminator loss: 0.039702, acc.: 96.88%] [Generator loss: 7.834391]\n",
      "15100 [Discriminator loss: 0.050962, acc.: 95.31%] [Generator loss: 7.212311]\n",
      "15101 [Discriminator loss: 0.047235, acc.: 98.44%] [Generator loss: 6.391872]\n",
      "15102 [Discriminator loss: 0.173342, acc.: 90.62%] [Generator loss: 7.502765]\n",
      "15103 [Discriminator loss: 0.054596, acc.: 98.44%] [Generator loss: 8.892357]\n",
      "15104 [Discriminator loss: 0.083220, acc.: 96.88%] [Generator loss: 7.940538]\n",
      "15105 [Discriminator loss: 0.041341, acc.: 100.00%] [Generator loss: 6.805927]\n",
      "15106 [Discriminator loss: 0.028054, acc.: 100.00%] [Generator loss: 7.253072]\n",
      "15107 [Discriminator loss: 0.052552, acc.: 98.44%] [Generator loss: 8.029408]\n",
      "15108 [Discriminator loss: 0.039273, acc.: 98.44%] [Generator loss: 8.397808]\n",
      "15109 [Discriminator loss: 0.054420, acc.: 98.44%] [Generator loss: 8.422621]\n",
      "15110 [Discriminator loss: 0.163140, acc.: 92.19%] [Generator loss: 6.695643]\n",
      "15111 [Discriminator loss: 0.088061, acc.: 95.31%] [Generator loss: 9.235655]\n",
      "15112 [Discriminator loss: 0.035526, acc.: 98.44%] [Generator loss: 9.603073]\n",
      "15113 [Discriminator loss: 0.046052, acc.: 98.44%] [Generator loss: 7.276643]\n",
      "15114 [Discriminator loss: 0.052341, acc.: 98.44%] [Generator loss: 6.185677]\n",
      "15115 [Discriminator loss: 0.063119, acc.: 98.44%] [Generator loss: 7.159243]\n",
      "15116 [Discriminator loss: 0.032958, acc.: 98.44%] [Generator loss: 7.913273]\n",
      "15117 [Discriminator loss: 0.140342, acc.: 93.75%] [Generator loss: 7.468091]\n",
      "15118 [Discriminator loss: 0.121649, acc.: 95.31%] [Generator loss: 6.996063]\n",
      "15119 [Discriminator loss: 0.112132, acc.: 95.31%] [Generator loss: 7.887831]\n",
      "15120 [Discriminator loss: 0.082907, acc.: 96.88%] [Generator loss: 7.257018]\n",
      "15121 [Discriminator loss: 0.053443, acc.: 98.44%] [Generator loss: 7.940927]\n",
      "15122 [Discriminator loss: 0.082084, acc.: 95.31%] [Generator loss: 8.167372]\n",
      "15123 [Discriminator loss: 0.133581, acc.: 93.75%] [Generator loss: 8.956047]\n",
      "15124 [Discriminator loss: 0.253858, acc.: 90.62%] [Generator loss: 5.260625]\n",
      "15125 [Discriminator loss: 0.064094, acc.: 98.44%] [Generator loss: 7.792132]\n",
      "15126 [Discriminator loss: 0.088730, acc.: 93.75%] [Generator loss: 9.128457]\n",
      "15127 [Discriminator loss: 0.172694, acc.: 92.19%] [Generator loss: 9.826334]\n",
      "15128 [Discriminator loss: 0.055755, acc.: 96.88%] [Generator loss: 8.439636]\n",
      "15129 [Discriminator loss: 0.127090, acc.: 92.19%] [Generator loss: 8.758694]\n",
      "15130 [Discriminator loss: 0.065646, acc.: 95.31%] [Generator loss: 9.947902]\n",
      "15131 [Discriminator loss: 0.042314, acc.: 96.88%] [Generator loss: 10.388695]\n",
      "15132 [Discriminator loss: 0.135532, acc.: 95.31%] [Generator loss: 7.188674]\n",
      "15133 [Discriminator loss: 0.245001, acc.: 89.06%] [Generator loss: 7.693304]\n",
      "15134 [Discriminator loss: 0.067238, acc.: 96.88%] [Generator loss: 6.665694]\n",
      "15135 [Discriminator loss: 0.101950, acc.: 95.31%] [Generator loss: 7.437802]\n",
      "15136 [Discriminator loss: 0.037849, acc.: 100.00%] [Generator loss: 7.099475]\n",
      "15137 [Discriminator loss: 0.090668, acc.: 96.88%] [Generator loss: 6.983007]\n",
      "15138 [Discriminator loss: 0.069528, acc.: 98.44%] [Generator loss: 6.623655]\n",
      "15139 [Discriminator loss: 0.015216, acc.: 100.00%] [Generator loss: 7.102141]\n",
      "15140 [Discriminator loss: 0.027707, acc.: 100.00%] [Generator loss: 9.951702]\n",
      "15141 [Discriminator loss: 0.159835, acc.: 92.19%] [Generator loss: 7.584380]\n",
      "15142 [Discriminator loss: 0.035561, acc.: 98.44%] [Generator loss: 7.505358]\n",
      "15143 [Discriminator loss: 0.117987, acc.: 93.75%] [Generator loss: 5.801206]\n",
      "15144 [Discriminator loss: 0.106133, acc.: 96.88%] [Generator loss: 6.569857]\n",
      "15145 [Discriminator loss: 0.084469, acc.: 93.75%] [Generator loss: 6.725472]\n",
      "15146 [Discriminator loss: 0.030833, acc.: 100.00%] [Generator loss: 7.640702]\n",
      "15147 [Discriminator loss: 0.066769, acc.: 100.00%] [Generator loss: 6.863032]\n",
      "15148 [Discriminator loss: 0.206192, acc.: 93.75%] [Generator loss: 6.066296]\n",
      "15149 [Discriminator loss: 0.031442, acc.: 100.00%] [Generator loss: 8.670769]\n",
      "15150 [Discriminator loss: 0.052044, acc.: 100.00%] [Generator loss: 8.227063]\n",
      "15151 [Discriminator loss: 0.074370, acc.: 98.44%] [Generator loss: 6.698588]\n",
      "15152 [Discriminator loss: 0.057239, acc.: 96.88%] [Generator loss: 8.636748]\n",
      "15153 [Discriminator loss: 0.023205, acc.: 100.00%] [Generator loss: 6.820844]\n",
      "15154 [Discriminator loss: 0.031606, acc.: 98.44%] [Generator loss: 7.431391]\n",
      "15155 [Discriminator loss: 0.154131, acc.: 92.19%] [Generator loss: 7.930549]\n",
      "15156 [Discriminator loss: 0.180407, acc.: 90.62%] [Generator loss: 7.658901]\n",
      "15157 [Discriminator loss: 0.078276, acc.: 95.31%] [Generator loss: 8.233461]\n",
      "15158 [Discriminator loss: 0.058616, acc.: 98.44%] [Generator loss: 8.854352]\n",
      "15159 [Discriminator loss: 0.124694, acc.: 95.31%] [Generator loss: 7.443162]\n",
      "15160 [Discriminator loss: 0.217318, acc.: 93.75%] [Generator loss: 7.587446]\n",
      "15161 [Discriminator loss: 0.049156, acc.: 96.88%] [Generator loss: 8.234135]\n",
      "15162 [Discriminator loss: 0.102253, acc.: 95.31%] [Generator loss: 7.341307]\n",
      "15163 [Discriminator loss: 0.003960, acc.: 100.00%] [Generator loss: 9.283978]\n",
      "15164 [Discriminator loss: 0.050082, acc.: 96.88%] [Generator loss: 6.379179]\n",
      "15165 [Discriminator loss: 0.110836, acc.: 96.88%] [Generator loss: 8.170947]\n",
      "15166 [Discriminator loss: 0.097609, acc.: 95.31%] [Generator loss: 8.949535]\n",
      "15167 [Discriminator loss: 0.113447, acc.: 96.88%] [Generator loss: 9.135417]\n",
      "15168 [Discriminator loss: 0.046763, acc.: 98.44%] [Generator loss: 8.149483]\n",
      "15169 [Discriminator loss: 0.168869, acc.: 93.75%] [Generator loss: 7.149088]\n",
      "15170 [Discriminator loss: 0.013996, acc.: 100.00%] [Generator loss: 7.384797]\n",
      "15171 [Discriminator loss: 0.076530, acc.: 95.31%] [Generator loss: 6.755466]\n",
      "15172 [Discriminator loss: 0.088568, acc.: 98.44%] [Generator loss: 6.118189]\n",
      "15173 [Discriminator loss: 0.052843, acc.: 98.44%] [Generator loss: 8.109340]\n",
      "15174 [Discriminator loss: 0.028389, acc.: 100.00%] [Generator loss: 6.558945]\n",
      "15175 [Discriminator loss: 0.120219, acc.: 92.19%] [Generator loss: 7.357362]\n",
      "15176 [Discriminator loss: 0.106862, acc.: 93.75%] [Generator loss: 8.092749]\n",
      "15177 [Discriminator loss: 0.083560, acc.: 96.88%] [Generator loss: 7.443614]\n",
      "15178 [Discriminator loss: 0.219249, acc.: 95.31%] [Generator loss: 7.132272]\n",
      "15179 [Discriminator loss: 0.069429, acc.: 96.88%] [Generator loss: 6.601417]\n",
      "15180 [Discriminator loss: 0.044641, acc.: 98.44%] [Generator loss: 8.122443]\n",
      "15181 [Discriminator loss: 0.066624, acc.: 96.88%] [Generator loss: 8.048853]\n",
      "15182 [Discriminator loss: 0.037685, acc.: 96.88%] [Generator loss: 9.718851]\n",
      "15183 [Discriminator loss: 0.105388, acc.: 95.31%] [Generator loss: 9.736261]\n",
      "15184 [Discriminator loss: 0.054938, acc.: 98.44%] [Generator loss: 7.762187]\n",
      "15185 [Discriminator loss: 0.144872, acc.: 92.19%] [Generator loss: 7.536339]\n",
      "15186 [Discriminator loss: 0.043861, acc.: 98.44%] [Generator loss: 8.888479]\n",
      "15187 [Discriminator loss: 0.143257, acc.: 92.19%] [Generator loss: 5.774620]\n",
      "15188 [Discriminator loss: 0.207625, acc.: 92.19%] [Generator loss: 7.587626]\n",
      "15189 [Discriminator loss: 0.096214, acc.: 93.75%] [Generator loss: 8.941401]\n",
      "15190 [Discriminator loss: 0.032173, acc.: 100.00%] [Generator loss: 9.036503]\n",
      "15191 [Discriminator loss: 0.026660, acc.: 100.00%] [Generator loss: 7.304790]\n",
      "15192 [Discriminator loss: 0.144412, acc.: 95.31%] [Generator loss: 4.636950]\n",
      "15193 [Discriminator loss: 0.197965, acc.: 93.75%] [Generator loss: 8.357500]\n",
      "15194 [Discriminator loss: 0.122249, acc.: 98.44%] [Generator loss: 7.497050]\n",
      "15195 [Discriminator loss: 0.019770, acc.: 100.00%] [Generator loss: 8.340164]\n",
      "15196 [Discriminator loss: 0.106142, acc.: 95.31%] [Generator loss: 6.733344]\n",
      "15197 [Discriminator loss: 0.126044, acc.: 95.31%] [Generator loss: 9.081488]\n",
      "15198 [Discriminator loss: 0.073146, acc.: 96.88%] [Generator loss: 8.895432]\n",
      "15199 [Discriminator loss: 0.077849, acc.: 96.88%] [Generator loss: 8.081709]\n",
      "15200 [Discriminator loss: 0.014433, acc.: 100.00%] [Generator loss: 7.273314]\n",
      "15201 [Discriminator loss: 0.045182, acc.: 98.44%] [Generator loss: 8.671014]\n",
      "15202 [Discriminator loss: 0.020663, acc.: 100.00%] [Generator loss: 9.715990]\n",
      "15203 [Discriminator loss: 0.072741, acc.: 96.88%] [Generator loss: 6.551333]\n",
      "15204 [Discriminator loss: 0.073386, acc.: 96.88%] [Generator loss: 6.969653]\n",
      "15205 [Discriminator loss: 0.029558, acc.: 98.44%] [Generator loss: 6.821124]\n",
      "15206 [Discriminator loss: 0.144790, acc.: 96.88%] [Generator loss: 5.313693]\n",
      "15207 [Discriminator loss: 0.121716, acc.: 93.75%] [Generator loss: 7.013550]\n",
      "15208 [Discriminator loss: 0.071428, acc.: 98.44%] [Generator loss: 6.672291]\n",
      "15209 [Discriminator loss: 0.050685, acc.: 98.44%] [Generator loss: 7.086975]\n",
      "15210 [Discriminator loss: 0.062242, acc.: 98.44%] [Generator loss: 6.071009]\n",
      "15211 [Discriminator loss: 0.036509, acc.: 98.44%] [Generator loss: 6.373731]\n",
      "15212 [Discriminator loss: 0.022998, acc.: 98.44%] [Generator loss: 7.008514]\n",
      "15213 [Discriminator loss: 0.012698, acc.: 100.00%] [Generator loss: 7.451241]\n",
      "15214 [Discriminator loss: 0.115668, acc.: 96.88%] [Generator loss: 7.001180]\n",
      "15215 [Discriminator loss: 0.058968, acc.: 98.44%] [Generator loss: 8.500304]\n",
      "15216 [Discriminator loss: 0.057142, acc.: 98.44%] [Generator loss: 9.224735]\n",
      "15217 [Discriminator loss: 0.042416, acc.: 98.44%] [Generator loss: 7.316996]\n",
      "15218 [Discriminator loss: 0.044069, acc.: 96.88%] [Generator loss: 7.083740]\n",
      "15219 [Discriminator loss: 0.154501, acc.: 92.19%] [Generator loss: 9.050204]\n",
      "15220 [Discriminator loss: 0.018543, acc.: 100.00%] [Generator loss: 10.505827]\n",
      "15221 [Discriminator loss: 0.211998, acc.: 92.19%] [Generator loss: 8.419132]\n",
      "15222 [Discriminator loss: 0.057016, acc.: 96.88%] [Generator loss: 8.088505]\n",
      "15223 [Discriminator loss: 0.028808, acc.: 100.00%] [Generator loss: 8.863893]\n",
      "15224 [Discriminator loss: 0.067061, acc.: 95.31%] [Generator loss: 8.122520]\n",
      "15225 [Discriminator loss: 0.102798, acc.: 96.88%] [Generator loss: 7.287385]\n",
      "15226 [Discriminator loss: 0.091116, acc.: 96.88%] [Generator loss: 8.002727]\n",
      "15227 [Discriminator loss: 0.015719, acc.: 100.00%] [Generator loss: 8.359856]\n",
      "15228 [Discriminator loss: 0.026527, acc.: 98.44%] [Generator loss: 7.439536]\n",
      "15229 [Discriminator loss: 0.302522, acc.: 92.19%] [Generator loss: 7.258875]\n",
      "15230 [Discriminator loss: 0.054096, acc.: 98.44%] [Generator loss: 6.206765]\n",
      "15231 [Discriminator loss: 0.331395, acc.: 90.62%] [Generator loss: 8.488598]\n",
      "15232 [Discriminator loss: 0.041699, acc.: 98.44%] [Generator loss: 8.517443]\n",
      "15233 [Discriminator loss: 0.085072, acc.: 96.88%] [Generator loss: 7.864740]\n",
      "15234 [Discriminator loss: 0.056224, acc.: 98.44%] [Generator loss: 6.666194]\n",
      "15235 [Discriminator loss: 0.035770, acc.: 98.44%] [Generator loss: 6.870721]\n",
      "15236 [Discriminator loss: 0.129258, acc.: 93.75%] [Generator loss: 8.031111]\n",
      "15237 [Discriminator loss: 0.017230, acc.: 100.00%] [Generator loss: 10.002868]\n",
      "15238 [Discriminator loss: 0.059286, acc.: 100.00%] [Generator loss: 8.775202]\n",
      "15239 [Discriminator loss: 0.065177, acc.: 98.44%] [Generator loss: 6.785368]\n",
      "15240 [Discriminator loss: 0.080865, acc.: 96.88%] [Generator loss: 7.599259]\n",
      "15241 [Discriminator loss: 0.069141, acc.: 96.88%] [Generator loss: 7.680247]\n",
      "15242 [Discriminator loss: 0.143143, acc.: 93.75%] [Generator loss: 7.269550]\n",
      "15243 [Discriminator loss: 0.099568, acc.: 95.31%] [Generator loss: 7.431849]\n",
      "15244 [Discriminator loss: 0.042635, acc.: 98.44%] [Generator loss: 9.802894]\n",
      "15245 [Discriminator loss: 0.141701, acc.: 95.31%] [Generator loss: 8.171465]\n",
      "15246 [Discriminator loss: 0.154188, acc.: 93.75%] [Generator loss: 4.323476]\n",
      "15247 [Discriminator loss: 0.138209, acc.: 93.75%] [Generator loss: 7.643035]\n",
      "15248 [Discriminator loss: 0.018618, acc.: 100.00%] [Generator loss: 8.932493]\n",
      "15249 [Discriminator loss: 0.047836, acc.: 98.44%] [Generator loss: 7.549545]\n",
      "15250 [Discriminator loss: 0.091827, acc.: 95.31%] [Generator loss: 7.365085]\n",
      "15251 [Discriminator loss: 0.094939, acc.: 95.31%] [Generator loss: 8.814441]\n",
      "15252 [Discriminator loss: 0.077179, acc.: 93.75%] [Generator loss: 8.740010]\n",
      "15253 [Discriminator loss: 0.056051, acc.: 96.88%] [Generator loss: 8.031855]\n",
      "15254 [Discriminator loss: 0.025881, acc.: 100.00%] [Generator loss: 7.999583]\n",
      "15255 [Discriminator loss: 0.036213, acc.: 100.00%] [Generator loss: 7.669256]\n",
      "15256 [Discriminator loss: 0.217970, acc.: 90.62%] [Generator loss: 7.505535]\n",
      "15257 [Discriminator loss: 0.051976, acc.: 96.88%] [Generator loss: 8.019655]\n",
      "15258 [Discriminator loss: 0.074044, acc.: 96.88%] [Generator loss: 6.923711]\n",
      "15259 [Discriminator loss: 0.034750, acc.: 96.88%] [Generator loss: 7.000559]\n",
      "15260 [Discriminator loss: 0.108867, acc.: 98.44%] [Generator loss: 8.706882]\n",
      "15261 [Discriminator loss: 0.038225, acc.: 100.00%] [Generator loss: 7.604024]\n",
      "15262 [Discriminator loss: 0.142791, acc.: 93.75%] [Generator loss: 8.064684]\n",
      "15263 [Discriminator loss: 0.118767, acc.: 96.88%] [Generator loss: 8.573105]\n",
      "15264 [Discriminator loss: 0.050324, acc.: 98.44%] [Generator loss: 9.170380]\n",
      "15265 [Discriminator loss: 0.144147, acc.: 93.75%] [Generator loss: 8.075971]\n",
      "15266 [Discriminator loss: 0.048682, acc.: 98.44%] [Generator loss: 9.109018]\n",
      "15267 [Discriminator loss: 0.167953, acc.: 93.75%] [Generator loss: 9.182074]\n",
      "15268 [Discriminator loss: 0.103044, acc.: 96.88%] [Generator loss: 7.837223]\n",
      "15269 [Discriminator loss: 0.167765, acc.: 93.75%] [Generator loss: 8.665678]\n",
      "15270 [Discriminator loss: 0.027740, acc.: 98.44%] [Generator loss: 10.353077]\n",
      "15271 [Discriminator loss: 0.024772, acc.: 100.00%] [Generator loss: 8.646044]\n",
      "15272 [Discriminator loss: 0.130645, acc.: 93.75%] [Generator loss: 7.484383]\n",
      "15273 [Discriminator loss: 0.038814, acc.: 98.44%] [Generator loss: 7.653279]\n",
      "15274 [Discriminator loss: 0.038802, acc.: 100.00%] [Generator loss: 7.735864]\n",
      "15275 [Discriminator loss: 0.094219, acc.: 93.75%] [Generator loss: 8.323075]\n",
      "15276 [Discriminator loss: 0.038504, acc.: 98.44%] [Generator loss: 9.203377]\n",
      "15277 [Discriminator loss: 0.072142, acc.: 98.44%] [Generator loss: 8.666117]\n",
      "15278 [Discriminator loss: 0.041214, acc.: 100.00%] [Generator loss: 8.182767]\n",
      "15279 [Discriminator loss: 0.082755, acc.: 95.31%] [Generator loss: 6.765394]\n",
      "15280 [Discriminator loss: 0.029018, acc.: 100.00%] [Generator loss: 8.072207]\n",
      "15281 [Discriminator loss: 0.019333, acc.: 100.00%] [Generator loss: 8.623388]\n",
      "15282 [Discriminator loss: 0.173052, acc.: 95.31%] [Generator loss: 6.289883]\n",
      "15283 [Discriminator loss: 0.080344, acc.: 95.31%] [Generator loss: 8.544605]\n",
      "15284 [Discriminator loss: 0.039789, acc.: 98.44%] [Generator loss: 9.072401]\n",
      "15285 [Discriminator loss: 0.044600, acc.: 100.00%] [Generator loss: 8.002510]\n",
      "15286 [Discriminator loss: 0.176042, acc.: 95.31%] [Generator loss: 6.974180]\n",
      "15287 [Discriminator loss: 0.051768, acc.: 98.44%] [Generator loss: 7.661313]\n",
      "15288 [Discriminator loss: 0.101265, acc.: 95.31%] [Generator loss: 7.882560]\n",
      "15289 [Discriminator loss: 0.111899, acc.: 93.75%] [Generator loss: 8.396043]\n",
      "15290 [Discriminator loss: 0.166247, acc.: 93.75%] [Generator loss: 4.590221]\n",
      "15291 [Discriminator loss: 0.317350, acc.: 87.50%] [Generator loss: 10.241734]\n",
      "15292 [Discriminator loss: 0.138013, acc.: 96.88%] [Generator loss: 9.876472]\n",
      "15293 [Discriminator loss: 0.167533, acc.: 92.19%] [Generator loss: 9.406948]\n",
      "15294 [Discriminator loss: 0.050377, acc.: 98.44%] [Generator loss: 7.189752]\n",
      "15295 [Discriminator loss: 0.078801, acc.: 98.44%] [Generator loss: 8.054702]\n",
      "15296 [Discriminator loss: 0.022588, acc.: 100.00%] [Generator loss: 7.363733]\n",
      "15297 [Discriminator loss: 0.020467, acc.: 100.00%] [Generator loss: 8.610345]\n",
      "15298 [Discriminator loss: 0.079701, acc.: 96.88%] [Generator loss: 9.452587]\n",
      "15299 [Discriminator loss: 0.122905, acc.: 95.31%] [Generator loss: 7.132377]\n",
      "15300 [Discriminator loss: 0.043704, acc.: 98.44%] [Generator loss: 7.975266]\n",
      "15301 [Discriminator loss: 0.097598, acc.: 96.88%] [Generator loss: 8.752691]\n",
      "15302 [Discriminator loss: 0.026840, acc.: 98.44%] [Generator loss: 7.854807]\n",
      "15303 [Discriminator loss: 0.053348, acc.: 96.88%] [Generator loss: 7.688882]\n",
      "15304 [Discriminator loss: 0.167988, acc.: 92.19%] [Generator loss: 7.903368]\n",
      "15305 [Discriminator loss: 0.281709, acc.: 87.50%] [Generator loss: 7.618392]\n",
      "15306 [Discriminator loss: 0.070716, acc.: 96.88%] [Generator loss: 7.270689]\n",
      "15307 [Discriminator loss: 0.060288, acc.: 96.88%] [Generator loss: 6.172152]\n",
      "15308 [Discriminator loss: 0.112044, acc.: 93.75%] [Generator loss: 6.397288]\n",
      "15309 [Discriminator loss: 0.104037, acc.: 95.31%] [Generator loss: 10.007128]\n",
      "15310 [Discriminator loss: 0.086235, acc.: 96.88%] [Generator loss: 6.980411]\n",
      "15311 [Discriminator loss: 0.224721, acc.: 96.88%] [Generator loss: 8.899868]\n",
      "15312 [Discriminator loss: 0.048708, acc.: 98.44%] [Generator loss: 10.501789]\n",
      "15313 [Discriminator loss: 0.341772, acc.: 89.06%] [Generator loss: 7.481092]\n",
      "15314 [Discriminator loss: 0.112531, acc.: 96.88%] [Generator loss: 8.799782]\n",
      "15315 [Discriminator loss: 0.039458, acc.: 100.00%] [Generator loss: 8.929857]\n",
      "15316 [Discriminator loss: 0.188978, acc.: 92.19%] [Generator loss: 5.218328]\n",
      "15317 [Discriminator loss: 0.069167, acc.: 96.88%] [Generator loss: 7.431962]\n",
      "15318 [Discriminator loss: 0.078483, acc.: 95.31%] [Generator loss: 8.405945]\n",
      "15319 [Discriminator loss: 0.068654, acc.: 96.88%] [Generator loss: 9.470047]\n",
      "15320 [Discriminator loss: 0.023183, acc.: 100.00%] [Generator loss: 8.600626]\n",
      "15321 [Discriminator loss: 0.089604, acc.: 96.88%] [Generator loss: 8.145427]\n",
      "15322 [Discriminator loss: 0.061431, acc.: 98.44%] [Generator loss: 8.367477]\n",
      "15323 [Discriminator loss: 0.060624, acc.: 98.44%] [Generator loss: 7.296151]\n",
      "15324 [Discriminator loss: 0.013295, acc.: 100.00%] [Generator loss: 7.744228]\n",
      "15325 [Discriminator loss: 0.169995, acc.: 92.19%] [Generator loss: 8.175914]\n",
      "15326 [Discriminator loss: 0.264573, acc.: 92.19%] [Generator loss: 8.874838]\n",
      "15327 [Discriminator loss: 0.054310, acc.: 98.44%] [Generator loss: 7.882220]\n",
      "15328 [Discriminator loss: 0.086510, acc.: 96.88%] [Generator loss: 7.018201]\n",
      "15329 [Discriminator loss: 0.061068, acc.: 98.44%] [Generator loss: 7.466021]\n",
      "15330 [Discriminator loss: 0.016813, acc.: 100.00%] [Generator loss: 7.161647]\n",
      "15331 [Discriminator loss: 0.061262, acc.: 98.44%] [Generator loss: 7.683548]\n",
      "15332 [Discriminator loss: 0.148948, acc.: 95.31%] [Generator loss: 7.492844]\n",
      "15333 [Discriminator loss: 0.068086, acc.: 98.44%] [Generator loss: 6.862491]\n",
      "15334 [Discriminator loss: 0.080577, acc.: 96.88%] [Generator loss: 6.539593]\n",
      "15335 [Discriminator loss: 0.042011, acc.: 98.44%] [Generator loss: 8.018664]\n",
      "15336 [Discriminator loss: 0.116169, acc.: 93.75%] [Generator loss: 8.433739]\n",
      "15337 [Discriminator loss: 0.016203, acc.: 100.00%] [Generator loss: 8.947174]\n",
      "15338 [Discriminator loss: 0.102246, acc.: 96.88%] [Generator loss: 7.657225]\n",
      "15339 [Discriminator loss: 0.034662, acc.: 100.00%] [Generator loss: 7.585696]\n",
      "15340 [Discriminator loss: 0.153692, acc.: 95.31%] [Generator loss: 8.506918]\n",
      "15341 [Discriminator loss: 0.083977, acc.: 95.31%] [Generator loss: 6.972888]\n",
      "15342 [Discriminator loss: 0.105663, acc.: 93.75%] [Generator loss: 6.954247]\n",
      "15343 [Discriminator loss: 0.069523, acc.: 98.44%] [Generator loss: 8.235743]\n",
      "15344 [Discriminator loss: 0.046483, acc.: 98.44%] [Generator loss: 8.055464]\n",
      "15345 [Discriminator loss: 0.107911, acc.: 95.31%] [Generator loss: 7.841958]\n",
      "15346 [Discriminator loss: 0.028660, acc.: 98.44%] [Generator loss: 7.889337]\n",
      "15347 [Discriminator loss: 0.032017, acc.: 100.00%] [Generator loss: 6.618458]\n",
      "15348 [Discriminator loss: 0.169399, acc.: 95.31%] [Generator loss: 7.995491]\n",
      "15349 [Discriminator loss: 0.056503, acc.: 98.44%] [Generator loss: 8.928243]\n",
      "15350 [Discriminator loss: 0.097684, acc.: 96.88%] [Generator loss: 7.841239]\n",
      "15351 [Discriminator loss: 0.036143, acc.: 98.44%] [Generator loss: 7.535533]\n",
      "15352 [Discriminator loss: 0.219017, acc.: 93.75%] [Generator loss: 7.529293]\n",
      "15353 [Discriminator loss: 0.010853, acc.: 100.00%] [Generator loss: 8.349126]\n",
      "15354 [Discriminator loss: 0.061746, acc.: 98.44%] [Generator loss: 8.185715]\n",
      "15355 [Discriminator loss: 0.181243, acc.: 93.75%] [Generator loss: 9.145632]\n",
      "15356 [Discriminator loss: 0.033736, acc.: 100.00%] [Generator loss: 10.096754]\n",
      "15357 [Discriminator loss: 0.111234, acc.: 93.75%] [Generator loss: 6.803734]\n",
      "15358 [Discriminator loss: 0.283045, acc.: 89.06%] [Generator loss: 9.036220]\n",
      "15359 [Discriminator loss: 0.090128, acc.: 96.88%] [Generator loss: 8.780678]\n",
      "15360 [Discriminator loss: 0.085402, acc.: 95.31%] [Generator loss: 7.281081]\n",
      "15361 [Discriminator loss: 0.078397, acc.: 95.31%] [Generator loss: 6.104739]\n",
      "15362 [Discriminator loss: 0.060925, acc.: 98.44%] [Generator loss: 7.179972]\n",
      "15363 [Discriminator loss: 0.095246, acc.: 96.88%] [Generator loss: 7.785785]\n",
      "15364 [Discriminator loss: 0.138929, acc.: 95.31%] [Generator loss: 7.056196]\n",
      "15365 [Discriminator loss: 0.140844, acc.: 93.75%] [Generator loss: 7.833953]\n",
      "15366 [Discriminator loss: 0.025278, acc.: 100.00%] [Generator loss: 8.282459]\n",
      "15367 [Discriminator loss: 0.169507, acc.: 92.19%] [Generator loss: 6.821538]\n",
      "15368 [Discriminator loss: 0.072921, acc.: 96.88%] [Generator loss: 6.927186]\n",
      "15369 [Discriminator loss: 0.093235, acc.: 96.88%] [Generator loss: 8.381296]\n",
      "15370 [Discriminator loss: 0.108218, acc.: 96.88%] [Generator loss: 9.154220]\n",
      "15371 [Discriminator loss: 0.118674, acc.: 93.75%] [Generator loss: 7.777406]\n",
      "15372 [Discriminator loss: 0.127840, acc.: 93.75%] [Generator loss: 8.126966]\n",
      "15373 [Discriminator loss: 0.057980, acc.: 96.88%] [Generator loss: 7.947841]\n",
      "15374 [Discriminator loss: 0.123701, acc.: 96.88%] [Generator loss: 7.381166]\n",
      "15375 [Discriminator loss: 0.038562, acc.: 100.00%] [Generator loss: 7.610078]\n",
      "15376 [Discriminator loss: 0.039595, acc.: 98.44%] [Generator loss: 7.908073]\n",
      "15377 [Discriminator loss: 0.110356, acc.: 95.31%] [Generator loss: 7.162540]\n",
      "15378 [Discriminator loss: 0.092701, acc.: 96.88%] [Generator loss: 7.619403]\n",
      "15379 [Discriminator loss: 0.110539, acc.: 95.31%] [Generator loss: 8.202291]\n",
      "15380 [Discriminator loss: 0.044545, acc.: 96.88%] [Generator loss: 8.457745]\n",
      "15381 [Discriminator loss: 0.107570, acc.: 95.31%] [Generator loss: 8.250637]\n",
      "15382 [Discriminator loss: 0.201068, acc.: 87.50%] [Generator loss: 7.414467]\n",
      "15383 [Discriminator loss: 0.066689, acc.: 98.44%] [Generator loss: 8.693096]\n",
      "15384 [Discriminator loss: 0.144303, acc.: 95.31%] [Generator loss: 8.848667]\n",
      "15385 [Discriminator loss: 0.086465, acc.: 96.88%] [Generator loss: 8.369112]\n",
      "15386 [Discriminator loss: 0.046560, acc.: 98.44%] [Generator loss: 7.769030]\n",
      "15387 [Discriminator loss: 0.084396, acc.: 96.88%] [Generator loss: 8.325444]\n",
      "15388 [Discriminator loss: 0.079423, acc.: 96.88%] [Generator loss: 8.166693]\n",
      "15389 [Discriminator loss: 0.095511, acc.: 95.31%] [Generator loss: 8.085742]\n",
      "15390 [Discriminator loss: 0.079707, acc.: 96.88%] [Generator loss: 6.563540]\n",
      "15391 [Discriminator loss: 0.244553, acc.: 87.50%] [Generator loss: 10.505846]\n",
      "15392 [Discriminator loss: 0.101751, acc.: 96.88%] [Generator loss: 12.345391]\n",
      "15393 [Discriminator loss: 0.231774, acc.: 87.50%] [Generator loss: 8.415165]\n",
      "15394 [Discriminator loss: 0.033578, acc.: 100.00%] [Generator loss: 8.048940]\n",
      "15395 [Discriminator loss: 0.016092, acc.: 100.00%] [Generator loss: 7.110435]\n",
      "15396 [Discriminator loss: 0.067338, acc.: 98.44%] [Generator loss: 6.309173]\n",
      "15397 [Discriminator loss: 0.155003, acc.: 93.75%] [Generator loss: 8.391294]\n",
      "15398 [Discriminator loss: 0.118058, acc.: 93.75%] [Generator loss: 8.555035]\n",
      "15399 [Discriminator loss: 0.182010, acc.: 93.75%] [Generator loss: 9.583099]\n",
      "15400 [Discriminator loss: 0.243691, acc.: 93.75%] [Generator loss: 7.953055]\n",
      "15401 [Discriminator loss: 0.129762, acc.: 92.19%] [Generator loss: 8.160137]\n",
      "15402 [Discriminator loss: 0.011510, acc.: 100.00%] [Generator loss: 8.516152]\n",
      "15403 [Discriminator loss: 0.103385, acc.: 96.88%] [Generator loss: 7.384260]\n",
      "15404 [Discriminator loss: 0.006510, acc.: 100.00%] [Generator loss: 8.717645]\n",
      "15405 [Discriminator loss: 0.058638, acc.: 96.88%] [Generator loss: 8.372231]\n",
      "15406 [Discriminator loss: 0.016211, acc.: 100.00%] [Generator loss: 6.934896]\n",
      "15407 [Discriminator loss: 0.036736, acc.: 98.44%] [Generator loss: 7.015486]\n",
      "15408 [Discriminator loss: 0.043600, acc.: 98.44%] [Generator loss: 7.032561]\n",
      "15409 [Discriminator loss: 0.023654, acc.: 100.00%] [Generator loss: 6.914030]\n",
      "15410 [Discriminator loss: 0.223556, acc.: 96.88%] [Generator loss: 6.457735]\n",
      "15411 [Discriminator loss: 0.080143, acc.: 95.31%] [Generator loss: 6.830357]\n",
      "15412 [Discriminator loss: 0.282151, acc.: 89.06%] [Generator loss: 6.639869]\n",
      "15413 [Discriminator loss: 0.015273, acc.: 100.00%] [Generator loss: 8.346881]\n",
      "15414 [Discriminator loss: 0.031379, acc.: 100.00%] [Generator loss: 6.631981]\n",
      "15415 [Discriminator loss: 0.101349, acc.: 95.31%] [Generator loss: 6.263871]\n",
      "15416 [Discriminator loss: 0.021867, acc.: 100.00%] [Generator loss: 5.576500]\n",
      "15417 [Discriminator loss: 0.126105, acc.: 92.19%] [Generator loss: 10.027668]\n",
      "15418 [Discriminator loss: 0.035770, acc.: 96.88%] [Generator loss: 9.541795]\n",
      "15419 [Discriminator loss: 0.145666, acc.: 96.88%] [Generator loss: 8.671461]\n",
      "15420 [Discriminator loss: 0.060396, acc.: 98.44%] [Generator loss: 7.907414]\n",
      "15421 [Discriminator loss: 0.015747, acc.: 100.00%] [Generator loss: 8.818060]\n",
      "15422 [Discriminator loss: 0.102601, acc.: 98.44%] [Generator loss: 7.778913]\n",
      "15423 [Discriminator loss: 0.036008, acc.: 100.00%] [Generator loss: 7.124622]\n",
      "15424 [Discriminator loss: 0.052506, acc.: 96.88%] [Generator loss: 6.762172]\n",
      "15425 [Discriminator loss: 0.113701, acc.: 95.31%] [Generator loss: 6.041870]\n",
      "15426 [Discriminator loss: 0.063484, acc.: 96.88%] [Generator loss: 7.646633]\n",
      "15427 [Discriminator loss: 0.026479, acc.: 100.00%] [Generator loss: 8.501194]\n",
      "15428 [Discriminator loss: 0.062612, acc.: 96.88%] [Generator loss: 8.412140]\n",
      "15429 [Discriminator loss: 0.058198, acc.: 96.88%] [Generator loss: 8.414248]\n",
      "15430 [Discriminator loss: 0.076331, acc.: 96.88%] [Generator loss: 7.947769]\n",
      "15431 [Discriminator loss: 0.018885, acc.: 100.00%] [Generator loss: 8.884314]\n",
      "15432 [Discriminator loss: 0.061754, acc.: 96.88%] [Generator loss: 6.818577]\n",
      "15433 [Discriminator loss: 0.075101, acc.: 96.88%] [Generator loss: 8.208599]\n",
      "15434 [Discriminator loss: 0.026082, acc.: 100.00%] [Generator loss: 8.231485]\n",
      "15435 [Discriminator loss: 0.080528, acc.: 95.31%] [Generator loss: 8.317494]\n",
      "15436 [Discriminator loss: 0.084253, acc.: 96.88%] [Generator loss: 6.175045]\n",
      "15437 [Discriminator loss: 0.022713, acc.: 100.00%] [Generator loss: 5.639922]\n",
      "15438 [Discriminator loss: 0.094916, acc.: 95.31%] [Generator loss: 8.341474]\n",
      "15439 [Discriminator loss: 0.053110, acc.: 98.44%] [Generator loss: 8.883663]\n",
      "15440 [Discriminator loss: 0.218911, acc.: 92.19%] [Generator loss: 7.875308]\n",
      "15441 [Discriminator loss: 0.110050, acc.: 93.75%] [Generator loss: 7.003304]\n",
      "15442 [Discriminator loss: 0.036754, acc.: 100.00%] [Generator loss: 7.512817]\n",
      "15443 [Discriminator loss: 0.020127, acc.: 100.00%] [Generator loss: 8.653461]\n",
      "15444 [Discriminator loss: 0.026684, acc.: 100.00%] [Generator loss: 8.189448]\n",
      "15445 [Discriminator loss: 0.068692, acc.: 98.44%] [Generator loss: 8.960348]\n",
      "15446 [Discriminator loss: 0.031552, acc.: 100.00%] [Generator loss: 8.129683]\n",
      "15447 [Discriminator loss: 0.103960, acc.: 96.88%] [Generator loss: 6.059892]\n",
      "15448 [Discriminator loss: 0.008352, acc.: 100.00%] [Generator loss: 5.767792]\n",
      "15449 [Discriminator loss: 0.171789, acc.: 95.31%] [Generator loss: 8.095436]\n",
      "15450 [Discriminator loss: 0.020283, acc.: 100.00%] [Generator loss: 8.093250]\n",
      "15451 [Discriminator loss: 0.189839, acc.: 92.19%] [Generator loss: 6.269556]\n",
      "15452 [Discriminator loss: 0.067847, acc.: 96.88%] [Generator loss: 8.903969]\n",
      "15453 [Discriminator loss: 0.010847, acc.: 100.00%] [Generator loss: 8.842087]\n",
      "15454 [Discriminator loss: 0.133572, acc.: 93.75%] [Generator loss: 5.536315]\n",
      "15455 [Discriminator loss: 0.085911, acc.: 95.31%] [Generator loss: 6.387426]\n",
      "15456 [Discriminator loss: 0.165304, acc.: 96.88%] [Generator loss: 10.074711]\n",
      "15457 [Discriminator loss: 0.055042, acc.: 98.44%] [Generator loss: 9.651684]\n",
      "15458 [Discriminator loss: 0.067364, acc.: 96.88%] [Generator loss: 8.393506]\n",
      "15459 [Discriminator loss: 0.082270, acc.: 95.31%] [Generator loss: 7.232403]\n",
      "15460 [Discriminator loss: 0.065197, acc.: 98.44%] [Generator loss: 7.255374]\n",
      "15461 [Discriminator loss: 0.041879, acc.: 98.44%] [Generator loss: 7.521219]\n",
      "15462 [Discriminator loss: 0.209225, acc.: 89.06%] [Generator loss: 8.547834]\n",
      "15463 [Discriminator loss: 0.114254, acc.: 95.31%] [Generator loss: 8.198463]\n",
      "15464 [Discriminator loss: 0.048391, acc.: 98.44%] [Generator loss: 8.906265]\n",
      "15465 [Discriminator loss: 0.106728, acc.: 96.88%] [Generator loss: 7.417225]\n",
      "15466 [Discriminator loss: 0.053120, acc.: 96.88%] [Generator loss: 10.330200]\n",
      "15467 [Discriminator loss: 0.024891, acc.: 100.00%] [Generator loss: 8.619209]\n",
      "15468 [Discriminator loss: 0.257830, acc.: 90.62%] [Generator loss: 5.803173]\n",
      "15469 [Discriminator loss: 0.113237, acc.: 93.75%] [Generator loss: 6.225218]\n",
      "15470 [Discriminator loss: 0.028858, acc.: 100.00%] [Generator loss: 7.919664]\n",
      "15471 [Discriminator loss: 0.086798, acc.: 95.31%] [Generator loss: 7.159614]\n",
      "15472 [Discriminator loss: 0.012830, acc.: 100.00%] [Generator loss: 7.531773]\n",
      "15473 [Discriminator loss: 0.066920, acc.: 96.88%] [Generator loss: 7.664924]\n",
      "15474 [Discriminator loss: 0.033234, acc.: 98.44%] [Generator loss: 8.152090]\n",
      "15475 [Discriminator loss: 0.122449, acc.: 93.75%] [Generator loss: 8.959924]\n",
      "15476 [Discriminator loss: 0.187188, acc.: 93.75%] [Generator loss: 8.588952]\n",
      "15477 [Discriminator loss: 0.033096, acc.: 100.00%] [Generator loss: 8.495355]\n",
      "15478 [Discriminator loss: 0.069198, acc.: 96.88%] [Generator loss: 7.567871]\n",
      "15479 [Discriminator loss: 0.093815, acc.: 95.31%] [Generator loss: 8.179664]\n",
      "15480 [Discriminator loss: 0.039886, acc.: 98.44%] [Generator loss: 9.050577]\n",
      "15481 [Discriminator loss: 0.258685, acc.: 89.06%] [Generator loss: 7.049146]\n",
      "15482 [Discriminator loss: 0.136309, acc.: 96.88%] [Generator loss: 7.677246]\n",
      "15483 [Discriminator loss: 0.130957, acc.: 93.75%] [Generator loss: 8.891117]\n",
      "15484 [Discriminator loss: 0.075358, acc.: 98.44%] [Generator loss: 8.695116]\n",
      "15485 [Discriminator loss: 0.056459, acc.: 96.88%] [Generator loss: 7.941074]\n",
      "15486 [Discriminator loss: 0.086669, acc.: 93.75%] [Generator loss: 7.417206]\n",
      "15487 [Discriminator loss: 0.035402, acc.: 98.44%] [Generator loss: 7.756947]\n",
      "15488 [Discriminator loss: 0.072553, acc.: 98.44%] [Generator loss: 7.563712]\n",
      "15489 [Discriminator loss: 0.016670, acc.: 100.00%] [Generator loss: 7.518273]\n",
      "15490 [Discriminator loss: 0.116967, acc.: 96.88%] [Generator loss: 5.018038]\n",
      "15491 [Discriminator loss: 0.305876, acc.: 87.50%] [Generator loss: 8.113985]\n",
      "15492 [Discriminator loss: 0.055428, acc.: 96.88%] [Generator loss: 8.923256]\n",
      "15493 [Discriminator loss: 0.115593, acc.: 95.31%] [Generator loss: 8.205614]\n",
      "15494 [Discriminator loss: 0.109267, acc.: 93.75%] [Generator loss: 7.286199]\n",
      "15495 [Discriminator loss: 0.053075, acc.: 96.88%] [Generator loss: 6.665480]\n",
      "15496 [Discriminator loss: 0.077401, acc.: 96.88%] [Generator loss: 7.925173]\n",
      "15497 [Discriminator loss: 0.075695, acc.: 98.44%] [Generator loss: 8.323570]\n",
      "15498 [Discriminator loss: 0.084478, acc.: 96.88%] [Generator loss: 6.016902]\n",
      "15499 [Discriminator loss: 0.041577, acc.: 100.00%] [Generator loss: 6.428510]\n",
      "15500 [Discriminator loss: 0.116613, acc.: 93.75%] [Generator loss: 9.680537]\n",
      "15501 [Discriminator loss: 0.066496, acc.: 98.44%] [Generator loss: 8.536238]\n",
      "15502 [Discriminator loss: 0.111558, acc.: 96.88%] [Generator loss: 5.662967]\n",
      "15503 [Discriminator loss: 0.122304, acc.: 93.75%] [Generator loss: 7.004383]\n",
      "15504 [Discriminator loss: 0.092091, acc.: 98.44%] [Generator loss: 9.022917]\n",
      "15505 [Discriminator loss: 0.033159, acc.: 100.00%] [Generator loss: 8.408613]\n",
      "15506 [Discriminator loss: 0.139030, acc.: 93.75%] [Generator loss: 7.935964]\n",
      "15507 [Discriminator loss: 0.185512, acc.: 93.75%] [Generator loss: 9.351046]\n",
      "15508 [Discriminator loss: 0.034434, acc.: 100.00%] [Generator loss: 8.698517]\n",
      "15509 [Discriminator loss: 0.065706, acc.: 98.44%] [Generator loss: 7.525887]\n",
      "15510 [Discriminator loss: 0.135116, acc.: 93.75%] [Generator loss: 8.956774]\n",
      "15511 [Discriminator loss: 0.061392, acc.: 96.88%] [Generator loss: 7.935402]\n",
      "15512 [Discriminator loss: 0.125668, acc.: 93.75%] [Generator loss: 7.990100]\n",
      "15513 [Discriminator loss: 0.044481, acc.: 98.44%] [Generator loss: 7.656460]\n",
      "15514 [Discriminator loss: 0.139184, acc.: 95.31%] [Generator loss: 6.875756]\n",
      "15515 [Discriminator loss: 0.054233, acc.: 98.44%] [Generator loss: 7.508471]\n",
      "15516 [Discriminator loss: 0.084912, acc.: 95.31%] [Generator loss: 8.851818]\n",
      "15517 [Discriminator loss: 0.068600, acc.: 96.88%] [Generator loss: 8.822526]\n",
      "15518 [Discriminator loss: 0.034818, acc.: 100.00%] [Generator loss: 9.137694]\n",
      "15519 [Discriminator loss: 0.099378, acc.: 95.31%] [Generator loss: 7.456132]\n",
      "15520 [Discriminator loss: 0.022256, acc.: 100.00%] [Generator loss: 7.251125]\n",
      "15521 [Discriminator loss: 0.017024, acc.: 100.00%] [Generator loss: 6.482443]\n",
      "15522 [Discriminator loss: 0.031989, acc.: 100.00%] [Generator loss: 7.674191]\n",
      "15523 [Discriminator loss: 0.090288, acc.: 98.44%] [Generator loss: 7.681963]\n",
      "15524 [Discriminator loss: 0.054931, acc.: 96.88%] [Generator loss: 7.551595]\n",
      "15525 [Discriminator loss: 0.049802, acc.: 100.00%] [Generator loss: 8.331412]\n",
      "15526 [Discriminator loss: 0.052112, acc.: 96.88%] [Generator loss: 8.288147]\n",
      "15527 [Discriminator loss: 0.077884, acc.: 96.88%] [Generator loss: 9.115836]\n",
      "15528 [Discriminator loss: 0.101590, acc.: 93.75%] [Generator loss: 7.012084]\n",
      "15529 [Discriminator loss: 0.086244, acc.: 96.88%] [Generator loss: 8.406217]\n",
      "15530 [Discriminator loss: 0.050588, acc.: 98.44%] [Generator loss: 7.413820]\n",
      "15531 [Discriminator loss: 0.080726, acc.: 95.31%] [Generator loss: 8.362049]\n",
      "15532 [Discriminator loss: 0.053041, acc.: 98.44%] [Generator loss: 8.492905]\n",
      "15533 [Discriminator loss: 0.014206, acc.: 100.00%] [Generator loss: 6.447833]\n",
      "15534 [Discriminator loss: 0.078943, acc.: 96.88%] [Generator loss: 7.931759]\n",
      "15535 [Discriminator loss: 0.113684, acc.: 95.31%] [Generator loss: 8.055618]\n",
      "15536 [Discriminator loss: 0.102977, acc.: 95.31%] [Generator loss: 8.858480]\n",
      "15537 [Discriminator loss: 0.217981, acc.: 96.88%] [Generator loss: 7.463241]\n",
      "15538 [Discriminator loss: 0.165062, acc.: 95.31%] [Generator loss: 7.378663]\n",
      "15539 [Discriminator loss: 0.254052, acc.: 93.75%] [Generator loss: 7.830936]\n",
      "15540 [Discriminator loss: 0.121396, acc.: 93.75%] [Generator loss: 7.547955]\n",
      "15541 [Discriminator loss: 0.102210, acc.: 96.88%] [Generator loss: 7.191092]\n",
      "15542 [Discriminator loss: 0.210019, acc.: 87.50%] [Generator loss: 9.274361]\n",
      "15543 [Discriminator loss: 0.021764, acc.: 100.00%] [Generator loss: 11.194403]\n",
      "15544 [Discriminator loss: 0.046364, acc.: 98.44%] [Generator loss: 10.048252]\n",
      "15545 [Discriminator loss: 0.104521, acc.: 95.31%] [Generator loss: 5.415728]\n",
      "15546 [Discriminator loss: 0.171259, acc.: 92.19%] [Generator loss: 8.461080]\n",
      "15547 [Discriminator loss: 0.003492, acc.: 100.00%] [Generator loss: 8.873975]\n",
      "15548 [Discriminator loss: 0.026013, acc.: 98.44%] [Generator loss: 10.258234]\n",
      "15549 [Discriminator loss: 0.089287, acc.: 96.88%] [Generator loss: 6.848858]\n",
      "15550 [Discriminator loss: 0.043020, acc.: 98.44%] [Generator loss: 8.079675]\n",
      "15551 [Discriminator loss: 0.090384, acc.: 96.88%] [Generator loss: 9.095880]\n",
      "15552 [Discriminator loss: 0.069638, acc.: 98.44%] [Generator loss: 7.165618]\n",
      "15553 [Discriminator loss: 0.091927, acc.: 96.88%] [Generator loss: 5.682216]\n",
      "15554 [Discriminator loss: 0.316593, acc.: 93.75%] [Generator loss: 9.333458]\n",
      "15555 [Discriminator loss: 0.073248, acc.: 96.88%] [Generator loss: 8.840533]\n",
      "15556 [Discriminator loss: 0.204316, acc.: 92.19%] [Generator loss: 7.003342]\n",
      "15557 [Discriminator loss: 0.070302, acc.: 95.31%] [Generator loss: 7.065754]\n",
      "15558 [Discriminator loss: 0.034717, acc.: 98.44%] [Generator loss: 8.086251]\n",
      "15559 [Discriminator loss: 0.108027, acc.: 96.88%] [Generator loss: 6.886117]\n",
      "15560 [Discriminator loss: 0.088160, acc.: 98.44%] [Generator loss: 8.002525]\n",
      "15561 [Discriminator loss: 0.049395, acc.: 98.44%] [Generator loss: 9.612608]\n",
      "15562 [Discriminator loss: 0.560973, acc.: 84.38%] [Generator loss: 5.626061]\n",
      "15563 [Discriminator loss: 0.576775, acc.: 84.38%] [Generator loss: 10.590502]\n",
      "15564 [Discriminator loss: 0.083302, acc.: 96.88%] [Generator loss: 10.469023]\n",
      "15565 [Discriminator loss: 0.162563, acc.: 95.31%] [Generator loss: 8.044252]\n",
      "15566 [Discriminator loss: 0.102220, acc.: 98.44%] [Generator loss: 6.951347]\n",
      "15567 [Discriminator loss: 0.023014, acc.: 100.00%] [Generator loss: 6.467898]\n",
      "15568 [Discriminator loss: 0.276325, acc.: 92.19%] [Generator loss: 8.708273]\n",
      "15569 [Discriminator loss: 0.043795, acc.: 98.44%] [Generator loss: 10.200344]\n",
      "15570 [Discriminator loss: 0.132355, acc.: 95.31%] [Generator loss: 7.159188]\n",
      "15571 [Discriminator loss: 0.251218, acc.: 90.62%] [Generator loss: 8.010956]\n",
      "15572 [Discriminator loss: 0.063416, acc.: 98.44%] [Generator loss: 10.636850]\n",
      "15573 [Discriminator loss: 0.043102, acc.: 98.44%] [Generator loss: 9.437988]\n",
      "15574 [Discriminator loss: 0.207145, acc.: 89.06%] [Generator loss: 6.636370]\n",
      "15575 [Discriminator loss: 0.119379, acc.: 96.88%] [Generator loss: 5.250020]\n",
      "15576 [Discriminator loss: 0.054797, acc.: 96.88%] [Generator loss: 7.057570]\n",
      "15577 [Discriminator loss: 0.055246, acc.: 100.00%] [Generator loss: 6.967575]\n",
      "15578 [Discriminator loss: 0.039536, acc.: 100.00%] [Generator loss: 5.911621]\n",
      "15579 [Discriminator loss: 0.259166, acc.: 95.31%] [Generator loss: 8.881779]\n",
      "15580 [Discriminator loss: 0.029562, acc.: 100.00%] [Generator loss: 7.563033]\n",
      "15581 [Discriminator loss: 0.218952, acc.: 93.75%] [Generator loss: 7.814495]\n",
      "15582 [Discriminator loss: 0.041823, acc.: 98.44%] [Generator loss: 9.827462]\n",
      "15583 [Discriminator loss: 0.093636, acc.: 96.88%] [Generator loss: 6.533050]\n",
      "15584 [Discriminator loss: 0.089539, acc.: 96.88%] [Generator loss: 8.942778]\n",
      "15585 [Discriminator loss: 0.049011, acc.: 96.88%] [Generator loss: 8.197811]\n",
      "15586 [Discriminator loss: 0.094999, acc.: 96.88%] [Generator loss: 6.914565]\n",
      "15587 [Discriminator loss: 0.177812, acc.: 92.19%] [Generator loss: 6.995663]\n",
      "15588 [Discriminator loss: 0.058391, acc.: 96.88%] [Generator loss: 8.632502]\n",
      "15589 [Discriminator loss: 0.036716, acc.: 98.44%] [Generator loss: 7.649735]\n",
      "15590 [Discriminator loss: 0.015271, acc.: 100.00%] [Generator loss: 7.906379]\n",
      "15591 [Discriminator loss: 0.187972, acc.: 89.06%] [Generator loss: 7.737063]\n",
      "15592 [Discriminator loss: 0.043782, acc.: 98.44%] [Generator loss: 9.275128]\n",
      "15593 [Discriminator loss: 0.424771, acc.: 82.81%] [Generator loss: 7.446288]\n",
      "15594 [Discriminator loss: 0.045383, acc.: 96.88%] [Generator loss: 8.539772]\n",
      "15595 [Discriminator loss: 0.049246, acc.: 96.88%] [Generator loss: 8.869215]\n",
      "15596 [Discriminator loss: 0.188445, acc.: 93.75%] [Generator loss: 7.735322]\n",
      "15597 [Discriminator loss: 0.073260, acc.: 95.31%] [Generator loss: 8.054757]\n",
      "15598 [Discriminator loss: 0.102339, acc.: 95.31%] [Generator loss: 8.712175]\n",
      "15599 [Discriminator loss: 0.060810, acc.: 95.31%] [Generator loss: 7.523061]\n",
      "15600 [Discriminator loss: 0.066020, acc.: 96.88%] [Generator loss: 9.525669]\n",
      "15601 [Discriminator loss: 0.023012, acc.: 100.00%] [Generator loss: 7.996044]\n",
      "15602 [Discriminator loss: 0.102450, acc.: 95.31%] [Generator loss: 7.692204]\n",
      "15603 [Discriminator loss: 0.180909, acc.: 93.75%] [Generator loss: 8.023726]\n",
      "15604 [Discriminator loss: 0.135920, acc.: 96.88%] [Generator loss: 8.491665]\n",
      "15605 [Discriminator loss: 0.130427, acc.: 95.31%] [Generator loss: 6.078256]\n",
      "15606 [Discriminator loss: 0.195521, acc.: 92.19%] [Generator loss: 7.408933]\n",
      "15607 [Discriminator loss: 0.099500, acc.: 96.88%] [Generator loss: 6.810894]\n",
      "15608 [Discriminator loss: 0.181157, acc.: 90.62%] [Generator loss: 8.118696]\n",
      "15609 [Discriminator loss: 0.021141, acc.: 100.00%] [Generator loss: 6.268242]\n",
      "15610 [Discriminator loss: 0.180999, acc.: 95.31%] [Generator loss: 9.108944]\n",
      "15611 [Discriminator loss: 0.065986, acc.: 98.44%] [Generator loss: 6.971788]\n",
      "15612 [Discriminator loss: 0.044823, acc.: 98.44%] [Generator loss: 6.597030]\n",
      "15613 [Discriminator loss: 0.133142, acc.: 92.19%] [Generator loss: 8.030745]\n",
      "15614 [Discriminator loss: 0.088764, acc.: 95.31%] [Generator loss: 9.264402]\n",
      "15615 [Discriminator loss: 0.067372, acc.: 96.88%] [Generator loss: 9.896387]\n",
      "15616 [Discriminator loss: 0.118250, acc.: 95.31%] [Generator loss: 7.370696]\n",
      "15617 [Discriminator loss: 0.083388, acc.: 98.44%] [Generator loss: 8.824128]\n",
      "15618 [Discriminator loss: 0.086199, acc.: 98.44%] [Generator loss: 9.261485]\n",
      "15619 [Discriminator loss: 0.035399, acc.: 98.44%] [Generator loss: 8.733300]\n",
      "15620 [Discriminator loss: 0.284938, acc.: 90.62%] [Generator loss: 8.128916]\n",
      "15621 [Discriminator loss: 0.032151, acc.: 100.00%] [Generator loss: 10.062277]\n",
      "15622 [Discriminator loss: 0.120674, acc.: 95.31%] [Generator loss: 8.481620]\n",
      "15623 [Discriminator loss: 0.039483, acc.: 100.00%] [Generator loss: 9.662268]\n",
      "15624 [Discriminator loss: 0.074660, acc.: 95.31%] [Generator loss: 8.783383]\n",
      "15625 [Discriminator loss: 0.059002, acc.: 96.88%] [Generator loss: 6.934017]\n",
      "15626 [Discriminator loss: 0.039016, acc.: 98.44%] [Generator loss: 7.791126]\n",
      "15627 [Discriminator loss: 0.167522, acc.: 96.88%] [Generator loss: 7.450189]\n",
      "15628 [Discriminator loss: 0.038952, acc.: 100.00%] [Generator loss: 8.066404]\n",
      "15629 [Discriminator loss: 0.230202, acc.: 95.31%] [Generator loss: 8.152511]\n",
      "15630 [Discriminator loss: 0.041659, acc.: 98.44%] [Generator loss: 7.038957]\n",
      "15631 [Discriminator loss: 0.047780, acc.: 100.00%] [Generator loss: 7.576776]\n",
      "15632 [Discriminator loss: 0.286622, acc.: 87.50%] [Generator loss: 7.889662]\n",
      "15633 [Discriminator loss: 0.006755, acc.: 100.00%] [Generator loss: 7.965363]\n",
      "15634 [Discriminator loss: 0.060274, acc.: 95.31%] [Generator loss: 7.398461]\n",
      "15635 [Discriminator loss: 0.112375, acc.: 92.19%] [Generator loss: 9.409796]\n",
      "15636 [Discriminator loss: 0.063130, acc.: 98.44%] [Generator loss: 7.137200]\n",
      "15637 [Discriminator loss: 0.087438, acc.: 96.88%] [Generator loss: 7.005277]\n",
      "15638 [Discriminator loss: 0.030297, acc.: 98.44%] [Generator loss: 6.869022]\n",
      "15639 [Discriminator loss: 0.030632, acc.: 100.00%] [Generator loss: 7.739289]\n",
      "15640 [Discriminator loss: 0.045392, acc.: 98.44%] [Generator loss: 7.786435]\n",
      "15641 [Discriminator loss: 0.101532, acc.: 96.88%] [Generator loss: 7.555254]\n",
      "15642 [Discriminator loss: 0.110313, acc.: 93.75%] [Generator loss: 7.620614]\n",
      "15643 [Discriminator loss: 0.272480, acc.: 87.50%] [Generator loss: 9.291524]\n",
      "15644 [Discriminator loss: 0.333792, acc.: 87.50%] [Generator loss: 4.766889]\n",
      "15645 [Discriminator loss: 0.087505, acc.: 96.88%] [Generator loss: 7.335508]\n",
      "15646 [Discriminator loss: 0.072641, acc.: 96.88%] [Generator loss: 7.354130]\n",
      "15647 [Discriminator loss: 0.176504, acc.: 92.19%] [Generator loss: 6.737691]\n",
      "15648 [Discriminator loss: 0.144607, acc.: 93.75%] [Generator loss: 8.986798]\n",
      "15649 [Discriminator loss: 0.054784, acc.: 98.44%] [Generator loss: 7.533075]\n",
      "15650 [Discriminator loss: 0.121626, acc.: 96.88%] [Generator loss: 7.646645]\n",
      "15651 [Discriminator loss: 0.025729, acc.: 100.00%] [Generator loss: 7.409975]\n",
      "15652 [Discriminator loss: 0.023943, acc.: 100.00%] [Generator loss: 7.873626]\n",
      "15653 [Discriminator loss: 0.101031, acc.: 95.31%] [Generator loss: 8.083517]\n",
      "15654 [Discriminator loss: 0.139118, acc.: 96.88%] [Generator loss: 7.170650]\n",
      "15655 [Discriminator loss: 0.102140, acc.: 95.31%] [Generator loss: 8.006595]\n",
      "15656 [Discriminator loss: 0.146557, acc.: 93.75%] [Generator loss: 7.445169]\n",
      "15657 [Discriminator loss: 0.093762, acc.: 96.88%] [Generator loss: 7.487257]\n",
      "15658 [Discriminator loss: 0.076775, acc.: 98.44%] [Generator loss: 8.324861]\n",
      "15659 [Discriminator loss: 0.043099, acc.: 100.00%] [Generator loss: 8.012619]\n",
      "15660 [Discriminator loss: 0.176660, acc.: 93.75%] [Generator loss: 6.955321]\n",
      "15661 [Discriminator loss: 0.085350, acc.: 95.31%] [Generator loss: 6.072542]\n",
      "15662 [Discriminator loss: 0.066494, acc.: 98.44%] [Generator loss: 6.251250]\n",
      "15663 [Discriminator loss: 0.141829, acc.: 93.75%] [Generator loss: 7.075507]\n",
      "15664 [Discriminator loss: 0.126244, acc.: 93.75%] [Generator loss: 9.684149]\n",
      "15665 [Discriminator loss: 0.051820, acc.: 98.44%] [Generator loss: 11.304372]\n",
      "15666 [Discriminator loss: 0.207434, acc.: 90.62%] [Generator loss: 5.854359]\n",
      "15667 [Discriminator loss: 0.160833, acc.: 96.88%] [Generator loss: 8.675323]\n",
      "15668 [Discriminator loss: 0.139814, acc.: 96.88%] [Generator loss: 8.245268]\n",
      "15669 [Discriminator loss: 0.033627, acc.: 98.44%] [Generator loss: 8.810517]\n",
      "15670 [Discriminator loss: 0.045277, acc.: 98.44%] [Generator loss: 9.441463]\n",
      "15671 [Discriminator loss: 0.080506, acc.: 98.44%] [Generator loss: 9.150520]\n",
      "15672 [Discriminator loss: 0.049225, acc.: 98.44%] [Generator loss: 9.210988]\n",
      "15673 [Discriminator loss: 0.049416, acc.: 98.44%] [Generator loss: 8.582616]\n",
      "15674 [Discriminator loss: 0.067301, acc.: 98.44%] [Generator loss: 7.939958]\n",
      "15675 [Discriminator loss: 0.071361, acc.: 95.31%] [Generator loss: 7.072517]\n",
      "15676 [Discriminator loss: 0.119091, acc.: 93.75%] [Generator loss: 7.959389]\n",
      "15677 [Discriminator loss: 0.077406, acc.: 96.88%] [Generator loss: 7.118481]\n",
      "15678 [Discriminator loss: 0.109724, acc.: 93.75%] [Generator loss: 8.630451]\n",
      "15679 [Discriminator loss: 0.058515, acc.: 96.88%] [Generator loss: 8.839526]\n",
      "15680 [Discriminator loss: 0.083141, acc.: 96.88%] [Generator loss: 9.079573]\n",
      "15681 [Discriminator loss: 0.125893, acc.: 92.19%] [Generator loss: 8.220087]\n",
      "15682 [Discriminator loss: 0.062062, acc.: 96.88%] [Generator loss: 9.331472]\n",
      "15683 [Discriminator loss: 0.096375, acc.: 95.31%] [Generator loss: 8.892601]\n",
      "15684 [Discriminator loss: 0.072202, acc.: 96.88%] [Generator loss: 9.354572]\n",
      "15685 [Discriminator loss: 0.072492, acc.: 96.88%] [Generator loss: 8.907183]\n",
      "15686 [Discriminator loss: 0.042429, acc.: 98.44%] [Generator loss: 7.691790]\n",
      "15687 [Discriminator loss: 0.158775, acc.: 96.88%] [Generator loss: 7.237696]\n",
      "15688 [Discriminator loss: 0.085518, acc.: 96.88%] [Generator loss: 8.840377]\n",
      "15689 [Discriminator loss: 0.185357, acc.: 95.31%] [Generator loss: 6.553397]\n",
      "15690 [Discriminator loss: 0.132039, acc.: 96.88%] [Generator loss: 8.293259]\n",
      "15691 [Discriminator loss: 0.039750, acc.: 98.44%] [Generator loss: 8.819263]\n",
      "15692 [Discriminator loss: 0.053991, acc.: 96.88%] [Generator loss: 8.806967]\n",
      "15693 [Discriminator loss: 0.039943, acc.: 100.00%] [Generator loss: 8.461123]\n",
      "15694 [Discriminator loss: 0.068930, acc.: 96.88%] [Generator loss: 7.363829]\n",
      "15695 [Discriminator loss: 0.044613, acc.: 98.44%] [Generator loss: 8.511593]\n",
      "15696 [Discriminator loss: 0.084645, acc.: 96.88%] [Generator loss: 8.423168]\n",
      "15697 [Discriminator loss: 0.108256, acc.: 95.31%] [Generator loss: 7.019444]\n",
      "15698 [Discriminator loss: 0.046230, acc.: 98.44%] [Generator loss: 8.231457]\n",
      "15699 [Discriminator loss: 0.029175, acc.: 100.00%] [Generator loss: 6.736232]\n",
      "15700 [Discriminator loss: 0.158878, acc.: 95.31%] [Generator loss: 8.576210]\n",
      "15701 [Discriminator loss: 0.095188, acc.: 96.88%] [Generator loss: 8.532918]\n",
      "15702 [Discriminator loss: 0.029575, acc.: 98.44%] [Generator loss: 7.682211]\n",
      "15703 [Discriminator loss: 0.100888, acc.: 93.75%] [Generator loss: 7.380061]\n",
      "15704 [Discriminator loss: 0.039388, acc.: 98.44%] [Generator loss: 7.916096]\n",
      "15705 [Discriminator loss: 0.162149, acc.: 92.19%] [Generator loss: 5.446352]\n",
      "15706 [Discriminator loss: 0.102725, acc.: 95.31%] [Generator loss: 7.921877]\n",
      "15707 [Discriminator loss: 0.033631, acc.: 98.44%] [Generator loss: 7.402286]\n",
      "15708 [Discriminator loss: 0.064997, acc.: 96.88%] [Generator loss: 7.393761]\n",
      "15709 [Discriminator loss: 0.023205, acc.: 100.00%] [Generator loss: 7.476178]\n",
      "15710 [Discriminator loss: 0.131411, acc.: 96.88%] [Generator loss: 6.609380]\n",
      "15711 [Discriminator loss: 0.079688, acc.: 96.88%] [Generator loss: 6.963868]\n",
      "15712 [Discriminator loss: 0.027185, acc.: 100.00%] [Generator loss: 7.255916]\n",
      "15713 [Discriminator loss: 0.170481, acc.: 93.75%] [Generator loss: 7.743648]\n",
      "15714 [Discriminator loss: 0.025180, acc.: 100.00%] [Generator loss: 7.633504]\n",
      "15715 [Discriminator loss: 0.009334, acc.: 100.00%] [Generator loss: 9.142351]\n",
      "15716 [Discriminator loss: 0.169569, acc.: 93.75%] [Generator loss: 8.797592]\n",
      "15717 [Discriminator loss: 0.069782, acc.: 96.88%] [Generator loss: 8.506306]\n",
      "15718 [Discriminator loss: 0.042169, acc.: 100.00%] [Generator loss: 8.540940]\n",
      "15719 [Discriminator loss: 0.046211, acc.: 98.44%] [Generator loss: 7.563676]\n",
      "15720 [Discriminator loss: 0.148576, acc.: 95.31%] [Generator loss: 8.117788]\n",
      "15721 [Discriminator loss: 0.080611, acc.: 95.31%] [Generator loss: 8.498835]\n",
      "15722 [Discriminator loss: 0.065806, acc.: 96.88%] [Generator loss: 6.446422]\n",
      "15723 [Discriminator loss: 0.142039, acc.: 96.88%] [Generator loss: 5.719749]\n",
      "15724 [Discriminator loss: 0.047083, acc.: 96.88%] [Generator loss: 7.872330]\n",
      "15725 [Discriminator loss: 0.057565, acc.: 98.44%] [Generator loss: 8.087812]\n",
      "15726 [Discriminator loss: 0.267058, acc.: 85.94%] [Generator loss: 7.942627]\n",
      "15727 [Discriminator loss: 0.022898, acc.: 100.00%] [Generator loss: 9.830979]\n",
      "15728 [Discriminator loss: 0.099957, acc.: 96.88%] [Generator loss: 7.777256]\n",
      "15729 [Discriminator loss: 0.126643, acc.: 92.19%] [Generator loss: 7.891338]\n",
      "15730 [Discriminator loss: 0.158094, acc.: 96.88%] [Generator loss: 7.708992]\n",
      "15731 [Discriminator loss: 0.087903, acc.: 95.31%] [Generator loss: 9.271554]\n",
      "15732 [Discriminator loss: 0.088683, acc.: 98.44%] [Generator loss: 7.466738]\n",
      "15733 [Discriminator loss: 0.029945, acc.: 100.00%] [Generator loss: 7.773541]\n",
      "15734 [Discriminator loss: 0.176062, acc.: 92.19%] [Generator loss: 10.332889]\n",
      "15735 [Discriminator loss: 0.056399, acc.: 100.00%] [Generator loss: 9.281441]\n",
      "15736 [Discriminator loss: 0.084820, acc.: 96.88%] [Generator loss: 9.016657]\n",
      "15737 [Discriminator loss: 0.031449, acc.: 100.00%] [Generator loss: 6.317493]\n",
      "15738 [Discriminator loss: 0.327738, acc.: 87.50%] [Generator loss: 9.801629]\n",
      "15739 [Discriminator loss: 0.058960, acc.: 98.44%] [Generator loss: 9.335442]\n",
      "15740 [Discriminator loss: 0.060784, acc.: 98.44%] [Generator loss: 7.689955]\n",
      "15741 [Discriminator loss: 0.018657, acc.: 100.00%] [Generator loss: 7.315774]\n",
      "15742 [Discriminator loss: 0.194513, acc.: 90.62%] [Generator loss: 5.671091]\n",
      "15743 [Discriminator loss: 0.274433, acc.: 85.94%] [Generator loss: 8.069780]\n",
      "15744 [Discriminator loss: 0.006582, acc.: 100.00%] [Generator loss: 11.744518]\n",
      "15745 [Discriminator loss: 0.140624, acc.: 92.19%] [Generator loss: 7.619013]\n",
      "15746 [Discriminator loss: 0.080130, acc.: 96.88%] [Generator loss: 7.667804]\n",
      "15747 [Discriminator loss: 0.065451, acc.: 96.88%] [Generator loss: 7.027822]\n",
      "15748 [Discriminator loss: 0.119794, acc.: 96.88%] [Generator loss: 7.755909]\n",
      "15749 [Discriminator loss: 0.009421, acc.: 100.00%] [Generator loss: 7.475037]\n",
      "15750 [Discriminator loss: 0.168384, acc.: 93.75%] [Generator loss: 8.240291]\n",
      "15751 [Discriminator loss: 0.117557, acc.: 95.31%] [Generator loss: 9.049818]\n",
      "15752 [Discriminator loss: 0.056601, acc.: 96.88%] [Generator loss: 8.132579]\n",
      "15753 [Discriminator loss: 0.042957, acc.: 100.00%] [Generator loss: 8.230994]\n",
      "15754 [Discriminator loss: 0.146990, acc.: 95.31%] [Generator loss: 7.085201]\n",
      "15755 [Discriminator loss: 0.046876, acc.: 98.44%] [Generator loss: 6.951163]\n",
      "15756 [Discriminator loss: 0.008428, acc.: 100.00%] [Generator loss: 7.559381]\n",
      "15757 [Discriminator loss: 0.016292, acc.: 100.00%] [Generator loss: 6.309009]\n",
      "15758 [Discriminator loss: 0.124563, acc.: 96.88%] [Generator loss: 6.009530]\n",
      "15759 [Discriminator loss: 0.010485, acc.: 100.00%] [Generator loss: 7.641760]\n",
      "15760 [Discriminator loss: 0.167169, acc.: 92.19%] [Generator loss: 9.058096]\n",
      "15761 [Discriminator loss: 0.200657, acc.: 93.75%] [Generator loss: 6.579217]\n",
      "15762 [Discriminator loss: 0.105116, acc.: 93.75%] [Generator loss: 7.164578]\n",
      "15763 [Discriminator loss: 0.037741, acc.: 98.44%] [Generator loss: 7.681297]\n",
      "15764 [Discriminator loss: 0.446935, acc.: 87.50%] [Generator loss: 7.803156]\n",
      "15765 [Discriminator loss: 0.113419, acc.: 95.31%] [Generator loss: 7.894460]\n",
      "15766 [Discriminator loss: 0.094577, acc.: 96.88%] [Generator loss: 6.302558]\n",
      "15767 [Discriminator loss: 0.038473, acc.: 98.44%] [Generator loss: 7.628914]\n",
      "15768 [Discriminator loss: 0.050806, acc.: 98.44%] [Generator loss: 6.685877]\n",
      "15769 [Discriminator loss: 0.018098, acc.: 100.00%] [Generator loss: 7.661703]\n",
      "15770 [Discriminator loss: 0.073675, acc.: 98.44%] [Generator loss: 7.924477]\n",
      "15771 [Discriminator loss: 0.054259, acc.: 96.88%] [Generator loss: 7.231877]\n",
      "15772 [Discriminator loss: 0.171252, acc.: 93.75%] [Generator loss: 7.126920]\n",
      "15773 [Discriminator loss: 0.022342, acc.: 100.00%] [Generator loss: 8.377504]\n",
      "15774 [Discriminator loss: 0.053352, acc.: 98.44%] [Generator loss: 6.930940]\n",
      "15775 [Discriminator loss: 0.086809, acc.: 95.31%] [Generator loss: 7.512690]\n",
      "15776 [Discriminator loss: 0.099851, acc.: 98.44%] [Generator loss: 7.046369]\n",
      "15777 [Discriminator loss: 0.201526, acc.: 90.62%] [Generator loss: 10.293748]\n",
      "15778 [Discriminator loss: 0.085860, acc.: 95.31%] [Generator loss: 10.340266]\n",
      "15779 [Discriminator loss: 0.046702, acc.: 96.88%] [Generator loss: 9.345675]\n",
      "15780 [Discriminator loss: 0.109757, acc.: 95.31%] [Generator loss: 6.661706]\n",
      "15781 [Discriminator loss: 0.036554, acc.: 98.44%] [Generator loss: 7.105135]\n",
      "15782 [Discriminator loss: 0.046934, acc.: 98.44%] [Generator loss: 6.968704]\n",
      "15783 [Discriminator loss: 0.021911, acc.: 100.00%] [Generator loss: 8.490973]\n",
      "15784 [Discriminator loss: 0.030860, acc.: 98.44%] [Generator loss: 6.967926]\n",
      "15785 [Discriminator loss: 0.067734, acc.: 96.88%] [Generator loss: 6.315794]\n",
      "15786 [Discriminator loss: 0.052252, acc.: 98.44%] [Generator loss: 7.540403]\n",
      "15787 [Discriminator loss: 0.041740, acc.: 100.00%] [Generator loss: 8.434879]\n",
      "15788 [Discriminator loss: 0.228225, acc.: 92.19%] [Generator loss: 7.285880]\n",
      "15789 [Discriminator loss: 0.119754, acc.: 96.88%] [Generator loss: 7.726566]\n",
      "15790 [Discriminator loss: 0.048405, acc.: 96.88%] [Generator loss: 9.052664]\n",
      "15791 [Discriminator loss: 0.208457, acc.: 92.19%] [Generator loss: 6.973923]\n",
      "15792 [Discriminator loss: 0.064182, acc.: 98.44%] [Generator loss: 8.756107]\n",
      "15793 [Discriminator loss: 0.068760, acc.: 95.31%] [Generator loss: 8.711576]\n",
      "15794 [Discriminator loss: 0.037552, acc.: 100.00%] [Generator loss: 8.375982]\n",
      "15795 [Discriminator loss: 0.084275, acc.: 98.44%] [Generator loss: 6.780362]\n",
      "15796 [Discriminator loss: 0.029601, acc.: 100.00%] [Generator loss: 7.183748]\n",
      "15797 [Discriminator loss: 0.218149, acc.: 92.19%] [Generator loss: 8.618235]\n",
      "15798 [Discriminator loss: 0.058591, acc.: 98.44%] [Generator loss: 9.307036]\n",
      "15799 [Discriminator loss: 0.200669, acc.: 92.19%] [Generator loss: 6.398691]\n",
      "15800 [Discriminator loss: 0.112000, acc.: 95.31%] [Generator loss: 6.954757]\n",
      "15801 [Discriminator loss: 0.042984, acc.: 98.44%] [Generator loss: 8.285741]\n",
      "15802 [Discriminator loss: 0.060125, acc.: 98.44%] [Generator loss: 7.434155]\n",
      "15803 [Discriminator loss: 0.057684, acc.: 98.44%] [Generator loss: 8.151227]\n",
      "15804 [Discriminator loss: 0.045932, acc.: 98.44%] [Generator loss: 9.276602]\n",
      "15805 [Discriminator loss: 0.038747, acc.: 98.44%] [Generator loss: 8.323864]\n",
      "15806 [Discriminator loss: 0.058079, acc.: 96.88%] [Generator loss: 7.233863]\n",
      "15807 [Discriminator loss: 0.034344, acc.: 100.00%] [Generator loss: 7.289800]\n",
      "15808 [Discriminator loss: 0.024531, acc.: 100.00%] [Generator loss: 7.198271]\n",
      "15809 [Discriminator loss: 0.024738, acc.: 100.00%] [Generator loss: 8.009380]\n",
      "15810 [Discriminator loss: 0.108620, acc.: 98.44%] [Generator loss: 7.699167]\n",
      "15811 [Discriminator loss: 0.041376, acc.: 98.44%] [Generator loss: 8.842325]\n",
      "15812 [Discriminator loss: 0.086958, acc.: 96.88%] [Generator loss: 7.156844]\n",
      "15813 [Discriminator loss: 0.089312, acc.: 95.31%] [Generator loss: 7.825166]\n",
      "15814 [Discriminator loss: 0.056671, acc.: 96.88%] [Generator loss: 6.984186]\n",
      "15815 [Discriminator loss: 0.010229, acc.: 100.00%] [Generator loss: 6.740375]\n",
      "15816 [Discriminator loss: 0.069975, acc.: 98.44%] [Generator loss: 6.599933]\n",
      "15817 [Discriminator loss: 0.103938, acc.: 96.88%] [Generator loss: 8.836292]\n",
      "15818 [Discriminator loss: 0.132336, acc.: 95.31%] [Generator loss: 6.312026]\n",
      "15819 [Discriminator loss: 0.075932, acc.: 98.44%] [Generator loss: 5.874021]\n",
      "15820 [Discriminator loss: 0.072144, acc.: 98.44%] [Generator loss: 7.470113]\n",
      "15821 [Discriminator loss: 0.041248, acc.: 98.44%] [Generator loss: 8.494835]\n",
      "15822 [Discriminator loss: 0.054535, acc.: 96.88%] [Generator loss: 7.886489]\n",
      "15823 [Discriminator loss: 0.060796, acc.: 98.44%] [Generator loss: 6.842782]\n",
      "15824 [Discriminator loss: 0.017829, acc.: 100.00%] [Generator loss: 6.487325]\n",
      "15825 [Discriminator loss: 0.048340, acc.: 98.44%] [Generator loss: 6.911649]\n",
      "15826 [Discriminator loss: 0.218740, acc.: 93.75%] [Generator loss: 8.320885]\n",
      "15827 [Discriminator loss: 0.124455, acc.: 96.88%] [Generator loss: 10.255518]\n",
      "15828 [Discriminator loss: 0.083422, acc.: 98.44%] [Generator loss: 9.817269]\n",
      "15829 [Discriminator loss: 0.126002, acc.: 93.75%] [Generator loss: 8.568815]\n",
      "15830 [Discriminator loss: 0.029598, acc.: 98.44%] [Generator loss: 8.367735]\n",
      "15831 [Discriminator loss: 0.174222, acc.: 95.31%] [Generator loss: 7.507538]\n",
      "15832 [Discriminator loss: 0.039514, acc.: 98.44%] [Generator loss: 8.747494]\n",
      "15833 [Discriminator loss: 0.059761, acc.: 98.44%] [Generator loss: 8.641165]\n",
      "15834 [Discriminator loss: 0.052287, acc.: 98.44%] [Generator loss: 7.499228]\n",
      "15835 [Discriminator loss: 0.034741, acc.: 98.44%] [Generator loss: 7.275755]\n",
      "15836 [Discriminator loss: 0.039659, acc.: 98.44%] [Generator loss: 6.750608]\n",
      "15837 [Discriminator loss: 0.111300, acc.: 95.31%] [Generator loss: 8.124226]\n",
      "15838 [Discriminator loss: 0.142887, acc.: 96.88%] [Generator loss: 7.342590]\n",
      "15839 [Discriminator loss: 0.117296, acc.: 95.31%] [Generator loss: 8.434747]\n",
      "15840 [Discriminator loss: 0.058310, acc.: 98.44%] [Generator loss: 10.045557]\n",
      "15841 [Discriminator loss: 0.031280, acc.: 100.00%] [Generator loss: 9.704947]\n",
      "15842 [Discriminator loss: 0.127275, acc.: 95.31%] [Generator loss: 8.011248]\n",
      "15843 [Discriminator loss: 0.037038, acc.: 98.44%] [Generator loss: 9.018471]\n",
      "15844 [Discriminator loss: 0.079615, acc.: 96.88%] [Generator loss: 6.902145]\n",
      "15845 [Discriminator loss: 0.051033, acc.: 98.44%] [Generator loss: 6.404496]\n",
      "15846 [Discriminator loss: 0.140694, acc.: 95.31%] [Generator loss: 7.892342]\n",
      "15847 [Discriminator loss: 0.049109, acc.: 98.44%] [Generator loss: 7.723705]\n",
      "15848 [Discriminator loss: 0.044202, acc.: 98.44%] [Generator loss: 7.557852]\n",
      "15849 [Discriminator loss: 0.038244, acc.: 98.44%] [Generator loss: 7.834079]\n",
      "15850 [Discriminator loss: 0.230688, acc.: 92.19%] [Generator loss: 8.566278]\n",
      "15851 [Discriminator loss: 0.148690, acc.: 93.75%] [Generator loss: 8.599369]\n",
      "15852 [Discriminator loss: 0.065020, acc.: 93.75%] [Generator loss: 8.400066]\n",
      "15853 [Discriminator loss: 0.113251, acc.: 96.88%] [Generator loss: 6.940251]\n",
      "15854 [Discriminator loss: 0.053067, acc.: 98.44%] [Generator loss: 8.299417]\n",
      "15855 [Discriminator loss: 0.041078, acc.: 98.44%] [Generator loss: 7.072029]\n",
      "15856 [Discriminator loss: 0.068443, acc.: 98.44%] [Generator loss: 6.956672]\n",
      "15857 [Discriminator loss: 0.063629, acc.: 96.88%] [Generator loss: 7.877650]\n",
      "15858 [Discriminator loss: 0.199754, acc.: 89.06%] [Generator loss: 7.700270]\n",
      "15859 [Discriminator loss: 0.017548, acc.: 100.00%] [Generator loss: 8.937054]\n",
      "15860 [Discriminator loss: 0.074966, acc.: 96.88%] [Generator loss: 6.550672]\n",
      "15861 [Discriminator loss: 0.035807, acc.: 98.44%] [Generator loss: 6.249271]\n",
      "15862 [Discriminator loss: 0.130050, acc.: 93.75%] [Generator loss: 7.865544]\n",
      "15863 [Discriminator loss: 0.244506, acc.: 92.19%] [Generator loss: 8.868829]\n",
      "15864 [Discriminator loss: 0.129190, acc.: 96.88%] [Generator loss: 7.739923]\n",
      "15865 [Discriminator loss: 0.051383, acc.: 98.44%] [Generator loss: 7.806208]\n",
      "15866 [Discriminator loss: 0.199685, acc.: 90.62%] [Generator loss: 9.840874]\n",
      "15867 [Discriminator loss: 0.058159, acc.: 96.88%] [Generator loss: 9.578195]\n",
      "15868 [Discriminator loss: 0.086576, acc.: 95.31%] [Generator loss: 8.854918]\n",
      "15869 [Discriminator loss: 0.110986, acc.: 93.75%] [Generator loss: 6.873953]\n",
      "15870 [Discriminator loss: 0.026474, acc.: 100.00%] [Generator loss: 7.765493]\n",
      "15871 [Discriminator loss: 0.016073, acc.: 100.00%] [Generator loss: 7.244998]\n",
      "15872 [Discriminator loss: 0.084492, acc.: 95.31%] [Generator loss: 8.699598]\n",
      "15873 [Discriminator loss: 0.044341, acc.: 96.88%] [Generator loss: 8.650146]\n",
      "15874 [Discriminator loss: 0.207350, acc.: 93.75%] [Generator loss: 6.919927]\n",
      "15875 [Discriminator loss: 0.133871, acc.: 95.31%] [Generator loss: 9.656987]\n",
      "15876 [Discriminator loss: 0.075265, acc.: 95.31%] [Generator loss: 8.209631]\n",
      "15877 [Discriminator loss: 0.178206, acc.: 95.31%] [Generator loss: 7.417706]\n",
      "15878 [Discriminator loss: 0.073336, acc.: 98.44%] [Generator loss: 9.738952]\n",
      "15879 [Discriminator loss: 0.100172, acc.: 96.88%] [Generator loss: 8.967538]\n",
      "15880 [Discriminator loss: 0.139741, acc.: 90.62%] [Generator loss: 7.911146]\n",
      "15881 [Discriminator loss: 0.088900, acc.: 96.88%] [Generator loss: 7.669066]\n",
      "15882 [Discriminator loss: 0.086015, acc.: 96.88%] [Generator loss: 9.331713]\n",
      "15883 [Discriminator loss: 0.026514, acc.: 98.44%] [Generator loss: 8.879466]\n",
      "15884 [Discriminator loss: 0.113764, acc.: 93.75%] [Generator loss: 7.606959]\n",
      "15885 [Discriminator loss: 0.027823, acc.: 98.44%] [Generator loss: 8.925714]\n",
      "15886 [Discriminator loss: 0.041232, acc.: 98.44%] [Generator loss: 8.638357]\n",
      "15887 [Discriminator loss: 0.106237, acc.: 95.31%] [Generator loss: 6.428259]\n",
      "15888 [Discriminator loss: 0.028545, acc.: 98.44%] [Generator loss: 8.055005]\n",
      "15889 [Discriminator loss: 0.064100, acc.: 95.31%] [Generator loss: 6.736985]\n",
      "15890 [Discriminator loss: 0.066935, acc.: 98.44%] [Generator loss: 6.858700]\n",
      "15891 [Discriminator loss: 0.099716, acc.: 98.44%] [Generator loss: 7.280793]\n",
      "15892 [Discriminator loss: 0.121152, acc.: 96.88%] [Generator loss: 5.429666]\n",
      "15893 [Discriminator loss: 0.145156, acc.: 95.31%] [Generator loss: 7.790467]\n",
      "15894 [Discriminator loss: 0.044165, acc.: 98.44%] [Generator loss: 9.739820]\n",
      "15895 [Discriminator loss: 0.052879, acc.: 98.44%] [Generator loss: 8.218282]\n",
      "15896 [Discriminator loss: 0.089965, acc.: 98.44%] [Generator loss: 6.610732]\n",
      "15897 [Discriminator loss: 0.156466, acc.: 93.75%] [Generator loss: 8.212919]\n",
      "15898 [Discriminator loss: 0.045571, acc.: 96.88%] [Generator loss: 8.095330]\n",
      "15899 [Discriminator loss: 0.093298, acc.: 95.31%] [Generator loss: 6.883040]\n",
      "15900 [Discriminator loss: 0.011317, acc.: 100.00%] [Generator loss: 7.155893]\n",
      "15901 [Discriminator loss: 0.096436, acc.: 98.44%] [Generator loss: 6.406717]\n",
      "15902 [Discriminator loss: 0.057725, acc.: 98.44%] [Generator loss: 7.707969]\n",
      "15903 [Discriminator loss: 0.156951, acc.: 93.75%] [Generator loss: 8.081272]\n",
      "15904 [Discriminator loss: 0.161663, acc.: 95.31%] [Generator loss: 8.912885]\n",
      "15905 [Discriminator loss: 0.093944, acc.: 93.75%] [Generator loss: 8.104623]\n",
      "15906 [Discriminator loss: 0.061078, acc.: 98.44%] [Generator loss: 8.501486]\n",
      "15907 [Discriminator loss: 0.145438, acc.: 93.75%] [Generator loss: 7.081597]\n",
      "15908 [Discriminator loss: 0.090482, acc.: 95.31%] [Generator loss: 7.418049]\n",
      "15909 [Discriminator loss: 0.053917, acc.: 98.44%] [Generator loss: 8.073486]\n",
      "15910 [Discriminator loss: 0.088576, acc.: 96.88%] [Generator loss: 8.469520]\n",
      "15911 [Discriminator loss: 0.047005, acc.: 98.44%] [Generator loss: 7.527852]\n",
      "15912 [Discriminator loss: 0.049414, acc.: 98.44%] [Generator loss: 8.714012]\n",
      "15913 [Discriminator loss: 0.132843, acc.: 93.75%] [Generator loss: 7.323096]\n",
      "15914 [Discriminator loss: 0.078186, acc.: 96.88%] [Generator loss: 7.631981]\n",
      "15915 [Discriminator loss: 0.053143, acc.: 98.44%] [Generator loss: 9.079794]\n",
      "15916 [Discriminator loss: 0.174157, acc.: 93.75%] [Generator loss: 9.012201]\n",
      "15917 [Discriminator loss: 0.223761, acc.: 92.19%] [Generator loss: 6.883385]\n",
      "15918 [Discriminator loss: 0.166431, acc.: 95.31%] [Generator loss: 7.132932]\n",
      "15919 [Discriminator loss: 0.011814, acc.: 100.00%] [Generator loss: 8.180958]\n",
      "15920 [Discriminator loss: 0.087176, acc.: 96.88%] [Generator loss: 7.629040]\n",
      "15921 [Discriminator loss: 0.057159, acc.: 96.88%] [Generator loss: 7.181501]\n",
      "15922 [Discriminator loss: 0.043018, acc.: 98.44%] [Generator loss: 6.922327]\n",
      "15923 [Discriminator loss: 0.047799, acc.: 98.44%] [Generator loss: 6.483436]\n",
      "15924 [Discriminator loss: 0.103342, acc.: 96.88%] [Generator loss: 9.460131]\n",
      "15925 [Discriminator loss: 0.028043, acc.: 100.00%] [Generator loss: 8.870029]\n",
      "15926 [Discriminator loss: 0.060803, acc.: 96.88%] [Generator loss: 7.691949]\n",
      "15927 [Discriminator loss: 0.106359, acc.: 95.31%] [Generator loss: 5.656033]\n",
      "15928 [Discriminator loss: 0.115985, acc.: 93.75%] [Generator loss: 7.867350]\n",
      "15929 [Discriminator loss: 0.024094, acc.: 100.00%] [Generator loss: 8.780396]\n",
      "15930 [Discriminator loss: 0.158909, acc.: 95.31%] [Generator loss: 9.348148]\n",
      "15931 [Discriminator loss: 0.083457, acc.: 96.88%] [Generator loss: 9.954204]\n",
      "15932 [Discriminator loss: 0.038993, acc.: 98.44%] [Generator loss: 8.406635]\n",
      "15933 [Discriminator loss: 0.101385, acc.: 96.88%] [Generator loss: 7.454353]\n",
      "15934 [Discriminator loss: 0.008216, acc.: 100.00%] [Generator loss: 8.480125]\n",
      "15935 [Discriminator loss: 0.057935, acc.: 98.44%] [Generator loss: 8.035313]\n",
      "15936 [Discriminator loss: 0.093129, acc.: 95.31%] [Generator loss: 6.156360]\n",
      "15937 [Discriminator loss: 0.136716, acc.: 92.19%] [Generator loss: 8.877665]\n",
      "15938 [Discriminator loss: 0.040564, acc.: 98.44%] [Generator loss: 9.240259]\n",
      "15939 [Discriminator loss: 0.121282, acc.: 93.75%] [Generator loss: 8.990458]\n",
      "15940 [Discriminator loss: 0.049194, acc.: 98.44%] [Generator loss: 8.018999]\n",
      "15941 [Discriminator loss: 0.038304, acc.: 100.00%] [Generator loss: 6.161316]\n",
      "15942 [Discriminator loss: 0.039351, acc.: 98.44%] [Generator loss: 8.379654]\n",
      "15943 [Discriminator loss: 0.037344, acc.: 98.44%] [Generator loss: 9.388573]\n",
      "15944 [Discriminator loss: 0.053006, acc.: 98.44%] [Generator loss: 10.069234]\n",
      "15945 [Discriminator loss: 0.031244, acc.: 100.00%] [Generator loss: 8.415932]\n",
      "15946 [Discriminator loss: 0.071303, acc.: 96.88%] [Generator loss: 5.603473]\n",
      "15947 [Discriminator loss: 0.042272, acc.: 98.44%] [Generator loss: 7.827576]\n",
      "15948 [Discriminator loss: 0.026289, acc.: 100.00%] [Generator loss: 7.115596]\n",
      "15949 [Discriminator loss: 0.019051, acc.: 100.00%] [Generator loss: 7.936098]\n",
      "15950 [Discriminator loss: 0.032851, acc.: 100.00%] [Generator loss: 6.464705]\n",
      "15951 [Discriminator loss: 0.084043, acc.: 95.31%] [Generator loss: 7.585433]\n",
      "15952 [Discriminator loss: 0.068772, acc.: 98.44%] [Generator loss: 9.930538]\n",
      "15953 [Discriminator loss: 0.117244, acc.: 95.31%] [Generator loss: 7.224566]\n",
      "15954 [Discriminator loss: 0.059190, acc.: 98.44%] [Generator loss: 8.261885]\n",
      "15955 [Discriminator loss: 0.086671, acc.: 98.44%] [Generator loss: 7.631186]\n",
      "15956 [Discriminator loss: 0.111804, acc.: 95.31%] [Generator loss: 9.789194]\n",
      "15957 [Discriminator loss: 0.127802, acc.: 95.31%] [Generator loss: 7.726611]\n",
      "15958 [Discriminator loss: 0.127048, acc.: 90.62%] [Generator loss: 6.657701]\n",
      "15959 [Discriminator loss: 0.120342, acc.: 95.31%] [Generator loss: 7.632842]\n",
      "15960 [Discriminator loss: 0.072234, acc.: 95.31%] [Generator loss: 8.385136]\n",
      "15961 [Discriminator loss: 0.051082, acc.: 96.88%] [Generator loss: 7.541485]\n",
      "15962 [Discriminator loss: 0.079228, acc.: 98.44%] [Generator loss: 5.867979]\n",
      "15963 [Discriminator loss: 0.086496, acc.: 98.44%] [Generator loss: 7.513292]\n",
      "15964 [Discriminator loss: 0.024604, acc.: 100.00%] [Generator loss: 7.380662]\n",
      "15965 [Discriminator loss: 0.104705, acc.: 96.88%] [Generator loss: 8.685469]\n",
      "15966 [Discriminator loss: 0.111439, acc.: 96.88%] [Generator loss: 7.875249]\n",
      "15967 [Discriminator loss: 0.162923, acc.: 92.19%] [Generator loss: 9.456804]\n",
      "15968 [Discriminator loss: 0.181153, acc.: 93.75%] [Generator loss: 8.933505]\n",
      "15969 [Discriminator loss: 0.002722, acc.: 100.00%] [Generator loss: 8.608431]\n",
      "15970 [Discriminator loss: 0.009216, acc.: 100.00%] [Generator loss: 6.788418]\n",
      "15971 [Discriminator loss: 0.032164, acc.: 100.00%] [Generator loss: 7.430026]\n",
      "15972 [Discriminator loss: 0.054578, acc.: 96.88%] [Generator loss: 6.225286]\n",
      "15973 [Discriminator loss: 0.122942, acc.: 96.88%] [Generator loss: 8.006096]\n",
      "15974 [Discriminator loss: 0.046855, acc.: 98.44%] [Generator loss: 9.099773]\n",
      "15975 [Discriminator loss: 0.096085, acc.: 96.88%] [Generator loss: 7.638613]\n",
      "15976 [Discriminator loss: 0.045507, acc.: 98.44%] [Generator loss: 8.634077]\n",
      "15977 [Discriminator loss: 0.188293, acc.: 92.19%] [Generator loss: 9.291738]\n",
      "15978 [Discriminator loss: 0.086856, acc.: 98.44%] [Generator loss: 8.950644]\n",
      "15979 [Discriminator loss: 0.186157, acc.: 95.31%] [Generator loss: 8.913986]\n",
      "15980 [Discriminator loss: 0.052734, acc.: 98.44%] [Generator loss: 8.430555]\n",
      "15981 [Discriminator loss: 0.049369, acc.: 98.44%] [Generator loss: 7.102669]\n",
      "15982 [Discriminator loss: 0.087313, acc.: 95.31%] [Generator loss: 6.970582]\n",
      "15983 [Discriminator loss: 0.042431, acc.: 98.44%] [Generator loss: 8.712334]\n",
      "15984 [Discriminator loss: 0.067178, acc.: 96.88%] [Generator loss: 9.522110]\n",
      "15985 [Discriminator loss: 0.111106, acc.: 95.31%] [Generator loss: 7.883555]\n",
      "15986 [Discriminator loss: 0.092942, acc.: 93.75%] [Generator loss: 8.562634]\n",
      "15987 [Discriminator loss: 0.054154, acc.: 95.31%] [Generator loss: 7.246715]\n",
      "15988 [Discriminator loss: 0.024051, acc.: 100.00%] [Generator loss: 8.191393]\n",
      "15989 [Discriminator loss: 0.159489, acc.: 93.75%] [Generator loss: 7.361668]\n",
      "15990 [Discriminator loss: 0.139642, acc.: 93.75%] [Generator loss: 7.390137]\n",
      "15991 [Discriminator loss: 0.079605, acc.: 95.31%] [Generator loss: 10.117588]\n",
      "15992 [Discriminator loss: 0.106614, acc.: 95.31%] [Generator loss: 6.698405]\n",
      "15993 [Discriminator loss: 0.091456, acc.: 98.44%] [Generator loss: 6.857769]\n",
      "15994 [Discriminator loss: 0.125929, acc.: 96.88%] [Generator loss: 9.024166]\n",
      "15995 [Discriminator loss: 0.033896, acc.: 100.00%] [Generator loss: 8.211483]\n",
      "15996 [Discriminator loss: 0.097629, acc.: 96.88%] [Generator loss: 7.375021]\n",
      "15997 [Discriminator loss: 0.042260, acc.: 100.00%] [Generator loss: 5.655054]\n",
      "15998 [Discriminator loss: 0.052583, acc.: 100.00%] [Generator loss: 6.335187]\n",
      "15999 [Discriminator loss: 0.081833, acc.: 98.44%] [Generator loss: 8.524199]\n",
      "16000 [Discriminator loss: 0.017571, acc.: 100.00%] [Generator loss: 7.630840]\n",
      "16001 [Discriminator loss: 0.068993, acc.: 96.88%] [Generator loss: 7.697390]\n",
      "16002 [Discriminator loss: 0.027196, acc.: 100.00%] [Generator loss: 6.902840]\n",
      "16003 [Discriminator loss: 0.093318, acc.: 96.88%] [Generator loss: 8.333118]\n",
      "16004 [Discriminator loss: 0.098397, acc.: 96.88%] [Generator loss: 9.526244]\n",
      "16005 [Discriminator loss: 0.121793, acc.: 93.75%] [Generator loss: 6.428328]\n",
      "16006 [Discriminator loss: 0.048082, acc.: 96.88%] [Generator loss: 8.190485]\n",
      "16007 [Discriminator loss: 0.043222, acc.: 100.00%] [Generator loss: 7.919460]\n",
      "16008 [Discriminator loss: 0.142423, acc.: 93.75%] [Generator loss: 8.192431]\n",
      "16009 [Discriminator loss: 0.035810, acc.: 98.44%] [Generator loss: 9.402414]\n",
      "16010 [Discriminator loss: 0.171162, acc.: 92.19%] [Generator loss: 6.554533]\n",
      "16011 [Discriminator loss: 0.065824, acc.: 96.88%] [Generator loss: 6.544473]\n",
      "16012 [Discriminator loss: 0.288120, acc.: 92.19%] [Generator loss: 8.453082]\n",
      "16013 [Discriminator loss: 0.094594, acc.: 96.88%] [Generator loss: 8.409084]\n",
      "16014 [Discriminator loss: 0.135454, acc.: 93.75%] [Generator loss: 8.733433]\n",
      "16015 [Discriminator loss: 0.075064, acc.: 98.44%] [Generator loss: 8.015912]\n",
      "16016 [Discriminator loss: 0.052665, acc.: 100.00%] [Generator loss: 7.369115]\n",
      "16017 [Discriminator loss: 0.075296, acc.: 95.31%] [Generator loss: 8.651662]\n",
      "16018 [Discriminator loss: 0.043313, acc.: 98.44%] [Generator loss: 8.779209]\n",
      "16019 [Discriminator loss: 0.051770, acc.: 96.88%] [Generator loss: 8.614565]\n",
      "16020 [Discriminator loss: 0.101836, acc.: 93.75%] [Generator loss: 7.440061]\n",
      "16021 [Discriminator loss: 0.063065, acc.: 98.44%] [Generator loss: 5.950945]\n",
      "16022 [Discriminator loss: 0.096406, acc.: 96.88%] [Generator loss: 7.015646]\n",
      "16023 [Discriminator loss: 0.097730, acc.: 93.75%] [Generator loss: 7.080908]\n",
      "16024 [Discriminator loss: 0.062359, acc.: 96.88%] [Generator loss: 7.681688]\n",
      "16025 [Discriminator loss: 0.019945, acc.: 100.00%] [Generator loss: 8.347694]\n",
      "16026 [Discriminator loss: 0.092306, acc.: 93.75%] [Generator loss: 6.736895]\n",
      "16027 [Discriminator loss: 0.030561, acc.: 100.00%] [Generator loss: 7.838862]\n",
      "16028 [Discriminator loss: 0.192964, acc.: 93.75%] [Generator loss: 7.936045]\n",
      "16029 [Discriminator loss: 0.023856, acc.: 100.00%] [Generator loss: 8.713198]\n",
      "16030 [Discriminator loss: 0.309572, acc.: 92.19%] [Generator loss: 8.439798]\n",
      "16031 [Discriminator loss: 0.087946, acc.: 96.88%] [Generator loss: 6.216897]\n",
      "16032 [Discriminator loss: 0.129855, acc.: 93.75%] [Generator loss: 7.167473]\n",
      "16033 [Discriminator loss: 0.026436, acc.: 100.00%] [Generator loss: 8.507644]\n",
      "16034 [Discriminator loss: 0.090264, acc.: 93.75%] [Generator loss: 8.372520]\n",
      "16035 [Discriminator loss: 0.006712, acc.: 100.00%] [Generator loss: 8.267559]\n",
      "16036 [Discriminator loss: 0.088008, acc.: 98.44%] [Generator loss: 7.598607]\n",
      "16037 [Discriminator loss: 0.116328, acc.: 93.75%] [Generator loss: 6.608840]\n",
      "16038 [Discriminator loss: 0.087529, acc.: 96.88%] [Generator loss: 7.685900]\n",
      "16039 [Discriminator loss: 0.053148, acc.: 98.44%] [Generator loss: 8.863686]\n",
      "16040 [Discriminator loss: 0.031021, acc.: 100.00%] [Generator loss: 7.750458]\n",
      "16041 [Discriminator loss: 0.149303, acc.: 92.19%] [Generator loss: 6.757333]\n",
      "16042 [Discriminator loss: 0.056712, acc.: 98.44%] [Generator loss: 8.118184]\n",
      "16043 [Discriminator loss: 0.336168, acc.: 87.50%] [Generator loss: 8.166600]\n",
      "16044 [Discriminator loss: 0.067499, acc.: 98.44%] [Generator loss: 9.267462]\n",
      "16045 [Discriminator loss: 0.044588, acc.: 98.44%] [Generator loss: 8.982285]\n",
      "16046 [Discriminator loss: 0.107335, acc.: 96.88%] [Generator loss: 9.271053]\n",
      "16047 [Discriminator loss: 0.043437, acc.: 100.00%] [Generator loss: 7.681090]\n",
      "16048 [Discriminator loss: 0.033273, acc.: 100.00%] [Generator loss: 7.421901]\n",
      "16049 [Discriminator loss: 0.085367, acc.: 98.44%] [Generator loss: 7.418266]\n",
      "16050 [Discriminator loss: 0.044736, acc.: 98.44%] [Generator loss: 8.454618]\n",
      "16051 [Discriminator loss: 0.241720, acc.: 90.62%] [Generator loss: 8.691189]\n",
      "16052 [Discriminator loss: 0.081716, acc.: 96.88%] [Generator loss: 9.145868]\n",
      "16053 [Discriminator loss: 0.157617, acc.: 93.75%] [Generator loss: 10.207005]\n",
      "16054 [Discriminator loss: 0.097200, acc.: 96.88%] [Generator loss: 7.921907]\n",
      "16055 [Discriminator loss: 0.134258, acc.: 95.31%] [Generator loss: 6.348217]\n",
      "16056 [Discriminator loss: 0.062282, acc.: 96.88%] [Generator loss: 9.561670]\n",
      "16057 [Discriminator loss: 0.160760, acc.: 95.31%] [Generator loss: 9.660460]\n",
      "16058 [Discriminator loss: 0.311980, acc.: 90.62%] [Generator loss: 8.970714]\n",
      "16059 [Discriminator loss: 0.037048, acc.: 98.44%] [Generator loss: 10.091698]\n",
      "16060 [Discriminator loss: 0.063356, acc.: 96.88%] [Generator loss: 8.930905]\n",
      "16061 [Discriminator loss: 0.058207, acc.: 98.44%] [Generator loss: 7.809479]\n",
      "16062 [Discriminator loss: 0.082498, acc.: 98.44%] [Generator loss: 7.552908]\n",
      "16063 [Discriminator loss: 0.072320, acc.: 98.44%] [Generator loss: 6.530479]\n",
      "16064 [Discriminator loss: 0.028905, acc.: 100.00%] [Generator loss: 8.452797]\n",
      "16065 [Discriminator loss: 0.009786, acc.: 100.00%] [Generator loss: 7.534245]\n",
      "16066 [Discriminator loss: 0.238227, acc.: 92.19%] [Generator loss: 7.715159]\n",
      "16067 [Discriminator loss: 0.033607, acc.: 98.44%] [Generator loss: 7.642451]\n",
      "16068 [Discriminator loss: 0.113266, acc.: 93.75%] [Generator loss: 10.107794]\n",
      "16069 [Discriminator loss: 0.099412, acc.: 96.88%] [Generator loss: 7.961176]\n",
      "16070 [Discriminator loss: 0.072077, acc.: 98.44%] [Generator loss: 8.083282]\n",
      "16071 [Discriminator loss: 0.199434, acc.: 89.06%] [Generator loss: 7.601387]\n",
      "16072 [Discriminator loss: 0.009445, acc.: 100.00%] [Generator loss: 7.361497]\n",
      "16073 [Discriminator loss: 0.086297, acc.: 96.88%] [Generator loss: 6.295941]\n",
      "16074 [Discriminator loss: 0.038972, acc.: 98.44%] [Generator loss: 8.408260]\n",
      "16075 [Discriminator loss: 0.159217, acc.: 93.75%] [Generator loss: 6.792881]\n",
      "16076 [Discriminator loss: 0.174023, acc.: 90.62%] [Generator loss: 6.999473]\n",
      "16077 [Discriminator loss: 0.060613, acc.: 96.88%] [Generator loss: 7.762206]\n",
      "16078 [Discriminator loss: 0.083273, acc.: 96.88%] [Generator loss: 8.940818]\n",
      "16079 [Discriminator loss: 0.044157, acc.: 98.44%] [Generator loss: 8.806535]\n",
      "16080 [Discriminator loss: 0.120784, acc.: 93.75%] [Generator loss: 8.314430]\n",
      "16081 [Discriminator loss: 0.022682, acc.: 100.00%] [Generator loss: 9.394186]\n",
      "16082 [Discriminator loss: 0.414655, acc.: 84.38%] [Generator loss: 7.022913]\n",
      "16083 [Discriminator loss: 0.069932, acc.: 98.44%] [Generator loss: 8.490374]\n",
      "16084 [Discriminator loss: 0.075305, acc.: 98.44%] [Generator loss: 7.124164]\n",
      "16085 [Discriminator loss: 0.045674, acc.: 96.88%] [Generator loss: 8.171168]\n",
      "16086 [Discriminator loss: 0.112494, acc.: 95.31%] [Generator loss: 7.755918]\n",
      "16087 [Discriminator loss: 0.101513, acc.: 96.88%] [Generator loss: 6.809844]\n",
      "16088 [Discriminator loss: 0.042113, acc.: 98.44%] [Generator loss: 9.219166]\n",
      "16089 [Discriminator loss: 0.014460, acc.: 100.00%] [Generator loss: 6.076721]\n",
      "16090 [Discriminator loss: 0.154678, acc.: 95.31%] [Generator loss: 7.231805]\n",
      "16091 [Discriminator loss: 0.104660, acc.: 96.88%] [Generator loss: 8.070580]\n",
      "16092 [Discriminator loss: 0.104594, acc.: 96.88%] [Generator loss: 7.437964]\n",
      "16093 [Discriminator loss: 0.075230, acc.: 95.31%] [Generator loss: 10.384993]\n",
      "16094 [Discriminator loss: 0.049158, acc.: 96.88%] [Generator loss: 9.223544]\n",
      "16095 [Discriminator loss: 0.290443, acc.: 85.94%] [Generator loss: 7.896761]\n",
      "16096 [Discriminator loss: 0.059079, acc.: 96.88%] [Generator loss: 7.332216]\n",
      "16097 [Discriminator loss: 0.232643, acc.: 93.75%] [Generator loss: 7.627063]\n",
      "16098 [Discriminator loss: 0.070936, acc.: 98.44%] [Generator loss: 8.632084]\n",
      "16099 [Discriminator loss: 0.070955, acc.: 96.88%] [Generator loss: 8.327854]\n",
      "16100 [Discriminator loss: 0.045107, acc.: 96.88%] [Generator loss: 8.216697]\n",
      "16101 [Discriminator loss: 0.037469, acc.: 98.44%] [Generator loss: 6.967577]\n",
      "16102 [Discriminator loss: 0.066651, acc.: 96.88%] [Generator loss: 8.805521]\n",
      "16103 [Discriminator loss: 0.035858, acc.: 98.44%] [Generator loss: 8.340088]\n",
      "16104 [Discriminator loss: 0.096563, acc.: 95.31%] [Generator loss: 9.345667]\n",
      "16105 [Discriminator loss: 0.031960, acc.: 100.00%] [Generator loss: 9.291080]\n",
      "16106 [Discriminator loss: 0.128492, acc.: 96.88%] [Generator loss: 8.408325]\n",
      "16107 [Discriminator loss: 0.117134, acc.: 96.88%] [Generator loss: 6.891177]\n",
      "16108 [Discriminator loss: 0.103108, acc.: 95.31%] [Generator loss: 8.791492]\n",
      "16109 [Discriminator loss: 0.014992, acc.: 100.00%] [Generator loss: 8.839767]\n",
      "16110 [Discriminator loss: 0.018278, acc.: 100.00%] [Generator loss: 7.801701]\n",
      "16111 [Discriminator loss: 0.042202, acc.: 100.00%] [Generator loss: 5.800633]\n",
      "16112 [Discriminator loss: 0.154733, acc.: 96.88%] [Generator loss: 7.834270]\n",
      "16113 [Discriminator loss: 0.022308, acc.: 100.00%] [Generator loss: 7.267142]\n",
      "16114 [Discriminator loss: 0.070011, acc.: 96.88%] [Generator loss: 8.663221]\n",
      "16115 [Discriminator loss: 0.031339, acc.: 100.00%] [Generator loss: 7.884947]\n",
      "16116 [Discriminator loss: 0.139137, acc.: 93.75%] [Generator loss: 7.813440]\n",
      "16117 [Discriminator loss: 0.038923, acc.: 98.44%] [Generator loss: 8.151083]\n",
      "16118 [Discriminator loss: 0.103726, acc.: 95.31%] [Generator loss: 6.660685]\n",
      "16119 [Discriminator loss: 0.175857, acc.: 95.31%] [Generator loss: 8.204349]\n",
      "16120 [Discriminator loss: 0.075507, acc.: 98.44%] [Generator loss: 9.877732]\n",
      "16121 [Discriminator loss: 0.157952, acc.: 95.31%] [Generator loss: 8.084469]\n",
      "16122 [Discriminator loss: 0.021567, acc.: 100.00%] [Generator loss: 8.572458]\n",
      "16123 [Discriminator loss: 0.007855, acc.: 100.00%] [Generator loss: 9.045679]\n",
      "16124 [Discriminator loss: 0.028222, acc.: 100.00%] [Generator loss: 7.813669]\n",
      "16125 [Discriminator loss: 0.037529, acc.: 98.44%] [Generator loss: 8.150970]\n",
      "16126 [Discriminator loss: 0.035613, acc.: 98.44%] [Generator loss: 8.123987]\n",
      "16127 [Discriminator loss: 0.019775, acc.: 100.00%] [Generator loss: 8.151052]\n",
      "16128 [Discriminator loss: 0.142205, acc.: 92.19%] [Generator loss: 7.269041]\n",
      "16129 [Discriminator loss: 0.069351, acc.: 95.31%] [Generator loss: 9.549274]\n",
      "16130 [Discriminator loss: 0.052375, acc.: 98.44%] [Generator loss: 8.702835]\n",
      "16131 [Discriminator loss: 0.053468, acc.: 98.44%] [Generator loss: 8.183825]\n",
      "16132 [Discriminator loss: 0.051727, acc.: 98.44%] [Generator loss: 6.020838]\n",
      "16133 [Discriminator loss: 0.031758, acc.: 100.00%] [Generator loss: 6.447193]\n",
      "16134 [Discriminator loss: 0.040934, acc.: 98.44%] [Generator loss: 7.408908]\n",
      "16135 [Discriminator loss: 0.054761, acc.: 95.31%] [Generator loss: 6.752237]\n",
      "16136 [Discriminator loss: 0.138497, acc.: 95.31%] [Generator loss: 9.371283]\n",
      "16137 [Discriminator loss: 0.070239, acc.: 98.44%] [Generator loss: 7.679544]\n",
      "16138 [Discriminator loss: 0.103562, acc.: 96.88%] [Generator loss: 8.545406]\n",
      "16139 [Discriminator loss: 0.225220, acc.: 90.62%] [Generator loss: 7.114131]\n",
      "16140 [Discriminator loss: 0.039376, acc.: 98.44%] [Generator loss: 5.946396]\n",
      "16141 [Discriminator loss: 0.148800, acc.: 93.75%] [Generator loss: 9.265244]\n",
      "16142 [Discriminator loss: 0.045412, acc.: 98.44%] [Generator loss: 8.802318]\n",
      "16143 [Discriminator loss: 0.015348, acc.: 100.00%] [Generator loss: 8.386330]\n",
      "16144 [Discriminator loss: 0.148072, acc.: 95.31%] [Generator loss: 7.604024]\n",
      "16145 [Discriminator loss: 0.008425, acc.: 100.00%] [Generator loss: 9.397751]\n",
      "16146 [Discriminator loss: 0.016525, acc.: 100.00%] [Generator loss: 9.532674]\n",
      "16147 [Discriminator loss: 0.013067, acc.: 100.00%] [Generator loss: 9.734131]\n",
      "16148 [Discriminator loss: 0.245464, acc.: 89.06%] [Generator loss: 8.352476]\n",
      "16149 [Discriminator loss: 0.019731, acc.: 100.00%] [Generator loss: 7.833921]\n",
      "16150 [Discriminator loss: 0.058417, acc.: 96.88%] [Generator loss: 9.156595]\n",
      "16151 [Discriminator loss: 0.038278, acc.: 100.00%] [Generator loss: 8.278718]\n",
      "16152 [Discriminator loss: 0.044270, acc.: 100.00%] [Generator loss: 8.812609]\n",
      "16153 [Discriminator loss: 0.126321, acc.: 93.75%] [Generator loss: 6.833584]\n",
      "16154 [Discriminator loss: 0.154339, acc.: 92.19%] [Generator loss: 9.009857]\n",
      "16155 [Discriminator loss: 0.067847, acc.: 98.44%] [Generator loss: 8.635389]\n",
      "16156 [Discriminator loss: 0.166417, acc.: 95.31%] [Generator loss: 7.374281]\n",
      "16157 [Discriminator loss: 0.039106, acc.: 100.00%] [Generator loss: 7.207792]\n",
      "16158 [Discriminator loss: 0.087640, acc.: 95.31%] [Generator loss: 8.792578]\n",
      "16159 [Discriminator loss: 0.043939, acc.: 98.44%] [Generator loss: 7.626356]\n",
      "16160 [Discriminator loss: 0.080233, acc.: 95.31%] [Generator loss: 7.163958]\n",
      "16161 [Discriminator loss: 0.134974, acc.: 95.31%] [Generator loss: 6.199031]\n",
      "16162 [Discriminator loss: 0.105032, acc.: 96.88%] [Generator loss: 7.672656]\n",
      "16163 [Discriminator loss: 0.082172, acc.: 98.44%] [Generator loss: 8.248590]\n",
      "16164 [Discriminator loss: 0.063451, acc.: 96.88%] [Generator loss: 8.209139]\n",
      "16165 [Discriminator loss: 0.100481, acc.: 92.19%] [Generator loss: 6.998801]\n",
      "16166 [Discriminator loss: 0.043859, acc.: 96.88%] [Generator loss: 5.587787]\n",
      "16167 [Discriminator loss: 0.085910, acc.: 95.31%] [Generator loss: 8.350294]\n",
      "16168 [Discriminator loss: 0.028019, acc.: 100.00%] [Generator loss: 8.439130]\n",
      "16169 [Discriminator loss: 0.075034, acc.: 98.44%] [Generator loss: 5.757047]\n",
      "16170 [Discriminator loss: 0.129967, acc.: 93.75%] [Generator loss: 9.202045]\n",
      "16171 [Discriminator loss: 0.050412, acc.: 98.44%] [Generator loss: 9.230624]\n",
      "16172 [Discriminator loss: 0.080217, acc.: 98.44%] [Generator loss: 8.250912]\n",
      "16173 [Discriminator loss: 0.019162, acc.: 100.00%] [Generator loss: 8.077621]\n",
      "16174 [Discriminator loss: 0.182371, acc.: 93.75%] [Generator loss: 9.145016]\n",
      "16175 [Discriminator loss: 0.049046, acc.: 96.88%] [Generator loss: 9.058058]\n",
      "16176 [Discriminator loss: 0.071124, acc.: 98.44%] [Generator loss: 7.569520]\n",
      "16177 [Discriminator loss: 0.150609, acc.: 92.19%] [Generator loss: 6.735590]\n",
      "16178 [Discriminator loss: 0.170385, acc.: 95.31%] [Generator loss: 8.129322]\n",
      "16179 [Discriminator loss: 0.025204, acc.: 98.44%] [Generator loss: 9.839729]\n",
      "16180 [Discriminator loss: 0.104965, acc.: 95.31%] [Generator loss: 7.921659]\n",
      "16181 [Discriminator loss: 0.076978, acc.: 95.31%] [Generator loss: 9.046192]\n",
      "16182 [Discriminator loss: 0.038772, acc.: 98.44%] [Generator loss: 7.645792]\n",
      "16183 [Discriminator loss: 0.077409, acc.: 98.44%] [Generator loss: 5.573137]\n",
      "16184 [Discriminator loss: 0.173760, acc.: 90.62%] [Generator loss: 8.552217]\n",
      "16185 [Discriminator loss: 0.020599, acc.: 100.00%] [Generator loss: 10.242903]\n",
      "16186 [Discriminator loss: 0.036472, acc.: 100.00%] [Generator loss: 7.079708]\n",
      "16187 [Discriminator loss: 0.055672, acc.: 96.88%] [Generator loss: 7.764441]\n",
      "16188 [Discriminator loss: 0.022278, acc.: 100.00%] [Generator loss: 8.323478]\n",
      "16189 [Discriminator loss: 0.046541, acc.: 98.44%] [Generator loss: 9.148495]\n",
      "16190 [Discriminator loss: 0.086578, acc.: 95.31%] [Generator loss: 7.977019]\n",
      "16191 [Discriminator loss: 0.146551, acc.: 90.62%] [Generator loss: 7.092863]\n",
      "16192 [Discriminator loss: 0.072502, acc.: 96.88%] [Generator loss: 8.253579]\n",
      "16193 [Discriminator loss: 0.091356, acc.: 96.88%] [Generator loss: 7.250105]\n",
      "16194 [Discriminator loss: 0.034637, acc.: 100.00%] [Generator loss: 8.853642]\n",
      "16195 [Discriminator loss: 0.063822, acc.: 98.44%] [Generator loss: 8.278271]\n",
      "16196 [Discriminator loss: 0.071799, acc.: 95.31%] [Generator loss: 8.309514]\n",
      "16197 [Discriminator loss: 0.039626, acc.: 100.00%] [Generator loss: 5.890664]\n",
      "16198 [Discriminator loss: 0.051834, acc.: 96.88%] [Generator loss: 7.647331]\n",
      "16199 [Discriminator loss: 0.109786, acc.: 95.31%] [Generator loss: 8.797714]\n",
      "16200 [Discriminator loss: 0.186910, acc.: 92.19%] [Generator loss: 7.791731]\n",
      "16201 [Discriminator loss: 0.038231, acc.: 98.44%] [Generator loss: 7.960095]\n",
      "16202 [Discriminator loss: 0.135832, acc.: 90.62%] [Generator loss: 5.931566]\n",
      "16203 [Discriminator loss: 0.103383, acc.: 95.31%] [Generator loss: 7.228465]\n",
      "16204 [Discriminator loss: 0.056170, acc.: 98.44%] [Generator loss: 9.558721]\n",
      "16205 [Discriminator loss: 0.057717, acc.: 96.88%] [Generator loss: 8.179273]\n",
      "16206 [Discriminator loss: 0.274045, acc.: 85.94%] [Generator loss: 9.011918]\n",
      "16207 [Discriminator loss: 0.013025, acc.: 100.00%] [Generator loss: 10.945971]\n",
      "16208 [Discriminator loss: 0.038387, acc.: 98.44%] [Generator loss: 9.705256]\n",
      "16209 [Discriminator loss: 0.022015, acc.: 100.00%] [Generator loss: 8.757223]\n",
      "16210 [Discriminator loss: 0.035950, acc.: 98.44%] [Generator loss: 7.967638]\n",
      "16211 [Discriminator loss: 0.054060, acc.: 98.44%] [Generator loss: 8.409840]\n",
      "16212 [Discriminator loss: 0.142428, acc.: 93.75%] [Generator loss: 6.775631]\n",
      "16213 [Discriminator loss: 0.067790, acc.: 96.88%] [Generator loss: 7.534596]\n",
      "16214 [Discriminator loss: 0.142678, acc.: 95.31%] [Generator loss: 9.188208]\n",
      "16215 [Discriminator loss: 0.053530, acc.: 95.31%] [Generator loss: 9.546725]\n",
      "16216 [Discriminator loss: 0.068480, acc.: 98.44%] [Generator loss: 8.969719]\n",
      "16217 [Discriminator loss: 0.164692, acc.: 93.75%] [Generator loss: 7.964166]\n",
      "16218 [Discriminator loss: 0.034983, acc.: 98.44%] [Generator loss: 7.758363]\n",
      "16219 [Discriminator loss: 0.151288, acc.: 95.31%] [Generator loss: 9.954382]\n",
      "16220 [Discriminator loss: 0.090584, acc.: 98.44%] [Generator loss: 9.271645]\n",
      "16221 [Discriminator loss: 0.066941, acc.: 98.44%] [Generator loss: 8.075813]\n",
      "16222 [Discriminator loss: 0.061652, acc.: 98.44%] [Generator loss: 7.281108]\n",
      "16223 [Discriminator loss: 0.025897, acc.: 98.44%] [Generator loss: 7.448832]\n",
      "16224 [Discriminator loss: 0.099875, acc.: 96.88%] [Generator loss: 6.994361]\n",
      "16225 [Discriminator loss: 0.122748, acc.: 95.31%] [Generator loss: 8.831198]\n",
      "16226 [Discriminator loss: 0.118754, acc.: 95.31%] [Generator loss: 8.514342]\n",
      "16227 [Discriminator loss: 0.032396, acc.: 98.44%] [Generator loss: 8.970295]\n",
      "16228 [Discriminator loss: 0.486249, acc.: 79.69%] [Generator loss: 6.617234]\n",
      "16229 [Discriminator loss: 0.071939, acc.: 96.88%] [Generator loss: 6.787669]\n",
      "16230 [Discriminator loss: 0.027843, acc.: 100.00%] [Generator loss: 7.589709]\n",
      "16231 [Discriminator loss: 0.011202, acc.: 100.00%] [Generator loss: 8.294218]\n",
      "16232 [Discriminator loss: 0.029497, acc.: 98.44%] [Generator loss: 8.372325]\n",
      "16233 [Discriminator loss: 0.443581, acc.: 87.50%] [Generator loss: 8.779489]\n",
      "16234 [Discriminator loss: 0.020893, acc.: 100.00%] [Generator loss: 8.723143]\n",
      "16235 [Discriminator loss: 0.122993, acc.: 95.31%] [Generator loss: 8.017427]\n",
      "16236 [Discriminator loss: 0.066602, acc.: 96.88%] [Generator loss: 8.304271]\n",
      "16237 [Discriminator loss: 0.021446, acc.: 100.00%] [Generator loss: 8.103975]\n",
      "16238 [Discriminator loss: 0.079837, acc.: 95.31%] [Generator loss: 7.447843]\n",
      "16239 [Discriminator loss: 0.075621, acc.: 96.88%] [Generator loss: 7.716148]\n",
      "16240 [Discriminator loss: 0.077627, acc.: 96.88%] [Generator loss: 9.763817]\n",
      "16241 [Discriminator loss: 0.156397, acc.: 93.75%] [Generator loss: 6.793297]\n",
      "16242 [Discriminator loss: 0.019449, acc.: 100.00%] [Generator loss: 8.462281]\n",
      "16243 [Discriminator loss: 0.046429, acc.: 96.88%] [Generator loss: 8.537318]\n",
      "16244 [Discriminator loss: 0.042145, acc.: 98.44%] [Generator loss: 8.570558]\n",
      "16245 [Discriminator loss: 0.075537, acc.: 95.31%] [Generator loss: 8.089061]\n",
      "16246 [Discriminator loss: 0.179390, acc.: 95.31%] [Generator loss: 9.698346]\n",
      "16247 [Discriminator loss: 0.045932, acc.: 100.00%] [Generator loss: 8.590316]\n",
      "16248 [Discriminator loss: 0.019366, acc.: 100.00%] [Generator loss: 8.575620]\n",
      "16249 [Discriminator loss: 0.019309, acc.: 100.00%] [Generator loss: 8.331549]\n",
      "16250 [Discriminator loss: 0.059624, acc.: 96.88%] [Generator loss: 7.324108]\n",
      "16251 [Discriminator loss: 0.036196, acc.: 98.44%] [Generator loss: 8.355243]\n",
      "16252 [Discriminator loss: 0.250303, acc.: 92.19%] [Generator loss: 7.498701]\n",
      "16253 [Discriminator loss: 0.109458, acc.: 93.75%] [Generator loss: 9.557409]\n",
      "16254 [Discriminator loss: 0.084770, acc.: 93.75%] [Generator loss: 8.658146]\n",
      "16255 [Discriminator loss: 0.109480, acc.: 92.19%] [Generator loss: 6.938933]\n",
      "16256 [Discriminator loss: 0.033828, acc.: 100.00%] [Generator loss: 7.315571]\n",
      "16257 [Discriminator loss: 0.070976, acc.: 96.88%] [Generator loss: 8.363059]\n",
      "16258 [Discriminator loss: 0.027089, acc.: 100.00%] [Generator loss: 9.077494]\n",
      "16259 [Discriminator loss: 0.050350, acc.: 96.88%] [Generator loss: 8.632902]\n",
      "16260 [Discriminator loss: 0.136251, acc.: 95.31%] [Generator loss: 6.663822]\n",
      "16261 [Discriminator loss: 0.089252, acc.: 95.31%] [Generator loss: 8.117965]\n",
      "16262 [Discriminator loss: 0.018036, acc.: 100.00%] [Generator loss: 9.290014]\n",
      "16263 [Discriminator loss: 0.065304, acc.: 96.88%] [Generator loss: 7.624516]\n",
      "16264 [Discriminator loss: 0.070064, acc.: 96.88%] [Generator loss: 8.052786]\n",
      "16265 [Discriminator loss: 0.049326, acc.: 98.44%] [Generator loss: 7.308422]\n",
      "16266 [Discriminator loss: 0.070874, acc.: 96.88%] [Generator loss: 8.802830]\n",
      "16267 [Discriminator loss: 0.179816, acc.: 92.19%] [Generator loss: 6.710929]\n",
      "16268 [Discriminator loss: 0.082874, acc.: 95.31%] [Generator loss: 7.669753]\n",
      "16269 [Discriminator loss: 0.054208, acc.: 98.44%] [Generator loss: 6.388433]\n",
      "16270 [Discriminator loss: 0.115751, acc.: 95.31%] [Generator loss: 7.258323]\n",
      "16271 [Discriminator loss: 0.022680, acc.: 100.00%] [Generator loss: 8.683558]\n",
      "16272 [Discriminator loss: 0.068316, acc.: 96.88%] [Generator loss: 7.671707]\n",
      "16273 [Discriminator loss: 0.045969, acc.: 100.00%] [Generator loss: 7.488525]\n",
      "16274 [Discriminator loss: 0.054860, acc.: 98.44%] [Generator loss: 7.691247]\n",
      "16275 [Discriminator loss: 0.029617, acc.: 98.44%] [Generator loss: 8.282594]\n",
      "16276 [Discriminator loss: 0.017853, acc.: 100.00%] [Generator loss: 9.459662]\n",
      "16277 [Discriminator loss: 0.137798, acc.: 95.31%] [Generator loss: 5.386257]\n",
      "16278 [Discriminator loss: 0.065558, acc.: 96.88%] [Generator loss: 6.356549]\n",
      "16279 [Discriminator loss: 0.072484, acc.: 98.44%] [Generator loss: 8.139860]\n",
      "16280 [Discriminator loss: 0.172294, acc.: 90.62%] [Generator loss: 7.999550]\n",
      "16281 [Discriminator loss: 0.057973, acc.: 98.44%] [Generator loss: 8.801952]\n",
      "16282 [Discriminator loss: 0.109173, acc.: 93.75%] [Generator loss: 9.821713]\n",
      "16283 [Discriminator loss: 0.028213, acc.: 98.44%] [Generator loss: 11.442246]\n",
      "16284 [Discriminator loss: 0.086247, acc.: 95.31%] [Generator loss: 8.613312]\n",
      "16285 [Discriminator loss: 0.169252, acc.: 93.75%] [Generator loss: 7.032176]\n",
      "16286 [Discriminator loss: 0.093605, acc.: 96.88%] [Generator loss: 7.913892]\n",
      "16287 [Discriminator loss: 0.074295, acc.: 95.31%] [Generator loss: 10.564127]\n",
      "16288 [Discriminator loss: 0.050922, acc.: 98.44%] [Generator loss: 9.278662]\n",
      "16289 [Discriminator loss: 0.053575, acc.: 98.44%] [Generator loss: 8.371488]\n",
      "16290 [Discriminator loss: 0.138603, acc.: 90.62%] [Generator loss: 10.591636]\n",
      "16291 [Discriminator loss: 0.069160, acc.: 96.88%] [Generator loss: 9.477421]\n",
      "16292 [Discriminator loss: 0.094760, acc.: 96.88%] [Generator loss: 6.554892]\n",
      "16293 [Discriminator loss: 0.069960, acc.: 96.88%] [Generator loss: 6.825945]\n",
      "16294 [Discriminator loss: 0.019403, acc.: 100.00%] [Generator loss: 7.409411]\n",
      "16295 [Discriminator loss: 0.069021, acc.: 98.44%] [Generator loss: 7.327362]\n",
      "16296 [Discriminator loss: 0.055307, acc.: 98.44%] [Generator loss: 7.166580]\n",
      "16297 [Discriminator loss: 0.031922, acc.: 98.44%] [Generator loss: 8.743752]\n",
      "16298 [Discriminator loss: 0.025753, acc.: 100.00%] [Generator loss: 9.136517]\n",
      "16299 [Discriminator loss: 0.098331, acc.: 95.31%] [Generator loss: 7.227554]\n",
      "16300 [Discriminator loss: 0.093012, acc.: 98.44%] [Generator loss: 8.889215]\n",
      "16301 [Discriminator loss: 0.058695, acc.: 96.88%] [Generator loss: 6.458364]\n",
      "16302 [Discriminator loss: 0.118283, acc.: 93.75%] [Generator loss: 8.037720]\n",
      "16303 [Discriminator loss: 0.106002, acc.: 95.31%] [Generator loss: 7.997463]\n",
      "16304 [Discriminator loss: 0.192428, acc.: 92.19%] [Generator loss: 9.503613]\n",
      "16305 [Discriminator loss: 0.029106, acc.: 98.44%] [Generator loss: 10.553695]\n",
      "16306 [Discriminator loss: 0.170931, acc.: 90.62%] [Generator loss: 7.347704]\n",
      "16307 [Discriminator loss: 0.042487, acc.: 98.44%] [Generator loss: 8.233015]\n",
      "16308 [Discriminator loss: 0.054326, acc.: 98.44%] [Generator loss: 6.363055]\n",
      "16309 [Discriminator loss: 0.060754, acc.: 96.88%] [Generator loss: 7.072104]\n",
      "16310 [Discriminator loss: 0.090704, acc.: 95.31%] [Generator loss: 7.423098]\n",
      "16311 [Discriminator loss: 0.130039, acc.: 95.31%] [Generator loss: 7.705263]\n",
      "16312 [Discriminator loss: 0.090264, acc.: 95.31%] [Generator loss: 9.102049]\n",
      "16313 [Discriminator loss: 0.098202, acc.: 96.88%] [Generator loss: 8.751301]\n",
      "16314 [Discriminator loss: 0.224117, acc.: 90.62%] [Generator loss: 6.366408]\n",
      "16315 [Discriminator loss: 0.097288, acc.: 96.88%] [Generator loss: 7.566774]\n",
      "16316 [Discriminator loss: 0.103646, acc.: 93.75%] [Generator loss: 8.047796]\n",
      "16317 [Discriminator loss: 0.122226, acc.: 95.31%] [Generator loss: 8.859825]\n",
      "16318 [Discriminator loss: 0.042677, acc.: 98.44%] [Generator loss: 7.965977]\n",
      "16319 [Discriminator loss: 0.067361, acc.: 96.88%] [Generator loss: 6.510927]\n",
      "16320 [Discriminator loss: 0.103017, acc.: 95.31%] [Generator loss: 7.723116]\n",
      "16321 [Discriminator loss: 0.084251, acc.: 96.88%] [Generator loss: 7.668400]\n",
      "16322 [Discriminator loss: 0.131743, acc.: 95.31%] [Generator loss: 8.358419]\n",
      "16323 [Discriminator loss: 0.101010, acc.: 95.31%] [Generator loss: 6.771318]\n",
      "16324 [Discriminator loss: 0.101628, acc.: 93.75%] [Generator loss: 7.807758]\n",
      "16325 [Discriminator loss: 0.038325, acc.: 100.00%] [Generator loss: 8.842538]\n",
      "16326 [Discriminator loss: 0.097614, acc.: 95.31%] [Generator loss: 8.194468]\n",
      "16327 [Discriminator loss: 0.055419, acc.: 98.44%] [Generator loss: 7.865160]\n",
      "16328 [Discriminator loss: 0.046182, acc.: 100.00%] [Generator loss: 8.114388]\n",
      "16329 [Discriminator loss: 0.042812, acc.: 98.44%] [Generator loss: 7.125482]\n",
      "16330 [Discriminator loss: 0.159521, acc.: 92.19%] [Generator loss: 9.197121]\n",
      "16331 [Discriminator loss: 0.103323, acc.: 96.88%] [Generator loss: 9.033382]\n",
      "16332 [Discriminator loss: 0.103557, acc.: 98.44%] [Generator loss: 6.930684]\n",
      "16333 [Discriminator loss: 0.037276, acc.: 100.00%] [Generator loss: 6.392598]\n",
      "16334 [Discriminator loss: 0.151452, acc.: 95.31%] [Generator loss: 7.763157]\n",
      "16335 [Discriminator loss: 0.033942, acc.: 98.44%] [Generator loss: 10.779626]\n",
      "16336 [Discriminator loss: 0.232380, acc.: 90.62%] [Generator loss: 6.054183]\n",
      "16337 [Discriminator loss: 0.195586, acc.: 93.75%] [Generator loss: 8.191109]\n",
      "16338 [Discriminator loss: 0.011121, acc.: 100.00%] [Generator loss: 9.368185]\n",
      "16339 [Discriminator loss: 0.120551, acc.: 96.88%] [Generator loss: 9.482719]\n",
      "16340 [Discriminator loss: 0.212721, acc.: 92.19%] [Generator loss: 8.002401]\n",
      "16341 [Discriminator loss: 0.026230, acc.: 98.44%] [Generator loss: 8.376117]\n",
      "16342 [Discriminator loss: 0.129439, acc.: 95.31%] [Generator loss: 5.505178]\n",
      "16343 [Discriminator loss: 0.094632, acc.: 95.31%] [Generator loss: 7.432513]\n",
      "16344 [Discriminator loss: 0.019075, acc.: 100.00%] [Generator loss: 8.221098]\n",
      "16345 [Discriminator loss: 0.085759, acc.: 93.75%] [Generator loss: 6.227998]\n",
      "16346 [Discriminator loss: 0.073948, acc.: 95.31%] [Generator loss: 7.442403]\n",
      "16347 [Discriminator loss: 0.014290, acc.: 100.00%] [Generator loss: 10.025920]\n",
      "16348 [Discriminator loss: 0.014031, acc.: 100.00%] [Generator loss: 7.971053]\n",
      "16349 [Discriminator loss: 0.022144, acc.: 100.00%] [Generator loss: 8.703220]\n",
      "16350 [Discriminator loss: 0.159584, acc.: 90.62%] [Generator loss: 4.949241]\n",
      "16351 [Discriminator loss: 0.037830, acc.: 98.44%] [Generator loss: 7.669868]\n",
      "16352 [Discriminator loss: 0.112247, acc.: 96.88%] [Generator loss: 9.404282]\n",
      "16353 [Discriminator loss: 0.112749, acc.: 96.88%] [Generator loss: 8.308353]\n",
      "16354 [Discriminator loss: 0.137727, acc.: 95.31%] [Generator loss: 8.958260]\n",
      "16355 [Discriminator loss: 0.046190, acc.: 98.44%] [Generator loss: 9.291330]\n",
      "16356 [Discriminator loss: 0.113483, acc.: 98.44%] [Generator loss: 8.069000]\n",
      "16357 [Discriminator loss: 0.107695, acc.: 96.88%] [Generator loss: 8.944699]\n",
      "16358 [Discriminator loss: 0.179397, acc.: 90.62%] [Generator loss: 8.613995]\n",
      "16359 [Discriminator loss: 0.071861, acc.: 96.88%] [Generator loss: 8.643593]\n",
      "16360 [Discriminator loss: 0.035166, acc.: 98.44%] [Generator loss: 9.670694]\n",
      "16361 [Discriminator loss: 0.040852, acc.: 100.00%] [Generator loss: 9.829894]\n",
      "16362 [Discriminator loss: 0.061381, acc.: 98.44%] [Generator loss: 10.014288]\n",
      "16363 [Discriminator loss: 0.035649, acc.: 98.44%] [Generator loss: 8.764170]\n",
      "16364 [Discriminator loss: 0.046956, acc.: 98.44%] [Generator loss: 8.258297]\n",
      "16365 [Discriminator loss: 0.109417, acc.: 93.75%] [Generator loss: 6.579484]\n",
      "16366 [Discriminator loss: 0.076056, acc.: 98.44%] [Generator loss: 8.205875]\n",
      "16367 [Discriminator loss: 0.022611, acc.: 100.00%] [Generator loss: 8.883572]\n",
      "16368 [Discriminator loss: 0.135658, acc.: 90.62%] [Generator loss: 6.783501]\n",
      "16369 [Discriminator loss: 0.027621, acc.: 100.00%] [Generator loss: 7.482460]\n",
      "16370 [Discriminator loss: 0.171668, acc.: 90.62%] [Generator loss: 4.891926]\n",
      "16371 [Discriminator loss: 0.118208, acc.: 98.44%] [Generator loss: 8.164661]\n",
      "16372 [Discriminator loss: 0.077800, acc.: 95.31%] [Generator loss: 10.452486]\n",
      "16373 [Discriminator loss: 0.020164, acc.: 100.00%] [Generator loss: 10.515982]\n",
      "16374 [Discriminator loss: 0.194534, acc.: 93.75%] [Generator loss: 5.413499]\n",
      "16375 [Discriminator loss: 0.234499, acc.: 92.19%] [Generator loss: 9.142527]\n",
      "16376 [Discriminator loss: 0.024714, acc.: 100.00%] [Generator loss: 8.106146]\n",
      "16377 [Discriminator loss: 0.087115, acc.: 95.31%] [Generator loss: 7.984035]\n",
      "16378 [Discriminator loss: 0.363470, acc.: 87.50%] [Generator loss: 6.653914]\n",
      "16379 [Discriminator loss: 0.053405, acc.: 98.44%] [Generator loss: 8.805126]\n",
      "16380 [Discriminator loss: 0.027330, acc.: 100.00%] [Generator loss: 8.827229]\n",
      "16381 [Discriminator loss: 0.115823, acc.: 93.75%] [Generator loss: 9.120232]\n",
      "16382 [Discriminator loss: 0.103033, acc.: 93.75%] [Generator loss: 8.180158]\n",
      "16383 [Discriminator loss: 0.072913, acc.: 96.88%] [Generator loss: 8.832738]\n",
      "16384 [Discriminator loss: 0.071517, acc.: 98.44%] [Generator loss: 8.441343]\n",
      "16385 [Discriminator loss: 0.140760, acc.: 95.31%] [Generator loss: 7.771995]\n",
      "16386 [Discriminator loss: 0.079783, acc.: 98.44%] [Generator loss: 6.605973]\n",
      "16387 [Discriminator loss: 0.065886, acc.: 96.88%] [Generator loss: 9.089069]\n",
      "16388 [Discriminator loss: 0.090964, acc.: 95.31%] [Generator loss: 7.854106]\n",
      "16389 [Discriminator loss: 0.102528, acc.: 93.75%] [Generator loss: 6.601251]\n",
      "16390 [Discriminator loss: 0.091206, acc.: 98.44%] [Generator loss: 7.288487]\n",
      "16391 [Discriminator loss: 0.051829, acc.: 98.44%] [Generator loss: 7.636753]\n",
      "16392 [Discriminator loss: 0.180064, acc.: 95.31%] [Generator loss: 7.051547]\n",
      "16393 [Discriminator loss: 0.057266, acc.: 98.44%] [Generator loss: 8.555942]\n",
      "16394 [Discriminator loss: 0.064247, acc.: 98.44%] [Generator loss: 7.444756]\n",
      "16395 [Discriminator loss: 0.111015, acc.: 96.88%] [Generator loss: 7.791736]\n",
      "16396 [Discriminator loss: 0.025100, acc.: 100.00%] [Generator loss: 9.085007]\n",
      "16397 [Discriminator loss: 0.122238, acc.: 92.19%] [Generator loss: 9.423996]\n",
      "16398 [Discriminator loss: 0.022462, acc.: 100.00%] [Generator loss: 9.250076]\n",
      "16399 [Discriminator loss: 0.146314, acc.: 95.31%] [Generator loss: 8.593603]\n",
      "16400 [Discriminator loss: 0.102020, acc.: 95.31%] [Generator loss: 8.638824]\n",
      "16401 [Discriminator loss: 0.065807, acc.: 96.88%] [Generator loss: 7.960230]\n",
      "16402 [Discriminator loss: 0.080441, acc.: 96.88%] [Generator loss: 9.010853]\n",
      "16403 [Discriminator loss: 0.137118, acc.: 93.75%] [Generator loss: 7.886612]\n",
      "16404 [Discriminator loss: 0.015459, acc.: 100.00%] [Generator loss: 8.173997]\n",
      "16405 [Discriminator loss: 0.098260, acc.: 96.88%] [Generator loss: 6.971449]\n",
      "16406 [Discriminator loss: 0.138475, acc.: 93.75%] [Generator loss: 8.346899]\n",
      "16407 [Discriminator loss: 0.017520, acc.: 100.00%] [Generator loss: 10.007915]\n",
      "16408 [Discriminator loss: 0.357583, acc.: 85.94%] [Generator loss: 8.475962]\n",
      "16409 [Discriminator loss: 0.020948, acc.: 100.00%] [Generator loss: 8.506691]\n",
      "16410 [Discriminator loss: 0.026999, acc.: 100.00%] [Generator loss: 8.132538]\n",
      "16411 [Discriminator loss: 0.054991, acc.: 96.88%] [Generator loss: 9.101158]\n",
      "16412 [Discriminator loss: 0.014972, acc.: 100.00%] [Generator loss: 7.170281]\n",
      "16413 [Discriminator loss: 0.069228, acc.: 98.44%] [Generator loss: 7.744688]\n",
      "16414 [Discriminator loss: 0.131021, acc.: 92.19%] [Generator loss: 9.746641]\n",
      "16415 [Discriminator loss: 0.125036, acc.: 95.31%] [Generator loss: 7.960337]\n",
      "16416 [Discriminator loss: 0.061742, acc.: 98.44%] [Generator loss: 8.026268]\n",
      "16417 [Discriminator loss: 0.051698, acc.: 96.88%] [Generator loss: 7.208201]\n",
      "16418 [Discriminator loss: 0.106308, acc.: 98.44%] [Generator loss: 8.016039]\n",
      "16419 [Discriminator loss: 0.086573, acc.: 95.31%] [Generator loss: 6.879564]\n",
      "16420 [Discriminator loss: 0.018475, acc.: 100.00%] [Generator loss: 6.903493]\n",
      "16421 [Discriminator loss: 0.162281, acc.: 90.62%] [Generator loss: 9.582804]\n",
      "16422 [Discriminator loss: 0.312742, acc.: 89.06%] [Generator loss: 8.518238]\n",
      "16423 [Discriminator loss: 0.026101, acc.: 100.00%] [Generator loss: 7.727291]\n",
      "16424 [Discriminator loss: 0.132273, acc.: 93.75%] [Generator loss: 8.030087]\n",
      "16425 [Discriminator loss: 0.049196, acc.: 96.88%] [Generator loss: 9.009258]\n",
      "16426 [Discriminator loss: 0.096311, acc.: 93.75%] [Generator loss: 7.000950]\n",
      "16427 [Discriminator loss: 0.114793, acc.: 93.75%] [Generator loss: 7.366705]\n",
      "16428 [Discriminator loss: 0.119069, acc.: 96.88%] [Generator loss: 8.440647]\n",
      "16429 [Discriminator loss: 0.116093, acc.: 95.31%] [Generator loss: 7.712201]\n",
      "16430 [Discriminator loss: 0.097950, acc.: 96.88%] [Generator loss: 6.906486]\n",
      "16431 [Discriminator loss: 0.015001, acc.: 100.00%] [Generator loss: 8.689407]\n",
      "16432 [Discriminator loss: 0.039229, acc.: 100.00%] [Generator loss: 6.438529]\n",
      "16433 [Discriminator loss: 0.089491, acc.: 96.88%] [Generator loss: 7.779742]\n",
      "16434 [Discriminator loss: 0.031733, acc.: 98.44%] [Generator loss: 7.005739]\n",
      "16435 [Discriminator loss: 0.077287, acc.: 96.88%] [Generator loss: 7.665656]\n",
      "16436 [Discriminator loss: 0.143528, acc.: 95.31%] [Generator loss: 6.567041]\n",
      "16437 [Discriminator loss: 0.079789, acc.: 96.88%] [Generator loss: 7.290398]\n",
      "16438 [Discriminator loss: 0.168637, acc.: 90.62%] [Generator loss: 9.090683]\n",
      "16439 [Discriminator loss: 0.202374, acc.: 90.62%] [Generator loss: 9.289267]\n",
      "16440 [Discriminator loss: 0.072329, acc.: 95.31%] [Generator loss: 8.901377]\n",
      "16441 [Discriminator loss: 0.038314, acc.: 98.44%] [Generator loss: 8.031199]\n",
      "16442 [Discriminator loss: 0.134322, acc.: 96.88%] [Generator loss: 6.955598]\n",
      "16443 [Discriminator loss: 0.069948, acc.: 98.44%] [Generator loss: 6.860888]\n",
      "16444 [Discriminator loss: 0.160323, acc.: 93.75%] [Generator loss: 8.847668]\n",
      "16445 [Discriminator loss: 0.067234, acc.: 98.44%] [Generator loss: 8.798032]\n",
      "16446 [Discriminator loss: 0.117380, acc.: 95.31%] [Generator loss: 8.509544]\n",
      "16447 [Discriminator loss: 0.189411, acc.: 89.06%] [Generator loss: 8.730088]\n",
      "16448 [Discriminator loss: 0.058613, acc.: 96.88%] [Generator loss: 8.623516]\n",
      "16449 [Discriminator loss: 0.020348, acc.: 100.00%] [Generator loss: 6.735514]\n",
      "16450 [Discriminator loss: 0.045290, acc.: 98.44%] [Generator loss: 8.990912]\n",
      "16451 [Discriminator loss: 0.107310, acc.: 93.75%] [Generator loss: 8.993290]\n",
      "16452 [Discriminator loss: 0.229469, acc.: 92.19%] [Generator loss: 9.108908]\n",
      "16453 [Discriminator loss: 0.012939, acc.: 100.00%] [Generator loss: 8.507082]\n",
      "16454 [Discriminator loss: 0.088763, acc.: 93.75%] [Generator loss: 7.996244]\n",
      "16455 [Discriminator loss: 0.071799, acc.: 98.44%] [Generator loss: 9.299740]\n",
      "16456 [Discriminator loss: 0.032461, acc.: 98.44%] [Generator loss: 7.699397]\n",
      "16457 [Discriminator loss: 0.048587, acc.: 98.44%] [Generator loss: 7.804464]\n",
      "16458 [Discriminator loss: 0.187120, acc.: 92.19%] [Generator loss: 7.894540]\n",
      "16459 [Discriminator loss: 0.079036, acc.: 98.44%] [Generator loss: 8.016397]\n",
      "16460 [Discriminator loss: 0.152068, acc.: 95.31%] [Generator loss: 8.471639]\n",
      "16461 [Discriminator loss: 0.039598, acc.: 98.44%] [Generator loss: 7.563945]\n",
      "16462 [Discriminator loss: 0.027927, acc.: 100.00%] [Generator loss: 8.090565]\n",
      "16463 [Discriminator loss: 0.072704, acc.: 98.44%] [Generator loss: 6.412186]\n",
      "16464 [Discriminator loss: 0.031562, acc.: 100.00%] [Generator loss: 6.698607]\n",
      "16465 [Discriminator loss: 0.121447, acc.: 95.31%] [Generator loss: 6.880742]\n",
      "16466 [Discriminator loss: 0.119178, acc.: 96.88%] [Generator loss: 8.694588]\n",
      "16467 [Discriminator loss: 0.049171, acc.: 96.88%] [Generator loss: 6.992760]\n",
      "16468 [Discriminator loss: 0.061778, acc.: 95.31%] [Generator loss: 7.379093]\n",
      "16469 [Discriminator loss: 0.037363, acc.: 98.44%] [Generator loss: 8.034502]\n",
      "16470 [Discriminator loss: 0.044773, acc.: 98.44%] [Generator loss: 8.098867]\n",
      "16471 [Discriminator loss: 0.111189, acc.: 96.88%] [Generator loss: 7.397269]\n",
      "16472 [Discriminator loss: 0.137113, acc.: 96.88%] [Generator loss: 6.763335]\n",
      "16473 [Discriminator loss: 0.163796, acc.: 96.88%] [Generator loss: 9.488959]\n",
      "16474 [Discriminator loss: 0.137694, acc.: 95.31%] [Generator loss: 6.801015]\n",
      "16475 [Discriminator loss: 0.231082, acc.: 92.19%] [Generator loss: 9.795226]\n",
      "16476 [Discriminator loss: 0.075497, acc.: 96.88%] [Generator loss: 9.307560]\n",
      "16477 [Discriminator loss: 0.186166, acc.: 93.75%] [Generator loss: 8.888933]\n",
      "16478 [Discriminator loss: 0.072998, acc.: 96.88%] [Generator loss: 8.278330]\n",
      "16479 [Discriminator loss: 0.029014, acc.: 100.00%] [Generator loss: 7.783527]\n",
      "16480 [Discriminator loss: 0.017323, acc.: 100.00%] [Generator loss: 8.414732]\n",
      "16481 [Discriminator loss: 0.106959, acc.: 95.31%] [Generator loss: 6.760178]\n",
      "16482 [Discriminator loss: 0.047010, acc.: 98.44%] [Generator loss: 6.010828]\n",
      "16483 [Discriminator loss: 0.122967, acc.: 95.31%] [Generator loss: 8.253765]\n",
      "16484 [Discriminator loss: 0.111622, acc.: 96.88%] [Generator loss: 7.037864]\n",
      "16485 [Discriminator loss: 0.100993, acc.: 98.44%] [Generator loss: 7.124761]\n",
      "16486 [Discriminator loss: 0.048319, acc.: 98.44%] [Generator loss: 8.308239]\n",
      "16487 [Discriminator loss: 0.160935, acc.: 93.75%] [Generator loss: 8.582681]\n",
      "16488 [Discriminator loss: 0.171193, acc.: 92.19%] [Generator loss: 7.490279]\n",
      "16489 [Discriminator loss: 0.092237, acc.: 96.88%] [Generator loss: 8.204000]\n",
      "16490 [Discriminator loss: 0.012956, acc.: 100.00%] [Generator loss: 8.554738]\n",
      "16491 [Discriminator loss: 0.091424, acc.: 95.31%] [Generator loss: 9.036947]\n",
      "16492 [Discriminator loss: 0.095530, acc.: 95.31%] [Generator loss: 7.238472]\n",
      "16493 [Discriminator loss: 0.091887, acc.: 95.31%] [Generator loss: 7.015308]\n",
      "16494 [Discriminator loss: 0.018371, acc.: 100.00%] [Generator loss: 7.190575]\n",
      "16495 [Discriminator loss: 0.089873, acc.: 96.88%] [Generator loss: 8.211954]\n",
      "16496 [Discriminator loss: 0.020359, acc.: 100.00%] [Generator loss: 8.253543]\n",
      "16497 [Discriminator loss: 0.053307, acc.: 98.44%] [Generator loss: 8.151246]\n",
      "16498 [Discriminator loss: 0.076917, acc.: 98.44%] [Generator loss: 7.897245]\n",
      "16499 [Discriminator loss: 0.076530, acc.: 95.31%] [Generator loss: 6.573228]\n",
      "16500 [Discriminator loss: 0.113767, acc.: 92.19%] [Generator loss: 7.055579]\n",
      "16501 [Discriminator loss: 0.177574, acc.: 93.75%] [Generator loss: 7.132174]\n",
      "16502 [Discriminator loss: 0.081586, acc.: 96.88%] [Generator loss: 7.949862]\n",
      "16503 [Discriminator loss: 0.047455, acc.: 98.44%] [Generator loss: 7.813096]\n",
      "16504 [Discriminator loss: 0.055689, acc.: 98.44%] [Generator loss: 6.229747]\n",
      "16505 [Discriminator loss: 0.232541, acc.: 92.19%] [Generator loss: 8.470896]\n",
      "16506 [Discriminator loss: 0.051027, acc.: 96.88%] [Generator loss: 9.318397]\n",
      "16507 [Discriminator loss: 0.078869, acc.: 96.88%] [Generator loss: 7.948510]\n",
      "16508 [Discriminator loss: 0.091403, acc.: 95.31%] [Generator loss: 7.791645]\n",
      "16509 [Discriminator loss: 0.008424, acc.: 100.00%] [Generator loss: 8.064652]\n",
      "16510 [Discriminator loss: 0.060381, acc.: 98.44%] [Generator loss: 7.608341]\n",
      "16511 [Discriminator loss: 0.045866, acc.: 96.88%] [Generator loss: 7.707128]\n",
      "16512 [Discriminator loss: 0.034838, acc.: 98.44%] [Generator loss: 5.952931]\n",
      "16513 [Discriminator loss: 0.031255, acc.: 100.00%] [Generator loss: 6.342408]\n",
      "16514 [Discriminator loss: 0.088597, acc.: 96.88%] [Generator loss: 8.520529]\n",
      "16515 [Discriminator loss: 0.051216, acc.: 98.44%] [Generator loss: 7.760244]\n",
      "16516 [Discriminator loss: 0.073943, acc.: 95.31%] [Generator loss: 7.803428]\n",
      "16517 [Discriminator loss: 0.013548, acc.: 100.00%] [Generator loss: 7.991203]\n",
      "16518 [Discriminator loss: 0.010324, acc.: 100.00%] [Generator loss: 7.012169]\n",
      "16519 [Discriminator loss: 0.140776, acc.: 92.19%] [Generator loss: 7.267734]\n",
      "16520 [Discriminator loss: 0.020580, acc.: 100.00%] [Generator loss: 8.424592]\n",
      "16521 [Discriminator loss: 0.060371, acc.: 96.88%] [Generator loss: 8.654682]\n",
      "16522 [Discriminator loss: 0.134944, acc.: 96.88%] [Generator loss: 7.036568]\n",
      "16523 [Discriminator loss: 0.140876, acc.: 96.88%] [Generator loss: 7.337573]\n",
      "16524 [Discriminator loss: 0.037682, acc.: 98.44%] [Generator loss: 8.067705]\n",
      "16525 [Discriminator loss: 0.030074, acc.: 98.44%] [Generator loss: 9.209867]\n",
      "16526 [Discriminator loss: 0.045815, acc.: 98.44%] [Generator loss: 7.864893]\n",
      "16527 [Discriminator loss: 0.039231, acc.: 98.44%] [Generator loss: 7.912957]\n",
      "16528 [Discriminator loss: 0.099620, acc.: 95.31%] [Generator loss: 6.820553]\n",
      "16529 [Discriminator loss: 0.025672, acc.: 100.00%] [Generator loss: 7.390669]\n",
      "16530 [Discriminator loss: 0.082943, acc.: 96.88%] [Generator loss: 6.753789]\n",
      "16531 [Discriminator loss: 0.098478, acc.: 95.31%] [Generator loss: 8.977627]\n",
      "16532 [Discriminator loss: 0.059367, acc.: 96.88%] [Generator loss: 8.438406]\n",
      "16533 [Discriminator loss: 0.052738, acc.: 96.88%] [Generator loss: 8.232075]\n",
      "16534 [Discriminator loss: 0.057240, acc.: 98.44%] [Generator loss: 8.206964]\n",
      "16535 [Discriminator loss: 0.073823, acc.: 96.88%] [Generator loss: 8.960884]\n",
      "16536 [Discriminator loss: 0.026943, acc.: 98.44%] [Generator loss: 8.270724]\n",
      "16537 [Discriminator loss: 0.122580, acc.: 95.31%] [Generator loss: 6.918054]\n",
      "16538 [Discriminator loss: 0.067602, acc.: 96.88%] [Generator loss: 6.792524]\n",
      "16539 [Discriminator loss: 0.038519, acc.: 98.44%] [Generator loss: 7.589540]\n",
      "16540 [Discriminator loss: 0.063751, acc.: 98.44%] [Generator loss: 6.806426]\n",
      "16541 [Discriminator loss: 0.042776, acc.: 98.44%] [Generator loss: 8.666301]\n",
      "16542 [Discriminator loss: 0.121257, acc.: 95.31%] [Generator loss: 7.254786]\n",
      "16543 [Discriminator loss: 0.026852, acc.: 100.00%] [Generator loss: 7.203347]\n",
      "16544 [Discriminator loss: 0.125309, acc.: 92.19%] [Generator loss: 7.690744]\n",
      "16545 [Discriminator loss: 0.100345, acc.: 98.44%] [Generator loss: 8.574094]\n",
      "16546 [Discriminator loss: 0.059528, acc.: 98.44%] [Generator loss: 8.073577]\n",
      "16547 [Discriminator loss: 0.023017, acc.: 100.00%] [Generator loss: 8.318707]\n",
      "16548 [Discriminator loss: 0.121862, acc.: 95.31%] [Generator loss: 9.680137]\n",
      "16549 [Discriminator loss: 0.120541, acc.: 93.75%] [Generator loss: 7.889709]\n",
      "16550 [Discriminator loss: 0.086617, acc.: 95.31%] [Generator loss: 8.856322]\n",
      "16551 [Discriminator loss: 0.017227, acc.: 100.00%] [Generator loss: 7.446937]\n",
      "16552 [Discriminator loss: 0.101376, acc.: 96.88%] [Generator loss: 7.433263]\n",
      "16553 [Discriminator loss: 0.041224, acc.: 100.00%] [Generator loss: 7.064762]\n",
      "16554 [Discriminator loss: 0.217010, acc.: 92.19%] [Generator loss: 7.128160]\n",
      "16555 [Discriminator loss: 0.104608, acc.: 93.75%] [Generator loss: 10.526077]\n",
      "16556 [Discriminator loss: 0.079171, acc.: 95.31%] [Generator loss: 10.469990]\n",
      "16557 [Discriminator loss: 0.069751, acc.: 98.44%] [Generator loss: 7.710338]\n",
      "16558 [Discriminator loss: 0.077913, acc.: 96.88%] [Generator loss: 8.475939]\n",
      "16559 [Discriminator loss: 0.151915, acc.: 95.31%] [Generator loss: 9.125672]\n",
      "16560 [Discriminator loss: 0.065940, acc.: 96.88%] [Generator loss: 8.394201]\n",
      "16561 [Discriminator loss: 0.024042, acc.: 100.00%] [Generator loss: 8.109863]\n",
      "16562 [Discriminator loss: 0.055428, acc.: 96.88%] [Generator loss: 8.883476]\n",
      "16563 [Discriminator loss: 0.040493, acc.: 96.88%] [Generator loss: 9.005993]\n",
      "16564 [Discriminator loss: 0.063680, acc.: 98.44%] [Generator loss: 8.810705]\n",
      "16565 [Discriminator loss: 0.060566, acc.: 96.88%] [Generator loss: 6.777482]\n",
      "16566 [Discriminator loss: 0.053982, acc.: 96.88%] [Generator loss: 7.660166]\n",
      "16567 [Discriminator loss: 0.199168, acc.: 92.19%] [Generator loss: 8.872818]\n",
      "16568 [Discriminator loss: 0.107173, acc.: 96.88%] [Generator loss: 7.750401]\n",
      "16569 [Discriminator loss: 0.019871, acc.: 100.00%] [Generator loss: 7.594535]\n",
      "16570 [Discriminator loss: 0.169902, acc.: 95.31%] [Generator loss: 7.358428]\n",
      "16571 [Discriminator loss: 0.052144, acc.: 98.44%] [Generator loss: 8.191616]\n",
      "16572 [Discriminator loss: 0.046778, acc.: 98.44%] [Generator loss: 7.894070]\n",
      "16573 [Discriminator loss: 0.082479, acc.: 96.88%] [Generator loss: 8.811169]\n",
      "16574 [Discriminator loss: 0.154429, acc.: 92.19%] [Generator loss: 8.500534]\n",
      "16575 [Discriminator loss: 0.151974, acc.: 93.75%] [Generator loss: 8.390192]\n",
      "16576 [Discriminator loss: 0.072973, acc.: 95.31%] [Generator loss: 9.472537]\n",
      "16577 [Discriminator loss: 0.236972, acc.: 90.62%] [Generator loss: 6.332040]\n",
      "16578 [Discriminator loss: 0.110215, acc.: 98.44%] [Generator loss: 6.311994]\n",
      "16579 [Discriminator loss: 0.060169, acc.: 96.88%] [Generator loss: 8.383266]\n",
      "16580 [Discriminator loss: 0.116356, acc.: 93.75%] [Generator loss: 9.140072]\n",
      "16581 [Discriminator loss: 0.035639, acc.: 98.44%] [Generator loss: 9.000370]\n",
      "16582 [Discriminator loss: 0.101017, acc.: 96.88%] [Generator loss: 9.242374]\n",
      "16583 [Discriminator loss: 0.080452, acc.: 93.75%] [Generator loss: 8.248340]\n",
      "16584 [Discriminator loss: 0.022550, acc.: 98.44%] [Generator loss: 8.191831]\n",
      "16585 [Discriminator loss: 0.049967, acc.: 98.44%] [Generator loss: 7.521555]\n",
      "16586 [Discriminator loss: 0.036138, acc.: 100.00%] [Generator loss: 6.869546]\n",
      "16587 [Discriminator loss: 0.060262, acc.: 96.88%] [Generator loss: 7.356313]\n",
      "16588 [Discriminator loss: 0.018034, acc.: 100.00%] [Generator loss: 8.655417]\n",
      "16589 [Discriminator loss: 0.231214, acc.: 90.62%] [Generator loss: 6.743129]\n",
      "16590 [Discriminator loss: 0.097944, acc.: 96.88%] [Generator loss: 7.605996]\n",
      "16591 [Discriminator loss: 0.106539, acc.: 95.31%] [Generator loss: 9.228009]\n",
      "16592 [Discriminator loss: 0.058493, acc.: 98.44%] [Generator loss: 8.788586]\n",
      "16593 [Discriminator loss: 0.137993, acc.: 96.88%] [Generator loss: 7.977901]\n",
      "16594 [Discriminator loss: 0.086688, acc.: 96.88%] [Generator loss: 7.438960]\n",
      "16595 [Discriminator loss: 0.045695, acc.: 98.44%] [Generator loss: 8.710377]\n",
      "16596 [Discriminator loss: 0.032445, acc.: 98.44%] [Generator loss: 9.516312]\n",
      "16597 [Discriminator loss: 0.063287, acc.: 98.44%] [Generator loss: 9.645782]\n",
      "16598 [Discriminator loss: 0.014915, acc.: 100.00%] [Generator loss: 11.270618]\n",
      "16599 [Discriminator loss: 0.191553, acc.: 95.31%] [Generator loss: 9.639837]\n",
      "16600 [Discriminator loss: 0.143498, acc.: 95.31%] [Generator loss: 8.294107]\n",
      "16601 [Discriminator loss: 0.030337, acc.: 100.00%] [Generator loss: 8.824070]\n",
      "16602 [Discriminator loss: 0.093019, acc.: 96.88%] [Generator loss: 6.957782]\n",
      "16603 [Discriminator loss: 0.028311, acc.: 100.00%] [Generator loss: 6.545074]\n",
      "16604 [Discriminator loss: 0.023944, acc.: 98.44%] [Generator loss: 8.321636]\n",
      "16605 [Discriminator loss: 0.101446, acc.: 98.44%] [Generator loss: 6.962956]\n",
      "16606 [Discriminator loss: 0.082891, acc.: 95.31%] [Generator loss: 7.768279]\n",
      "16607 [Discriminator loss: 0.071538, acc.: 95.31%] [Generator loss: 8.913279]\n",
      "16608 [Discriminator loss: 0.114642, acc.: 95.31%] [Generator loss: 8.898714]\n",
      "16609 [Discriminator loss: 0.095178, acc.: 95.31%] [Generator loss: 7.493441]\n",
      "16610 [Discriminator loss: 0.040941, acc.: 98.44%] [Generator loss: 8.761841]\n",
      "16611 [Discriminator loss: 0.145994, acc.: 96.88%] [Generator loss: 5.522547]\n",
      "16612 [Discriminator loss: 0.112536, acc.: 95.31%] [Generator loss: 8.235995]\n",
      "16613 [Discriminator loss: 0.014969, acc.: 100.00%] [Generator loss: 8.551668]\n",
      "16614 [Discriminator loss: 0.026403, acc.: 98.44%] [Generator loss: 7.365443]\n",
      "16615 [Discriminator loss: 0.022867, acc.: 100.00%] [Generator loss: 7.174254]\n",
      "16616 [Discriminator loss: 0.052408, acc.: 98.44%] [Generator loss: 7.604773]\n",
      "16617 [Discriminator loss: 0.129460, acc.: 95.31%] [Generator loss: 9.411019]\n",
      "16618 [Discriminator loss: 0.232101, acc.: 87.50%] [Generator loss: 8.913530]\n",
      "16619 [Discriminator loss: 0.032448, acc.: 100.00%] [Generator loss: 6.228805]\n",
      "16620 [Discriminator loss: 0.043306, acc.: 98.44%] [Generator loss: 7.284830]\n",
      "16621 [Discriminator loss: 0.024050, acc.: 100.00%] [Generator loss: 7.478510]\n",
      "16622 [Discriminator loss: 0.091978, acc.: 95.31%] [Generator loss: 7.262456]\n",
      "16623 [Discriminator loss: 0.087307, acc.: 96.88%] [Generator loss: 8.238888]\n",
      "16624 [Discriminator loss: 0.042425, acc.: 98.44%] [Generator loss: 9.043128]\n",
      "16625 [Discriminator loss: 0.033172, acc.: 98.44%] [Generator loss: 7.347640]\n",
      "16626 [Discriminator loss: 0.089656, acc.: 95.31%] [Generator loss: 8.739282]\n",
      "16627 [Discriminator loss: 0.022784, acc.: 100.00%] [Generator loss: 7.094891]\n",
      "16628 [Discriminator loss: 0.111994, acc.: 96.88%] [Generator loss: 8.144923]\n",
      "16629 [Discriminator loss: 0.346367, acc.: 87.50%] [Generator loss: 9.206044]\n",
      "16630 [Discriminator loss: 0.060653, acc.: 98.44%] [Generator loss: 10.107941]\n",
      "16631 [Discriminator loss: 0.032239, acc.: 98.44%] [Generator loss: 9.153995]\n",
      "16632 [Discriminator loss: 0.106484, acc.: 95.31%] [Generator loss: 7.113244]\n",
      "16633 [Discriminator loss: 0.071581, acc.: 96.88%] [Generator loss: 7.874268]\n",
      "16634 [Discriminator loss: 0.076746, acc.: 95.31%] [Generator loss: 7.923863]\n",
      "16635 [Discriminator loss: 0.029892, acc.: 98.44%] [Generator loss: 6.799293]\n",
      "16636 [Discriminator loss: 0.028676, acc.: 100.00%] [Generator loss: 6.937181]\n",
      "16637 [Discriminator loss: 0.051704, acc.: 96.88%] [Generator loss: 7.121619]\n",
      "16638 [Discriminator loss: 0.091676, acc.: 95.31%] [Generator loss: 9.222816]\n",
      "16639 [Discriminator loss: 0.104397, acc.: 96.88%] [Generator loss: 6.115322]\n",
      "16640 [Discriminator loss: 0.063061, acc.: 100.00%] [Generator loss: 6.789077]\n",
      "16641 [Discriminator loss: 0.140186, acc.: 96.88%] [Generator loss: 8.304379]\n",
      "16642 [Discriminator loss: 0.056471, acc.: 98.44%] [Generator loss: 9.852612]\n",
      "16643 [Discriminator loss: 0.072565, acc.: 98.44%] [Generator loss: 9.341346]\n",
      "16644 [Discriminator loss: 0.215729, acc.: 90.62%] [Generator loss: 8.230685]\n",
      "16645 [Discriminator loss: 0.054448, acc.: 98.44%] [Generator loss: 7.644878]\n",
      "16646 [Discriminator loss: 0.087365, acc.: 96.88%] [Generator loss: 6.745935]\n",
      "16647 [Discriminator loss: 0.147294, acc.: 95.31%] [Generator loss: 7.361145]\n",
      "16648 [Discriminator loss: 0.146441, acc.: 96.88%] [Generator loss: 9.840870]\n",
      "16649 [Discriminator loss: 0.064316, acc.: 96.88%] [Generator loss: 8.902552]\n",
      "16650 [Discriminator loss: 0.071401, acc.: 98.44%] [Generator loss: 7.028326]\n",
      "16651 [Discriminator loss: 0.074171, acc.: 96.88%] [Generator loss: 6.606884]\n",
      "16652 [Discriminator loss: 0.042821, acc.: 98.44%] [Generator loss: 7.596728]\n",
      "16653 [Discriminator loss: 0.069117, acc.: 96.88%] [Generator loss: 7.265103]\n",
      "16654 [Discriminator loss: 0.027308, acc.: 100.00%] [Generator loss: 8.993437]\n",
      "16655 [Discriminator loss: 0.057931, acc.: 98.44%] [Generator loss: 9.410474]\n",
      "16656 [Discriminator loss: 0.074152, acc.: 96.88%] [Generator loss: 6.980885]\n",
      "16657 [Discriminator loss: 0.179784, acc.: 93.75%] [Generator loss: 7.188053]\n",
      "16658 [Discriminator loss: 0.022899, acc.: 100.00%] [Generator loss: 6.998228]\n",
      "16659 [Discriminator loss: 0.078335, acc.: 96.88%] [Generator loss: 8.017805]\n",
      "16660 [Discriminator loss: 0.059750, acc.: 98.44%] [Generator loss: 7.766320]\n",
      "16661 [Discriminator loss: 0.118244, acc.: 95.31%] [Generator loss: 8.032707]\n",
      "16662 [Discriminator loss: 0.050019, acc.: 98.44%] [Generator loss: 8.646507]\n",
      "16663 [Discriminator loss: 0.098923, acc.: 96.88%] [Generator loss: 9.930305]\n",
      "16664 [Discriminator loss: 0.278636, acc.: 92.19%] [Generator loss: 6.313800]\n",
      "16665 [Discriminator loss: 0.062549, acc.: 96.88%] [Generator loss: 6.209310]\n",
      "16666 [Discriminator loss: 0.043027, acc.: 98.44%] [Generator loss: 8.443415]\n",
      "16667 [Discriminator loss: 0.018542, acc.: 98.44%] [Generator loss: 7.660122]\n",
      "16668 [Discriminator loss: 0.167982, acc.: 95.31%] [Generator loss: 8.617974]\n",
      "16669 [Discriminator loss: 0.122831, acc.: 93.75%] [Generator loss: 9.898750]\n",
      "16670 [Discriminator loss: 0.082836, acc.: 96.88%] [Generator loss: 8.103765]\n",
      "16671 [Discriminator loss: 0.052127, acc.: 95.31%] [Generator loss: 7.972197]\n",
      "16672 [Discriminator loss: 0.038115, acc.: 98.44%] [Generator loss: 7.965735]\n",
      "16673 [Discriminator loss: 0.110465, acc.: 96.88%] [Generator loss: 7.986835]\n",
      "16674 [Discriminator loss: 0.119937, acc.: 95.31%] [Generator loss: 6.401425]\n",
      "16675 [Discriminator loss: 0.068172, acc.: 96.88%] [Generator loss: 8.259821]\n",
      "16676 [Discriminator loss: 0.026425, acc.: 100.00%] [Generator loss: 7.989692]\n",
      "16677 [Discriminator loss: 0.161569, acc.: 93.75%] [Generator loss: 8.751410]\n",
      "16678 [Discriminator loss: 0.215709, acc.: 92.19%] [Generator loss: 7.337715]\n",
      "16679 [Discriminator loss: 0.024752, acc.: 98.44%] [Generator loss: 8.300501]\n",
      "16680 [Discriminator loss: 0.034871, acc.: 100.00%] [Generator loss: 8.095922]\n",
      "16681 [Discriminator loss: 0.014295, acc.: 100.00%] [Generator loss: 9.586353]\n",
      "16682 [Discriminator loss: 0.094045, acc.: 95.31%] [Generator loss: 7.071683]\n",
      "16683 [Discriminator loss: 0.029868, acc.: 98.44%] [Generator loss: 6.971841]\n",
      "16684 [Discriminator loss: 0.165239, acc.: 95.31%] [Generator loss: 8.960215]\n",
      "16685 [Discriminator loss: 0.023508, acc.: 100.00%] [Generator loss: 9.945230]\n",
      "16686 [Discriminator loss: 0.033950, acc.: 100.00%] [Generator loss: 9.081173]\n",
      "16687 [Discriminator loss: 0.043199, acc.: 96.88%] [Generator loss: 7.987095]\n",
      "16688 [Discriminator loss: 0.027599, acc.: 98.44%] [Generator loss: 8.210753]\n",
      "16689 [Discriminator loss: 0.023078, acc.: 100.00%] [Generator loss: 8.938376]\n",
      "16690 [Discriminator loss: 0.027425, acc.: 98.44%] [Generator loss: 6.422858]\n",
      "16691 [Discriminator loss: 0.068493, acc.: 95.31%] [Generator loss: 8.165807]\n",
      "16692 [Discriminator loss: 0.010312, acc.: 100.00%] [Generator loss: 9.985691]\n",
      "16693 [Discriminator loss: 0.091715, acc.: 95.31%] [Generator loss: 6.905092]\n",
      "16694 [Discriminator loss: 0.064888, acc.: 98.44%] [Generator loss: 8.392054]\n",
      "16695 [Discriminator loss: 0.101419, acc.: 96.88%] [Generator loss: 8.440972]\n",
      "16696 [Discriminator loss: 0.014893, acc.: 100.00%] [Generator loss: 8.167448]\n",
      "16697 [Discriminator loss: 0.013756, acc.: 100.00%] [Generator loss: 8.339141]\n",
      "16698 [Discriminator loss: 0.170843, acc.: 90.62%] [Generator loss: 5.907117]\n",
      "16699 [Discriminator loss: 0.078187, acc.: 96.88%] [Generator loss: 8.323461]\n",
      "16700 [Discriminator loss: 0.071621, acc.: 96.88%] [Generator loss: 7.372491]\n",
      "16701 [Discriminator loss: 0.091574, acc.: 96.88%] [Generator loss: 8.101900]\n",
      "16702 [Discriminator loss: 0.060360, acc.: 96.88%] [Generator loss: 8.723603]\n",
      "16703 [Discriminator loss: 0.035333, acc.: 100.00%] [Generator loss: 8.435503]\n",
      "16704 [Discriminator loss: 0.030722, acc.: 100.00%] [Generator loss: 7.248706]\n",
      "16705 [Discriminator loss: 0.084265, acc.: 96.88%] [Generator loss: 6.817351]\n",
      "16706 [Discriminator loss: 0.072882, acc.: 96.88%] [Generator loss: 8.314856]\n",
      "16707 [Discriminator loss: 0.017109, acc.: 100.00%] [Generator loss: 9.130465]\n",
      "16708 [Discriminator loss: 0.055602, acc.: 100.00%] [Generator loss: 7.387549]\n",
      "16709 [Discriminator loss: 0.057769, acc.: 98.44%] [Generator loss: 8.291916]\n",
      "16710 [Discriminator loss: 0.047352, acc.: 98.44%] [Generator loss: 8.489397]\n",
      "16711 [Discriminator loss: 0.069866, acc.: 96.88%] [Generator loss: 6.578484]\n",
      "16712 [Discriminator loss: 0.024350, acc.: 100.00%] [Generator loss: 5.231795]\n",
      "16713 [Discriminator loss: 0.124277, acc.: 93.75%] [Generator loss: 9.224382]\n",
      "16714 [Discriminator loss: 0.034239, acc.: 98.44%] [Generator loss: 9.887081]\n",
      "16715 [Discriminator loss: 0.139078, acc.: 95.31%] [Generator loss: 7.470798]\n",
      "16716 [Discriminator loss: 0.068237, acc.: 96.88%] [Generator loss: 7.117356]\n",
      "16717 [Discriminator loss: 0.055033, acc.: 98.44%] [Generator loss: 10.501427]\n",
      "16718 [Discriminator loss: 0.105405, acc.: 93.75%] [Generator loss: 6.272679]\n",
      "16719 [Discriminator loss: 0.018294, acc.: 100.00%] [Generator loss: 7.326925]\n",
      "16720 [Discriminator loss: 0.017786, acc.: 100.00%] [Generator loss: 8.650368]\n",
      "16721 [Discriminator loss: 0.041492, acc.: 96.88%] [Generator loss: 8.425400]\n",
      "16722 [Discriminator loss: 0.042310, acc.: 98.44%] [Generator loss: 7.963829]\n",
      "16723 [Discriminator loss: 0.058390, acc.: 96.88%] [Generator loss: 9.397326]\n",
      "16724 [Discriminator loss: 0.013450, acc.: 100.00%] [Generator loss: 9.477909]\n",
      "16725 [Discriminator loss: 0.154831, acc.: 93.75%] [Generator loss: 9.865463]\n",
      "16726 [Discriminator loss: 0.124543, acc.: 95.31%] [Generator loss: 6.439109]\n",
      "16727 [Discriminator loss: 0.089057, acc.: 96.88%] [Generator loss: 7.616038]\n",
      "16728 [Discriminator loss: 0.073916, acc.: 98.44%] [Generator loss: 7.825996]\n",
      "16729 [Discriminator loss: 0.060513, acc.: 98.44%] [Generator loss: 7.900378]\n",
      "16730 [Discriminator loss: 0.129839, acc.: 90.62%] [Generator loss: 8.376805]\n",
      "16731 [Discriminator loss: 0.028900, acc.: 100.00%] [Generator loss: 9.726248]\n",
      "16732 [Discriminator loss: 0.045919, acc.: 98.44%] [Generator loss: 8.480085]\n",
      "16733 [Discriminator loss: 0.053765, acc.: 98.44%] [Generator loss: 8.995596]\n",
      "16734 [Discriminator loss: 0.068424, acc.: 98.44%] [Generator loss: 9.079361]\n",
      "16735 [Discriminator loss: 0.061197, acc.: 96.88%] [Generator loss: 6.909785]\n",
      "16736 [Discriminator loss: 0.076354, acc.: 96.88%] [Generator loss: 7.475221]\n",
      "16737 [Discriminator loss: 0.045744, acc.: 98.44%] [Generator loss: 8.367565]\n",
      "16738 [Discriminator loss: 0.079199, acc.: 96.88%] [Generator loss: 8.531496]\n",
      "16739 [Discriminator loss: 0.140893, acc.: 93.75%] [Generator loss: 10.605186]\n",
      "16740 [Discriminator loss: 0.144902, acc.: 93.75%] [Generator loss: 8.010541]\n",
      "16741 [Discriminator loss: 0.203558, acc.: 95.31%] [Generator loss: 7.464508]\n",
      "16742 [Discriminator loss: 0.090084, acc.: 98.44%] [Generator loss: 7.386271]\n",
      "16743 [Discriminator loss: 0.125722, acc.: 95.31%] [Generator loss: 6.261134]\n",
      "16744 [Discriminator loss: 0.148007, acc.: 92.19%] [Generator loss: 9.017003]\n",
      "16745 [Discriminator loss: 0.018530, acc.: 100.00%] [Generator loss: 11.090390]\n",
      "16746 [Discriminator loss: 0.063793, acc.: 96.88%] [Generator loss: 8.836618]\n",
      "16747 [Discriminator loss: 0.199634, acc.: 89.06%] [Generator loss: 8.532265]\n",
      "16748 [Discriminator loss: 0.060277, acc.: 96.88%] [Generator loss: 8.094006]\n",
      "16749 [Discriminator loss: 0.062556, acc.: 98.44%] [Generator loss: 8.833080]\n",
      "16750 [Discriminator loss: 0.063673, acc.: 96.88%] [Generator loss: 7.904436]\n",
      "16751 [Discriminator loss: 0.045906, acc.: 98.44%] [Generator loss: 8.457450]\n",
      "16752 [Discriminator loss: 0.070983, acc.: 95.31%] [Generator loss: 9.761906]\n",
      "16753 [Discriminator loss: 0.110959, acc.: 93.75%] [Generator loss: 7.019848]\n",
      "16754 [Discriminator loss: 0.178718, acc.: 90.62%] [Generator loss: 8.884127]\n",
      "16755 [Discriminator loss: 0.023670, acc.: 98.44%] [Generator loss: 9.327578]\n",
      "16756 [Discriminator loss: 0.244939, acc.: 90.62%] [Generator loss: 6.640520]\n",
      "16757 [Discriminator loss: 0.072115, acc.: 98.44%] [Generator loss: 7.087729]\n",
      "16758 [Discriminator loss: 0.140161, acc.: 93.75%] [Generator loss: 8.829966]\n",
      "16759 [Discriminator loss: 0.199098, acc.: 89.06%] [Generator loss: 8.629723]\n",
      "16760 [Discriminator loss: 0.006002, acc.: 100.00%] [Generator loss: 8.649866]\n",
      "16761 [Discriminator loss: 0.132228, acc.: 96.88%] [Generator loss: 9.500521]\n",
      "16762 [Discriminator loss: 0.118729, acc.: 96.88%] [Generator loss: 8.736069]\n",
      "16763 [Discriminator loss: 0.035367, acc.: 100.00%] [Generator loss: 9.886859]\n",
      "16764 [Discriminator loss: 0.128153, acc.: 93.75%] [Generator loss: 9.021284]\n",
      "16765 [Discriminator loss: 0.183960, acc.: 95.31%] [Generator loss: 8.561630]\n",
      "16766 [Discriminator loss: 0.048798, acc.: 100.00%] [Generator loss: 7.657147]\n",
      "16767 [Discriminator loss: 0.063789, acc.: 96.88%] [Generator loss: 7.166724]\n",
      "16768 [Discriminator loss: 0.123621, acc.: 95.31%] [Generator loss: 8.110088]\n",
      "16769 [Discriminator loss: 0.079624, acc.: 93.75%] [Generator loss: 9.834988]\n",
      "16770 [Discriminator loss: 0.072374, acc.: 96.88%] [Generator loss: 8.153437]\n",
      "16771 [Discriminator loss: 0.102033, acc.: 95.31%] [Generator loss: 7.589077]\n",
      "16772 [Discriminator loss: 0.073332, acc.: 95.31%] [Generator loss: 6.319088]\n",
      "16773 [Discriminator loss: 0.032963, acc.: 98.44%] [Generator loss: 6.787755]\n",
      "16774 [Discriminator loss: 0.127347, acc.: 95.31%] [Generator loss: 8.732204]\n",
      "16775 [Discriminator loss: 0.040399, acc.: 98.44%] [Generator loss: 7.747806]\n",
      "16776 [Discriminator loss: 0.385950, acc.: 89.06%] [Generator loss: 9.216911]\n",
      "16777 [Discriminator loss: 0.025685, acc.: 100.00%] [Generator loss: 10.080215]\n",
      "16778 [Discriminator loss: 0.146942, acc.: 93.75%] [Generator loss: 8.138598]\n",
      "16779 [Discriminator loss: 0.073174, acc.: 95.31%] [Generator loss: 7.660285]\n",
      "16780 [Discriminator loss: 0.032701, acc.: 100.00%] [Generator loss: 7.539097]\n",
      "16781 [Discriminator loss: 0.045549, acc.: 96.88%] [Generator loss: 5.487693]\n",
      "16782 [Discriminator loss: 0.040666, acc.: 98.44%] [Generator loss: 7.048900]\n",
      "16783 [Discriminator loss: 0.040804, acc.: 98.44%] [Generator loss: 8.696044]\n",
      "16784 [Discriminator loss: 0.034080, acc.: 100.00%] [Generator loss: 7.217120]\n",
      "16785 [Discriminator loss: 0.055274, acc.: 98.44%] [Generator loss: 8.193648]\n",
      "16786 [Discriminator loss: 0.136660, acc.: 93.75%] [Generator loss: 7.451435]\n",
      "16787 [Discriminator loss: 0.031037, acc.: 98.44%] [Generator loss: 6.929920]\n",
      "16788 [Discriminator loss: 0.157126, acc.: 93.75%] [Generator loss: 5.190970]\n",
      "16789 [Discriminator loss: 0.141645, acc.: 95.31%] [Generator loss: 8.394910]\n",
      "16790 [Discriminator loss: 0.110560, acc.: 93.75%] [Generator loss: 7.022894]\n",
      "16791 [Discriminator loss: 0.050683, acc.: 98.44%] [Generator loss: 8.253819]\n",
      "16792 [Discriminator loss: 0.226081, acc.: 89.06%] [Generator loss: 6.980928]\n",
      "16793 [Discriminator loss: 0.018666, acc.: 100.00%] [Generator loss: 9.392594]\n",
      "16794 [Discriminator loss: 0.115233, acc.: 95.31%] [Generator loss: 8.461380]\n",
      "16795 [Discriminator loss: 0.060424, acc.: 98.44%] [Generator loss: 6.563210]\n",
      "16796 [Discriminator loss: 0.013312, acc.: 100.00%] [Generator loss: 7.588818]\n",
      "16797 [Discriminator loss: 0.124874, acc.: 93.75%] [Generator loss: 9.091038]\n",
      "16798 [Discriminator loss: 0.094488, acc.: 96.88%] [Generator loss: 7.761804]\n",
      "16799 [Discriminator loss: 0.246892, acc.: 92.19%] [Generator loss: 9.648315]\n",
      "16800 [Discriminator loss: 0.063430, acc.: 96.88%] [Generator loss: 7.994562]\n",
      "16801 [Discriminator loss: 0.057624, acc.: 98.44%] [Generator loss: 8.116934]\n",
      "16802 [Discriminator loss: 0.189311, acc.: 93.75%] [Generator loss: 7.278481]\n",
      "16803 [Discriminator loss: 0.064903, acc.: 96.88%] [Generator loss: 8.740922]\n",
      "16804 [Discriminator loss: 0.211691, acc.: 95.31%] [Generator loss: 7.499572]\n",
      "16805 [Discriminator loss: 0.339482, acc.: 89.06%] [Generator loss: 7.606768]\n",
      "16806 [Discriminator loss: 0.075679, acc.: 98.44%] [Generator loss: 9.579081]\n",
      "16807 [Discriminator loss: 0.029255, acc.: 100.00%] [Generator loss: 7.818836]\n",
      "16808 [Discriminator loss: 0.198641, acc.: 92.19%] [Generator loss: 7.762101]\n",
      "16809 [Discriminator loss: 0.075603, acc.: 98.44%] [Generator loss: 9.447269]\n",
      "16810 [Discriminator loss: 0.319871, acc.: 82.81%] [Generator loss: 8.573743]\n",
      "16811 [Discriminator loss: 0.146573, acc.: 93.75%] [Generator loss: 7.684892]\n",
      "16812 [Discriminator loss: 0.150150, acc.: 95.31%] [Generator loss: 8.901443]\n",
      "16813 [Discriminator loss: 0.071532, acc.: 96.88%] [Generator loss: 8.601093]\n",
      "16814 [Discriminator loss: 0.127619, acc.: 92.19%] [Generator loss: 7.923390]\n",
      "16815 [Discriminator loss: 0.075689, acc.: 98.44%] [Generator loss: 6.242394]\n",
      "16816 [Discriminator loss: 0.035194, acc.: 100.00%] [Generator loss: 9.481668]\n",
      "16817 [Discriminator loss: 0.049923, acc.: 98.44%] [Generator loss: 6.935806]\n",
      "16818 [Discriminator loss: 0.167582, acc.: 95.31%] [Generator loss: 6.843898]\n",
      "16819 [Discriminator loss: 0.016121, acc.: 100.00%] [Generator loss: 9.114272]\n",
      "16820 [Discriminator loss: 0.241648, acc.: 96.88%] [Generator loss: 7.589487]\n",
      "16821 [Discriminator loss: 0.064833, acc.: 96.88%] [Generator loss: 8.095002]\n",
      "16822 [Discriminator loss: 0.030316, acc.: 98.44%] [Generator loss: 8.774576]\n",
      "16823 [Discriminator loss: 0.011786, acc.: 100.00%] [Generator loss: 8.154081]\n",
      "16824 [Discriminator loss: 0.110872, acc.: 95.31%] [Generator loss: 7.404238]\n",
      "16825 [Discriminator loss: 0.132012, acc.: 95.31%] [Generator loss: 7.415446]\n",
      "16826 [Discriminator loss: 0.249635, acc.: 89.06%] [Generator loss: 8.509748]\n",
      "16827 [Discriminator loss: 0.062483, acc.: 98.44%] [Generator loss: 7.982580]\n",
      "16828 [Discriminator loss: 0.027819, acc.: 100.00%] [Generator loss: 8.928549]\n",
      "16829 [Discriminator loss: 0.096869, acc.: 95.31%] [Generator loss: 8.641012]\n",
      "16830 [Discriminator loss: 0.019166, acc.: 100.00%] [Generator loss: 7.168128]\n",
      "16831 [Discriminator loss: 0.021034, acc.: 98.44%] [Generator loss: 6.994456]\n",
      "16832 [Discriminator loss: 0.064283, acc.: 96.88%] [Generator loss: 6.588943]\n",
      "16833 [Discriminator loss: 0.158163, acc.: 90.62%] [Generator loss: 8.247490]\n",
      "16834 [Discriminator loss: 0.096562, acc.: 95.31%] [Generator loss: 8.230598]\n",
      "16835 [Discriminator loss: 0.024848, acc.: 100.00%] [Generator loss: 8.859838]\n",
      "16836 [Discriminator loss: 0.157490, acc.: 93.75%] [Generator loss: 7.739617]\n",
      "16837 [Discriminator loss: 0.031833, acc.: 100.00%] [Generator loss: 7.745292]\n",
      "16838 [Discriminator loss: 0.093360, acc.: 96.88%] [Generator loss: 8.770563]\n",
      "16839 [Discriminator loss: 0.069930, acc.: 95.31%] [Generator loss: 8.003333]\n",
      "16840 [Discriminator loss: 0.175422, acc.: 92.19%] [Generator loss: 7.797562]\n",
      "16841 [Discriminator loss: 0.093249, acc.: 95.31%] [Generator loss: 8.998184]\n",
      "16842 [Discriminator loss: 0.090409, acc.: 95.31%] [Generator loss: 8.119595]\n",
      "16843 [Discriminator loss: 0.043526, acc.: 98.44%] [Generator loss: 7.580699]\n",
      "16844 [Discriminator loss: 0.025061, acc.: 100.00%] [Generator loss: 6.172076]\n",
      "16845 [Discriminator loss: 0.120139, acc.: 95.31%] [Generator loss: 7.500723]\n",
      "16846 [Discriminator loss: 0.124159, acc.: 93.75%] [Generator loss: 8.991684]\n",
      "16847 [Discriminator loss: 0.177559, acc.: 95.31%] [Generator loss: 9.463772]\n",
      "16848 [Discriminator loss: 0.039332, acc.: 100.00%] [Generator loss: 7.449975]\n",
      "16849 [Discriminator loss: 0.077387, acc.: 98.44%] [Generator loss: 8.383816]\n",
      "16850 [Discriminator loss: 0.087691, acc.: 98.44%] [Generator loss: 6.137679]\n",
      "16851 [Discriminator loss: 0.096360, acc.: 98.44%] [Generator loss: 7.996770]\n",
      "16852 [Discriminator loss: 0.098910, acc.: 95.31%] [Generator loss: 6.694081]\n",
      "16853 [Discriminator loss: 0.046503, acc.: 98.44%] [Generator loss: 8.545987]\n",
      "16854 [Discriminator loss: 0.068806, acc.: 95.31%] [Generator loss: 7.893694]\n",
      "16855 [Discriminator loss: 0.072279, acc.: 98.44%] [Generator loss: 7.418979]\n",
      "16856 [Discriminator loss: 0.042051, acc.: 98.44%] [Generator loss: 7.762015]\n",
      "16857 [Discriminator loss: 0.087707, acc.: 95.31%] [Generator loss: 7.705169]\n",
      "16858 [Discriminator loss: 0.202000, acc.: 90.62%] [Generator loss: 9.369720]\n",
      "16859 [Discriminator loss: 0.123983, acc.: 93.75%] [Generator loss: 8.211115]\n",
      "16860 [Discriminator loss: 0.041623, acc.: 96.88%] [Generator loss: 8.305673]\n",
      "16861 [Discriminator loss: 0.180309, acc.: 95.31%] [Generator loss: 8.431587]\n",
      "16862 [Discriminator loss: 0.035431, acc.: 96.88%] [Generator loss: 9.372694]\n",
      "16863 [Discriminator loss: 0.124287, acc.: 96.88%] [Generator loss: 9.642303]\n",
      "16864 [Discriminator loss: 0.208189, acc.: 92.19%] [Generator loss: 8.619215]\n",
      "16865 [Discriminator loss: 0.041385, acc.: 96.88%] [Generator loss: 6.444884]\n",
      "16866 [Discriminator loss: 0.068073, acc.: 96.88%] [Generator loss: 7.127082]\n",
      "16867 [Discriminator loss: 0.041814, acc.: 98.44%] [Generator loss: 8.130457]\n",
      "16868 [Discriminator loss: 0.087354, acc.: 96.88%] [Generator loss: 9.217591]\n",
      "16869 [Discriminator loss: 0.115318, acc.: 93.75%] [Generator loss: 7.393645]\n",
      "16870 [Discriminator loss: 0.055452, acc.: 96.88%] [Generator loss: 8.095692]\n",
      "16871 [Discriminator loss: 0.112903, acc.: 95.31%] [Generator loss: 8.210119]\n",
      "16872 [Discriminator loss: 0.036323, acc.: 98.44%] [Generator loss: 9.844090]\n",
      "16873 [Discriminator loss: 0.092052, acc.: 95.31%] [Generator loss: 6.460082]\n",
      "16874 [Discriminator loss: 0.082697, acc.: 96.88%] [Generator loss: 8.309180]\n",
      "16875 [Discriminator loss: 0.076565, acc.: 96.88%] [Generator loss: 8.209543]\n",
      "16876 [Discriminator loss: 0.058034, acc.: 96.88%] [Generator loss: 7.244694]\n",
      "16877 [Discriminator loss: 0.077161, acc.: 96.88%] [Generator loss: 7.806479]\n",
      "16878 [Discriminator loss: 0.045326, acc.: 98.44%] [Generator loss: 7.047129]\n",
      "16879 [Discriminator loss: 0.051437, acc.: 96.88%] [Generator loss: 7.784369]\n",
      "16880 [Discriminator loss: 0.197332, acc.: 93.75%] [Generator loss: 8.591496]\n",
      "16881 [Discriminator loss: 0.322484, acc.: 85.94%] [Generator loss: 8.416828]\n",
      "16882 [Discriminator loss: 0.020498, acc.: 100.00%] [Generator loss: 8.518137]\n",
      "16883 [Discriminator loss: 0.085722, acc.: 96.88%] [Generator loss: 10.294233]\n",
      "16884 [Discriminator loss: 0.301688, acc.: 85.94%] [Generator loss: 8.467632]\n",
      "16885 [Discriminator loss: 0.140265, acc.: 90.62%] [Generator loss: 9.899488]\n",
      "16886 [Discriminator loss: 0.104359, acc.: 95.31%] [Generator loss: 10.430065]\n",
      "16887 [Discriminator loss: 0.061339, acc.: 96.88%] [Generator loss: 8.087335]\n",
      "16888 [Discriminator loss: 0.053397, acc.: 96.88%] [Generator loss: 7.712622]\n",
      "16889 [Discriminator loss: 0.021814, acc.: 98.44%] [Generator loss: 8.329754]\n",
      "16890 [Discriminator loss: 0.066433, acc.: 96.88%] [Generator loss: 7.631075]\n",
      "16891 [Discriminator loss: 0.083025, acc.: 96.88%] [Generator loss: 7.013685]\n",
      "16892 [Discriminator loss: 0.150487, acc.: 90.62%] [Generator loss: 7.780447]\n",
      "16893 [Discriminator loss: 0.008401, acc.: 100.00%] [Generator loss: 8.105537]\n",
      "16894 [Discriminator loss: 0.094480, acc.: 93.75%] [Generator loss: 6.602665]\n",
      "16895 [Discriminator loss: 0.155835, acc.: 95.31%] [Generator loss: 7.277867]\n",
      "16896 [Discriminator loss: 0.042314, acc.: 100.00%] [Generator loss: 8.092121]\n",
      "16897 [Discriminator loss: 0.134112, acc.: 95.31%] [Generator loss: 9.572520]\n",
      "16898 [Discriminator loss: 0.156920, acc.: 93.75%] [Generator loss: 9.293550]\n",
      "16899 [Discriminator loss: 0.050267, acc.: 100.00%] [Generator loss: 9.495321]\n",
      "16900 [Discriminator loss: 0.076476, acc.: 96.88%] [Generator loss: 7.812164]\n",
      "16901 [Discriminator loss: 0.188270, acc.: 92.19%] [Generator loss: 6.539194]\n",
      "16902 [Discriminator loss: 0.041212, acc.: 98.44%] [Generator loss: 7.735445]\n",
      "16903 [Discriminator loss: 0.038600, acc.: 98.44%] [Generator loss: 7.777085]\n",
      "16904 [Discriminator loss: 0.064165, acc.: 96.88%] [Generator loss: 9.537418]\n",
      "16905 [Discriminator loss: 0.036866, acc.: 98.44%] [Generator loss: 8.499567]\n",
      "16906 [Discriminator loss: 0.157727, acc.: 96.88%] [Generator loss: 10.008491]\n",
      "16907 [Discriminator loss: 0.022012, acc.: 100.00%] [Generator loss: 9.294645]\n",
      "16908 [Discriminator loss: 0.064002, acc.: 98.44%] [Generator loss: 7.833126]\n",
      "16909 [Discriminator loss: 0.054089, acc.: 98.44%] [Generator loss: 7.607598]\n",
      "16910 [Discriminator loss: 0.087959, acc.: 95.31%] [Generator loss: 9.010526]\n",
      "16911 [Discriminator loss: 0.074471, acc.: 96.88%] [Generator loss: 8.204054]\n",
      "16912 [Discriminator loss: 0.229009, acc.: 84.38%] [Generator loss: 9.631678]\n",
      "16913 [Discriminator loss: 0.093956, acc.: 95.31%] [Generator loss: 8.993713]\n",
      "16914 [Discriminator loss: 0.127058, acc.: 95.31%] [Generator loss: 8.876945]\n",
      "16915 [Discriminator loss: 0.107829, acc.: 95.31%] [Generator loss: 7.457736]\n",
      "16916 [Discriminator loss: 0.058721, acc.: 98.44%] [Generator loss: 6.253948]\n",
      "16917 [Discriminator loss: 0.101730, acc.: 95.31%] [Generator loss: 8.195621]\n",
      "16918 [Discriminator loss: 0.083450, acc.: 98.44%] [Generator loss: 9.767052]\n",
      "16919 [Discriminator loss: 0.082332, acc.: 95.31%] [Generator loss: 9.786327]\n",
      "16920 [Discriminator loss: 0.052942, acc.: 98.44%] [Generator loss: 7.854290]\n",
      "16921 [Discriminator loss: 0.131037, acc.: 95.31%] [Generator loss: 6.829194]\n",
      "16922 [Discriminator loss: 0.100593, acc.: 93.75%] [Generator loss: 6.976964]\n",
      "16923 [Discriminator loss: 0.178639, acc.: 95.31%] [Generator loss: 7.350772]\n",
      "16924 [Discriminator loss: 0.065980, acc.: 98.44%] [Generator loss: 8.244570]\n",
      "16925 [Discriminator loss: 0.025445, acc.: 100.00%] [Generator loss: 9.222692]\n",
      "16926 [Discriminator loss: 0.068284, acc.: 98.44%] [Generator loss: 8.121794]\n",
      "16927 [Discriminator loss: 0.221228, acc.: 92.19%] [Generator loss: 7.838189]\n",
      "16928 [Discriminator loss: 0.039604, acc.: 98.44%] [Generator loss: 9.699903]\n",
      "16929 [Discriminator loss: 0.082138, acc.: 98.44%] [Generator loss: 9.264227]\n",
      "16930 [Discriminator loss: 0.026779, acc.: 100.00%] [Generator loss: 8.848662]\n",
      "16931 [Discriminator loss: 0.157255, acc.: 93.75%] [Generator loss: 7.021759]\n",
      "16932 [Discriminator loss: 0.087036, acc.: 96.88%] [Generator loss: 6.187295]\n",
      "16933 [Discriminator loss: 0.183525, acc.: 95.31%] [Generator loss: 8.847595]\n",
      "16934 [Discriminator loss: 0.019542, acc.: 100.00%] [Generator loss: 8.586712]\n",
      "16935 [Discriminator loss: 0.050025, acc.: 98.44%] [Generator loss: 8.919310]\n",
      "16936 [Discriminator loss: 0.076064, acc.: 95.31%] [Generator loss: 8.924025]\n",
      "16937 [Discriminator loss: 0.116385, acc.: 96.88%] [Generator loss: 7.541662]\n",
      "16938 [Discriminator loss: 0.077951, acc.: 96.88%] [Generator loss: 9.403608]\n",
      "16939 [Discriminator loss: 0.117549, acc.: 95.31%] [Generator loss: 8.892260]\n",
      "16940 [Discriminator loss: 0.123639, acc.: 95.31%] [Generator loss: 8.431221]\n",
      "16941 [Discriminator loss: 0.039078, acc.: 96.88%] [Generator loss: 9.613375]\n",
      "16942 [Discriminator loss: 0.034378, acc.: 100.00%] [Generator loss: 8.212277]\n",
      "16943 [Discriminator loss: 0.218788, acc.: 92.19%] [Generator loss: 8.341589]\n",
      "16944 [Discriminator loss: 0.053093, acc.: 98.44%] [Generator loss: 8.211594]\n",
      "16945 [Discriminator loss: 0.064904, acc.: 95.31%] [Generator loss: 8.699981]\n",
      "16946 [Discriminator loss: 0.133552, acc.: 95.31%] [Generator loss: 9.857880]\n",
      "16947 [Discriminator loss: 0.180160, acc.: 95.31%] [Generator loss: 7.229766]\n",
      "16948 [Discriminator loss: 0.076217, acc.: 96.88%] [Generator loss: 7.058642]\n",
      "16949 [Discriminator loss: 0.084959, acc.: 95.31%] [Generator loss: 8.737642]\n",
      "16950 [Discriminator loss: 0.066762, acc.: 96.88%] [Generator loss: 9.282825]\n",
      "16951 [Discriminator loss: 0.128955, acc.: 95.31%] [Generator loss: 8.067652]\n",
      "16952 [Discriminator loss: 0.028418, acc.: 100.00%] [Generator loss: 9.482014]\n",
      "16953 [Discriminator loss: 0.188306, acc.: 92.19%] [Generator loss: 8.601137]\n",
      "16954 [Discriminator loss: 0.139858, acc.: 96.88%] [Generator loss: 7.426980]\n",
      "16955 [Discriminator loss: 0.029539, acc.: 98.44%] [Generator loss: 7.445231]\n",
      "16956 [Discriminator loss: 0.059118, acc.: 98.44%] [Generator loss: 8.117983]\n",
      "16957 [Discriminator loss: 0.043493, acc.: 98.44%] [Generator loss: 9.058458]\n",
      "16958 [Discriminator loss: 0.051317, acc.: 96.88%] [Generator loss: 6.769936]\n",
      "16959 [Discriminator loss: 0.058852, acc.: 98.44%] [Generator loss: 6.203241]\n",
      "16960 [Discriminator loss: 0.076312, acc.: 98.44%] [Generator loss: 8.129536]\n",
      "16961 [Discriminator loss: 0.015698, acc.: 100.00%] [Generator loss: 10.511709]\n",
      "16962 [Discriminator loss: 0.155309, acc.: 95.31%] [Generator loss: 8.034042]\n",
      "16963 [Discriminator loss: 0.034051, acc.: 100.00%] [Generator loss: 6.281894]\n",
      "16964 [Discriminator loss: 0.025133, acc.: 100.00%] [Generator loss: 7.766644]\n",
      "16965 [Discriminator loss: 0.062397, acc.: 95.31%] [Generator loss: 8.339561]\n",
      "16966 [Discriminator loss: 0.072931, acc.: 95.31%] [Generator loss: 8.416203]\n",
      "16967 [Discriminator loss: 0.077335, acc.: 95.31%] [Generator loss: 7.160774]\n",
      "16968 [Discriminator loss: 0.059171, acc.: 98.44%] [Generator loss: 7.531939]\n",
      "16969 [Discriminator loss: 0.064649, acc.: 98.44%] [Generator loss: 10.078322]\n",
      "16970 [Discriminator loss: 0.059938, acc.: 98.44%] [Generator loss: 8.420588]\n",
      "16971 [Discriminator loss: 0.023694, acc.: 100.00%] [Generator loss: 8.329476]\n",
      "16972 [Discriminator loss: 0.054196, acc.: 98.44%] [Generator loss: 6.813704]\n",
      "16973 [Discriminator loss: 0.196596, acc.: 95.31%] [Generator loss: 6.779819]\n",
      "16974 [Discriminator loss: 0.078947, acc.: 96.88%] [Generator loss: 8.124706]\n",
      "16975 [Discriminator loss: 0.047177, acc.: 95.31%] [Generator loss: 7.722786]\n",
      "16976 [Discriminator loss: 0.088050, acc.: 98.44%] [Generator loss: 8.105730]\n",
      "16977 [Discriminator loss: 0.089228, acc.: 95.31%] [Generator loss: 9.204069]\n",
      "16978 [Discriminator loss: 0.045843, acc.: 98.44%] [Generator loss: 7.227379]\n",
      "16979 [Discriminator loss: 0.078403, acc.: 95.31%] [Generator loss: 7.378457]\n",
      "16980 [Discriminator loss: 0.099164, acc.: 96.88%] [Generator loss: 8.533168]\n",
      "16981 [Discriminator loss: 0.053268, acc.: 98.44%] [Generator loss: 8.999528]\n",
      "16982 [Discriminator loss: 0.046934, acc.: 98.44%] [Generator loss: 9.299845]\n",
      "16983 [Discriminator loss: 0.181062, acc.: 95.31%] [Generator loss: 7.818414]\n",
      "16984 [Discriminator loss: 0.051980, acc.: 98.44%] [Generator loss: 9.476298]\n",
      "16985 [Discriminator loss: 0.145080, acc.: 92.19%] [Generator loss: 7.585773]\n",
      "16986 [Discriminator loss: 0.057657, acc.: 98.44%] [Generator loss: 6.739330]\n",
      "16987 [Discriminator loss: 0.015557, acc.: 100.00%] [Generator loss: 8.537156]\n",
      "16988 [Discriminator loss: 0.049822, acc.: 98.44%] [Generator loss: 9.569629]\n",
      "16989 [Discriminator loss: 0.046557, acc.: 100.00%] [Generator loss: 8.271437]\n",
      "16990 [Discriminator loss: 0.032101, acc.: 98.44%] [Generator loss: 8.930263]\n",
      "16991 [Discriminator loss: 0.069730, acc.: 96.88%] [Generator loss: 8.713873]\n",
      "16992 [Discriminator loss: 0.144998, acc.: 92.19%] [Generator loss: 5.815905]\n",
      "16993 [Discriminator loss: 0.009516, acc.: 100.00%] [Generator loss: 6.853895]\n",
      "16994 [Discriminator loss: 0.187531, acc.: 95.31%] [Generator loss: 9.904036]\n",
      "16995 [Discriminator loss: 0.091496, acc.: 96.88%] [Generator loss: 8.782122]\n",
      "16996 [Discriminator loss: 0.047707, acc.: 98.44%] [Generator loss: 10.599229]\n",
      "16997 [Discriminator loss: 0.036130, acc.: 100.00%] [Generator loss: 8.347809]\n",
      "16998 [Discriminator loss: 0.080600, acc.: 96.88%] [Generator loss: 7.336737]\n",
      "16999 [Discriminator loss: 0.298004, acc.: 90.62%] [Generator loss: 9.470540]\n",
      "17000 [Discriminator loss: 0.078917, acc.: 96.88%] [Generator loss: 10.251535]\n",
      "17001 [Discriminator loss: 0.244856, acc.: 92.19%] [Generator loss: 6.741209]\n",
      "17002 [Discriminator loss: 0.064450, acc.: 95.31%] [Generator loss: 8.480773]\n",
      "17003 [Discriminator loss: 0.044045, acc.: 100.00%] [Generator loss: 8.617067]\n",
      "17004 [Discriminator loss: 0.029292, acc.: 100.00%] [Generator loss: 8.792484]\n",
      "17005 [Discriminator loss: 0.126974, acc.: 95.31%] [Generator loss: 7.957581]\n",
      "17006 [Discriminator loss: 0.056055, acc.: 95.31%] [Generator loss: 8.555970]\n",
      "17007 [Discriminator loss: 0.066420, acc.: 96.88%] [Generator loss: 9.257566]\n",
      "17008 [Discriminator loss: 0.042065, acc.: 98.44%] [Generator loss: 7.631866]\n",
      "17009 [Discriminator loss: 0.038824, acc.: 100.00%] [Generator loss: 8.791207]\n",
      "17010 [Discriminator loss: 0.101955, acc.: 95.31%] [Generator loss: 7.375504]\n",
      "17011 [Discriminator loss: 0.023140, acc.: 100.00%] [Generator loss: 8.163050]\n",
      "17012 [Discriminator loss: 0.190930, acc.: 95.31%] [Generator loss: 9.538628]\n",
      "17013 [Discriminator loss: 0.141121, acc.: 92.19%] [Generator loss: 7.095406]\n",
      "17014 [Discriminator loss: 0.183555, acc.: 95.31%] [Generator loss: 8.393993]\n",
      "17015 [Discriminator loss: 0.018850, acc.: 100.00%] [Generator loss: 8.635932]\n",
      "17016 [Discriminator loss: 0.065353, acc.: 96.88%] [Generator loss: 9.831439]\n",
      "17017 [Discriminator loss: 0.058209, acc.: 98.44%] [Generator loss: 7.431105]\n",
      "17018 [Discriminator loss: 0.154081, acc.: 92.19%] [Generator loss: 6.992073]\n",
      "17019 [Discriminator loss: 0.104500, acc.: 95.31%] [Generator loss: 8.249005]\n",
      "17020 [Discriminator loss: 0.065596, acc.: 96.88%] [Generator loss: 8.197598]\n",
      "17021 [Discriminator loss: 0.015762, acc.: 100.00%] [Generator loss: 9.615576]\n",
      "17022 [Discriminator loss: 0.041763, acc.: 98.44%] [Generator loss: 9.179539]\n",
      "17023 [Discriminator loss: 0.148582, acc.: 92.19%] [Generator loss: 7.459874]\n",
      "17024 [Discriminator loss: 0.074698, acc.: 95.31%] [Generator loss: 8.622738]\n",
      "17025 [Discriminator loss: 0.126917, acc.: 96.88%] [Generator loss: 8.947015]\n",
      "17026 [Discriminator loss: 0.082910, acc.: 96.88%] [Generator loss: 8.124577]\n",
      "17027 [Discriminator loss: 0.046854, acc.: 98.44%] [Generator loss: 7.581094]\n",
      "17028 [Discriminator loss: 0.032764, acc.: 98.44%] [Generator loss: 7.743006]\n",
      "17029 [Discriminator loss: 0.229959, acc.: 89.06%] [Generator loss: 9.400314]\n",
      "17030 [Discriminator loss: 0.129553, acc.: 96.88%] [Generator loss: 10.057744]\n",
      "17031 [Discriminator loss: 0.031670, acc.: 98.44%] [Generator loss: 8.959532]\n",
      "17032 [Discriminator loss: 0.029523, acc.: 98.44%] [Generator loss: 7.685890]\n",
      "17033 [Discriminator loss: 0.085683, acc.: 98.44%] [Generator loss: 7.973518]\n",
      "17034 [Discriminator loss: 0.076977, acc.: 98.44%] [Generator loss: 7.291829]\n",
      "17035 [Discriminator loss: 0.076590, acc.: 96.88%] [Generator loss: 6.823927]\n",
      "17036 [Discriminator loss: 0.100256, acc.: 93.75%] [Generator loss: 8.173823]\n",
      "17037 [Discriminator loss: 0.034630, acc.: 100.00%] [Generator loss: 8.497505]\n",
      "17038 [Discriminator loss: 0.040735, acc.: 98.44%] [Generator loss: 9.705487]\n",
      "17039 [Discriminator loss: 0.055716, acc.: 96.88%] [Generator loss: 8.813586]\n",
      "17040 [Discriminator loss: 0.112897, acc.: 96.88%] [Generator loss: 6.537865]\n",
      "17041 [Discriminator loss: 0.038110, acc.: 100.00%] [Generator loss: 7.186879]\n",
      "17042 [Discriminator loss: 0.052887, acc.: 96.88%] [Generator loss: 6.864388]\n",
      "17043 [Discriminator loss: 0.073236, acc.: 98.44%] [Generator loss: 6.798630]\n",
      "17044 [Discriminator loss: 0.038119, acc.: 98.44%] [Generator loss: 7.854092]\n",
      "17045 [Discriminator loss: 0.139598, acc.: 95.31%] [Generator loss: 6.531312]\n",
      "17046 [Discriminator loss: 0.039684, acc.: 100.00%] [Generator loss: 9.872844]\n",
      "17047 [Discriminator loss: 0.016355, acc.: 100.00%] [Generator loss: 9.555704]\n",
      "17048 [Discriminator loss: 0.050806, acc.: 98.44%] [Generator loss: 7.154063]\n",
      "17049 [Discriminator loss: 0.069888, acc.: 98.44%] [Generator loss: 9.237644]\n",
      "17050 [Discriminator loss: 0.026857, acc.: 100.00%] [Generator loss: 6.495067]\n",
      "17051 [Discriminator loss: 0.068085, acc.: 98.44%] [Generator loss: 7.954556]\n",
      "17052 [Discriminator loss: 0.103068, acc.: 95.31%] [Generator loss: 8.396931]\n",
      "17053 [Discriminator loss: 0.180323, acc.: 95.31%] [Generator loss: 6.571314]\n",
      "17054 [Discriminator loss: 0.123066, acc.: 95.31%] [Generator loss: 7.860146]\n",
      "17055 [Discriminator loss: 0.014931, acc.: 100.00%] [Generator loss: 8.511084]\n",
      "17056 [Discriminator loss: 0.233122, acc.: 89.06%] [Generator loss: 10.181967]\n",
      "17057 [Discriminator loss: 0.053816, acc.: 98.44%] [Generator loss: 8.016189]\n",
      "17058 [Discriminator loss: 0.128618, acc.: 95.31%] [Generator loss: 8.018984]\n",
      "17059 [Discriminator loss: 0.080465, acc.: 98.44%] [Generator loss: 9.914749]\n",
      "17060 [Discriminator loss: 0.126284, acc.: 98.44%] [Generator loss: 8.551780]\n",
      "17061 [Discriminator loss: 0.227225, acc.: 89.06%] [Generator loss: 5.952830]\n",
      "17062 [Discriminator loss: 0.144598, acc.: 95.31%] [Generator loss: 7.156300]\n",
      "17063 [Discriminator loss: 0.052663, acc.: 98.44%] [Generator loss: 7.553789]\n",
      "17064 [Discriminator loss: 0.216664, acc.: 95.31%] [Generator loss: 7.411761]\n",
      "17065 [Discriminator loss: 0.111465, acc.: 93.75%] [Generator loss: 8.244122]\n",
      "17066 [Discriminator loss: 0.031020, acc.: 100.00%] [Generator loss: 8.253055]\n",
      "17067 [Discriminator loss: 0.032746, acc.: 98.44%] [Generator loss: 7.733144]\n",
      "17068 [Discriminator loss: 0.030036, acc.: 98.44%] [Generator loss: 9.459143]\n",
      "17069 [Discriminator loss: 0.104135, acc.: 95.31%] [Generator loss: 7.798813]\n",
      "17070 [Discriminator loss: 0.055331, acc.: 96.88%] [Generator loss: 6.639798]\n",
      "17071 [Discriminator loss: 0.055053, acc.: 96.88%] [Generator loss: 8.190453]\n",
      "17072 [Discriminator loss: 0.020507, acc.: 100.00%] [Generator loss: 7.392283]\n",
      "17073 [Discriminator loss: 0.048438, acc.: 96.88%] [Generator loss: 7.016426]\n",
      "17074 [Discriminator loss: 0.093586, acc.: 96.88%] [Generator loss: 7.805927]\n",
      "17075 [Discriminator loss: 0.043816, acc.: 98.44%] [Generator loss: 8.552216]\n",
      "17076 [Discriminator loss: 0.105952, acc.: 93.75%] [Generator loss: 9.314478]\n",
      "17077 [Discriminator loss: 0.091879, acc.: 96.88%] [Generator loss: 6.869070]\n",
      "17078 [Discriminator loss: 0.086875, acc.: 96.88%] [Generator loss: 6.669804]\n",
      "17079 [Discriminator loss: 0.118421, acc.: 96.88%] [Generator loss: 6.944261]\n",
      "17080 [Discriminator loss: 0.135842, acc.: 93.75%] [Generator loss: 7.563128]\n",
      "17081 [Discriminator loss: 0.035623, acc.: 98.44%] [Generator loss: 7.928105]\n",
      "17082 [Discriminator loss: 0.148002, acc.: 95.31%] [Generator loss: 6.667557]\n",
      "17083 [Discriminator loss: 0.027480, acc.: 100.00%] [Generator loss: 7.648457]\n",
      "17084 [Discriminator loss: 0.138094, acc.: 95.31%] [Generator loss: 8.829912]\n",
      "17085 [Discriminator loss: 0.016582, acc.: 100.00%] [Generator loss: 9.821864]\n",
      "17086 [Discriminator loss: 0.163880, acc.: 92.19%] [Generator loss: 7.207883]\n",
      "17087 [Discriminator loss: 0.093991, acc.: 93.75%] [Generator loss: 7.660385]\n",
      "17088 [Discriminator loss: 0.040508, acc.: 100.00%] [Generator loss: 8.750608]\n",
      "17089 [Discriminator loss: 0.066918, acc.: 98.44%] [Generator loss: 7.226365]\n",
      "17090 [Discriminator loss: 0.060633, acc.: 98.44%] [Generator loss: 5.937483]\n",
      "17091 [Discriminator loss: 0.057589, acc.: 98.44%] [Generator loss: 5.761122]\n",
      "17092 [Discriminator loss: 0.064665, acc.: 96.88%] [Generator loss: 7.091135]\n",
      "17093 [Discriminator loss: 0.151844, acc.: 95.31%] [Generator loss: 8.245349]\n",
      "17094 [Discriminator loss: 0.287327, acc.: 90.62%] [Generator loss: 8.021477]\n",
      "17095 [Discriminator loss: 0.049118, acc.: 98.44%] [Generator loss: 8.656679]\n",
      "17096 [Discriminator loss: 0.156588, acc.: 93.75%] [Generator loss: 9.157200]\n",
      "17097 [Discriminator loss: 0.032471, acc.: 98.44%] [Generator loss: 9.190416]\n",
      "17098 [Discriminator loss: 0.109409, acc.: 95.31%] [Generator loss: 8.297110]\n",
      "17099 [Discriminator loss: 0.092167, acc.: 96.88%] [Generator loss: 8.421560]\n",
      "17100 [Discriminator loss: 0.079538, acc.: 95.31%] [Generator loss: 9.125223]\n",
      "17101 [Discriminator loss: 0.143115, acc.: 89.06%] [Generator loss: 7.103110]\n",
      "17102 [Discriminator loss: 0.004745, acc.: 100.00%] [Generator loss: 7.310651]\n",
      "17103 [Discriminator loss: 0.033335, acc.: 98.44%] [Generator loss: 6.799220]\n",
      "17104 [Discriminator loss: 0.026411, acc.: 98.44%] [Generator loss: 8.337225]\n",
      "17105 [Discriminator loss: 0.081487, acc.: 98.44%] [Generator loss: 7.329602]\n",
      "17106 [Discriminator loss: 0.049837, acc.: 96.88%] [Generator loss: 7.029360]\n",
      "17107 [Discriminator loss: 0.012012, acc.: 100.00%] [Generator loss: 8.236805]\n",
      "17108 [Discriminator loss: 0.037649, acc.: 98.44%] [Generator loss: 8.846112]\n",
      "17109 [Discriminator loss: 0.079591, acc.: 95.31%] [Generator loss: 9.490959]\n",
      "17110 [Discriminator loss: 0.071413, acc.: 96.88%] [Generator loss: 7.659569]\n",
      "17111 [Discriminator loss: 0.065034, acc.: 98.44%] [Generator loss: 7.678876]\n",
      "17112 [Discriminator loss: 0.083154, acc.: 93.75%] [Generator loss: 7.345307]\n",
      "17113 [Discriminator loss: 0.085070, acc.: 95.31%] [Generator loss: 7.549072]\n",
      "17114 [Discriminator loss: 0.306032, acc.: 89.06%] [Generator loss: 8.427842]\n",
      "17115 [Discriminator loss: 0.185391, acc.: 98.44%] [Generator loss: 8.275137]\n",
      "17116 [Discriminator loss: 0.141577, acc.: 96.88%] [Generator loss: 11.187314]\n",
      "17117 [Discriminator loss: 0.064848, acc.: 96.88%] [Generator loss: 9.081606]\n",
      "17118 [Discriminator loss: 0.344882, acc.: 87.50%] [Generator loss: 4.899281]\n",
      "17119 [Discriminator loss: 0.153476, acc.: 95.31%] [Generator loss: 7.291944]\n",
      "17120 [Discriminator loss: 0.030210, acc.: 98.44%] [Generator loss: 7.966876]\n",
      "17121 [Discriminator loss: 0.104362, acc.: 95.31%] [Generator loss: 8.480500]\n",
      "17122 [Discriminator loss: 0.105156, acc.: 93.75%] [Generator loss: 7.846738]\n",
      "17123 [Discriminator loss: 0.024269, acc.: 100.00%] [Generator loss: 8.574414]\n",
      "17124 [Discriminator loss: 0.127135, acc.: 96.88%] [Generator loss: 9.901958]\n",
      "17125 [Discriminator loss: 0.132337, acc.: 95.31%] [Generator loss: 8.396858]\n",
      "17126 [Discriminator loss: 0.049163, acc.: 98.44%] [Generator loss: 10.227661]\n",
      "17127 [Discriminator loss: 0.127590, acc.: 95.31%] [Generator loss: 6.772501]\n",
      "17128 [Discriminator loss: 0.123962, acc.: 95.31%] [Generator loss: 6.300446]\n",
      "17129 [Discriminator loss: 0.085715, acc.: 96.88%] [Generator loss: 7.214796]\n",
      "17130 [Discriminator loss: 0.038048, acc.: 98.44%] [Generator loss: 8.322696]\n",
      "17131 [Discriminator loss: 0.065119, acc.: 98.44%] [Generator loss: 7.360538]\n",
      "17132 [Discriminator loss: 0.100213, acc.: 95.31%] [Generator loss: 8.467892]\n",
      "17133 [Discriminator loss: 0.133266, acc.: 95.31%] [Generator loss: 10.525003]\n",
      "17134 [Discriminator loss: 0.032855, acc.: 100.00%] [Generator loss: 9.374081]\n",
      "17135 [Discriminator loss: 0.027962, acc.: 98.44%] [Generator loss: 8.022205]\n",
      "17136 [Discriminator loss: 0.018904, acc.: 100.00%] [Generator loss: 7.343824]\n",
      "17137 [Discriminator loss: 0.110506, acc.: 96.88%] [Generator loss: 7.726774]\n",
      "17138 [Discriminator loss: 0.120377, acc.: 95.31%] [Generator loss: 9.957260]\n",
      "17139 [Discriminator loss: 0.057088, acc.: 96.88%] [Generator loss: 7.749802]\n",
      "17140 [Discriminator loss: 0.252240, acc.: 92.19%] [Generator loss: 9.699867]\n",
      "17141 [Discriminator loss: 0.045627, acc.: 98.44%] [Generator loss: 8.986957]\n",
      "17142 [Discriminator loss: 0.038035, acc.: 98.44%] [Generator loss: 8.958157]\n",
      "17143 [Discriminator loss: 0.197502, acc.: 92.19%] [Generator loss: 6.293799]\n",
      "17144 [Discriminator loss: 0.127894, acc.: 96.88%] [Generator loss: 9.256272]\n",
      "17145 [Discriminator loss: 0.087106, acc.: 96.88%] [Generator loss: 8.445147]\n",
      "17146 [Discriminator loss: 0.169632, acc.: 90.62%] [Generator loss: 7.244082]\n",
      "17147 [Discriminator loss: 0.107383, acc.: 92.19%] [Generator loss: 7.617618]\n",
      "17148 [Discriminator loss: 0.038137, acc.: 96.88%] [Generator loss: 8.161188]\n",
      "17149 [Discriminator loss: 0.128336, acc.: 95.31%] [Generator loss: 9.618541]\n",
      "17150 [Discriminator loss: 0.026199, acc.: 100.00%] [Generator loss: 8.835377]\n",
      "17151 [Discriminator loss: 0.124947, acc.: 96.88%] [Generator loss: 9.457335]\n",
      "17152 [Discriminator loss: 0.052855, acc.: 98.44%] [Generator loss: 9.312281]\n",
      "17153 [Discriminator loss: 0.025499, acc.: 100.00%] [Generator loss: 6.847239]\n",
      "17154 [Discriminator loss: 0.029201, acc.: 96.88%] [Generator loss: 7.814364]\n",
      "17155 [Discriminator loss: 0.041645, acc.: 100.00%] [Generator loss: 8.904723]\n",
      "17156 [Discriminator loss: 0.059849, acc.: 96.88%] [Generator loss: 8.782417]\n",
      "17157 [Discriminator loss: 0.030196, acc.: 98.44%] [Generator loss: 7.794004]\n",
      "17158 [Discriminator loss: 0.045479, acc.: 98.44%] [Generator loss: 7.020092]\n",
      "17159 [Discriminator loss: 0.172899, acc.: 93.75%] [Generator loss: 8.208202]\n",
      "17160 [Discriminator loss: 0.039891, acc.: 98.44%] [Generator loss: 8.934233]\n",
      "17161 [Discriminator loss: 0.038164, acc.: 100.00%] [Generator loss: 9.068702]\n",
      "17162 [Discriminator loss: 0.210687, acc.: 92.19%] [Generator loss: 6.094165]\n",
      "17163 [Discriminator loss: 0.146386, acc.: 93.75%] [Generator loss: 8.531137]\n",
      "17164 [Discriminator loss: 0.036702, acc.: 98.44%] [Generator loss: 9.746555]\n",
      "17165 [Discriminator loss: 0.197543, acc.: 92.19%] [Generator loss: 8.691483]\n",
      "17166 [Discriminator loss: 0.209953, acc.: 95.31%] [Generator loss: 7.890069]\n",
      "17167 [Discriminator loss: 0.031246, acc.: 98.44%] [Generator loss: 9.385663]\n",
      "17168 [Discriminator loss: 0.025189, acc.: 100.00%] [Generator loss: 8.613329]\n",
      "17169 [Discriminator loss: 0.168638, acc.: 93.75%] [Generator loss: 7.444167]\n",
      "17170 [Discriminator loss: 0.035091, acc.: 98.44%] [Generator loss: 7.584268]\n",
      "17171 [Discriminator loss: 0.045784, acc.: 98.44%] [Generator loss: 8.157714]\n",
      "17172 [Discriminator loss: 0.030796, acc.: 98.44%] [Generator loss: 9.798890]\n",
      "17173 [Discriminator loss: 0.105209, acc.: 96.88%] [Generator loss: 8.553465]\n",
      "17174 [Discriminator loss: 0.156475, acc.: 93.75%] [Generator loss: 8.618627]\n",
      "17175 [Discriminator loss: 0.053178, acc.: 96.88%] [Generator loss: 7.516473]\n",
      "17176 [Discriminator loss: 0.059646, acc.: 96.88%] [Generator loss: 8.687052]\n",
      "17177 [Discriminator loss: 0.116028, acc.: 95.31%] [Generator loss: 7.805419]\n",
      "17178 [Discriminator loss: 0.209030, acc.: 93.75%] [Generator loss: 9.657072]\n",
      "17179 [Discriminator loss: 0.079815, acc.: 95.31%] [Generator loss: 8.780310]\n",
      "17180 [Discriminator loss: 0.159855, acc.: 90.62%] [Generator loss: 7.841753]\n",
      "17181 [Discriminator loss: 0.032409, acc.: 98.44%] [Generator loss: 8.908914]\n",
      "17182 [Discriminator loss: 0.069107, acc.: 96.88%] [Generator loss: 8.789582]\n",
      "17183 [Discriminator loss: 0.078751, acc.: 96.88%] [Generator loss: 9.719820]\n",
      "17184 [Discriminator loss: 0.038847, acc.: 98.44%] [Generator loss: 8.246467]\n",
      "17185 [Discriminator loss: 0.044218, acc.: 98.44%] [Generator loss: 6.902166]\n",
      "17186 [Discriminator loss: 0.118957, acc.: 95.31%] [Generator loss: 9.578596]\n",
      "17187 [Discriminator loss: 0.055681, acc.: 96.88%] [Generator loss: 9.190822]\n",
      "17188 [Discriminator loss: 0.041868, acc.: 100.00%] [Generator loss: 8.154667]\n",
      "17189 [Discriminator loss: 0.052222, acc.: 96.88%] [Generator loss: 6.605933]\n",
      "17190 [Discriminator loss: 0.128039, acc.: 93.75%] [Generator loss: 7.081627]\n",
      "17191 [Discriminator loss: 0.075830, acc.: 96.88%] [Generator loss: 9.274237]\n",
      "17192 [Discriminator loss: 0.084135, acc.: 96.88%] [Generator loss: 9.817015]\n",
      "17193 [Discriminator loss: 0.084368, acc.: 96.88%] [Generator loss: 6.115617]\n",
      "17194 [Discriminator loss: 0.235477, acc.: 93.75%] [Generator loss: 8.473283]\n",
      "17195 [Discriminator loss: 0.040736, acc.: 98.44%] [Generator loss: 9.375981]\n",
      "17196 [Discriminator loss: 0.124974, acc.: 95.31%] [Generator loss: 8.543541]\n",
      "17197 [Discriminator loss: 0.030187, acc.: 98.44%] [Generator loss: 8.908591]\n",
      "17198 [Discriminator loss: 0.031438, acc.: 98.44%] [Generator loss: 7.671851]\n",
      "17199 [Discriminator loss: 0.024552, acc.: 98.44%] [Generator loss: 9.304949]\n",
      "17200 [Discriminator loss: 0.019460, acc.: 100.00%] [Generator loss: 7.672294]\n",
      "17201 [Discriminator loss: 0.137258, acc.: 95.31%] [Generator loss: 7.265953]\n",
      "17202 [Discriminator loss: 0.118768, acc.: 96.88%] [Generator loss: 7.427516]\n",
      "17203 [Discriminator loss: 0.044332, acc.: 98.44%] [Generator loss: 9.183012]\n",
      "17204 [Discriminator loss: 0.028179, acc.: 100.00%] [Generator loss: 9.131871]\n",
      "17205 [Discriminator loss: 0.161189, acc.: 92.19%] [Generator loss: 6.779778]\n",
      "17206 [Discriminator loss: 0.127180, acc.: 95.31%] [Generator loss: 7.895312]\n",
      "17207 [Discriminator loss: 0.049346, acc.: 98.44%] [Generator loss: 8.429503]\n",
      "17208 [Discriminator loss: 0.036368, acc.: 98.44%] [Generator loss: 7.787013]\n",
      "17209 [Discriminator loss: 0.035724, acc.: 100.00%] [Generator loss: 8.790060]\n",
      "17210 [Discriminator loss: 0.066250, acc.: 96.88%] [Generator loss: 6.545973]\n",
      "17211 [Discriminator loss: 0.063552, acc.: 100.00%] [Generator loss: 6.997664]\n",
      "17212 [Discriminator loss: 0.066577, acc.: 98.44%] [Generator loss: 6.325377]\n",
      "17213 [Discriminator loss: 0.007877, acc.: 100.00%] [Generator loss: 7.296249]\n",
      "17214 [Discriminator loss: 0.082852, acc.: 96.88%] [Generator loss: 7.531054]\n",
      "17215 [Discriminator loss: 0.044339, acc.: 98.44%] [Generator loss: 8.485426]\n",
      "17216 [Discriminator loss: 0.092195, acc.: 95.31%] [Generator loss: 7.394641]\n",
      "17217 [Discriminator loss: 0.105002, acc.: 95.31%] [Generator loss: 9.162203]\n",
      "17218 [Discriminator loss: 0.020758, acc.: 100.00%] [Generator loss: 9.416061]\n",
      "17219 [Discriminator loss: 0.067310, acc.: 96.88%] [Generator loss: 7.617674]\n",
      "17220 [Discriminator loss: 0.055640, acc.: 96.88%] [Generator loss: 7.540263]\n",
      "17221 [Discriminator loss: 0.025308, acc.: 100.00%] [Generator loss: 7.872531]\n",
      "17222 [Discriminator loss: 0.015085, acc.: 100.00%] [Generator loss: 9.345922]\n",
      "17223 [Discriminator loss: 0.014900, acc.: 100.00%] [Generator loss: 7.509147]\n",
      "17224 [Discriminator loss: 0.072620, acc.: 96.88%] [Generator loss: 9.453372]\n",
      "17225 [Discriminator loss: 0.130037, acc.: 93.75%] [Generator loss: 7.303257]\n",
      "17226 [Discriminator loss: 0.082867, acc.: 95.31%] [Generator loss: 7.340614]\n",
      "17227 [Discriminator loss: 0.098993, acc.: 96.88%] [Generator loss: 9.374278]\n",
      "17228 [Discriminator loss: 0.011872, acc.: 100.00%] [Generator loss: 10.191673]\n",
      "17229 [Discriminator loss: 0.179545, acc.: 95.31%] [Generator loss: 7.239242]\n",
      "17230 [Discriminator loss: 0.073383, acc.: 98.44%] [Generator loss: 7.130188]\n",
      "17231 [Discriminator loss: 0.065812, acc.: 98.44%] [Generator loss: 7.460020]\n",
      "17232 [Discriminator loss: 0.025715, acc.: 100.00%] [Generator loss: 7.137836]\n",
      "17233 [Discriminator loss: 0.084936, acc.: 95.31%] [Generator loss: 8.915567]\n",
      "17234 [Discriminator loss: 0.013911, acc.: 100.00%] [Generator loss: 8.133964]\n",
      "17235 [Discriminator loss: 0.058046, acc.: 96.88%] [Generator loss: 8.165239]\n",
      "17236 [Discriminator loss: 0.018073, acc.: 100.00%] [Generator loss: 7.311948]\n",
      "17237 [Discriminator loss: 0.137783, acc.: 92.19%] [Generator loss: 7.779097]\n",
      "17238 [Discriminator loss: 0.071764, acc.: 96.88%] [Generator loss: 8.426912]\n",
      "17239 [Discriminator loss: 0.077118, acc.: 96.88%] [Generator loss: 8.001141]\n",
      "17240 [Discriminator loss: 0.141854, acc.: 93.75%] [Generator loss: 9.684440]\n",
      "17241 [Discriminator loss: 0.053956, acc.: 96.88%] [Generator loss: 10.215866]\n",
      "17242 [Discriminator loss: 0.183597, acc.: 95.31%] [Generator loss: 7.923124]\n",
      "17243 [Discriminator loss: 0.042658, acc.: 100.00%] [Generator loss: 6.649761]\n",
      "17244 [Discriminator loss: 0.157433, acc.: 96.88%] [Generator loss: 7.345484]\n",
      "17245 [Discriminator loss: 0.087719, acc.: 95.31%] [Generator loss: 7.560634]\n",
      "17246 [Discriminator loss: 0.115481, acc.: 95.31%] [Generator loss: 7.264245]\n",
      "17247 [Discriminator loss: 0.020313, acc.: 98.44%] [Generator loss: 7.783519]\n",
      "17248 [Discriminator loss: 0.037094, acc.: 98.44%] [Generator loss: 6.811441]\n",
      "17249 [Discriminator loss: 0.051233, acc.: 98.44%] [Generator loss: 6.367749]\n",
      "17250 [Discriminator loss: 0.039772, acc.: 100.00%] [Generator loss: 6.153780]\n",
      "17251 [Discriminator loss: 0.215399, acc.: 92.19%] [Generator loss: 7.952856]\n",
      "17252 [Discriminator loss: 0.053293, acc.: 98.44%] [Generator loss: 9.015695]\n",
      "17253 [Discriminator loss: 0.096530, acc.: 95.31%] [Generator loss: 8.928589]\n",
      "17254 [Discriminator loss: 0.185313, acc.: 93.75%] [Generator loss: 6.901207]\n",
      "17255 [Discriminator loss: 0.081976, acc.: 96.88%] [Generator loss: 7.670620]\n",
      "17256 [Discriminator loss: 0.035583, acc.: 96.88%] [Generator loss: 7.152450]\n",
      "17257 [Discriminator loss: 0.049176, acc.: 96.88%] [Generator loss: 6.938103]\n",
      "17258 [Discriminator loss: 0.049660, acc.: 100.00%] [Generator loss: 6.623836]\n",
      "17259 [Discriminator loss: 0.130387, acc.: 95.31%] [Generator loss: 5.037939]\n",
      "17260 [Discriminator loss: 0.158803, acc.: 96.88%] [Generator loss: 7.783910]\n",
      "17261 [Discriminator loss: 0.111442, acc.: 93.75%] [Generator loss: 8.312624]\n",
      "17262 [Discriminator loss: 0.078599, acc.: 98.44%] [Generator loss: 7.164790]\n",
      "17263 [Discriminator loss: 0.042209, acc.: 98.44%] [Generator loss: 9.820293]\n",
      "17264 [Discriminator loss: 0.015288, acc.: 100.00%] [Generator loss: 8.433737]\n",
      "17265 [Discriminator loss: 0.207238, acc.: 92.19%] [Generator loss: 9.847564]\n",
      "17266 [Discriminator loss: 0.029117, acc.: 100.00%] [Generator loss: 9.583229]\n",
      "17267 [Discriminator loss: 0.038516, acc.: 98.44%] [Generator loss: 9.083839]\n",
      "17268 [Discriminator loss: 0.145977, acc.: 95.31%] [Generator loss: 7.964248]\n",
      "17269 [Discriminator loss: 0.042693, acc.: 98.44%] [Generator loss: 8.755772]\n",
      "17270 [Discriminator loss: 0.036575, acc.: 98.44%] [Generator loss: 6.842992]\n",
      "17271 [Discriminator loss: 0.059945, acc.: 96.88%] [Generator loss: 7.358311]\n",
      "17272 [Discriminator loss: 0.019210, acc.: 100.00%] [Generator loss: 6.479573]\n",
      "17273 [Discriminator loss: 0.294930, acc.: 89.06%] [Generator loss: 9.299522]\n",
      "17274 [Discriminator loss: 0.076195, acc.: 95.31%] [Generator loss: 10.161451]\n",
      "17275 [Discriminator loss: 0.084843, acc.: 96.88%] [Generator loss: 8.266581]\n",
      "17276 [Discriminator loss: 0.111785, acc.: 93.75%] [Generator loss: 5.956556]\n",
      "17277 [Discriminator loss: 0.081610, acc.: 96.88%] [Generator loss: 7.650521]\n",
      "17278 [Discriminator loss: 0.050202, acc.: 98.44%] [Generator loss: 7.462550]\n",
      "17279 [Discriminator loss: 0.028711, acc.: 98.44%] [Generator loss: 8.161572]\n",
      "17280 [Discriminator loss: 0.039557, acc.: 100.00%] [Generator loss: 7.327608]\n",
      "17281 [Discriminator loss: 0.039451, acc.: 98.44%] [Generator loss: 7.596711]\n",
      "17282 [Discriminator loss: 0.022901, acc.: 100.00%] [Generator loss: 7.263359]\n",
      "17283 [Discriminator loss: 0.116574, acc.: 96.88%] [Generator loss: 6.841835]\n",
      "17284 [Discriminator loss: 0.044857, acc.: 98.44%] [Generator loss: 8.348469]\n",
      "17285 [Discriminator loss: 0.036051, acc.: 98.44%] [Generator loss: 7.349932]\n",
      "17286 [Discriminator loss: 0.061685, acc.: 98.44%] [Generator loss: 6.694599]\n",
      "17287 [Discriminator loss: 0.063953, acc.: 98.44%] [Generator loss: 7.533748]\n",
      "17288 [Discriminator loss: 0.118930, acc.: 95.31%] [Generator loss: 6.328126]\n",
      "17289 [Discriminator loss: 0.060268, acc.: 98.44%] [Generator loss: 9.425475]\n",
      "17290 [Discriminator loss: 0.071491, acc.: 96.88%] [Generator loss: 6.850761]\n",
      "17291 [Discriminator loss: 0.129899, acc.: 95.31%] [Generator loss: 7.979045]\n",
      "17292 [Discriminator loss: 0.209136, acc.: 92.19%] [Generator loss: 7.799939]\n",
      "17293 [Discriminator loss: 0.048537, acc.: 98.44%] [Generator loss: 8.812289]\n",
      "17294 [Discriminator loss: 0.067135, acc.: 96.88%] [Generator loss: 9.324371]\n",
      "17295 [Discriminator loss: 0.098897, acc.: 95.31%] [Generator loss: 9.074025]\n",
      "17296 [Discriminator loss: 0.053626, acc.: 96.88%] [Generator loss: 8.578086]\n",
      "17297 [Discriminator loss: 0.064565, acc.: 96.88%] [Generator loss: 8.669249]\n",
      "17298 [Discriminator loss: 0.115137, acc.: 95.31%] [Generator loss: 7.565998]\n",
      "17299 [Discriminator loss: 0.094908, acc.: 96.88%] [Generator loss: 7.045825]\n",
      "17300 [Discriminator loss: 0.109942, acc.: 96.88%] [Generator loss: 8.087856]\n",
      "17301 [Discriminator loss: 0.078005, acc.: 98.44%] [Generator loss: 7.932917]\n",
      "17302 [Discriminator loss: 0.068332, acc.: 96.88%] [Generator loss: 8.543527]\n",
      "17303 [Discriminator loss: 0.042324, acc.: 98.44%] [Generator loss: 7.986245]\n",
      "17304 [Discriminator loss: 0.042411, acc.: 98.44%] [Generator loss: 7.841046]\n",
      "17305 [Discriminator loss: 0.054016, acc.: 96.88%] [Generator loss: 8.153871]\n",
      "17306 [Discriminator loss: 0.051497, acc.: 96.88%] [Generator loss: 8.581068]\n",
      "17307 [Discriminator loss: 0.161221, acc.: 93.75%] [Generator loss: 10.080820]\n",
      "17308 [Discriminator loss: 0.119859, acc.: 93.75%] [Generator loss: 6.505800]\n",
      "17309 [Discriminator loss: 0.174728, acc.: 92.19%] [Generator loss: 6.981815]\n",
      "17310 [Discriminator loss: 0.021544, acc.: 100.00%] [Generator loss: 8.990528]\n",
      "17311 [Discriminator loss: 0.146833, acc.: 92.19%] [Generator loss: 10.147573]\n",
      "17312 [Discriminator loss: 0.044726, acc.: 96.88%] [Generator loss: 7.844025]\n",
      "17313 [Discriminator loss: 0.033227, acc.: 98.44%] [Generator loss: 8.086853]\n",
      "17314 [Discriminator loss: 0.118246, acc.: 93.75%] [Generator loss: 7.642127]\n",
      "17315 [Discriminator loss: 0.140404, acc.: 92.19%] [Generator loss: 7.829066]\n",
      "17316 [Discriminator loss: 0.043968, acc.: 100.00%] [Generator loss: 8.147482]\n",
      "17317 [Discriminator loss: 0.065760, acc.: 96.88%] [Generator loss: 8.347673]\n",
      "17318 [Discriminator loss: 0.037438, acc.: 98.44%] [Generator loss: 8.146774]\n",
      "17319 [Discriminator loss: 0.043819, acc.: 100.00%] [Generator loss: 7.945348]\n",
      "17320 [Discriminator loss: 0.064489, acc.: 98.44%] [Generator loss: 9.244542]\n",
      "17321 [Discriminator loss: 0.097385, acc.: 96.88%] [Generator loss: 9.569115]\n",
      "17322 [Discriminator loss: 0.051253, acc.: 96.88%] [Generator loss: 8.922670]\n",
      "17323 [Discriminator loss: 0.050546, acc.: 98.44%] [Generator loss: 7.609649]\n",
      "17324 [Discriminator loss: 0.087231, acc.: 98.44%] [Generator loss: 8.093443]\n",
      "17325 [Discriminator loss: 0.143520, acc.: 95.31%] [Generator loss: 8.878011]\n",
      "17326 [Discriminator loss: 0.021851, acc.: 100.00%] [Generator loss: 8.005897]\n",
      "17327 [Discriminator loss: 0.053598, acc.: 98.44%] [Generator loss: 8.453066]\n",
      "17328 [Discriminator loss: 0.058208, acc.: 98.44%] [Generator loss: 8.905483]\n",
      "17329 [Discriminator loss: 0.051174, acc.: 98.44%] [Generator loss: 7.914542]\n",
      "17330 [Discriminator loss: 0.030392, acc.: 98.44%] [Generator loss: 8.433486]\n",
      "17331 [Discriminator loss: 0.047311, acc.: 98.44%] [Generator loss: 6.721693]\n",
      "17332 [Discriminator loss: 0.119216, acc.: 95.31%] [Generator loss: 7.262674]\n",
      "17333 [Discriminator loss: 0.071626, acc.: 96.88%] [Generator loss: 8.390259]\n",
      "17334 [Discriminator loss: 0.022021, acc.: 98.44%] [Generator loss: 7.521203]\n",
      "17335 [Discriminator loss: 0.112771, acc.: 95.31%] [Generator loss: 9.520117]\n",
      "17336 [Discriminator loss: 0.105055, acc.: 93.75%] [Generator loss: 8.512028]\n",
      "17337 [Discriminator loss: 0.091420, acc.: 96.88%] [Generator loss: 9.130452]\n",
      "17338 [Discriminator loss: 0.163307, acc.: 92.19%] [Generator loss: 9.941378]\n",
      "17339 [Discriminator loss: 0.024871, acc.: 100.00%] [Generator loss: 8.564495]\n",
      "17340 [Discriminator loss: 0.053319, acc.: 96.88%] [Generator loss: 7.308682]\n",
      "17341 [Discriminator loss: 0.121044, acc.: 96.88%] [Generator loss: 8.649335]\n",
      "17342 [Discriminator loss: 0.175705, acc.: 95.31%] [Generator loss: 10.594292]\n",
      "17343 [Discriminator loss: 0.086214, acc.: 95.31%] [Generator loss: 8.019394]\n",
      "17344 [Discriminator loss: 0.062692, acc.: 96.88%] [Generator loss: 8.155983]\n",
      "17345 [Discriminator loss: 0.042438, acc.: 98.44%] [Generator loss: 8.203873]\n",
      "17346 [Discriminator loss: 0.070970, acc.: 96.88%] [Generator loss: 8.768275]\n",
      "17347 [Discriminator loss: 0.159871, acc.: 93.75%] [Generator loss: 6.274411]\n",
      "17348 [Discriminator loss: 0.022412, acc.: 100.00%] [Generator loss: 7.298709]\n",
      "17349 [Discriminator loss: 0.107513, acc.: 93.75%] [Generator loss: 7.041532]\n",
      "17350 [Discriminator loss: 0.053186, acc.: 98.44%] [Generator loss: 9.174158]\n",
      "17351 [Discriminator loss: 0.070303, acc.: 95.31%] [Generator loss: 8.409284]\n",
      "17352 [Discriminator loss: 0.092446, acc.: 93.75%] [Generator loss: 8.603848]\n",
      "17353 [Discriminator loss: 0.028790, acc.: 98.44%] [Generator loss: 7.819748]\n",
      "17354 [Discriminator loss: 0.092083, acc.: 96.88%] [Generator loss: 7.585823]\n",
      "17355 [Discriminator loss: 0.115324, acc.: 95.31%] [Generator loss: 9.509028]\n",
      "17356 [Discriminator loss: 0.103982, acc.: 98.44%] [Generator loss: 7.128289]\n",
      "17357 [Discriminator loss: 0.065829, acc.: 96.88%] [Generator loss: 7.659645]\n",
      "17358 [Discriminator loss: 0.081468, acc.: 95.31%] [Generator loss: 7.017445]\n",
      "17359 [Discriminator loss: 0.080881, acc.: 96.88%] [Generator loss: 6.910980]\n",
      "17360 [Discriminator loss: 0.029154, acc.: 100.00%] [Generator loss: 7.138442]\n",
      "17361 [Discriminator loss: 0.204545, acc.: 92.19%] [Generator loss: 7.923415]\n",
      "17362 [Discriminator loss: 0.047127, acc.: 98.44%] [Generator loss: 8.913722]\n",
      "17363 [Discriminator loss: 0.021264, acc.: 98.44%] [Generator loss: 8.641610]\n",
      "17364 [Discriminator loss: 0.221679, acc.: 90.62%] [Generator loss: 7.767138]\n",
      "17365 [Discriminator loss: 0.018478, acc.: 100.00%] [Generator loss: 9.035420]\n",
      "17366 [Discriminator loss: 0.010892, acc.: 100.00%] [Generator loss: 9.309118]\n",
      "17367 [Discriminator loss: 0.072868, acc.: 96.88%] [Generator loss: 7.331795]\n",
      "17368 [Discriminator loss: 0.172095, acc.: 96.88%] [Generator loss: 9.333438]\n",
      "17369 [Discriminator loss: 0.022484, acc.: 98.44%] [Generator loss: 9.515839]\n",
      "17370 [Discriminator loss: 0.051853, acc.: 95.31%] [Generator loss: 8.047366]\n",
      "17371 [Discriminator loss: 0.050784, acc.: 95.31%] [Generator loss: 8.458599]\n",
      "17372 [Discriminator loss: 0.085903, acc.: 95.31%] [Generator loss: 6.381872]\n",
      "17373 [Discriminator loss: 0.284460, acc.: 89.06%] [Generator loss: 11.342481]\n",
      "17374 [Discriminator loss: 0.075115, acc.: 95.31%] [Generator loss: 11.657064]\n",
      "17375 [Discriminator loss: 0.048477, acc.: 98.44%] [Generator loss: 10.090136]\n",
      "17376 [Discriminator loss: 0.065356, acc.: 96.88%] [Generator loss: 8.926375]\n",
      "17377 [Discriminator loss: 0.188987, acc.: 92.19%] [Generator loss: 7.062552]\n",
      "17378 [Discriminator loss: 0.050400, acc.: 98.44%] [Generator loss: 8.140003]\n",
      "17379 [Discriminator loss: 0.181915, acc.: 93.75%] [Generator loss: 8.490574]\n",
      "17380 [Discriminator loss: 0.152196, acc.: 95.31%] [Generator loss: 8.471949]\n",
      "17381 [Discriminator loss: 0.096838, acc.: 96.88%] [Generator loss: 8.727063]\n",
      "17382 [Discriminator loss: 0.020517, acc.: 100.00%] [Generator loss: 7.640928]\n",
      "17383 [Discriminator loss: 0.112472, acc.: 95.31%] [Generator loss: 7.565045]\n",
      "17384 [Discriminator loss: 0.031188, acc.: 100.00%] [Generator loss: 7.707759]\n",
      "17385 [Discriminator loss: 0.037445, acc.: 96.88%] [Generator loss: 8.591984]\n",
      "17386 [Discriminator loss: 0.064590, acc.: 96.88%] [Generator loss: 8.508139]\n",
      "17387 [Discriminator loss: 0.145540, acc.: 96.88%] [Generator loss: 7.158247]\n",
      "17388 [Discriminator loss: 0.022607, acc.: 100.00%] [Generator loss: 7.512468]\n",
      "17389 [Discriminator loss: 0.081728, acc.: 95.31%] [Generator loss: 7.553572]\n",
      "17390 [Discriminator loss: 0.053714, acc.: 96.88%] [Generator loss: 7.202408]\n",
      "17391 [Discriminator loss: 0.137410, acc.: 96.88%] [Generator loss: 8.696906]\n",
      "17392 [Discriminator loss: 0.145802, acc.: 95.31%] [Generator loss: 9.638429]\n",
      "17393 [Discriminator loss: 0.117645, acc.: 95.31%] [Generator loss: 7.547848]\n",
      "17394 [Discriminator loss: 0.076039, acc.: 96.88%] [Generator loss: 8.621091]\n",
      "17395 [Discriminator loss: 0.043547, acc.: 98.44%] [Generator loss: 8.771753]\n",
      "17396 [Discriminator loss: 0.023756, acc.: 98.44%] [Generator loss: 9.601465]\n",
      "17397 [Discriminator loss: 0.042083, acc.: 98.44%] [Generator loss: 7.055768]\n",
      "17398 [Discriminator loss: 0.128093, acc.: 93.75%] [Generator loss: 7.277966]\n",
      "17399 [Discriminator loss: 0.036144, acc.: 98.44%] [Generator loss: 8.696383]\n",
      "17400 [Discriminator loss: 0.056724, acc.: 98.44%] [Generator loss: 8.312304]\n",
      "17401 [Discriminator loss: 0.042054, acc.: 96.88%] [Generator loss: 7.260102]\n",
      "17402 [Discriminator loss: 0.033546, acc.: 98.44%] [Generator loss: 6.383196]\n",
      "17403 [Discriminator loss: 0.017319, acc.: 100.00%] [Generator loss: 6.142055]\n",
      "17404 [Discriminator loss: 0.207218, acc.: 90.62%] [Generator loss: 11.410398]\n",
      "17405 [Discriminator loss: 0.252900, acc.: 87.50%] [Generator loss: 8.148470]\n",
      "17406 [Discriminator loss: 0.033866, acc.: 100.00%] [Generator loss: 7.740753]\n",
      "17407 [Discriminator loss: 0.166565, acc.: 90.62%] [Generator loss: 8.824760]\n",
      "17408 [Discriminator loss: 0.105710, acc.: 96.88%] [Generator loss: 6.761831]\n",
      "17409 [Discriminator loss: 0.032587, acc.: 100.00%] [Generator loss: 9.290312]\n",
      "17410 [Discriminator loss: 0.222971, acc.: 96.88%] [Generator loss: 8.370639]\n",
      "17411 [Discriminator loss: 0.084851, acc.: 95.31%] [Generator loss: 7.712344]\n",
      "17412 [Discriminator loss: 0.077695, acc.: 96.88%] [Generator loss: 8.056417]\n",
      "17413 [Discriminator loss: 0.038001, acc.: 98.44%] [Generator loss: 9.812605]\n",
      "17414 [Discriminator loss: 0.085718, acc.: 96.88%] [Generator loss: 7.489725]\n",
      "17415 [Discriminator loss: 0.126603, acc.: 95.31%] [Generator loss: 7.397409]\n",
      "17416 [Discriminator loss: 0.046848, acc.: 98.44%] [Generator loss: 7.162947]\n",
      "17417 [Discriminator loss: 0.060430, acc.: 96.88%] [Generator loss: 8.179583]\n",
      "17418 [Discriminator loss: 0.039875, acc.: 98.44%] [Generator loss: 7.600131]\n",
      "17419 [Discriminator loss: 0.111883, acc.: 95.31%] [Generator loss: 7.012630]\n",
      "17420 [Discriminator loss: 0.019460, acc.: 100.00%] [Generator loss: 7.423403]\n",
      "17421 [Discriminator loss: 0.087731, acc.: 96.88%] [Generator loss: 6.745090]\n",
      "17422 [Discriminator loss: 0.016432, acc.: 100.00%] [Generator loss: 7.741113]\n",
      "17423 [Discriminator loss: 0.077862, acc.: 95.31%] [Generator loss: 8.862987]\n",
      "17424 [Discriminator loss: 0.032264, acc.: 98.44%] [Generator loss: 8.042485]\n",
      "17425 [Discriminator loss: 0.053970, acc.: 98.44%] [Generator loss: 7.180956]\n",
      "17426 [Discriminator loss: 0.093880, acc.: 93.75%] [Generator loss: 6.730729]\n",
      "17427 [Discriminator loss: 0.045693, acc.: 98.44%] [Generator loss: 10.303011]\n",
      "17428 [Discriminator loss: 0.011483, acc.: 100.00%] [Generator loss: 9.421761]\n",
      "17429 [Discriminator loss: 0.199846, acc.: 95.31%] [Generator loss: 8.690819]\n",
      "17430 [Discriminator loss: 0.133584, acc.: 96.88%] [Generator loss: 7.315784]\n",
      "17431 [Discriminator loss: 0.014753, acc.: 100.00%] [Generator loss: 6.842112]\n",
      "17432 [Discriminator loss: 0.111441, acc.: 93.75%] [Generator loss: 9.349960]\n",
      "17433 [Discriminator loss: 0.021956, acc.: 100.00%] [Generator loss: 9.908760]\n",
      "17434 [Discriminator loss: 0.103744, acc.: 93.75%] [Generator loss: 6.655710]\n",
      "17435 [Discriminator loss: 0.040853, acc.: 98.44%] [Generator loss: 6.667418]\n",
      "17436 [Discriminator loss: 0.144381, acc.: 95.31%] [Generator loss: 7.129186]\n",
      "17437 [Discriminator loss: 0.033466, acc.: 98.44%] [Generator loss: 7.639350]\n",
      "17438 [Discriminator loss: 0.066191, acc.: 98.44%] [Generator loss: 7.342052]\n",
      "17439 [Discriminator loss: 0.047030, acc.: 98.44%] [Generator loss: 8.879517]\n",
      "17440 [Discriminator loss: 0.044033, acc.: 100.00%] [Generator loss: 8.714573]\n",
      "17441 [Discriminator loss: 0.037219, acc.: 98.44%] [Generator loss: 8.725430]\n",
      "17442 [Discriminator loss: 0.065480, acc.: 95.31%] [Generator loss: 7.887714]\n",
      "17443 [Discriminator loss: 0.058872, acc.: 96.88%] [Generator loss: 7.886359]\n",
      "17444 [Discriminator loss: 0.122545, acc.: 95.31%] [Generator loss: 8.073645]\n",
      "17445 [Discriminator loss: 0.062211, acc.: 98.44%] [Generator loss: 7.562359]\n",
      "17446 [Discriminator loss: 0.063959, acc.: 96.88%] [Generator loss: 7.855365]\n",
      "17447 [Discriminator loss: 0.042780, acc.: 98.44%] [Generator loss: 8.414406]\n",
      "17448 [Discriminator loss: 0.065484, acc.: 98.44%] [Generator loss: 8.085617]\n",
      "17449 [Discriminator loss: 0.109944, acc.: 95.31%] [Generator loss: 5.947385]\n",
      "17450 [Discriminator loss: 0.021981, acc.: 100.00%] [Generator loss: 6.307173]\n",
      "17451 [Discriminator loss: 0.036202, acc.: 100.00%] [Generator loss: 7.204787]\n",
      "17452 [Discriminator loss: 0.094038, acc.: 95.31%] [Generator loss: 9.741417]\n",
      "17453 [Discriminator loss: 0.019306, acc.: 100.00%] [Generator loss: 8.982973]\n",
      "17454 [Discriminator loss: 0.135752, acc.: 95.31%] [Generator loss: 7.425622]\n",
      "17455 [Discriminator loss: 0.006581, acc.: 100.00%] [Generator loss: 7.706381]\n",
      "17456 [Discriminator loss: 0.077358, acc.: 95.31%] [Generator loss: 7.633243]\n",
      "17457 [Discriminator loss: 0.071652, acc.: 95.31%] [Generator loss: 7.467768]\n",
      "17458 [Discriminator loss: 0.023309, acc.: 98.44%] [Generator loss: 8.643948]\n",
      "17459 [Discriminator loss: 0.064547, acc.: 98.44%] [Generator loss: 7.122617]\n",
      "17460 [Discriminator loss: 0.075291, acc.: 95.31%] [Generator loss: 9.105905]\n",
      "17461 [Discriminator loss: 0.022509, acc.: 100.00%] [Generator loss: 9.269843]\n",
      "17462 [Discriminator loss: 0.104635, acc.: 96.88%] [Generator loss: 7.104713]\n",
      "17463 [Discriminator loss: 0.037766, acc.: 98.44%] [Generator loss: 6.581146]\n",
      "17464 [Discriminator loss: 0.007775, acc.: 100.00%] [Generator loss: 7.737591]\n",
      "17465 [Discriminator loss: 0.089981, acc.: 96.88%] [Generator loss: 8.514519]\n",
      "17466 [Discriminator loss: 0.033015, acc.: 100.00%] [Generator loss: 7.368179]\n",
      "17467 [Discriminator loss: 0.083259, acc.: 95.31%] [Generator loss: 7.690537]\n",
      "17468 [Discriminator loss: 0.049599, acc.: 98.44%] [Generator loss: 8.868461]\n",
      "17469 [Discriminator loss: 0.179436, acc.: 90.62%] [Generator loss: 7.552842]\n",
      "17470 [Discriminator loss: 0.028641, acc.: 98.44%] [Generator loss: 9.871021]\n",
      "17471 [Discriminator loss: 0.119633, acc.: 95.31%] [Generator loss: 8.777903]\n",
      "17472 [Discriminator loss: 0.049666, acc.: 98.44%] [Generator loss: 6.744736]\n",
      "17473 [Discriminator loss: 0.036830, acc.: 98.44%] [Generator loss: 6.389023]\n",
      "17474 [Discriminator loss: 0.155454, acc.: 95.31%] [Generator loss: 8.589520]\n",
      "17475 [Discriminator loss: 0.002029, acc.: 100.00%] [Generator loss: 10.357390]\n",
      "17476 [Discriminator loss: 0.034279, acc.: 98.44%] [Generator loss: 9.634010]\n",
      "17477 [Discriminator loss: 0.020296, acc.: 100.00%] [Generator loss: 8.774074]\n",
      "17478 [Discriminator loss: 0.053784, acc.: 95.31%] [Generator loss: 8.340081]\n",
      "17479 [Discriminator loss: 0.010523, acc.: 100.00%] [Generator loss: 7.381159]\n",
      "17480 [Discriminator loss: 0.011243, acc.: 100.00%] [Generator loss: 7.637874]\n",
      "17481 [Discriminator loss: 0.060912, acc.: 96.88%] [Generator loss: 8.136974]\n",
      "17482 [Discriminator loss: 0.192115, acc.: 90.62%] [Generator loss: 9.269632]\n",
      "17483 [Discriminator loss: 0.045367, acc.: 98.44%] [Generator loss: 8.959727]\n",
      "17484 [Discriminator loss: 0.151947, acc.: 95.31%] [Generator loss: 5.918612]\n",
      "17485 [Discriminator loss: 0.183195, acc.: 93.75%] [Generator loss: 8.394167]\n",
      "17486 [Discriminator loss: 0.059347, acc.: 98.44%] [Generator loss: 10.201061]\n",
      "17487 [Discriminator loss: 0.270495, acc.: 90.62%] [Generator loss: 8.532832]\n",
      "17488 [Discriminator loss: 0.005300, acc.: 100.00%] [Generator loss: 9.736164]\n",
      "17489 [Discriminator loss: 0.031425, acc.: 98.44%] [Generator loss: 8.191729]\n",
      "17490 [Discriminator loss: 0.039665, acc.: 100.00%] [Generator loss: 7.628357]\n",
      "17491 [Discriminator loss: 0.026662, acc.: 100.00%] [Generator loss: 7.861074]\n",
      "17492 [Discriminator loss: 0.047274, acc.: 98.44%] [Generator loss: 8.957594]\n",
      "17493 [Discriminator loss: 0.026708, acc.: 98.44%] [Generator loss: 9.656429]\n",
      "17494 [Discriminator loss: 0.029391, acc.: 98.44%] [Generator loss: 9.823229]\n",
      "17495 [Discriminator loss: 0.099661, acc.: 96.88%] [Generator loss: 6.680069]\n",
      "17496 [Discriminator loss: 0.035780, acc.: 100.00%] [Generator loss: 7.534389]\n",
      "17497 [Discriminator loss: 0.115750, acc.: 93.75%] [Generator loss: 9.963124]\n",
      "17498 [Discriminator loss: 0.080055, acc.: 95.31%] [Generator loss: 9.865644]\n",
      "17499 [Discriminator loss: 0.124458, acc.: 95.31%] [Generator loss: 7.927430]\n",
      "17500 [Discriminator loss: 0.088788, acc.: 95.31%] [Generator loss: 6.462559]\n",
      "17501 [Discriminator loss: 0.074317, acc.: 96.88%] [Generator loss: 9.288689]\n",
      "17502 [Discriminator loss: 0.071923, acc.: 96.88%] [Generator loss: 8.095941]\n",
      "17503 [Discriminator loss: 0.020053, acc.: 98.44%] [Generator loss: 8.986381]\n",
      "17504 [Discriminator loss: 0.176677, acc.: 96.88%] [Generator loss: 7.611051]\n",
      "17505 [Discriminator loss: 0.016742, acc.: 100.00%] [Generator loss: 8.544933]\n",
      "17506 [Discriminator loss: 0.073200, acc.: 96.88%] [Generator loss: 9.139086]\n",
      "17507 [Discriminator loss: 0.114942, acc.: 92.19%] [Generator loss: 7.593984]\n",
      "17508 [Discriminator loss: 0.021968, acc.: 100.00%] [Generator loss: 7.835979]\n",
      "17509 [Discriminator loss: 0.099246, acc.: 95.31%] [Generator loss: 8.163406]\n",
      "17510 [Discriminator loss: 0.050626, acc.: 98.44%] [Generator loss: 7.768661]\n",
      "17511 [Discriminator loss: 0.126565, acc.: 98.44%] [Generator loss: 7.147051]\n",
      "17512 [Discriminator loss: 0.058957, acc.: 96.88%] [Generator loss: 8.273241]\n",
      "17513 [Discriminator loss: 0.100708, acc.: 95.31%] [Generator loss: 8.123467]\n",
      "17514 [Discriminator loss: 0.072306, acc.: 98.44%] [Generator loss: 11.238333]\n",
      "17515 [Discriminator loss: 0.176713, acc.: 92.19%] [Generator loss: 8.180785]\n",
      "17516 [Discriminator loss: 0.095929, acc.: 95.31%] [Generator loss: 6.934151]\n",
      "17517 [Discriminator loss: 0.156282, acc.: 95.31%] [Generator loss: 10.004732]\n",
      "17518 [Discriminator loss: 0.140178, acc.: 95.31%] [Generator loss: 9.469016]\n",
      "17519 [Discriminator loss: 0.032375, acc.: 100.00%] [Generator loss: 7.619464]\n",
      "17520 [Discriminator loss: 0.027821, acc.: 100.00%] [Generator loss: 8.124393]\n",
      "17521 [Discriminator loss: 0.142089, acc.: 93.75%] [Generator loss: 8.970303]\n",
      "17522 [Discriminator loss: 0.041500, acc.: 98.44%] [Generator loss: 9.526144]\n",
      "17523 [Discriminator loss: 0.132734, acc.: 93.75%] [Generator loss: 7.634189]\n",
      "17524 [Discriminator loss: 0.041760, acc.: 98.44%] [Generator loss: 8.339353]\n",
      "17525 [Discriminator loss: 0.085327, acc.: 96.88%] [Generator loss: 9.021539]\n",
      "17526 [Discriminator loss: 0.018334, acc.: 100.00%] [Generator loss: 9.344361]\n",
      "17527 [Discriminator loss: 0.030695, acc.: 98.44%] [Generator loss: 8.233827]\n",
      "17528 [Discriminator loss: 0.066815, acc.: 98.44%] [Generator loss: 7.408804]\n",
      "17529 [Discriminator loss: 0.092417, acc.: 93.75%] [Generator loss: 7.262011]\n",
      "17530 [Discriminator loss: 0.111918, acc.: 95.31%] [Generator loss: 7.303439]\n",
      "17531 [Discriminator loss: 0.060693, acc.: 98.44%] [Generator loss: 9.159152]\n",
      "17532 [Discriminator loss: 0.037421, acc.: 100.00%] [Generator loss: 8.945168]\n",
      "17533 [Discriminator loss: 0.070294, acc.: 98.44%] [Generator loss: 8.528223]\n",
      "17534 [Discriminator loss: 0.192871, acc.: 95.31%] [Generator loss: 7.961609]\n",
      "17535 [Discriminator loss: 0.047241, acc.: 100.00%] [Generator loss: 10.953260]\n",
      "17536 [Discriminator loss: 0.059854, acc.: 96.88%] [Generator loss: 8.734446]\n",
      "17537 [Discriminator loss: 0.024806, acc.: 100.00%] [Generator loss: 8.079742]\n",
      "17538 [Discriminator loss: 0.120556, acc.: 95.31%] [Generator loss: 5.471217]\n",
      "17539 [Discriminator loss: 0.023641, acc.: 100.00%] [Generator loss: 5.321609]\n",
      "17540 [Discriminator loss: 0.158999, acc.: 93.75%] [Generator loss: 8.266844]\n",
      "17541 [Discriminator loss: 0.009033, acc.: 100.00%] [Generator loss: 10.401521]\n",
      "17542 [Discriminator loss: 0.164789, acc.: 93.75%] [Generator loss: 7.739400]\n",
      "17543 [Discriminator loss: 0.031188, acc.: 100.00%] [Generator loss: 8.123363]\n",
      "17544 [Discriminator loss: 0.167765, acc.: 96.88%] [Generator loss: 7.538515]\n",
      "17545 [Discriminator loss: 0.023827, acc.: 98.44%] [Generator loss: 7.978550]\n",
      "17546 [Discriminator loss: 0.083947, acc.: 96.88%] [Generator loss: 7.474642]\n",
      "17547 [Discriminator loss: 0.142364, acc.: 95.31%] [Generator loss: 10.293559]\n",
      "17548 [Discriminator loss: 0.012359, acc.: 100.00%] [Generator loss: 9.720336]\n",
      "17549 [Discriminator loss: 0.230402, acc.: 95.31%] [Generator loss: 6.829640]\n",
      "17550 [Discriminator loss: 0.220736, acc.: 90.62%] [Generator loss: 8.887281]\n",
      "17551 [Discriminator loss: 0.022514, acc.: 100.00%] [Generator loss: 10.901424]\n",
      "17552 [Discriminator loss: 0.148283, acc.: 93.75%] [Generator loss: 8.396493]\n",
      "17553 [Discriminator loss: 0.136146, acc.: 93.75%] [Generator loss: 6.389098]\n",
      "17554 [Discriminator loss: 0.010434, acc.: 100.00%] [Generator loss: 8.684128]\n",
      "17555 [Discriminator loss: 0.104018, acc.: 95.31%] [Generator loss: 10.436346]\n",
      "17556 [Discriminator loss: 0.046807, acc.: 100.00%] [Generator loss: 8.311687]\n",
      "17557 [Discriminator loss: 0.115578, acc.: 95.31%] [Generator loss: 8.553229]\n",
      "17558 [Discriminator loss: 0.052460, acc.: 98.44%] [Generator loss: 9.614203]\n",
      "17559 [Discriminator loss: 0.177556, acc.: 93.75%] [Generator loss: 7.451987]\n",
      "17560 [Discriminator loss: 0.044167, acc.: 98.44%] [Generator loss: 7.227104]\n",
      "17561 [Discriminator loss: 0.007993, acc.: 100.00%] [Generator loss: 9.325635]\n",
      "17562 [Discriminator loss: 0.072321, acc.: 96.88%] [Generator loss: 8.742603]\n",
      "17563 [Discriminator loss: 0.059993, acc.: 96.88%] [Generator loss: 9.108880]\n",
      "17564 [Discriminator loss: 0.012953, acc.: 100.00%] [Generator loss: 8.137043]\n",
      "17565 [Discriminator loss: 0.091646, acc.: 96.88%] [Generator loss: 8.283039]\n",
      "17566 [Discriminator loss: 0.202133, acc.: 90.62%] [Generator loss: 7.468091]\n",
      "17567 [Discriminator loss: 0.045735, acc.: 98.44%] [Generator loss: 8.327234]\n",
      "17568 [Discriminator loss: 0.020541, acc.: 100.00%] [Generator loss: 7.873791]\n",
      "17569 [Discriminator loss: 0.042839, acc.: 98.44%] [Generator loss: 9.371534]\n",
      "17570 [Discriminator loss: 0.034743, acc.: 98.44%] [Generator loss: 7.251060]\n",
      "17571 [Discriminator loss: 0.080736, acc.: 95.31%] [Generator loss: 7.696305]\n",
      "17572 [Discriminator loss: 0.071779, acc.: 98.44%] [Generator loss: 7.450973]\n",
      "17573 [Discriminator loss: 0.080552, acc.: 96.88%] [Generator loss: 7.049423]\n",
      "17574 [Discriminator loss: 0.073421, acc.: 96.88%] [Generator loss: 7.187806]\n",
      "17575 [Discriminator loss: 0.125617, acc.: 95.31%] [Generator loss: 7.674135]\n",
      "17576 [Discriminator loss: 0.156396, acc.: 93.75%] [Generator loss: 9.202585]\n",
      "17577 [Discriminator loss: 0.083659, acc.: 98.44%] [Generator loss: 9.401695]\n",
      "17578 [Discriminator loss: 0.245265, acc.: 92.19%] [Generator loss: 7.030189]\n",
      "17579 [Discriminator loss: 0.080577, acc.: 96.88%] [Generator loss: 8.772557]\n",
      "17580 [Discriminator loss: 0.021649, acc.: 100.00%] [Generator loss: 10.097958]\n",
      "17581 [Discriminator loss: 0.066827, acc.: 96.88%] [Generator loss: 9.123085]\n",
      "17582 [Discriminator loss: 0.121556, acc.: 95.31%] [Generator loss: 8.634243]\n",
      "17583 [Discriminator loss: 0.119555, acc.: 96.88%] [Generator loss: 8.829926]\n",
      "17584 [Discriminator loss: 0.053554, acc.: 98.44%] [Generator loss: 8.823158]\n",
      "17585 [Discriminator loss: 0.102518, acc.: 96.88%] [Generator loss: 8.541679]\n",
      "17586 [Discriminator loss: 0.054928, acc.: 96.88%] [Generator loss: 8.283851]\n",
      "17587 [Discriminator loss: 0.023909, acc.: 98.44%] [Generator loss: 8.673803]\n",
      "17588 [Discriminator loss: 0.045160, acc.: 98.44%] [Generator loss: 9.774832]\n",
      "17589 [Discriminator loss: 0.044662, acc.: 100.00%] [Generator loss: 8.137737]\n",
      "17590 [Discriminator loss: 0.073500, acc.: 96.88%] [Generator loss: 8.868473]\n",
      "17591 [Discriminator loss: 0.085033, acc.: 96.88%] [Generator loss: 8.152486]\n",
      "17592 [Discriminator loss: 0.054174, acc.: 98.44%] [Generator loss: 9.475716]\n",
      "17593 [Discriminator loss: 0.031813, acc.: 100.00%] [Generator loss: 7.865732]\n",
      "17594 [Discriminator loss: 0.078673, acc.: 96.88%] [Generator loss: 8.409012]\n",
      "17595 [Discriminator loss: 0.061045, acc.: 96.88%] [Generator loss: 8.878177]\n",
      "17596 [Discriminator loss: 0.075434, acc.: 93.75%] [Generator loss: 7.967113]\n",
      "17597 [Discriminator loss: 0.085788, acc.: 95.31%] [Generator loss: 7.373423]\n",
      "17598 [Discriminator loss: 0.078803, acc.: 96.88%] [Generator loss: 10.140022]\n",
      "17599 [Discriminator loss: 0.014051, acc.: 100.00%] [Generator loss: 9.447556]\n",
      "17600 [Discriminator loss: 0.099826, acc.: 96.88%] [Generator loss: 7.658417]\n",
      "17601 [Discriminator loss: 0.111351, acc.: 96.88%] [Generator loss: 6.772820]\n",
      "17602 [Discriminator loss: 0.053884, acc.: 95.31%] [Generator loss: 5.952800]\n",
      "17603 [Discriminator loss: 0.165847, acc.: 93.75%] [Generator loss: 6.384521]\n",
      "17604 [Discriminator loss: 0.084880, acc.: 95.31%] [Generator loss: 7.317566]\n",
      "17605 [Discriminator loss: 0.141081, acc.: 93.75%] [Generator loss: 7.827306]\n",
      "17606 [Discriminator loss: 0.032153, acc.: 98.44%] [Generator loss: 8.436419]\n",
      "17607 [Discriminator loss: 0.180248, acc.: 93.75%] [Generator loss: 7.043930]\n",
      "17608 [Discriminator loss: 0.133567, acc.: 96.88%] [Generator loss: 8.635544]\n",
      "17609 [Discriminator loss: 0.066370, acc.: 96.88%] [Generator loss: 8.104958]\n",
      "17610 [Discriminator loss: 0.029583, acc.: 100.00%] [Generator loss: 8.548310]\n",
      "17611 [Discriminator loss: 0.051193, acc.: 98.44%] [Generator loss: 7.700779]\n",
      "17612 [Discriminator loss: 0.092802, acc.: 93.75%] [Generator loss: 6.994777]\n",
      "17613 [Discriminator loss: 0.073284, acc.: 98.44%] [Generator loss: 8.776674]\n",
      "17614 [Discriminator loss: 0.007933, acc.: 100.00%] [Generator loss: 8.810367]\n",
      "17615 [Discriminator loss: 0.094519, acc.: 98.44%] [Generator loss: 7.643834]\n",
      "17616 [Discriminator loss: 0.062286, acc.: 98.44%] [Generator loss: 7.735373]\n",
      "17617 [Discriminator loss: 0.022264, acc.: 100.00%] [Generator loss: 9.118429]\n",
      "17618 [Discriminator loss: 0.078749, acc.: 96.88%] [Generator loss: 7.850992]\n",
      "17619 [Discriminator loss: 0.084330, acc.: 95.31%] [Generator loss: 7.913684]\n",
      "17620 [Discriminator loss: 0.063571, acc.: 95.31%] [Generator loss: 7.802398]\n",
      "17621 [Discriminator loss: 0.057052, acc.: 96.88%] [Generator loss: 9.152350]\n",
      "17622 [Discriminator loss: 0.010733, acc.: 100.00%] [Generator loss: 8.136548]\n",
      "17623 [Discriminator loss: 0.034085, acc.: 98.44%] [Generator loss: 8.373479]\n",
      "17624 [Discriminator loss: 0.053505, acc.: 96.88%] [Generator loss: 7.650300]\n",
      "17625 [Discriminator loss: 0.030576, acc.: 98.44%] [Generator loss: 8.869968]\n",
      "17626 [Discriminator loss: 0.064553, acc.: 96.88%] [Generator loss: 9.091126]\n",
      "17627 [Discriminator loss: 0.239629, acc.: 90.62%] [Generator loss: 6.065866]\n",
      "17628 [Discriminator loss: 0.047210, acc.: 98.44%] [Generator loss: 6.644352]\n",
      "17629 [Discriminator loss: 0.009564, acc.: 100.00%] [Generator loss: 8.540979]\n",
      "17630 [Discriminator loss: 0.037178, acc.: 98.44%] [Generator loss: 7.015023]\n",
      "17631 [Discriminator loss: 0.229296, acc.: 93.75%] [Generator loss: 8.101641]\n",
      "17632 [Discriminator loss: 0.076168, acc.: 95.31%] [Generator loss: 7.491150]\n",
      "17633 [Discriminator loss: 0.038596, acc.: 96.88%] [Generator loss: 10.423265]\n",
      "17634 [Discriminator loss: 0.049806, acc.: 98.44%] [Generator loss: 8.873499]\n",
      "17635 [Discriminator loss: 0.026297, acc.: 100.00%] [Generator loss: 8.430022]\n",
      "17636 [Discriminator loss: 0.064229, acc.: 98.44%] [Generator loss: 8.704552]\n",
      "17637 [Discriminator loss: 0.098161, acc.: 98.44%] [Generator loss: 12.079290]\n",
      "17638 [Discriminator loss: 0.049580, acc.: 98.44%] [Generator loss: 9.968388]\n",
      "17639 [Discriminator loss: 0.127552, acc.: 92.19%] [Generator loss: 7.352731]\n",
      "17640 [Discriminator loss: 0.032300, acc.: 100.00%] [Generator loss: 6.406325]\n",
      "17641 [Discriminator loss: 0.030424, acc.: 98.44%] [Generator loss: 7.412553]\n",
      "17642 [Discriminator loss: 0.052319, acc.: 96.88%] [Generator loss: 7.135247]\n",
      "17643 [Discriminator loss: 0.151930, acc.: 96.88%] [Generator loss: 7.237816]\n",
      "17644 [Discriminator loss: 0.053830, acc.: 98.44%] [Generator loss: 6.936601]\n",
      "17645 [Discriminator loss: 0.059157, acc.: 98.44%] [Generator loss: 9.268608]\n",
      "17646 [Discriminator loss: 0.119235, acc.: 95.31%] [Generator loss: 6.606925]\n",
      "17647 [Discriminator loss: 0.166657, acc.: 92.19%] [Generator loss: 9.604475]\n",
      "17648 [Discriminator loss: 0.024437, acc.: 100.00%] [Generator loss: 11.830830]\n",
      "17649 [Discriminator loss: 0.008931, acc.: 100.00%] [Generator loss: 11.207373]\n",
      "17650 [Discriminator loss: 0.214043, acc.: 89.06%] [Generator loss: 6.445134]\n",
      "17651 [Discriminator loss: 0.167164, acc.: 93.75%] [Generator loss: 8.516216]\n",
      "17652 [Discriminator loss: 0.095768, acc.: 95.31%] [Generator loss: 6.356808]\n",
      "17653 [Discriminator loss: 0.021846, acc.: 100.00%] [Generator loss: 9.068831]\n",
      "17654 [Discriminator loss: 0.027668, acc.: 98.44%] [Generator loss: 8.453148]\n",
      "17655 [Discriminator loss: 0.087013, acc.: 95.31%] [Generator loss: 8.822893]\n",
      "17656 [Discriminator loss: 0.029720, acc.: 100.00%] [Generator loss: 7.877543]\n",
      "17657 [Discriminator loss: 0.040021, acc.: 96.88%] [Generator loss: 8.305164]\n",
      "17658 [Discriminator loss: 0.042528, acc.: 98.44%] [Generator loss: 6.551291]\n",
      "17659 [Discriminator loss: 0.032664, acc.: 100.00%] [Generator loss: 7.521351]\n",
      "17660 [Discriminator loss: 0.125077, acc.: 96.88%] [Generator loss: 7.396185]\n",
      "17661 [Discriminator loss: 0.106951, acc.: 95.31%] [Generator loss: 6.999513]\n",
      "17662 [Discriminator loss: 0.101359, acc.: 95.31%] [Generator loss: 8.930211]\n",
      "17663 [Discriminator loss: 0.043513, acc.: 98.44%] [Generator loss: 7.093334]\n",
      "17664 [Discriminator loss: 0.051227, acc.: 98.44%] [Generator loss: 9.407701]\n",
      "17665 [Discriminator loss: 0.096198, acc.: 93.75%] [Generator loss: 6.526339]\n",
      "17666 [Discriminator loss: 0.035178, acc.: 100.00%] [Generator loss: 7.898393]\n",
      "17667 [Discriminator loss: 0.021016, acc.: 100.00%] [Generator loss: 7.768761]\n",
      "17668 [Discriminator loss: 0.072891, acc.: 98.44%] [Generator loss: 9.401754]\n",
      "17669 [Discriminator loss: 0.022607, acc.: 100.00%] [Generator loss: 8.908886]\n",
      "17670 [Discriminator loss: 0.014935, acc.: 100.00%] [Generator loss: 9.709145]\n",
      "17671 [Discriminator loss: 0.299326, acc.: 84.38%] [Generator loss: 7.906565]\n",
      "17672 [Discriminator loss: 0.046088, acc.: 98.44%] [Generator loss: 8.475870]\n",
      "17673 [Discriminator loss: 0.107515, acc.: 96.88%] [Generator loss: 9.155879]\n",
      "17674 [Discriminator loss: 0.076927, acc.: 98.44%] [Generator loss: 8.432492]\n",
      "17675 [Discriminator loss: 0.034293, acc.: 98.44%] [Generator loss: 7.724164]\n",
      "17676 [Discriminator loss: 0.079231, acc.: 95.31%] [Generator loss: 8.810371]\n",
      "17677 [Discriminator loss: 0.177341, acc.: 90.62%] [Generator loss: 6.896247]\n",
      "17678 [Discriminator loss: 0.038853, acc.: 98.44%] [Generator loss: 9.023754]\n",
      "17679 [Discriminator loss: 0.030952, acc.: 98.44%] [Generator loss: 10.101373]\n",
      "17680 [Discriminator loss: 0.048423, acc.: 98.44%] [Generator loss: 8.634180]\n",
      "17681 [Discriminator loss: 0.045625, acc.: 98.44%] [Generator loss: 8.597893]\n",
      "17682 [Discriminator loss: 0.084777, acc.: 96.88%] [Generator loss: 8.172217]\n",
      "17683 [Discriminator loss: 0.037270, acc.: 98.44%] [Generator loss: 10.144732]\n",
      "17684 [Discriminator loss: 0.018274, acc.: 100.00%] [Generator loss: 8.824604]\n",
      "17685 [Discriminator loss: 0.015045, acc.: 100.00%] [Generator loss: 8.514900]\n",
      "17686 [Discriminator loss: 0.075672, acc.: 96.88%] [Generator loss: 7.506644]\n",
      "17687 [Discriminator loss: 0.151355, acc.: 93.75%] [Generator loss: 7.348860]\n",
      "17688 [Discriminator loss: 0.018099, acc.: 98.44%] [Generator loss: 8.150108]\n",
      "17689 [Discriminator loss: 0.052669, acc.: 96.88%] [Generator loss: 7.872307]\n",
      "17690 [Discriminator loss: 0.177699, acc.: 89.06%] [Generator loss: 7.504634]\n",
      "17691 [Discriminator loss: 0.285542, acc.: 93.75%] [Generator loss: 8.094153]\n",
      "17692 [Discriminator loss: 0.138121, acc.: 93.75%] [Generator loss: 8.527990]\n",
      "17693 [Discriminator loss: 0.045033, acc.: 98.44%] [Generator loss: 8.005396]\n",
      "17694 [Discriminator loss: 0.114517, acc.: 93.75%] [Generator loss: 7.587886]\n",
      "17695 [Discriminator loss: 0.030079, acc.: 98.44%] [Generator loss: 8.743648]\n",
      "17696 [Discriminator loss: 0.017194, acc.: 100.00%] [Generator loss: 9.447471]\n",
      "17697 [Discriminator loss: 0.053438, acc.: 96.88%] [Generator loss: 8.023261]\n",
      "17698 [Discriminator loss: 0.045695, acc.: 96.88%] [Generator loss: 8.434933]\n",
      "17699 [Discriminator loss: 0.140411, acc.: 93.75%] [Generator loss: 9.905163]\n",
      "17700 [Discriminator loss: 0.049567, acc.: 98.44%] [Generator loss: 9.026143]\n",
      "17701 [Discriminator loss: 0.055770, acc.: 96.88%] [Generator loss: 8.866927]\n",
      "17702 [Discriminator loss: 0.041424, acc.: 100.00%] [Generator loss: 7.408803]\n",
      "17703 [Discriminator loss: 0.052453, acc.: 98.44%] [Generator loss: 7.979488]\n",
      "17704 [Discriminator loss: 0.029742, acc.: 98.44%] [Generator loss: 8.893107]\n",
      "17705 [Discriminator loss: 0.039047, acc.: 96.88%] [Generator loss: 7.874242]\n",
      "17706 [Discriminator loss: 0.048987, acc.: 98.44%] [Generator loss: 8.024888]\n",
      "17707 [Discriminator loss: 0.095570, acc.: 95.31%] [Generator loss: 7.338615]\n",
      "17708 [Discriminator loss: 0.101279, acc.: 95.31%] [Generator loss: 6.356771]\n",
      "17709 [Discriminator loss: 0.033884, acc.: 98.44%] [Generator loss: 8.252904]\n",
      "17710 [Discriminator loss: 0.032585, acc.: 98.44%] [Generator loss: 6.377414]\n",
      "17711 [Discriminator loss: 0.049654, acc.: 98.44%] [Generator loss: 6.765559]\n",
      "17712 [Discriminator loss: 0.009083, acc.: 100.00%] [Generator loss: 7.693584]\n",
      "17713 [Discriminator loss: 0.081768, acc.: 95.31%] [Generator loss: 7.168487]\n",
      "17714 [Discriminator loss: 0.144707, acc.: 98.44%] [Generator loss: 7.855024]\n",
      "17715 [Discriminator loss: 0.096140, acc.: 93.75%] [Generator loss: 7.734478]\n",
      "17716 [Discriminator loss: 0.058470, acc.: 96.88%] [Generator loss: 8.267870]\n",
      "17717 [Discriminator loss: 0.047563, acc.: 98.44%] [Generator loss: 10.368029]\n",
      "17718 [Discriminator loss: 0.074647, acc.: 96.88%] [Generator loss: 7.611636]\n",
      "17719 [Discriminator loss: 0.119091, acc.: 93.75%] [Generator loss: 6.619998]\n",
      "17720 [Discriminator loss: 0.017768, acc.: 100.00%] [Generator loss: 8.032200]\n",
      "17721 [Discriminator loss: 0.051949, acc.: 98.44%] [Generator loss: 8.201884]\n",
      "17722 [Discriminator loss: 0.033960, acc.: 98.44%] [Generator loss: 9.138453]\n",
      "17723 [Discriminator loss: 0.060415, acc.: 98.44%] [Generator loss: 8.497540]\n",
      "17724 [Discriminator loss: 0.046981, acc.: 96.88%] [Generator loss: 7.416826]\n",
      "17725 [Discriminator loss: 0.016039, acc.: 100.00%] [Generator loss: 9.345922]\n",
      "17726 [Discriminator loss: 0.096950, acc.: 95.31%] [Generator loss: 7.046818]\n",
      "17727 [Discriminator loss: 0.163967, acc.: 95.31%] [Generator loss: 7.513442]\n",
      "17728 [Discriminator loss: 0.029878, acc.: 98.44%] [Generator loss: 8.467565]\n",
      "17729 [Discriminator loss: 0.046042, acc.: 96.88%] [Generator loss: 10.103650]\n",
      "17730 [Discriminator loss: 0.012405, acc.: 100.00%] [Generator loss: 9.833731]\n",
      "17731 [Discriminator loss: 0.181116, acc.: 92.19%] [Generator loss: 9.110329]\n",
      "17732 [Discriminator loss: 0.060085, acc.: 98.44%] [Generator loss: 8.599123]\n",
      "17733 [Discriminator loss: 0.228432, acc.: 93.75%] [Generator loss: 6.976536]\n",
      "17734 [Discriminator loss: 0.084451, acc.: 95.31%] [Generator loss: 8.344995]\n",
      "17735 [Discriminator loss: 0.013966, acc.: 100.00%] [Generator loss: 9.595119]\n",
      "17736 [Discriminator loss: 0.033495, acc.: 100.00%] [Generator loss: 9.271172]\n",
      "17737 [Discriminator loss: 0.229604, acc.: 92.19%] [Generator loss: 8.576233]\n",
      "17738 [Discriminator loss: 0.013958, acc.: 100.00%] [Generator loss: 8.941907]\n",
      "17739 [Discriminator loss: 0.061045, acc.: 100.00%] [Generator loss: 7.267992]\n",
      "17740 [Discriminator loss: 0.061045, acc.: 96.88%] [Generator loss: 8.292259]\n",
      "17741 [Discriminator loss: 0.021064, acc.: 100.00%] [Generator loss: 9.186152]\n",
      "17742 [Discriminator loss: 0.209956, acc.: 90.62%] [Generator loss: 5.994892]\n",
      "17743 [Discriminator loss: 0.024742, acc.: 100.00%] [Generator loss: 7.941398]\n",
      "17744 [Discriminator loss: 0.127657, acc.: 92.19%] [Generator loss: 9.749624]\n",
      "17745 [Discriminator loss: 0.033465, acc.: 98.44%] [Generator loss: 8.959282]\n",
      "17746 [Discriminator loss: 0.086158, acc.: 95.31%] [Generator loss: 8.934666]\n",
      "17747 [Discriminator loss: 0.063385, acc.: 95.31%] [Generator loss: 7.188546]\n",
      "17748 [Discriminator loss: 0.019726, acc.: 100.00%] [Generator loss: 9.553893]\n",
      "17749 [Discriminator loss: 0.053833, acc.: 98.44%] [Generator loss: 8.249971]\n",
      "17750 [Discriminator loss: 0.050856, acc.: 98.44%] [Generator loss: 10.007980]\n",
      "17751 [Discriminator loss: 0.216279, acc.: 93.75%] [Generator loss: 7.011276]\n",
      "17752 [Discriminator loss: 0.223018, acc.: 90.62%] [Generator loss: 8.403084]\n",
      "17753 [Discriminator loss: 0.025830, acc.: 100.00%] [Generator loss: 7.839223]\n",
      "17754 [Discriminator loss: 0.024473, acc.: 100.00%] [Generator loss: 8.812471]\n",
      "17755 [Discriminator loss: 0.087354, acc.: 95.31%] [Generator loss: 8.154881]\n",
      "17756 [Discriminator loss: 0.248413, acc.: 89.06%] [Generator loss: 8.414550]\n",
      "17757 [Discriminator loss: 0.012369, acc.: 100.00%] [Generator loss: 8.208161]\n",
      "17758 [Discriminator loss: 0.081432, acc.: 96.88%] [Generator loss: 8.393972]\n",
      "17759 [Discriminator loss: 0.124381, acc.: 95.31%] [Generator loss: 9.487146]\n",
      "17760 [Discriminator loss: 0.127746, acc.: 95.31%] [Generator loss: 10.272361]\n",
      "17761 [Discriminator loss: 0.014799, acc.: 100.00%] [Generator loss: 9.424471]\n",
      "17762 [Discriminator loss: 0.038078, acc.: 98.44%] [Generator loss: 8.058847]\n",
      "17763 [Discriminator loss: 0.067682, acc.: 96.88%] [Generator loss: 7.132601]\n",
      "17764 [Discriminator loss: 0.111269, acc.: 96.88%] [Generator loss: 10.156783]\n",
      "17765 [Discriminator loss: 0.017107, acc.: 100.00%] [Generator loss: 10.385115]\n",
      "17766 [Discriminator loss: 0.106954, acc.: 95.31%] [Generator loss: 7.111846]\n",
      "17767 [Discriminator loss: 0.114032, acc.: 93.75%] [Generator loss: 9.359348]\n",
      "17768 [Discriminator loss: 0.139147, acc.: 96.88%] [Generator loss: 9.763042]\n",
      "17769 [Discriminator loss: 0.061673, acc.: 98.44%] [Generator loss: 7.869878]\n",
      "17770 [Discriminator loss: 0.109604, acc.: 96.88%] [Generator loss: 7.748250]\n",
      "17771 [Discriminator loss: 0.031365, acc.: 98.44%] [Generator loss: 7.205591]\n",
      "17772 [Discriminator loss: 0.189866, acc.: 93.75%] [Generator loss: 8.607853]\n",
      "17773 [Discriminator loss: 0.073053, acc.: 96.88%] [Generator loss: 8.154052]\n",
      "17774 [Discriminator loss: 0.021878, acc.: 100.00%] [Generator loss: 9.148949]\n",
      "17775 [Discriminator loss: 0.064767, acc.: 98.44%] [Generator loss: 8.824455]\n",
      "17776 [Discriminator loss: 0.084895, acc.: 98.44%] [Generator loss: 7.620568]\n",
      "17777 [Discriminator loss: 0.117696, acc.: 96.88%] [Generator loss: 7.306934]\n",
      "17778 [Discriminator loss: 0.072402, acc.: 96.88%] [Generator loss: 9.759828]\n",
      "17779 [Discriminator loss: 0.115881, acc.: 93.75%] [Generator loss: 8.917839]\n",
      "17780 [Discriminator loss: 0.015268, acc.: 100.00%] [Generator loss: 9.770837]\n",
      "17781 [Discriminator loss: 0.077448, acc.: 96.88%] [Generator loss: 7.263495]\n",
      "17782 [Discriminator loss: 0.040548, acc.: 96.88%] [Generator loss: 7.907132]\n",
      "17783 [Discriminator loss: 0.056749, acc.: 96.88%] [Generator loss: 7.905995]\n",
      "17784 [Discriminator loss: 0.043374, acc.: 100.00%] [Generator loss: 10.180392]\n",
      "17785 [Discriminator loss: 0.136154, acc.: 95.31%] [Generator loss: 7.086179]\n",
      "17786 [Discriminator loss: 0.054607, acc.: 98.44%] [Generator loss: 7.738013]\n",
      "17787 [Discriminator loss: 0.015188, acc.: 100.00%] [Generator loss: 7.008548]\n",
      "17788 [Discriminator loss: 0.113934, acc.: 95.31%] [Generator loss: 8.334913]\n",
      "17789 [Discriminator loss: 0.041299, acc.: 96.88%] [Generator loss: 8.761621]\n",
      "17790 [Discriminator loss: 0.034786, acc.: 100.00%] [Generator loss: 8.845722]\n",
      "17791 [Discriminator loss: 0.082717, acc.: 96.88%] [Generator loss: 7.650204]\n",
      "17792 [Discriminator loss: 0.165224, acc.: 95.31%] [Generator loss: 7.629688]\n",
      "17793 [Discriminator loss: 0.013354, acc.: 100.00%] [Generator loss: 7.356401]\n",
      "17794 [Discriminator loss: 0.052485, acc.: 95.31%] [Generator loss: 7.721711]\n",
      "17795 [Discriminator loss: 0.050894, acc.: 98.44%] [Generator loss: 7.695709]\n",
      "17796 [Discriminator loss: 0.017259, acc.: 100.00%] [Generator loss: 7.303441]\n",
      "17797 [Discriminator loss: 0.102367, acc.: 92.19%] [Generator loss: 6.541914]\n",
      "17798 [Discriminator loss: 0.055961, acc.: 96.88%] [Generator loss: 7.997546]\n",
      "17799 [Discriminator loss: 0.075144, acc.: 98.44%] [Generator loss: 7.855785]\n",
      "17800 [Discriminator loss: 0.024060, acc.: 98.44%] [Generator loss: 7.802246]\n",
      "17801 [Discriminator loss: 0.068494, acc.: 98.44%] [Generator loss: 8.421624]\n",
      "17802 [Discriminator loss: 0.025225, acc.: 100.00%] [Generator loss: 9.388521]\n",
      "17803 [Discriminator loss: 0.081110, acc.: 98.44%] [Generator loss: 9.783015]\n",
      "17804 [Discriminator loss: 0.145427, acc.: 93.75%] [Generator loss: 7.833609]\n",
      "17805 [Discriminator loss: 0.086689, acc.: 96.88%] [Generator loss: 9.457994]\n",
      "17806 [Discriminator loss: 0.049494, acc.: 98.44%] [Generator loss: 6.909720]\n",
      "17807 [Discriminator loss: 0.038080, acc.: 96.88%] [Generator loss: 10.168874]\n",
      "17808 [Discriminator loss: 0.051957, acc.: 96.88%] [Generator loss: 7.370962]\n",
      "17809 [Discriminator loss: 0.013508, acc.: 100.00%] [Generator loss: 6.767028]\n",
      "17810 [Discriminator loss: 0.213954, acc.: 95.31%] [Generator loss: 9.901908]\n",
      "17811 [Discriminator loss: 0.113466, acc.: 96.88%] [Generator loss: 8.586188]\n",
      "17812 [Discriminator loss: 0.136018, acc.: 96.88%] [Generator loss: 7.196193]\n",
      "17813 [Discriminator loss: 0.057638, acc.: 98.44%] [Generator loss: 6.572948]\n",
      "17814 [Discriminator loss: 0.098407, acc.: 95.31%] [Generator loss: 7.162529]\n",
      "17815 [Discriminator loss: 0.025855, acc.: 100.00%] [Generator loss: 7.364096]\n",
      "17816 [Discriminator loss: 0.077164, acc.: 98.44%] [Generator loss: 7.194729]\n",
      "17817 [Discriminator loss: 0.028440, acc.: 98.44%] [Generator loss: 7.046035]\n",
      "17818 [Discriminator loss: 0.163464, acc.: 93.75%] [Generator loss: 8.559401]\n",
      "17819 [Discriminator loss: 0.020999, acc.: 100.00%] [Generator loss: 9.364736]\n",
      "17820 [Discriminator loss: 0.243983, acc.: 89.06%] [Generator loss: 8.368670]\n",
      "17821 [Discriminator loss: 0.048926, acc.: 98.44%] [Generator loss: 7.425840]\n",
      "17822 [Discriminator loss: 0.038511, acc.: 100.00%] [Generator loss: 8.493433]\n",
      "17823 [Discriminator loss: 0.070435, acc.: 96.88%] [Generator loss: 9.764709]\n",
      "17824 [Discriminator loss: 0.021112, acc.: 100.00%] [Generator loss: 7.931795]\n",
      "17825 [Discriminator loss: 0.023131, acc.: 100.00%] [Generator loss: 7.849271]\n",
      "17826 [Discriminator loss: 0.089936, acc.: 95.31%] [Generator loss: 6.718091]\n",
      "17827 [Discriminator loss: 0.253545, acc.: 93.75%] [Generator loss: 9.802570]\n",
      "17828 [Discriminator loss: 0.214119, acc.: 90.62%] [Generator loss: 9.080904]\n",
      "17829 [Discriminator loss: 0.142319, acc.: 95.31%] [Generator loss: 9.159807]\n",
      "17830 [Discriminator loss: 0.092989, acc.: 95.31%] [Generator loss: 9.205163]\n",
      "17831 [Discriminator loss: 0.018602, acc.: 100.00%] [Generator loss: 9.187033]\n",
      "17832 [Discriminator loss: 0.308240, acc.: 87.50%] [Generator loss: 7.778364]\n",
      "17833 [Discriminator loss: 0.019724, acc.: 100.00%] [Generator loss: 9.465900]\n",
      "17834 [Discriminator loss: 0.052795, acc.: 98.44%] [Generator loss: 9.513393]\n",
      "17835 [Discriminator loss: 0.056186, acc.: 98.44%] [Generator loss: 7.326951]\n",
      "17836 [Discriminator loss: 0.049212, acc.: 98.44%] [Generator loss: 8.128266]\n",
      "17837 [Discriminator loss: 0.058642, acc.: 96.88%] [Generator loss: 6.750902]\n",
      "17838 [Discriminator loss: 0.077392, acc.: 96.88%] [Generator loss: 9.892391]\n",
      "17839 [Discriminator loss: 0.094836, acc.: 95.31%] [Generator loss: 8.695926]\n",
      "17840 [Discriminator loss: 0.035116, acc.: 100.00%] [Generator loss: 7.307958]\n",
      "17841 [Discriminator loss: 0.057532, acc.: 96.88%] [Generator loss: 8.322803]\n",
      "17842 [Discriminator loss: 0.163507, acc.: 96.88%] [Generator loss: 6.979115]\n",
      "17843 [Discriminator loss: 0.240179, acc.: 90.62%] [Generator loss: 9.134443]\n",
      "17844 [Discriminator loss: 0.122909, acc.: 96.88%] [Generator loss: 7.934275]\n",
      "17845 [Discriminator loss: 0.062893, acc.: 96.88%] [Generator loss: 9.431578]\n",
      "17846 [Discriminator loss: 0.061893, acc.: 96.88%] [Generator loss: 7.475326]\n",
      "17847 [Discriminator loss: 0.143606, acc.: 96.88%] [Generator loss: 7.546747]\n",
      "17848 [Discriminator loss: 0.123974, acc.: 96.88%] [Generator loss: 7.753628]\n",
      "17849 [Discriminator loss: 0.111939, acc.: 96.88%] [Generator loss: 8.310699]\n",
      "17850 [Discriminator loss: 0.024224, acc.: 100.00%] [Generator loss: 7.710237]\n",
      "17851 [Discriminator loss: 0.029528, acc.: 100.00%] [Generator loss: 8.632502]\n",
      "17852 [Discriminator loss: 0.174471, acc.: 92.19%] [Generator loss: 9.332658]\n",
      "17853 [Discriminator loss: 0.135928, acc.: 93.75%] [Generator loss: 9.289074]\n",
      "17854 [Discriminator loss: 0.019820, acc.: 100.00%] [Generator loss: 7.528072]\n",
      "17855 [Discriminator loss: 0.037650, acc.: 98.44%] [Generator loss: 6.884124]\n",
      "17856 [Discriminator loss: 0.014113, acc.: 100.00%] [Generator loss: 7.343572]\n",
      "17857 [Discriminator loss: 0.059809, acc.: 96.88%] [Generator loss: 7.529467]\n",
      "17858 [Discriminator loss: 0.062058, acc.: 98.44%] [Generator loss: 9.398752]\n",
      "17859 [Discriminator loss: 0.051053, acc.: 98.44%] [Generator loss: 9.488048]\n",
      "17860 [Discriminator loss: 0.059257, acc.: 100.00%] [Generator loss: 8.702803]\n",
      "17861 [Discriminator loss: 0.028855, acc.: 98.44%] [Generator loss: 9.308286]\n",
      "17862 [Discriminator loss: 0.027975, acc.: 98.44%] [Generator loss: 8.445393]\n",
      "17863 [Discriminator loss: 0.117406, acc.: 96.88%] [Generator loss: 6.999351]\n",
      "17864 [Discriminator loss: 0.041391, acc.: 98.44%] [Generator loss: 7.317450]\n",
      "17865 [Discriminator loss: 0.054005, acc.: 96.88%] [Generator loss: 8.961885]\n",
      "17866 [Discriminator loss: 0.094110, acc.: 95.31%] [Generator loss: 7.534676]\n",
      "17867 [Discriminator loss: 0.035769, acc.: 98.44%] [Generator loss: 10.943811]\n",
      "17868 [Discriminator loss: 0.322449, acc.: 89.06%] [Generator loss: 8.763283]\n",
      "17869 [Discriminator loss: 0.060516, acc.: 96.88%] [Generator loss: 9.666229]\n",
      "17870 [Discriminator loss: 0.049252, acc.: 96.88%] [Generator loss: 8.413185]\n",
      "17871 [Discriminator loss: 0.019846, acc.: 100.00%] [Generator loss: 8.278721]\n",
      "17872 [Discriminator loss: 0.098786, acc.: 95.31%] [Generator loss: 7.564260]\n",
      "17873 [Discriminator loss: 0.100654, acc.: 95.31%] [Generator loss: 9.383698]\n",
      "17874 [Discriminator loss: 0.131282, acc.: 95.31%] [Generator loss: 10.320236]\n",
      "17875 [Discriminator loss: 0.056740, acc.: 96.88%] [Generator loss: 9.093341]\n",
      "17876 [Discriminator loss: 0.050946, acc.: 100.00%] [Generator loss: 7.720376]\n",
      "17877 [Discriminator loss: 0.021885, acc.: 100.00%] [Generator loss: 8.626072]\n",
      "17878 [Discriminator loss: 0.119438, acc.: 93.75%] [Generator loss: 5.874145]\n",
      "17879 [Discriminator loss: 0.056848, acc.: 98.44%] [Generator loss: 8.230923]\n",
      "17880 [Discriminator loss: 0.018063, acc.: 100.00%] [Generator loss: 9.368106]\n",
      "17881 [Discriminator loss: 0.022186, acc.: 100.00%] [Generator loss: 8.531773]\n",
      "17882 [Discriminator loss: 0.092012, acc.: 96.88%] [Generator loss: 7.507355]\n",
      "17883 [Discriminator loss: 0.018607, acc.: 100.00%] [Generator loss: 6.503589]\n",
      "17884 [Discriminator loss: 0.054254, acc.: 96.88%] [Generator loss: 7.961123]\n",
      "17885 [Discriminator loss: 0.154894, acc.: 92.19%] [Generator loss: 8.914504]\n",
      "17886 [Discriminator loss: 0.042939, acc.: 98.44%] [Generator loss: 9.273186]\n",
      "17887 [Discriminator loss: 0.084899, acc.: 96.88%] [Generator loss: 8.353858]\n",
      "17888 [Discriminator loss: 0.238730, acc.: 93.75%] [Generator loss: 7.851126]\n",
      "17889 [Discriminator loss: 0.050266, acc.: 98.44%] [Generator loss: 8.204578]\n",
      "17890 [Discriminator loss: 0.152149, acc.: 95.31%] [Generator loss: 8.688457]\n",
      "17891 [Discriminator loss: 0.072786, acc.: 96.88%] [Generator loss: 9.118269]\n",
      "17892 [Discriminator loss: 0.012972, acc.: 100.00%] [Generator loss: 8.471771]\n",
      "17893 [Discriminator loss: 0.030566, acc.: 100.00%] [Generator loss: 7.426434]\n",
      "17894 [Discriminator loss: 0.037939, acc.: 98.44%] [Generator loss: 7.292603]\n",
      "17895 [Discriminator loss: 0.142824, acc.: 90.62%] [Generator loss: 8.708038]\n",
      "17896 [Discriminator loss: 0.059856, acc.: 96.88%] [Generator loss: 9.458059]\n",
      "17897 [Discriminator loss: 0.056281, acc.: 98.44%] [Generator loss: 8.229067]\n",
      "17898 [Discriminator loss: 0.125182, acc.: 96.88%] [Generator loss: 6.872937]\n",
      "17899 [Discriminator loss: 0.044630, acc.: 96.88%] [Generator loss: 9.697669]\n",
      "17900 [Discriminator loss: 0.053440, acc.: 98.44%] [Generator loss: 7.811034]\n",
      "17901 [Discriminator loss: 0.031225, acc.: 98.44%] [Generator loss: 8.584039]\n",
      "17902 [Discriminator loss: 0.075678, acc.: 98.44%] [Generator loss: 9.188442]\n",
      "17903 [Discriminator loss: 0.234908, acc.: 90.62%] [Generator loss: 7.208022]\n",
      "17904 [Discriminator loss: 0.020543, acc.: 100.00%] [Generator loss: 7.067386]\n",
      "17905 [Discriminator loss: 0.119217, acc.: 93.75%] [Generator loss: 8.313153]\n",
      "17906 [Discriminator loss: 0.096900, acc.: 96.88%] [Generator loss: 11.922800]\n",
      "17907 [Discriminator loss: 0.117050, acc.: 96.88%] [Generator loss: 9.188441]\n",
      "17908 [Discriminator loss: 0.033233, acc.: 98.44%] [Generator loss: 7.667986]\n",
      "17909 [Discriminator loss: 0.101254, acc.: 95.31%] [Generator loss: 7.379884]\n",
      "17910 [Discriminator loss: 0.024301, acc.: 100.00%] [Generator loss: 8.547560]\n",
      "17911 [Discriminator loss: 0.119440, acc.: 95.31%] [Generator loss: 10.428780]\n",
      "17912 [Discriminator loss: 0.017620, acc.: 100.00%] [Generator loss: 8.195763]\n",
      "17913 [Discriminator loss: 0.026599, acc.: 100.00%] [Generator loss: 9.778755]\n",
      "17914 [Discriminator loss: 0.152467, acc.: 92.19%] [Generator loss: 8.165414]\n",
      "17915 [Discriminator loss: 0.018571, acc.: 100.00%] [Generator loss: 8.484146]\n",
      "17916 [Discriminator loss: 0.119369, acc.: 93.75%] [Generator loss: 6.632128]\n",
      "17917 [Discriminator loss: 0.197719, acc.: 95.31%] [Generator loss: 7.284691]\n",
      "17918 [Discriminator loss: 0.031397, acc.: 98.44%] [Generator loss: 8.743025]\n",
      "17919 [Discriminator loss: 0.087970, acc.: 96.88%] [Generator loss: 6.692588]\n",
      "17920 [Discriminator loss: 0.062010, acc.: 98.44%] [Generator loss: 9.128991]\n",
      "17921 [Discriminator loss: 0.069932, acc.: 96.88%] [Generator loss: 9.943031]\n",
      "17922 [Discriminator loss: 0.066419, acc.: 96.88%] [Generator loss: 7.252995]\n",
      "17923 [Discriminator loss: 0.232964, acc.: 90.62%] [Generator loss: 7.786202]\n",
      "17924 [Discriminator loss: 0.107547, acc.: 93.75%] [Generator loss: 8.630995]\n",
      "17925 [Discriminator loss: 0.050395, acc.: 98.44%] [Generator loss: 8.336695]\n",
      "17926 [Discriminator loss: 0.074383, acc.: 96.88%] [Generator loss: 8.579900]\n",
      "17927 [Discriminator loss: 0.215410, acc.: 92.19%] [Generator loss: 8.548100]\n",
      "17928 [Discriminator loss: 0.102430, acc.: 95.31%] [Generator loss: 6.810338]\n",
      "17929 [Discriminator loss: 0.187132, acc.: 95.31%] [Generator loss: 8.465469]\n",
      "17930 [Discriminator loss: 0.052398, acc.: 96.88%] [Generator loss: 8.002052]\n",
      "17931 [Discriminator loss: 0.049319, acc.: 100.00%] [Generator loss: 7.827403]\n",
      "17932 [Discriminator loss: 0.122986, acc.: 95.31%] [Generator loss: 7.579787]\n",
      "17933 [Discriminator loss: 0.067916, acc.: 96.88%] [Generator loss: 9.145810]\n",
      "17934 [Discriminator loss: 0.025890, acc.: 98.44%] [Generator loss: 8.547924]\n",
      "17935 [Discriminator loss: 0.013063, acc.: 100.00%] [Generator loss: 7.852592]\n",
      "17936 [Discriminator loss: 0.023189, acc.: 98.44%] [Generator loss: 7.074165]\n",
      "17937 [Discriminator loss: 0.150035, acc.: 93.75%] [Generator loss: 7.930627]\n",
      "17938 [Discriminator loss: 0.060702, acc.: 98.44%] [Generator loss: 9.308709]\n",
      "17939 [Discriminator loss: 0.093496, acc.: 98.44%] [Generator loss: 9.672305]\n",
      "17940 [Discriminator loss: 0.086289, acc.: 96.88%] [Generator loss: 7.764116]\n",
      "17941 [Discriminator loss: 0.078379, acc.: 96.88%] [Generator loss: 7.802780]\n",
      "17942 [Discriminator loss: 0.125008, acc.: 95.31%] [Generator loss: 8.571671]\n",
      "17943 [Discriminator loss: 0.125087, acc.: 93.75%] [Generator loss: 7.824714]\n",
      "17944 [Discriminator loss: 0.019547, acc.: 100.00%] [Generator loss: 6.605635]\n",
      "17945 [Discriminator loss: 0.022471, acc.: 100.00%] [Generator loss: 7.231952]\n",
      "17946 [Discriminator loss: 0.099090, acc.: 95.31%] [Generator loss: 7.498270]\n",
      "17947 [Discriminator loss: 0.009739, acc.: 100.00%] [Generator loss: 8.816078]\n",
      "17948 [Discriminator loss: 0.162597, acc.: 95.31%] [Generator loss: 7.603410]\n",
      "17949 [Discriminator loss: 0.047697, acc.: 98.44%] [Generator loss: 8.567508]\n",
      "17950 [Discriminator loss: 0.032132, acc.: 98.44%] [Generator loss: 7.542342]\n",
      "17951 [Discriminator loss: 0.036947, acc.: 98.44%] [Generator loss: 8.463938]\n",
      "17952 [Discriminator loss: 0.209703, acc.: 96.88%] [Generator loss: 8.363454]\n",
      "17953 [Discriminator loss: 0.225043, acc.: 90.62%] [Generator loss: 8.221063]\n",
      "17954 [Discriminator loss: 0.146949, acc.: 95.31%] [Generator loss: 9.283912]\n",
      "17955 [Discriminator loss: 0.091652, acc.: 98.44%] [Generator loss: 8.427906]\n",
      "17956 [Discriminator loss: 0.134813, acc.: 95.31%] [Generator loss: 7.726643]\n",
      "17957 [Discriminator loss: 0.128354, acc.: 93.75%] [Generator loss: 7.540009]\n",
      "17958 [Discriminator loss: 0.123139, acc.: 90.62%] [Generator loss: 11.160672]\n",
      "17959 [Discriminator loss: 0.250857, acc.: 92.19%] [Generator loss: 7.825113]\n",
      "17960 [Discriminator loss: 0.052931, acc.: 98.44%] [Generator loss: 8.072007]\n",
      "17961 [Discriminator loss: 0.105956, acc.: 96.88%] [Generator loss: 7.022027]\n",
      "17962 [Discriminator loss: 0.032584, acc.: 98.44%] [Generator loss: 8.798622]\n",
      "17963 [Discriminator loss: 0.092483, acc.: 96.88%] [Generator loss: 7.414475]\n",
      "17964 [Discriminator loss: 0.119344, acc.: 95.31%] [Generator loss: 9.214289]\n",
      "17965 [Discriminator loss: 0.160714, acc.: 93.75%] [Generator loss: 9.283482]\n",
      "17966 [Discriminator loss: 0.029100, acc.: 100.00%] [Generator loss: 8.177788]\n",
      "17967 [Discriminator loss: 0.063290, acc.: 96.88%] [Generator loss: 7.732290]\n",
      "17968 [Discriminator loss: 0.172559, acc.: 93.75%] [Generator loss: 6.188076]\n",
      "17969 [Discriminator loss: 0.100713, acc.: 95.31%] [Generator loss: 8.800741]\n",
      "17970 [Discriminator loss: 0.017152, acc.: 100.00%] [Generator loss: 8.848255]\n",
      "17971 [Discriminator loss: 0.042958, acc.: 98.44%] [Generator loss: 8.408678]\n",
      "17972 [Discriminator loss: 0.100323, acc.: 98.44%] [Generator loss: 7.152512]\n",
      "17973 [Discriminator loss: 0.049886, acc.: 96.88%] [Generator loss: 7.432655]\n",
      "17974 [Discriminator loss: 0.048760, acc.: 98.44%] [Generator loss: 9.404593]\n",
      "17975 [Discriminator loss: 0.052669, acc.: 98.44%] [Generator loss: 7.776937]\n",
      "17976 [Discriminator loss: 0.146651, acc.: 96.88%] [Generator loss: 7.728924]\n",
      "17977 [Discriminator loss: 0.063226, acc.: 98.44%] [Generator loss: 7.019776]\n",
      "17978 [Discriminator loss: 0.093014, acc.: 93.75%] [Generator loss: 8.577939]\n",
      "17979 [Discriminator loss: 0.056475, acc.: 98.44%] [Generator loss: 7.974070]\n",
      "17980 [Discriminator loss: 0.015294, acc.: 100.00%] [Generator loss: 9.928436]\n",
      "17981 [Discriminator loss: 0.218915, acc.: 89.06%] [Generator loss: 7.590487]\n",
      "17982 [Discriminator loss: 0.052248, acc.: 96.88%] [Generator loss: 8.784118]\n",
      "17983 [Discriminator loss: 0.056034, acc.: 98.44%] [Generator loss: 8.401913]\n",
      "17984 [Discriminator loss: 0.242549, acc.: 89.06%] [Generator loss: 6.954529]\n",
      "17985 [Discriminator loss: 0.073273, acc.: 98.44%] [Generator loss: 8.435507]\n",
      "17986 [Discriminator loss: 0.063312, acc.: 95.31%] [Generator loss: 6.691141]\n",
      "17987 [Discriminator loss: 0.128260, acc.: 95.31%] [Generator loss: 7.570264]\n",
      "17988 [Discriminator loss: 0.057366, acc.: 96.88%] [Generator loss: 8.723498]\n",
      "17989 [Discriminator loss: 0.040640, acc.: 98.44%] [Generator loss: 9.211055]\n",
      "17990 [Discriminator loss: 0.044012, acc.: 98.44%] [Generator loss: 7.812021]\n",
      "17991 [Discriminator loss: 0.167160, acc.: 95.31%] [Generator loss: 6.444514]\n",
      "17992 [Discriminator loss: 0.102098, acc.: 95.31%] [Generator loss: 7.767116]\n",
      "17993 [Discriminator loss: 0.043228, acc.: 98.44%] [Generator loss: 7.588011]\n",
      "17994 [Discriminator loss: 0.065487, acc.: 96.88%] [Generator loss: 7.190915]\n",
      "17995 [Discriminator loss: 0.206873, acc.: 90.62%] [Generator loss: 7.905345]\n",
      "17996 [Discriminator loss: 0.057576, acc.: 98.44%] [Generator loss: 8.051207]\n",
      "17997 [Discriminator loss: 0.142057, acc.: 95.31%] [Generator loss: 8.158128]\n",
      "17998 [Discriminator loss: 0.045944, acc.: 98.44%] [Generator loss: 7.120759]\n",
      "17999 [Discriminator loss: 0.088971, acc.: 96.88%] [Generator loss: 8.942777]\n",
      "18000 [Discriminator loss: 0.327702, acc.: 89.06%] [Generator loss: 8.699140]\n",
      "18001 [Discriminator loss: 0.047990, acc.: 98.44%] [Generator loss: 9.242963]\n",
      "18002 [Discriminator loss: 0.036644, acc.: 98.44%] [Generator loss: 8.820333]\n",
      "18003 [Discriminator loss: 0.078529, acc.: 96.88%] [Generator loss: 9.483836]\n",
      "18004 [Discriminator loss: 0.005946, acc.: 100.00%] [Generator loss: 7.956655]\n",
      "18005 [Discriminator loss: 0.081561, acc.: 96.88%] [Generator loss: 10.157287]\n",
      "18006 [Discriminator loss: 0.028135, acc.: 98.44%] [Generator loss: 7.198552]\n",
      "18007 [Discriminator loss: 0.079039, acc.: 96.88%] [Generator loss: 6.400768]\n",
      "18008 [Discriminator loss: 0.089928, acc.: 96.88%] [Generator loss: 8.411848]\n",
      "18009 [Discriminator loss: 0.120178, acc.: 93.75%] [Generator loss: 8.835844]\n",
      "18010 [Discriminator loss: 0.205372, acc.: 92.19%] [Generator loss: 8.562181]\n",
      "18011 [Discriminator loss: 0.071168, acc.: 96.88%] [Generator loss: 8.497794]\n",
      "18012 [Discriminator loss: 0.068027, acc.: 96.88%] [Generator loss: 7.824982]\n",
      "18013 [Discriminator loss: 0.106398, acc.: 93.75%] [Generator loss: 9.040276]\n",
      "18014 [Discriminator loss: 0.057385, acc.: 96.88%] [Generator loss: 7.797416]\n",
      "18015 [Discriminator loss: 0.093232, acc.: 95.31%] [Generator loss: 9.899267]\n",
      "18016 [Discriminator loss: 0.137386, acc.: 93.75%] [Generator loss: 10.963584]\n",
      "18017 [Discriminator loss: 0.099161, acc.: 95.31%] [Generator loss: 10.656705]\n",
      "18018 [Discriminator loss: 0.189226, acc.: 89.06%] [Generator loss: 9.350470]\n",
      "18019 [Discriminator loss: 0.059834, acc.: 96.88%] [Generator loss: 9.760408]\n",
      "18020 [Discriminator loss: 0.032137, acc.: 100.00%] [Generator loss: 8.007611]\n",
      "18021 [Discriminator loss: 0.172952, acc.: 93.75%] [Generator loss: 8.436654]\n",
      "18022 [Discriminator loss: 0.060061, acc.: 96.88%] [Generator loss: 10.128618]\n",
      "18023 [Discriminator loss: 0.147338, acc.: 93.75%] [Generator loss: 6.724070]\n",
      "18024 [Discriminator loss: 0.057472, acc.: 98.44%] [Generator loss: 6.983245]\n",
      "18025 [Discriminator loss: 0.070311, acc.: 95.31%] [Generator loss: 9.151751]\n",
      "18026 [Discriminator loss: 0.035442, acc.: 96.88%] [Generator loss: 9.185743]\n",
      "18027 [Discriminator loss: 0.040685, acc.: 98.44%] [Generator loss: 8.216448]\n",
      "18028 [Discriminator loss: 0.173757, acc.: 93.75%] [Generator loss: 7.517596]\n",
      "18029 [Discriminator loss: 0.105640, acc.: 93.75%] [Generator loss: 8.855196]\n",
      "18030 [Discriminator loss: 0.094987, acc.: 96.88%] [Generator loss: 9.184278]\n",
      "18031 [Discriminator loss: 0.156844, acc.: 93.75%] [Generator loss: 8.259963]\n",
      "18032 [Discriminator loss: 0.234193, acc.: 95.31%] [Generator loss: 6.898725]\n",
      "18033 [Discriminator loss: 0.032959, acc.: 98.44%] [Generator loss: 8.682484]\n",
      "18034 [Discriminator loss: 0.012237, acc.: 100.00%] [Generator loss: 7.660192]\n",
      "18035 [Discriminator loss: 0.019138, acc.: 100.00%] [Generator loss: 8.259270]\n",
      "18036 [Discriminator loss: 0.024903, acc.: 98.44%] [Generator loss: 7.738091]\n",
      "18037 [Discriminator loss: 0.053043, acc.: 98.44%] [Generator loss: 6.529374]\n",
      "18038 [Discriminator loss: 0.015797, acc.: 100.00%] [Generator loss: 7.460008]\n",
      "18039 [Discriminator loss: 0.032379, acc.: 100.00%] [Generator loss: 7.904387]\n",
      "18040 [Discriminator loss: 0.094569, acc.: 96.88%] [Generator loss: 8.533831]\n",
      "18041 [Discriminator loss: 0.033831, acc.: 100.00%] [Generator loss: 8.930836]\n",
      "18042 [Discriminator loss: 0.075735, acc.: 96.88%] [Generator loss: 7.876136]\n",
      "18043 [Discriminator loss: 0.065595, acc.: 96.88%] [Generator loss: 8.005047]\n",
      "18044 [Discriminator loss: 0.035265, acc.: 98.44%] [Generator loss: 8.125408]\n",
      "18045 [Discriminator loss: 0.209538, acc.: 87.50%] [Generator loss: 7.881079]\n",
      "18046 [Discriminator loss: 0.073138, acc.: 95.31%] [Generator loss: 6.819748]\n",
      "18047 [Discriminator loss: 0.071457, acc.: 98.44%] [Generator loss: 8.400826]\n",
      "18048 [Discriminator loss: 0.065368, acc.: 96.88%] [Generator loss: 8.108709]\n",
      "18049 [Discriminator loss: 0.041564, acc.: 98.44%] [Generator loss: 9.194654]\n",
      "18050 [Discriminator loss: 0.094827, acc.: 96.88%] [Generator loss: 7.826871]\n",
      "18051 [Discriminator loss: 0.069139, acc.: 96.88%] [Generator loss: 7.312572]\n",
      "18052 [Discriminator loss: 0.070939, acc.: 96.88%] [Generator loss: 8.652281]\n",
      "18053 [Discriminator loss: 0.013721, acc.: 100.00%] [Generator loss: 9.829426]\n",
      "18054 [Discriminator loss: 0.044159, acc.: 96.88%] [Generator loss: 9.048784]\n",
      "18055 [Discriminator loss: 0.047323, acc.: 98.44%] [Generator loss: 8.371469]\n",
      "18056 [Discriminator loss: 0.073292, acc.: 98.44%] [Generator loss: 6.098729]\n",
      "18057 [Discriminator loss: 0.273077, acc.: 93.75%] [Generator loss: 8.156002]\n",
      "18058 [Discriminator loss: 0.063777, acc.: 98.44%] [Generator loss: 8.627092]\n",
      "18059 [Discriminator loss: 0.036664, acc.: 98.44%] [Generator loss: 9.800301]\n",
      "18060 [Discriminator loss: 0.037264, acc.: 98.44%] [Generator loss: 8.424397]\n",
      "18061 [Discriminator loss: 0.053943, acc.: 98.44%] [Generator loss: 8.520315]\n",
      "18062 [Discriminator loss: 0.129352, acc.: 95.31%] [Generator loss: 9.322926]\n",
      "18063 [Discriminator loss: 0.134116, acc.: 93.75%] [Generator loss: 7.705638]\n",
      "18064 [Discriminator loss: 0.030336, acc.: 98.44%] [Generator loss: 7.715434]\n",
      "18065 [Discriminator loss: 0.028531, acc.: 98.44%] [Generator loss: 7.678989]\n",
      "18066 [Discriminator loss: 0.116699, acc.: 96.88%] [Generator loss: 8.628748]\n",
      "18067 [Discriminator loss: 0.127458, acc.: 93.75%] [Generator loss: 8.103394]\n",
      "18068 [Discriminator loss: 0.067704, acc.: 95.31%] [Generator loss: 7.497440]\n",
      "18069 [Discriminator loss: 0.032900, acc.: 98.44%] [Generator loss: 7.908986]\n",
      "18070 [Discriminator loss: 0.040966, acc.: 98.44%] [Generator loss: 8.855635]\n",
      "18071 [Discriminator loss: 0.034517, acc.: 98.44%] [Generator loss: 8.485803]\n",
      "18072 [Discriminator loss: 0.107361, acc.: 95.31%] [Generator loss: 7.722216]\n",
      "18073 [Discriminator loss: 0.026807, acc.: 98.44%] [Generator loss: 7.515661]\n",
      "18074 [Discriminator loss: 0.131459, acc.: 93.75%] [Generator loss: 8.338872]\n",
      "18075 [Discriminator loss: 0.021726, acc.: 100.00%] [Generator loss: 8.841553]\n",
      "18076 [Discriminator loss: 0.030549, acc.: 98.44%] [Generator loss: 9.342505]\n",
      "18077 [Discriminator loss: 0.060061, acc.: 98.44%] [Generator loss: 6.372063]\n",
      "18078 [Discriminator loss: 0.036938, acc.: 100.00%] [Generator loss: 7.589294]\n",
      "18079 [Discriminator loss: 0.027615, acc.: 100.00%] [Generator loss: 8.040027]\n",
      "18080 [Discriminator loss: 0.015609, acc.: 98.44%] [Generator loss: 8.804766]\n",
      "18081 [Discriminator loss: 0.123286, acc.: 93.75%] [Generator loss: 6.538541]\n",
      "18082 [Discriminator loss: 0.076106, acc.: 96.88%] [Generator loss: 7.615759]\n",
      "18083 [Discriminator loss: 0.021908, acc.: 100.00%] [Generator loss: 7.132013]\n",
      "18084 [Discriminator loss: 0.045767, acc.: 98.44%] [Generator loss: 7.233119]\n",
      "18085 [Discriminator loss: 0.083904, acc.: 96.88%] [Generator loss: 8.697956]\n",
      "18086 [Discriminator loss: 0.053329, acc.: 100.00%] [Generator loss: 7.322728]\n",
      "18087 [Discriminator loss: 0.138575, acc.: 93.75%] [Generator loss: 6.250769]\n",
      "18088 [Discriminator loss: 0.084213, acc.: 96.88%] [Generator loss: 8.483732]\n",
      "18089 [Discriminator loss: 0.043181, acc.: 98.44%] [Generator loss: 8.216771]\n",
      "18090 [Discriminator loss: 0.024855, acc.: 100.00%] [Generator loss: 7.704024]\n",
      "18091 [Discriminator loss: 0.073243, acc.: 95.31%] [Generator loss: 9.835886]\n",
      "18092 [Discriminator loss: 0.075159, acc.: 96.88%] [Generator loss: 9.811116]\n",
      "18093 [Discriminator loss: 0.036498, acc.: 98.44%] [Generator loss: 8.180630]\n",
      "18094 [Discriminator loss: 0.137543, acc.: 96.88%] [Generator loss: 8.915773]\n",
      "18095 [Discriminator loss: 0.009763, acc.: 100.00%] [Generator loss: 8.604055]\n",
      "18096 [Discriminator loss: 0.043476, acc.: 98.44%] [Generator loss: 7.702538]\n",
      "18097 [Discriminator loss: 0.128263, acc.: 95.31%] [Generator loss: 7.655572]\n",
      "18098 [Discriminator loss: 0.038491, acc.: 98.44%] [Generator loss: 8.476099]\n",
      "18099 [Discriminator loss: 0.043630, acc.: 98.44%] [Generator loss: 7.272854]\n",
      "18100 [Discriminator loss: 0.078159, acc.: 95.31%] [Generator loss: 8.922449]\n",
      "18101 [Discriminator loss: 0.110524, acc.: 90.62%] [Generator loss: 6.656390]\n",
      "18102 [Discriminator loss: 0.123480, acc.: 96.88%] [Generator loss: 7.924761]\n",
      "18103 [Discriminator loss: 0.070201, acc.: 95.31%] [Generator loss: 10.539826]\n",
      "18104 [Discriminator loss: 0.049290, acc.: 98.44%] [Generator loss: 9.692987]\n",
      "18105 [Discriminator loss: 0.017310, acc.: 100.00%] [Generator loss: 8.594501]\n",
      "18106 [Discriminator loss: 0.028058, acc.: 98.44%] [Generator loss: 7.550423]\n",
      "18107 [Discriminator loss: 0.117455, acc.: 95.31%] [Generator loss: 6.124630]\n",
      "18108 [Discriminator loss: 0.059903, acc.: 98.44%] [Generator loss: 7.975471]\n",
      "18109 [Discriminator loss: 0.066366, acc.: 96.88%] [Generator loss: 7.550068]\n",
      "18110 [Discriminator loss: 0.030412, acc.: 100.00%] [Generator loss: 8.804444]\n",
      "18111 [Discriminator loss: 0.104964, acc.: 95.31%] [Generator loss: 6.439009]\n",
      "18112 [Discriminator loss: 0.306073, acc.: 92.19%] [Generator loss: 7.463074]\n",
      "18113 [Discriminator loss: 0.011460, acc.: 100.00%] [Generator loss: 9.407182]\n",
      "18114 [Discriminator loss: 0.054897, acc.: 96.88%] [Generator loss: 7.478436]\n",
      "18115 [Discriminator loss: 0.091515, acc.: 98.44%] [Generator loss: 7.119992]\n",
      "18116 [Discriminator loss: 0.182137, acc.: 90.62%] [Generator loss: 7.606491]\n",
      "18117 [Discriminator loss: 0.187726, acc.: 93.75%] [Generator loss: 8.807689]\n",
      "18118 [Discriminator loss: 0.044292, acc.: 98.44%] [Generator loss: 10.388469]\n",
      "18119 [Discriminator loss: 0.019002, acc.: 100.00%] [Generator loss: 11.097372]\n",
      "18120 [Discriminator loss: 0.118748, acc.: 96.88%] [Generator loss: 9.962302]\n",
      "18121 [Discriminator loss: 0.023397, acc.: 100.00%] [Generator loss: 9.088175]\n",
      "18122 [Discriminator loss: 0.067036, acc.: 96.88%] [Generator loss: 8.232645]\n",
      "18123 [Discriminator loss: 0.078292, acc.: 96.88%] [Generator loss: 7.724871]\n",
      "18124 [Discriminator loss: 0.009818, acc.: 100.00%] [Generator loss: 8.848106]\n",
      "18125 [Discriminator loss: 0.061021, acc.: 100.00%] [Generator loss: 6.689760]\n",
      "18126 [Discriminator loss: 0.082251, acc.: 98.44%] [Generator loss: 8.254290]\n",
      "18127 [Discriminator loss: 0.008567, acc.: 100.00%] [Generator loss: 8.594412]\n",
      "18128 [Discriminator loss: 0.024750, acc.: 100.00%] [Generator loss: 9.666827]\n",
      "18129 [Discriminator loss: 0.136247, acc.: 95.31%] [Generator loss: 9.086927]\n",
      "18130 [Discriminator loss: 0.049598, acc.: 98.44%] [Generator loss: 9.062391]\n",
      "18131 [Discriminator loss: 0.048185, acc.: 98.44%] [Generator loss: 8.576016]\n",
      "18132 [Discriminator loss: 0.041294, acc.: 98.44%] [Generator loss: 8.934655]\n",
      "18133 [Discriminator loss: 0.073760, acc.: 96.88%] [Generator loss: 8.440016]\n",
      "18134 [Discriminator loss: 0.013127, acc.: 100.00%] [Generator loss: 9.349005]\n",
      "18135 [Discriminator loss: 0.017484, acc.: 100.00%] [Generator loss: 9.156114]\n",
      "18136 [Discriminator loss: 0.127136, acc.: 93.75%] [Generator loss: 6.735754]\n",
      "18137 [Discriminator loss: 0.055804, acc.: 96.88%] [Generator loss: 7.541974]\n",
      "18138 [Discriminator loss: 0.026024, acc.: 98.44%] [Generator loss: 8.844230]\n",
      "18139 [Discriminator loss: 0.039577, acc.: 98.44%] [Generator loss: 10.009714]\n",
      "18140 [Discriminator loss: 0.086815, acc.: 93.75%] [Generator loss: 7.358063]\n",
      "18141 [Discriminator loss: 0.033178, acc.: 100.00%] [Generator loss: 7.158659]\n",
      "18142 [Discriminator loss: 0.048698, acc.: 98.44%] [Generator loss: 7.887155]\n",
      "18143 [Discriminator loss: 0.070682, acc.: 96.88%] [Generator loss: 7.732827]\n",
      "18144 [Discriminator loss: 0.017260, acc.: 100.00%] [Generator loss: 7.329691]\n",
      "18145 [Discriminator loss: 0.063722, acc.: 95.31%] [Generator loss: 7.186089]\n",
      "18146 [Discriminator loss: 0.099426, acc.: 95.31%] [Generator loss: 8.290455]\n",
      "18147 [Discriminator loss: 0.024915, acc.: 98.44%] [Generator loss: 9.242633]\n",
      "18148 [Discriminator loss: 0.071910, acc.: 98.44%] [Generator loss: 7.388183]\n",
      "18149 [Discriminator loss: 0.060907, acc.: 95.31%] [Generator loss: 7.114123]\n",
      "18150 [Discriminator loss: 0.035024, acc.: 98.44%] [Generator loss: 8.660829]\n",
      "18151 [Discriminator loss: 0.064596, acc.: 96.88%] [Generator loss: 8.823078]\n",
      "18152 [Discriminator loss: 0.090564, acc.: 95.31%] [Generator loss: 8.456430]\n",
      "18153 [Discriminator loss: 0.020064, acc.: 100.00%] [Generator loss: 8.627007]\n",
      "18154 [Discriminator loss: 0.026048, acc.: 100.00%] [Generator loss: 8.859711]\n",
      "18155 [Discriminator loss: 0.135214, acc.: 98.44%] [Generator loss: 8.053194]\n",
      "18156 [Discriminator loss: 0.044160, acc.: 98.44%] [Generator loss: 8.100718]\n",
      "18157 [Discriminator loss: 0.027182, acc.: 100.00%] [Generator loss: 6.492171]\n",
      "18158 [Discriminator loss: 0.158612, acc.: 92.19%] [Generator loss: 8.966961]\n",
      "18159 [Discriminator loss: 0.014357, acc.: 100.00%] [Generator loss: 10.531853]\n",
      "18160 [Discriminator loss: 0.144845, acc.: 95.31%] [Generator loss: 9.191395]\n",
      "18161 [Discriminator loss: 0.070181, acc.: 95.31%] [Generator loss: 9.412399]\n",
      "18162 [Discriminator loss: 0.123182, acc.: 93.75%] [Generator loss: 9.247684]\n",
      "18163 [Discriminator loss: 0.019608, acc.: 100.00%] [Generator loss: 9.426331]\n",
      "18164 [Discriminator loss: 0.120608, acc.: 93.75%] [Generator loss: 7.560467]\n",
      "18165 [Discriminator loss: 0.013595, acc.: 100.00%] [Generator loss: 8.706087]\n",
      "18166 [Discriminator loss: 0.088453, acc.: 98.44%] [Generator loss: 7.225265]\n",
      "18167 [Discriminator loss: 0.167152, acc.: 93.75%] [Generator loss: 8.199068]\n",
      "18168 [Discriminator loss: 0.127809, acc.: 95.31%] [Generator loss: 8.782104]\n",
      "18169 [Discriminator loss: 0.036971, acc.: 98.44%] [Generator loss: 7.886597]\n",
      "18170 [Discriminator loss: 0.034885, acc.: 98.44%] [Generator loss: 7.188907]\n",
      "18171 [Discriminator loss: 0.069279, acc.: 96.88%] [Generator loss: 7.798577]\n",
      "18172 [Discriminator loss: 0.103078, acc.: 93.75%] [Generator loss: 8.318084]\n",
      "18173 [Discriminator loss: 0.034879, acc.: 98.44%] [Generator loss: 9.370108]\n",
      "18174 [Discriminator loss: 0.018672, acc.: 100.00%] [Generator loss: 8.585272]\n",
      "18175 [Discriminator loss: 0.092952, acc.: 93.75%] [Generator loss: 9.309048]\n",
      "18176 [Discriminator loss: 0.146356, acc.: 95.31%] [Generator loss: 7.270626]\n",
      "18177 [Discriminator loss: 0.120901, acc.: 96.88%] [Generator loss: 7.468629]\n",
      "18178 [Discriminator loss: 0.170761, acc.: 92.19%] [Generator loss: 8.368679]\n",
      "18179 [Discriminator loss: 0.040428, acc.: 98.44%] [Generator loss: 8.741889]\n",
      "18180 [Discriminator loss: 0.162566, acc.: 92.19%] [Generator loss: 6.903084]\n",
      "18181 [Discriminator loss: 0.106961, acc.: 95.31%] [Generator loss: 7.652159]\n",
      "18182 [Discriminator loss: 0.031084, acc.: 100.00%] [Generator loss: 9.380636]\n",
      "18183 [Discriminator loss: 0.094072, acc.: 98.44%] [Generator loss: 10.035340]\n",
      "18184 [Discriminator loss: 0.007932, acc.: 100.00%] [Generator loss: 9.405697]\n",
      "18185 [Discriminator loss: 0.030431, acc.: 98.44%] [Generator loss: 9.309739]\n",
      "18186 [Discriminator loss: 0.040723, acc.: 98.44%] [Generator loss: 9.630811]\n",
      "18187 [Discriminator loss: 0.076474, acc.: 96.88%] [Generator loss: 8.484266]\n",
      "18188 [Discriminator loss: 0.033835, acc.: 98.44%] [Generator loss: 8.633158]\n",
      "18189 [Discriminator loss: 0.158273, acc.: 96.88%] [Generator loss: 6.642274]\n",
      "18190 [Discriminator loss: 0.092463, acc.: 95.31%] [Generator loss: 7.284875]\n",
      "18191 [Discriminator loss: 0.098396, acc.: 93.75%] [Generator loss: 8.633581]\n",
      "18192 [Discriminator loss: 0.024061, acc.: 100.00%] [Generator loss: 8.694483]\n",
      "18193 [Discriminator loss: 0.065866, acc.: 96.88%] [Generator loss: 7.870015]\n",
      "18194 [Discriminator loss: 0.077480, acc.: 96.88%] [Generator loss: 9.114124]\n",
      "18195 [Discriminator loss: 0.065440, acc.: 96.88%] [Generator loss: 8.640220]\n",
      "18196 [Discriminator loss: 0.086574, acc.: 98.44%] [Generator loss: 7.541645]\n",
      "18197 [Discriminator loss: 0.055879, acc.: 98.44%] [Generator loss: 9.092182]\n",
      "18198 [Discriminator loss: 0.024835, acc.: 100.00%] [Generator loss: 8.428480]\n",
      "18199 [Discriminator loss: 0.028004, acc.: 98.44%] [Generator loss: 8.969391]\n",
      "18200 [Discriminator loss: 0.049333, acc.: 98.44%] [Generator loss: 7.503653]\n",
      "18201 [Discriminator loss: 0.049604, acc.: 96.88%] [Generator loss: 8.726655]\n",
      "18202 [Discriminator loss: 0.017950, acc.: 100.00%] [Generator loss: 8.705602]\n",
      "18203 [Discriminator loss: 0.133464, acc.: 93.75%] [Generator loss: 8.686579]\n",
      "18204 [Discriminator loss: 0.097344, acc.: 96.88%] [Generator loss: 7.668565]\n",
      "18205 [Discriminator loss: 0.058576, acc.: 98.44%] [Generator loss: 9.393123]\n",
      "18206 [Discriminator loss: 0.055010, acc.: 98.44%] [Generator loss: 8.362828]\n",
      "18207 [Discriminator loss: 0.078063, acc.: 98.44%] [Generator loss: 8.494437]\n",
      "18208 [Discriminator loss: 0.064990, acc.: 96.88%] [Generator loss: 9.827851]\n",
      "18209 [Discriminator loss: 0.114600, acc.: 93.75%] [Generator loss: 8.746699]\n",
      "18210 [Discriminator loss: 0.041553, acc.: 98.44%] [Generator loss: 7.719429]\n",
      "18211 [Discriminator loss: 0.045008, acc.: 98.44%] [Generator loss: 7.774194]\n",
      "18212 [Discriminator loss: 0.049003, acc.: 98.44%] [Generator loss: 7.446586]\n",
      "18213 [Discriminator loss: 0.056674, acc.: 98.44%] [Generator loss: 9.306921]\n",
      "18214 [Discriminator loss: 0.043601, acc.: 100.00%] [Generator loss: 8.914968]\n",
      "18215 [Discriminator loss: 0.091182, acc.: 96.88%] [Generator loss: 8.958364]\n",
      "18216 [Discriminator loss: 0.061830, acc.: 98.44%] [Generator loss: 7.268314]\n",
      "18217 [Discriminator loss: 0.172840, acc.: 92.19%] [Generator loss: 8.077457]\n",
      "18218 [Discriminator loss: 0.108901, acc.: 95.31%] [Generator loss: 7.435787]\n",
      "18219 [Discriminator loss: 0.018228, acc.: 100.00%] [Generator loss: 6.900801]\n",
      "18220 [Discriminator loss: 0.038049, acc.: 98.44%] [Generator loss: 8.539935]\n",
      "18221 [Discriminator loss: 0.178421, acc.: 95.31%] [Generator loss: 9.465292]\n",
      "18222 [Discriminator loss: 0.028352, acc.: 98.44%] [Generator loss: 9.440527]\n",
      "18223 [Discriminator loss: 0.128928, acc.: 93.75%] [Generator loss: 9.372723]\n",
      "18224 [Discriminator loss: 0.061998, acc.: 96.88%] [Generator loss: 8.979239]\n",
      "18225 [Discriminator loss: 0.023924, acc.: 100.00%] [Generator loss: 8.026493]\n",
      "18226 [Discriminator loss: 0.028006, acc.: 100.00%] [Generator loss: 8.937665]\n",
      "18227 [Discriminator loss: 0.078082, acc.: 98.44%] [Generator loss: 7.194141]\n",
      "18228 [Discriminator loss: 0.187478, acc.: 93.75%] [Generator loss: 7.419583]\n",
      "18229 [Discriminator loss: 0.156871, acc.: 93.75%] [Generator loss: 10.401476]\n",
      "18230 [Discriminator loss: 0.101286, acc.: 95.31%] [Generator loss: 7.571886]\n",
      "18231 [Discriminator loss: 0.037794, acc.: 98.44%] [Generator loss: 8.393518]\n",
      "18232 [Discriminator loss: 0.009814, acc.: 100.00%] [Generator loss: 8.183697]\n",
      "18233 [Discriminator loss: 0.122227, acc.: 93.75%] [Generator loss: 8.812512]\n",
      "18234 [Discriminator loss: 0.008715, acc.: 100.00%] [Generator loss: 9.902092]\n",
      "18235 [Discriminator loss: 0.034770, acc.: 98.44%] [Generator loss: 7.780721]\n",
      "18236 [Discriminator loss: 0.056186, acc.: 98.44%] [Generator loss: 7.104255]\n",
      "18237 [Discriminator loss: 0.062712, acc.: 96.88%] [Generator loss: 8.643121]\n",
      "18238 [Discriminator loss: 0.093123, acc.: 93.75%] [Generator loss: 10.727863]\n",
      "18239 [Discriminator loss: 0.019410, acc.: 100.00%] [Generator loss: 10.094984]\n",
      "18240 [Discriminator loss: 0.069610, acc.: 96.88%] [Generator loss: 7.221355]\n",
      "18241 [Discriminator loss: 0.031004, acc.: 98.44%] [Generator loss: 7.311910]\n",
      "18242 [Discriminator loss: 0.175142, acc.: 95.31%] [Generator loss: 9.053966]\n",
      "18243 [Discriminator loss: 0.056377, acc.: 98.44%] [Generator loss: 10.010056]\n",
      "18244 [Discriminator loss: 0.118522, acc.: 93.75%] [Generator loss: 8.517746]\n",
      "18245 [Discriminator loss: 0.124594, acc.: 95.31%] [Generator loss: 8.416372]\n",
      "18246 [Discriminator loss: 0.045505, acc.: 98.44%] [Generator loss: 7.989481]\n",
      "18247 [Discriminator loss: 0.032785, acc.: 100.00%] [Generator loss: 7.289828]\n",
      "18248 [Discriminator loss: 0.258293, acc.: 90.62%] [Generator loss: 7.793101]\n",
      "18249 [Discriminator loss: 0.060006, acc.: 96.88%] [Generator loss: 8.452350]\n",
      "18250 [Discriminator loss: 0.019107, acc.: 100.00%] [Generator loss: 7.117299]\n",
      "18251 [Discriminator loss: 0.053028, acc.: 98.44%] [Generator loss: 9.040363]\n",
      "18252 [Discriminator loss: 0.041494, acc.: 98.44%] [Generator loss: 9.541553]\n",
      "18253 [Discriminator loss: 0.181433, acc.: 93.75%] [Generator loss: 6.733583]\n",
      "18254 [Discriminator loss: 0.095845, acc.: 95.31%] [Generator loss: 8.214256]\n",
      "18255 [Discriminator loss: 0.136783, acc.: 93.75%] [Generator loss: 9.409915]\n",
      "18256 [Discriminator loss: 0.209197, acc.: 92.19%] [Generator loss: 5.274883]\n",
      "18257 [Discriminator loss: 0.011938, acc.: 100.00%] [Generator loss: 7.229942]\n",
      "18258 [Discriminator loss: 0.136481, acc.: 93.75%] [Generator loss: 8.948674]\n",
      "18259 [Discriminator loss: 0.009830, acc.: 100.00%] [Generator loss: 9.677121]\n",
      "18260 [Discriminator loss: 0.060132, acc.: 96.88%] [Generator loss: 8.476124]\n",
      "18261 [Discriminator loss: 0.126505, acc.: 93.75%] [Generator loss: 7.560987]\n",
      "18262 [Discriminator loss: 0.190839, acc.: 92.19%] [Generator loss: 8.613266]\n",
      "18263 [Discriminator loss: 0.080104, acc.: 96.88%] [Generator loss: 9.548093]\n",
      "18264 [Discriminator loss: 0.079164, acc.: 96.88%] [Generator loss: 8.018763]\n",
      "18265 [Discriminator loss: 0.151106, acc.: 93.75%] [Generator loss: 9.082040]\n",
      "18266 [Discriminator loss: 0.052758, acc.: 98.44%] [Generator loss: 9.531734]\n",
      "18267 [Discriminator loss: 0.031442, acc.: 100.00%] [Generator loss: 7.138439]\n",
      "18268 [Discriminator loss: 0.122377, acc.: 95.31%] [Generator loss: 9.819258]\n",
      "18269 [Discriminator loss: 0.024384, acc.: 100.00%] [Generator loss: 10.825788]\n",
      "18270 [Discriminator loss: 0.162993, acc.: 95.31%] [Generator loss: 9.073065]\n",
      "18271 [Discriminator loss: 0.009081, acc.: 100.00%] [Generator loss: 7.653924]\n",
      "18272 [Discriminator loss: 0.035973, acc.: 98.44%] [Generator loss: 8.178796]\n",
      "18273 [Discriminator loss: 0.019308, acc.: 98.44%] [Generator loss: 8.155131]\n",
      "18274 [Discriminator loss: 0.081353, acc.: 98.44%] [Generator loss: 9.068132]\n",
      "18275 [Discriminator loss: 0.065936, acc.: 95.31%] [Generator loss: 9.752037]\n",
      "18276 [Discriminator loss: 0.013744, acc.: 100.00%] [Generator loss: 9.227450]\n",
      "18277 [Discriminator loss: 0.113591, acc.: 93.75%] [Generator loss: 6.477857]\n",
      "18278 [Discriminator loss: 0.040100, acc.: 98.44%] [Generator loss: 7.229164]\n",
      "18279 [Discriminator loss: 0.022343, acc.: 98.44%] [Generator loss: 9.149509]\n",
      "18280 [Discriminator loss: 0.058367, acc.: 98.44%] [Generator loss: 7.150500]\n",
      "18281 [Discriminator loss: 0.222946, acc.: 93.75%] [Generator loss: 9.895418]\n",
      "18282 [Discriminator loss: 0.130501, acc.: 92.19%] [Generator loss: 8.858317]\n",
      "18283 [Discriminator loss: 0.126827, acc.: 92.19%] [Generator loss: 7.841544]\n",
      "18284 [Discriminator loss: 0.017920, acc.: 100.00%] [Generator loss: 6.822964]\n",
      "18285 [Discriminator loss: 0.023699, acc.: 100.00%] [Generator loss: 8.196469]\n",
      "18286 [Discriminator loss: 0.126886, acc.: 96.88%] [Generator loss: 9.429935]\n",
      "18287 [Discriminator loss: 0.033768, acc.: 100.00%] [Generator loss: 8.127975]\n",
      "18288 [Discriminator loss: 0.150931, acc.: 95.31%] [Generator loss: 7.810887]\n",
      "18289 [Discriminator loss: 0.060594, acc.: 98.44%] [Generator loss: 9.143188]\n",
      "18290 [Discriminator loss: 0.165154, acc.: 95.31%] [Generator loss: 8.576462]\n",
      "18291 [Discriminator loss: 0.124828, acc.: 95.31%] [Generator loss: 8.736570]\n",
      "18292 [Discriminator loss: 0.055026, acc.: 98.44%] [Generator loss: 8.945253]\n",
      "18293 [Discriminator loss: 0.073390, acc.: 96.88%] [Generator loss: 6.805309]\n",
      "18294 [Discriminator loss: 0.135379, acc.: 95.31%] [Generator loss: 8.214102]\n",
      "18295 [Discriminator loss: 0.033145, acc.: 98.44%] [Generator loss: 9.771269]\n",
      "18296 [Discriminator loss: 0.110123, acc.: 95.31%] [Generator loss: 7.139596]\n",
      "18297 [Discriminator loss: 0.074077, acc.: 96.88%] [Generator loss: 7.151423]\n",
      "18298 [Discriminator loss: 0.077417, acc.: 98.44%] [Generator loss: 7.451038]\n",
      "18299 [Discriminator loss: 0.058854, acc.: 96.88%] [Generator loss: 9.299809]\n",
      "18300 [Discriminator loss: 0.063921, acc.: 96.88%] [Generator loss: 9.848311]\n",
      "18301 [Discriminator loss: 0.056625, acc.: 96.88%] [Generator loss: 10.064619]\n",
      "18302 [Discriminator loss: 0.050950, acc.: 98.44%] [Generator loss: 9.379889]\n",
      "18303 [Discriminator loss: 0.059995, acc.: 98.44%] [Generator loss: 8.843398]\n",
      "18304 [Discriminator loss: 0.128416, acc.: 95.31%] [Generator loss: 7.888018]\n",
      "18305 [Discriminator loss: 0.016136, acc.: 100.00%] [Generator loss: 7.294458]\n",
      "18306 [Discriminator loss: 0.032434, acc.: 98.44%] [Generator loss: 9.401814]\n",
      "18307 [Discriminator loss: 0.030873, acc.: 100.00%] [Generator loss: 9.684584]\n",
      "18308 [Discriminator loss: 0.118675, acc.: 96.88%] [Generator loss: 8.524229]\n",
      "18309 [Discriminator loss: 0.160097, acc.: 90.62%] [Generator loss: 7.854939]\n",
      "18310 [Discriminator loss: 0.021443, acc.: 98.44%] [Generator loss: 9.094709]\n",
      "18311 [Discriminator loss: 0.081580, acc.: 95.31%] [Generator loss: 8.765506]\n",
      "18312 [Discriminator loss: 0.046794, acc.: 96.88%] [Generator loss: 8.187080]\n",
      "18313 [Discriminator loss: 0.048776, acc.: 98.44%] [Generator loss: 8.230129]\n",
      "18314 [Discriminator loss: 0.014374, acc.: 100.00%] [Generator loss: 8.181213]\n",
      "18315 [Discriminator loss: 0.033602, acc.: 98.44%] [Generator loss: 6.587682]\n",
      "18316 [Discriminator loss: 0.138280, acc.: 95.31%] [Generator loss: 6.896441]\n",
      "18317 [Discriminator loss: 0.024490, acc.: 98.44%] [Generator loss: 8.362153]\n",
      "18318 [Discriminator loss: 0.029534, acc.: 100.00%] [Generator loss: 8.810646]\n",
      "18319 [Discriminator loss: 0.234702, acc.: 89.06%] [Generator loss: 8.007641]\n",
      "18320 [Discriminator loss: 0.044848, acc.: 100.00%] [Generator loss: 10.805649]\n",
      "18321 [Discriminator loss: 0.020900, acc.: 100.00%] [Generator loss: 9.593161]\n",
      "18322 [Discriminator loss: 0.342084, acc.: 84.38%] [Generator loss: 6.261947]\n",
      "18323 [Discriminator loss: 0.151666, acc.: 95.31%] [Generator loss: 8.698018]\n",
      "18324 [Discriminator loss: 0.154421, acc.: 93.75%] [Generator loss: 7.418340]\n",
      "18325 [Discriminator loss: 0.043251, acc.: 100.00%] [Generator loss: 8.242706]\n",
      "18326 [Discriminator loss: 0.065040, acc.: 96.88%] [Generator loss: 7.101474]\n",
      "18327 [Discriminator loss: 0.034634, acc.: 98.44%] [Generator loss: 8.086322]\n",
      "18328 [Discriminator loss: 0.047106, acc.: 98.44%] [Generator loss: 9.071343]\n",
      "18329 [Discriminator loss: 0.022717, acc.: 100.00%] [Generator loss: 8.363080]\n",
      "18330 [Discriminator loss: 0.130623, acc.: 93.75%] [Generator loss: 6.658832]\n",
      "18331 [Discriminator loss: 0.018644, acc.: 100.00%] [Generator loss: 8.394039]\n",
      "18332 [Discriminator loss: 0.053416, acc.: 96.88%] [Generator loss: 7.168211]\n",
      "18333 [Discriminator loss: 0.137706, acc.: 95.31%] [Generator loss: 8.181269]\n",
      "18334 [Discriminator loss: 0.054983, acc.: 96.88%] [Generator loss: 9.401072]\n",
      "18335 [Discriminator loss: 0.036789, acc.: 100.00%] [Generator loss: 7.537499]\n",
      "18336 [Discriminator loss: 0.085238, acc.: 96.88%] [Generator loss: 7.871108]\n",
      "18337 [Discriminator loss: 0.075923, acc.: 96.88%] [Generator loss: 9.181922]\n",
      "18338 [Discriminator loss: 0.148090, acc.: 95.31%] [Generator loss: 8.584617]\n",
      "18339 [Discriminator loss: 0.032972, acc.: 98.44%] [Generator loss: 9.256921]\n",
      "18340 [Discriminator loss: 0.138330, acc.: 96.88%] [Generator loss: 7.569291]\n",
      "18341 [Discriminator loss: 0.105374, acc.: 98.44%] [Generator loss: 6.596906]\n",
      "18342 [Discriminator loss: 0.194150, acc.: 95.31%] [Generator loss: 8.238583]\n",
      "18343 [Discriminator loss: 0.048248, acc.: 96.88%] [Generator loss: 9.299275]\n",
      "18344 [Discriminator loss: 0.096189, acc.: 93.75%] [Generator loss: 8.746435]\n",
      "18345 [Discriminator loss: 0.110836, acc.: 93.75%] [Generator loss: 8.334331]\n",
      "18346 [Discriminator loss: 0.058001, acc.: 96.88%] [Generator loss: 8.376038]\n",
      "18347 [Discriminator loss: 0.018917, acc.: 98.44%] [Generator loss: 8.928555]\n",
      "18348 [Discriminator loss: 0.043640, acc.: 100.00%] [Generator loss: 7.515700]\n",
      "18349 [Discriminator loss: 0.016644, acc.: 100.00%] [Generator loss: 8.284344]\n",
      "18350 [Discriminator loss: 0.181354, acc.: 92.19%] [Generator loss: 9.393207]\n",
      "18351 [Discriminator loss: 0.078254, acc.: 98.44%] [Generator loss: 9.618710]\n",
      "18352 [Discriminator loss: 0.093488, acc.: 95.31%] [Generator loss: 7.749642]\n",
      "18353 [Discriminator loss: 0.031068, acc.: 100.00%] [Generator loss: 7.800403]\n",
      "18354 [Discriminator loss: 0.079613, acc.: 96.88%] [Generator loss: 8.772764]\n",
      "18355 [Discriminator loss: 0.200777, acc.: 95.31%] [Generator loss: 8.919371]\n",
      "18356 [Discriminator loss: 0.026214, acc.: 98.44%] [Generator loss: 9.586773]\n",
      "18357 [Discriminator loss: 0.047681, acc.: 96.88%] [Generator loss: 9.380768]\n",
      "18358 [Discriminator loss: 0.078655, acc.: 96.88%] [Generator loss: 7.860831]\n",
      "18359 [Discriminator loss: 0.113417, acc.: 93.75%] [Generator loss: 8.405668]\n",
      "18360 [Discriminator loss: 0.072361, acc.: 95.31%] [Generator loss: 10.488926]\n",
      "18361 [Discriminator loss: 0.162339, acc.: 92.19%] [Generator loss: 8.039248]\n",
      "18362 [Discriminator loss: 0.020560, acc.: 100.00%] [Generator loss: 7.281958]\n",
      "18363 [Discriminator loss: 0.083995, acc.: 96.88%] [Generator loss: 8.810546]\n",
      "18364 [Discriminator loss: 0.035408, acc.: 98.44%] [Generator loss: 8.939192]\n",
      "18365 [Discriminator loss: 0.019102, acc.: 98.44%] [Generator loss: 9.567326]\n",
      "18366 [Discriminator loss: 0.051767, acc.: 96.88%] [Generator loss: 8.083342]\n",
      "18367 [Discriminator loss: 0.041339, acc.: 98.44%] [Generator loss: 8.534255]\n",
      "18368 [Discriminator loss: 0.101223, acc.: 96.88%] [Generator loss: 8.555035]\n",
      "18369 [Discriminator loss: 0.122028, acc.: 93.75%] [Generator loss: 8.380821]\n",
      "18370 [Discriminator loss: 0.075338, acc.: 98.44%] [Generator loss: 8.833468]\n",
      "18371 [Discriminator loss: 0.171801, acc.: 93.75%] [Generator loss: 7.983761]\n",
      "18372 [Discriminator loss: 0.099865, acc.: 95.31%] [Generator loss: 8.905291]\n",
      "18373 [Discriminator loss: 0.042273, acc.: 98.44%] [Generator loss: 8.260904]\n",
      "18374 [Discriminator loss: 0.094226, acc.: 95.31%] [Generator loss: 7.769645]\n",
      "18375 [Discriminator loss: 0.134446, acc.: 93.75%] [Generator loss: 9.183232]\n",
      "18376 [Discriminator loss: 0.166126, acc.: 92.19%] [Generator loss: 8.255877]\n",
      "18377 [Discriminator loss: 0.321173, acc.: 90.62%] [Generator loss: 11.378674]\n",
      "18378 [Discriminator loss: 0.111595, acc.: 93.75%] [Generator loss: 10.068250]\n",
      "18379 [Discriminator loss: 0.302614, acc.: 92.19%] [Generator loss: 7.566376]\n",
      "18380 [Discriminator loss: 0.053736, acc.: 96.88%] [Generator loss: 6.864347]\n",
      "18381 [Discriminator loss: 0.022857, acc.: 100.00%] [Generator loss: 7.634270]\n",
      "18382 [Discriminator loss: 0.093569, acc.: 96.88%] [Generator loss: 6.894333]\n",
      "18383 [Discriminator loss: 0.199159, acc.: 93.75%] [Generator loss: 10.251278]\n",
      "18384 [Discriminator loss: 0.007949, acc.: 100.00%] [Generator loss: 9.652639]\n",
      "18385 [Discriminator loss: 0.045504, acc.: 96.88%] [Generator loss: 9.751369]\n",
      "18386 [Discriminator loss: 0.113101, acc.: 98.44%] [Generator loss: 7.429906]\n",
      "18387 [Discriminator loss: 0.114930, acc.: 93.75%] [Generator loss: 7.490743]\n",
      "18388 [Discriminator loss: 0.025775, acc.: 98.44%] [Generator loss: 7.842725]\n",
      "18389 [Discriminator loss: 0.148657, acc.: 95.31%] [Generator loss: 7.999269]\n",
      "18390 [Discriminator loss: 0.051084, acc.: 98.44%] [Generator loss: 8.642941]\n",
      "18391 [Discriminator loss: 0.338292, acc.: 92.19%] [Generator loss: 7.823549]\n",
      "18392 [Discriminator loss: 0.063386, acc.: 96.88%] [Generator loss: 8.392345]\n",
      "18393 [Discriminator loss: 0.055742, acc.: 96.88%] [Generator loss: 7.286537]\n",
      "18394 [Discriminator loss: 0.031428, acc.: 98.44%] [Generator loss: 7.620425]\n",
      "18395 [Discriminator loss: 0.023846, acc.: 100.00%] [Generator loss: 7.804619]\n",
      "18396 [Discriminator loss: 0.255776, acc.: 82.81%] [Generator loss: 9.163146]\n",
      "18397 [Discriminator loss: 0.036485, acc.: 100.00%] [Generator loss: 8.577082]\n",
      "18398 [Discriminator loss: 0.109685, acc.: 96.88%] [Generator loss: 7.675440]\n",
      "18399 [Discriminator loss: 0.113220, acc.: 95.31%] [Generator loss: 7.477815]\n",
      "18400 [Discriminator loss: 0.043204, acc.: 98.44%] [Generator loss: 8.187348]\n",
      "18401 [Discriminator loss: 0.038932, acc.: 98.44%] [Generator loss: 8.723057]\n",
      "18402 [Discriminator loss: 0.062987, acc.: 98.44%] [Generator loss: 8.607550]\n",
      "18403 [Discriminator loss: 0.018846, acc.: 100.00%] [Generator loss: 8.586563]\n",
      "18404 [Discriminator loss: 0.026616, acc.: 100.00%] [Generator loss: 10.011709]\n",
      "18405 [Discriminator loss: 0.064070, acc.: 96.88%] [Generator loss: 8.238956]\n",
      "18406 [Discriminator loss: 0.157258, acc.: 90.62%] [Generator loss: 7.330620]\n",
      "18407 [Discriminator loss: 0.081775, acc.: 96.88%] [Generator loss: 6.729258]\n",
      "18408 [Discriminator loss: 0.059947, acc.: 95.31%] [Generator loss: 8.710945]\n",
      "18409 [Discriminator loss: 0.038160, acc.: 98.44%] [Generator loss: 6.788671]\n",
      "18410 [Discriminator loss: 0.009716, acc.: 100.00%] [Generator loss: 10.271883]\n",
      "18411 [Discriminator loss: 0.136438, acc.: 95.31%] [Generator loss: 7.051603]\n",
      "18412 [Discriminator loss: 0.016955, acc.: 100.00%] [Generator loss: 7.896089]\n",
      "18413 [Discriminator loss: 0.125140, acc.: 96.88%] [Generator loss: 7.360276]\n",
      "18414 [Discriminator loss: 0.050300, acc.: 98.44%] [Generator loss: 7.324623]\n",
      "18415 [Discriminator loss: 0.046389, acc.: 98.44%] [Generator loss: 7.826024]\n",
      "18416 [Discriminator loss: 0.127329, acc.: 90.62%] [Generator loss: 10.109129]\n",
      "18417 [Discriminator loss: 0.152674, acc.: 96.88%] [Generator loss: 9.307524]\n",
      "18418 [Discriminator loss: 0.058099, acc.: 98.44%] [Generator loss: 7.360064]\n",
      "18419 [Discriminator loss: 0.070543, acc.: 95.31%] [Generator loss: 8.174715]\n",
      "18420 [Discriminator loss: 0.119140, acc.: 95.31%] [Generator loss: 10.539312]\n",
      "18421 [Discriminator loss: 0.009777, acc.: 100.00%] [Generator loss: 10.313939]\n",
      "18422 [Discriminator loss: 0.233987, acc.: 93.75%] [Generator loss: 8.408039]\n",
      "18423 [Discriminator loss: 0.038652, acc.: 98.44%] [Generator loss: 7.789494]\n",
      "18424 [Discriminator loss: 0.259893, acc.: 90.62%] [Generator loss: 8.825495]\n",
      "18425 [Discriminator loss: 0.019832, acc.: 100.00%] [Generator loss: 8.988651]\n",
      "18426 [Discriminator loss: 0.116086, acc.: 96.88%] [Generator loss: 7.016762]\n",
      "18427 [Discriminator loss: 0.097619, acc.: 98.44%] [Generator loss: 8.131697]\n",
      "18428 [Discriminator loss: 0.071332, acc.: 96.88%] [Generator loss: 9.493181]\n",
      "18429 [Discriminator loss: 0.128202, acc.: 93.75%] [Generator loss: 7.669224]\n",
      "18430 [Discriminator loss: 0.138240, acc.: 92.19%] [Generator loss: 7.028883]\n",
      "18431 [Discriminator loss: 0.177548, acc.: 95.31%] [Generator loss: 8.736708]\n",
      "18432 [Discriminator loss: 0.036034, acc.: 96.88%] [Generator loss: 10.186981]\n",
      "18433 [Discriminator loss: 0.068893, acc.: 98.44%] [Generator loss: 7.794028]\n",
      "18434 [Discriminator loss: 0.184892, acc.: 92.19%] [Generator loss: 10.457466]\n",
      "18435 [Discriminator loss: 0.015027, acc.: 100.00%] [Generator loss: 9.453976]\n",
      "18436 [Discriminator loss: 0.064343, acc.: 98.44%] [Generator loss: 9.233320]\n",
      "18437 [Discriminator loss: 0.050833, acc.: 100.00%] [Generator loss: 9.109786]\n",
      "18438 [Discriminator loss: 0.051890, acc.: 96.88%] [Generator loss: 8.939312]\n",
      "18439 [Discriminator loss: 0.131485, acc.: 93.75%] [Generator loss: 7.310127]\n",
      "18440 [Discriminator loss: 0.058369, acc.: 96.88%] [Generator loss: 7.177627]\n",
      "18441 [Discriminator loss: 0.016063, acc.: 100.00%] [Generator loss: 7.056323]\n",
      "18442 [Discriminator loss: 0.076707, acc.: 98.44%] [Generator loss: 8.439752]\n",
      "18443 [Discriminator loss: 0.173578, acc.: 90.62%] [Generator loss: 10.621548]\n",
      "18444 [Discriminator loss: 0.134616, acc.: 95.31%] [Generator loss: 10.408027]\n",
      "18445 [Discriminator loss: 0.169677, acc.: 95.31%] [Generator loss: 7.902534]\n",
      "18446 [Discriminator loss: 0.083019, acc.: 95.31%] [Generator loss: 8.593164]\n",
      "18447 [Discriminator loss: 0.125812, acc.: 93.75%] [Generator loss: 9.204786]\n",
      "18448 [Discriminator loss: 0.028800, acc.: 100.00%] [Generator loss: 10.075847]\n",
      "18449 [Discriminator loss: 0.142809, acc.: 93.75%] [Generator loss: 9.287763]\n",
      "18450 [Discriminator loss: 0.109855, acc.: 96.88%] [Generator loss: 8.621061]\n",
      "18451 [Discriminator loss: 0.008763, acc.: 100.00%] [Generator loss: 9.207607]\n",
      "18452 [Discriminator loss: 0.105876, acc.: 96.88%] [Generator loss: 7.008358]\n",
      "18453 [Discriminator loss: 0.108314, acc.: 93.75%] [Generator loss: 9.424623]\n",
      "18454 [Discriminator loss: 0.027246, acc.: 100.00%] [Generator loss: 10.163113]\n",
      "18455 [Discriminator loss: 0.115609, acc.: 95.31%] [Generator loss: 8.078349]\n",
      "18456 [Discriminator loss: 0.087058, acc.: 96.88%] [Generator loss: 7.304089]\n",
      "18457 [Discriminator loss: 0.086824, acc.: 95.31%] [Generator loss: 8.290565]\n",
      "18458 [Discriminator loss: 0.057557, acc.: 96.88%] [Generator loss: 8.070465]\n",
      "18459 [Discriminator loss: 0.114158, acc.: 92.19%] [Generator loss: 8.283909]\n",
      "18460 [Discriminator loss: 0.056861, acc.: 96.88%] [Generator loss: 8.214465]\n",
      "18461 [Discriminator loss: 0.128046, acc.: 95.31%] [Generator loss: 7.959879]\n",
      "18462 [Discriminator loss: 0.042859, acc.: 96.88%] [Generator loss: 8.379380]\n",
      "18463 [Discriminator loss: 0.057587, acc.: 96.88%] [Generator loss: 8.122198]\n",
      "18464 [Discriminator loss: 0.039917, acc.: 100.00%] [Generator loss: 7.549895]\n",
      "18465 [Discriminator loss: 0.074513, acc.: 96.88%] [Generator loss: 9.614098]\n",
      "18466 [Discriminator loss: 0.115744, acc.: 98.44%] [Generator loss: 8.221546]\n",
      "18467 [Discriminator loss: 0.047627, acc.: 96.88%] [Generator loss: 9.079214]\n",
      "18468 [Discriminator loss: 0.094936, acc.: 95.31%] [Generator loss: 8.644339]\n",
      "18469 [Discriminator loss: 0.054171, acc.: 96.88%] [Generator loss: 7.897884]\n",
      "18470 [Discriminator loss: 0.111483, acc.: 95.31%] [Generator loss: 9.439077]\n",
      "18471 [Discriminator loss: 0.148359, acc.: 95.31%] [Generator loss: 7.257239]\n",
      "18472 [Discriminator loss: 0.079235, acc.: 95.31%] [Generator loss: 6.855162]\n",
      "18473 [Discriminator loss: 0.018011, acc.: 100.00%] [Generator loss: 7.200041]\n",
      "18474 [Discriminator loss: 0.037236, acc.: 98.44%] [Generator loss: 7.221393]\n",
      "18475 [Discriminator loss: 0.051503, acc.: 98.44%] [Generator loss: 7.343198]\n",
      "18476 [Discriminator loss: 0.092471, acc.: 95.31%] [Generator loss: 7.985719]\n",
      "18477 [Discriminator loss: 0.082619, acc.: 95.31%] [Generator loss: 8.639224]\n",
      "18478 [Discriminator loss: 0.079166, acc.: 95.31%] [Generator loss: 8.595087]\n",
      "18479 [Discriminator loss: 0.022099, acc.: 98.44%] [Generator loss: 7.256324]\n",
      "18480 [Discriminator loss: 0.081252, acc.: 93.75%] [Generator loss: 8.304224]\n",
      "18481 [Discriminator loss: 0.055567, acc.: 98.44%] [Generator loss: 7.979160]\n",
      "18482 [Discriminator loss: 0.035374, acc.: 98.44%] [Generator loss: 6.301435]\n",
      "18483 [Discriminator loss: 0.061615, acc.: 95.31%] [Generator loss: 8.425703]\n",
      "18484 [Discriminator loss: 0.064995, acc.: 98.44%] [Generator loss: 9.680029]\n",
      "18485 [Discriminator loss: 0.056992, acc.: 98.44%] [Generator loss: 8.316115]\n",
      "18486 [Discriminator loss: 0.035071, acc.: 98.44%] [Generator loss: 9.362570]\n",
      "18487 [Discriminator loss: 0.075372, acc.: 96.88%] [Generator loss: 8.513356]\n",
      "18488 [Discriminator loss: 0.088762, acc.: 96.88%] [Generator loss: 7.031662]\n",
      "18489 [Discriminator loss: 0.090586, acc.: 92.19%] [Generator loss: 7.871198]\n",
      "18490 [Discriminator loss: 0.053442, acc.: 98.44%] [Generator loss: 8.505598]\n",
      "18491 [Discriminator loss: 0.061863, acc.: 96.88%] [Generator loss: 7.948584]\n",
      "18492 [Discriminator loss: 0.036655, acc.: 100.00%] [Generator loss: 7.623469]\n",
      "18493 [Discriminator loss: 0.018177, acc.: 100.00%] [Generator loss: 8.921050]\n",
      "18494 [Discriminator loss: 0.038029, acc.: 100.00%] [Generator loss: 8.849882]\n",
      "18495 [Discriminator loss: 0.121330, acc.: 93.75%] [Generator loss: 5.197959]\n",
      "18496 [Discriminator loss: 0.170956, acc.: 95.31%] [Generator loss: 8.780232]\n",
      "18497 [Discriminator loss: 0.038288, acc.: 98.44%] [Generator loss: 9.112846]\n",
      "18498 [Discriminator loss: 0.311405, acc.: 92.19%] [Generator loss: 6.495008]\n",
      "18499 [Discriminator loss: 0.086985, acc.: 96.88%] [Generator loss: 9.110100]\n",
      "18500 [Discriminator loss: 0.015812, acc.: 100.00%] [Generator loss: 9.525825]\n",
      "18501 [Discriminator loss: 0.018689, acc.: 100.00%] [Generator loss: 8.808678]\n",
      "18502 [Discriminator loss: 0.026892, acc.: 100.00%] [Generator loss: 7.939515]\n",
      "18503 [Discriminator loss: 0.033209, acc.: 98.44%] [Generator loss: 9.078469]\n",
      "18504 [Discriminator loss: 0.026403, acc.: 100.00%] [Generator loss: 7.646218]\n",
      "18505 [Discriminator loss: 0.109099, acc.: 96.88%] [Generator loss: 9.397178]\n",
      "18506 [Discriminator loss: 0.087540, acc.: 95.31%] [Generator loss: 8.956511]\n",
      "18507 [Discriminator loss: 0.110998, acc.: 95.31%] [Generator loss: 8.434336]\n",
      "18508 [Discriminator loss: 0.022064, acc.: 100.00%] [Generator loss: 8.473992]\n",
      "18509 [Discriminator loss: 0.018138, acc.: 100.00%] [Generator loss: 8.181576]\n",
      "18510 [Discriminator loss: 0.120770, acc.: 96.88%] [Generator loss: 7.636111]\n",
      "18511 [Discriminator loss: 0.107941, acc.: 92.19%] [Generator loss: 8.627078]\n",
      "18512 [Discriminator loss: 0.075395, acc.: 95.31%] [Generator loss: 9.287443]\n",
      "18513 [Discriminator loss: 0.075182, acc.: 95.31%] [Generator loss: 7.619007]\n",
      "18514 [Discriminator loss: 0.064319, acc.: 96.88%] [Generator loss: 8.050711]\n",
      "18515 [Discriminator loss: 0.061645, acc.: 96.88%] [Generator loss: 8.286043]\n",
      "18516 [Discriminator loss: 0.056269, acc.: 96.88%] [Generator loss: 8.521238]\n",
      "18517 [Discriminator loss: 0.017672, acc.: 100.00%] [Generator loss: 8.342052]\n",
      "18518 [Discriminator loss: 0.060682, acc.: 96.88%] [Generator loss: 8.379041]\n",
      "18519 [Discriminator loss: 0.176659, acc.: 92.19%] [Generator loss: 8.905753]\n",
      "18520 [Discriminator loss: 0.092671, acc.: 96.88%] [Generator loss: 8.640048]\n",
      "18521 [Discriminator loss: 0.154313, acc.: 95.31%] [Generator loss: 9.755572]\n",
      "18522 [Discriminator loss: 0.084626, acc.: 96.88%] [Generator loss: 8.237102]\n",
      "18523 [Discriminator loss: 0.108196, acc.: 95.31%] [Generator loss: 7.914994]\n",
      "18524 [Discriminator loss: 0.084468, acc.: 95.31%] [Generator loss: 7.508957]\n",
      "18525 [Discriminator loss: 0.095358, acc.: 96.88%] [Generator loss: 7.171402]\n",
      "18526 [Discriminator loss: 0.240730, acc.: 90.62%] [Generator loss: 8.580299]\n",
      "18527 [Discriminator loss: 0.039607, acc.: 98.44%] [Generator loss: 8.608247]\n",
      "18528 [Discriminator loss: 0.014972, acc.: 100.00%] [Generator loss: 9.463464]\n",
      "18529 [Discriminator loss: 0.084644, acc.: 95.31%] [Generator loss: 8.121576]\n",
      "18530 [Discriminator loss: 0.066005, acc.: 98.44%] [Generator loss: 8.566065]\n",
      "18531 [Discriminator loss: 0.041462, acc.: 100.00%] [Generator loss: 10.223845]\n",
      "18532 [Discriminator loss: 0.037211, acc.: 98.44%] [Generator loss: 8.628634]\n",
      "18533 [Discriminator loss: 0.157363, acc.: 90.62%] [Generator loss: 10.232288]\n",
      "18534 [Discriminator loss: 0.138967, acc.: 92.19%] [Generator loss: 9.879865]\n",
      "18535 [Discriminator loss: 0.030458, acc.: 100.00%] [Generator loss: 8.555758]\n",
      "18536 [Discriminator loss: 0.078527, acc.: 98.44%] [Generator loss: 7.803157]\n",
      "18537 [Discriminator loss: 0.159482, acc.: 90.62%] [Generator loss: 9.981580]\n",
      "18538 [Discriminator loss: 0.021709, acc.: 100.00%] [Generator loss: 11.275598]\n",
      "18539 [Discriminator loss: 0.017292, acc.: 100.00%] [Generator loss: 8.759994]\n",
      "18540 [Discriminator loss: 0.041066, acc.: 98.44%] [Generator loss: 7.875803]\n",
      "18541 [Discriminator loss: 0.098827, acc.: 95.31%] [Generator loss: 7.560369]\n",
      "18542 [Discriminator loss: 0.121047, acc.: 96.88%] [Generator loss: 7.940376]\n",
      "18543 [Discriminator loss: 0.078425, acc.: 98.44%] [Generator loss: 8.003403]\n",
      "18544 [Discriminator loss: 0.024325, acc.: 98.44%] [Generator loss: 6.055874]\n",
      "18545 [Discriminator loss: 0.017251, acc.: 100.00%] [Generator loss: 7.117538]\n",
      "18546 [Discriminator loss: 0.083271, acc.: 96.88%] [Generator loss: 8.479286]\n",
      "18547 [Discriminator loss: 0.119465, acc.: 93.75%] [Generator loss: 7.764664]\n",
      "18548 [Discriminator loss: 0.010340, acc.: 100.00%] [Generator loss: 7.909021]\n",
      "18549 [Discriminator loss: 0.034901, acc.: 100.00%] [Generator loss: 8.105660]\n",
      "18550 [Discriminator loss: 0.108779, acc.: 93.75%] [Generator loss: 8.162314]\n",
      "18551 [Discriminator loss: 0.048115, acc.: 98.44%] [Generator loss: 8.489764]\n",
      "18552 [Discriminator loss: 0.080536, acc.: 98.44%] [Generator loss: 9.494518]\n",
      "18553 [Discriminator loss: 0.046673, acc.: 98.44%] [Generator loss: 9.346695]\n",
      "18554 [Discriminator loss: 0.103265, acc.: 92.19%] [Generator loss: 9.534062]\n",
      "18555 [Discriminator loss: 0.092620, acc.: 95.31%] [Generator loss: 8.197701]\n",
      "18556 [Discriminator loss: 0.049031, acc.: 98.44%] [Generator loss: 8.834566]\n",
      "18557 [Discriminator loss: 0.051604, acc.: 96.88%] [Generator loss: 7.611744]\n",
      "18558 [Discriminator loss: 0.122750, acc.: 95.31%] [Generator loss: 7.530768]\n",
      "18559 [Discriminator loss: 0.037240, acc.: 98.44%] [Generator loss: 9.160928]\n",
      "18560 [Discriminator loss: 0.053701, acc.: 96.88%] [Generator loss: 8.321902]\n",
      "18561 [Discriminator loss: 0.024653, acc.: 98.44%] [Generator loss: 8.867252]\n",
      "18562 [Discriminator loss: 0.024995, acc.: 100.00%] [Generator loss: 7.684278]\n",
      "18563 [Discriminator loss: 0.025973, acc.: 100.00%] [Generator loss: 8.071595]\n",
      "18564 [Discriminator loss: 0.068214, acc.: 96.88%] [Generator loss: 7.200767]\n",
      "18565 [Discriminator loss: 0.022553, acc.: 100.00%] [Generator loss: 5.600914]\n",
      "18566 [Discriminator loss: 0.017721, acc.: 100.00%] [Generator loss: 7.066049]\n",
      "18567 [Discriminator loss: 0.384014, acc.: 89.06%] [Generator loss: 8.883152]\n",
      "18568 [Discriminator loss: 0.029745, acc.: 100.00%] [Generator loss: 10.559762]\n",
      "18569 [Discriminator loss: 0.043828, acc.: 100.00%] [Generator loss: 9.721992]\n",
      "18570 [Discriminator loss: 0.071945, acc.: 95.31%] [Generator loss: 7.592192]\n",
      "18571 [Discriminator loss: 0.063659, acc.: 95.31%] [Generator loss: 7.230775]\n",
      "18572 [Discriminator loss: 0.140538, acc.: 96.88%] [Generator loss: 8.799438]\n",
      "18573 [Discriminator loss: 0.037696, acc.: 98.44%] [Generator loss: 9.070189]\n",
      "18574 [Discriminator loss: 0.037825, acc.: 98.44%] [Generator loss: 9.911957]\n",
      "18575 [Discriminator loss: 0.091716, acc.: 95.31%] [Generator loss: 9.011054]\n",
      "18576 [Discriminator loss: 0.044984, acc.: 98.44%] [Generator loss: 9.244721]\n",
      "18577 [Discriminator loss: 0.146919, acc.: 95.31%] [Generator loss: 10.003031]\n",
      "18578 [Discriminator loss: 0.049157, acc.: 96.88%] [Generator loss: 9.957940]\n",
      "18579 [Discriminator loss: 0.088662, acc.: 93.75%] [Generator loss: 7.780020]\n",
      "18580 [Discriminator loss: 0.072263, acc.: 95.31%] [Generator loss: 8.367798]\n",
      "18581 [Discriminator loss: 0.073527, acc.: 95.31%] [Generator loss: 6.613813]\n",
      "18582 [Discriminator loss: 0.082084, acc.: 95.31%] [Generator loss: 8.204395]\n",
      "18583 [Discriminator loss: 0.020870, acc.: 100.00%] [Generator loss: 8.599286]\n",
      "18584 [Discriminator loss: 0.112228, acc.: 95.31%] [Generator loss: 7.182640]\n",
      "18585 [Discriminator loss: 0.031685, acc.: 100.00%] [Generator loss: 8.687000]\n",
      "18586 [Discriminator loss: 0.135587, acc.: 90.62%] [Generator loss: 8.826096]\n",
      "18587 [Discriminator loss: 0.031843, acc.: 100.00%] [Generator loss: 8.880280]\n",
      "18588 [Discriminator loss: 0.010796, acc.: 100.00%] [Generator loss: 9.719795]\n",
      "18589 [Discriminator loss: 0.065038, acc.: 96.88%] [Generator loss: 8.555230]\n",
      "18590 [Discriminator loss: 0.007023, acc.: 100.00%] [Generator loss: 9.317298]\n",
      "18591 [Discriminator loss: 0.036907, acc.: 100.00%] [Generator loss: 8.793459]\n",
      "18592 [Discriminator loss: 0.003953, acc.: 100.00%] [Generator loss: 9.477324]\n",
      "18593 [Discriminator loss: 0.048039, acc.: 98.44%] [Generator loss: 9.182405]\n",
      "18594 [Discriminator loss: 0.047441, acc.: 98.44%] [Generator loss: 7.258019]\n",
      "18595 [Discriminator loss: 0.041681, acc.: 100.00%] [Generator loss: 7.217475]\n",
      "18596 [Discriminator loss: 0.038228, acc.: 100.00%] [Generator loss: 8.487551]\n",
      "18597 [Discriminator loss: 0.123300, acc.: 93.75%] [Generator loss: 8.295208]\n",
      "18598 [Discriminator loss: 0.033040, acc.: 100.00%] [Generator loss: 6.913574]\n",
      "18599 [Discriminator loss: 0.065505, acc.: 96.88%] [Generator loss: 8.707811]\n",
      "18600 [Discriminator loss: 0.061522, acc.: 96.88%] [Generator loss: 8.647409]\n",
      "18601 [Discriminator loss: 0.063640, acc.: 98.44%] [Generator loss: 8.922892]\n",
      "18602 [Discriminator loss: 0.060795, acc.: 95.31%] [Generator loss: 8.101674]\n",
      "18603 [Discriminator loss: 0.061882, acc.: 98.44%] [Generator loss: 7.725936]\n",
      "18604 [Discriminator loss: 0.079867, acc.: 98.44%] [Generator loss: 7.651475]\n",
      "18605 [Discriminator loss: 0.089606, acc.: 96.88%] [Generator loss: 8.003499]\n",
      "18606 [Discriminator loss: 0.020478, acc.: 100.00%] [Generator loss: 8.995299]\n",
      "18607 [Discriminator loss: 0.131678, acc.: 92.19%] [Generator loss: 7.583037]\n",
      "18608 [Discriminator loss: 0.041793, acc.: 100.00%] [Generator loss: 8.824826]\n",
      "18609 [Discriminator loss: 0.134081, acc.: 96.88%] [Generator loss: 8.720485]\n",
      "18610 [Discriminator loss: 0.132351, acc.: 95.31%] [Generator loss: 9.978615]\n",
      "18611 [Discriminator loss: 0.052006, acc.: 96.88%] [Generator loss: 9.581764]\n",
      "18612 [Discriminator loss: 0.102239, acc.: 96.88%] [Generator loss: 7.823699]\n",
      "18613 [Discriminator loss: 0.060020, acc.: 96.88%] [Generator loss: 7.940824]\n",
      "18614 [Discriminator loss: 0.049607, acc.: 98.44%] [Generator loss: 6.972732]\n",
      "18615 [Discriminator loss: 0.082824, acc.: 96.88%] [Generator loss: 7.802789]\n",
      "18616 [Discriminator loss: 0.052410, acc.: 96.88%] [Generator loss: 9.168613]\n",
      "18617 [Discriminator loss: 0.012154, acc.: 100.00%] [Generator loss: 8.865596]\n",
      "18618 [Discriminator loss: 0.108751, acc.: 93.75%] [Generator loss: 8.391353]\n",
      "18619 [Discriminator loss: 0.070226, acc.: 96.88%] [Generator loss: 8.392165]\n",
      "18620 [Discriminator loss: 0.063488, acc.: 98.44%] [Generator loss: 9.066708]\n",
      "18621 [Discriminator loss: 0.075134, acc.: 98.44%] [Generator loss: 8.118596]\n",
      "18622 [Discriminator loss: 0.159199, acc.: 92.19%] [Generator loss: 8.138777]\n",
      "18623 [Discriminator loss: 0.039468, acc.: 98.44%] [Generator loss: 8.456459]\n",
      "18624 [Discriminator loss: 0.115918, acc.: 93.75%] [Generator loss: 9.066802]\n",
      "18625 [Discriminator loss: 0.016441, acc.: 100.00%] [Generator loss: 9.060266]\n",
      "18626 [Discriminator loss: 0.055878, acc.: 96.88%] [Generator loss: 8.571679]\n",
      "18627 [Discriminator loss: 0.131246, acc.: 93.75%] [Generator loss: 7.456304]\n",
      "18628 [Discriminator loss: 0.054686, acc.: 98.44%] [Generator loss: 6.954316]\n",
      "18629 [Discriminator loss: 0.221759, acc.: 96.88%] [Generator loss: 8.049021]\n",
      "18630 [Discriminator loss: 0.119025, acc.: 93.75%] [Generator loss: 8.629362]\n",
      "18631 [Discriminator loss: 0.043041, acc.: 98.44%] [Generator loss: 9.349868]\n",
      "18632 [Discriminator loss: 0.072672, acc.: 96.88%] [Generator loss: 9.113049]\n",
      "18633 [Discriminator loss: 0.211054, acc.: 95.31%] [Generator loss: 9.576171]\n",
      "18634 [Discriminator loss: 0.042173, acc.: 96.88%] [Generator loss: 9.387002]\n",
      "18635 [Discriminator loss: 0.020453, acc.: 100.00%] [Generator loss: 9.846909]\n",
      "18636 [Discriminator loss: 0.029457, acc.: 98.44%] [Generator loss: 10.906330]\n",
      "18637 [Discriminator loss: 0.102991, acc.: 90.62%] [Generator loss: 8.376622]\n",
      "18638 [Discriminator loss: 0.011830, acc.: 100.00%] [Generator loss: 8.445900]\n",
      "18639 [Discriminator loss: 0.167100, acc.: 90.62%] [Generator loss: 8.974974]\n",
      "18640 [Discriminator loss: 0.069158, acc.: 98.44%] [Generator loss: 11.063859]\n",
      "18641 [Discriminator loss: 0.066211, acc.: 96.88%] [Generator loss: 7.713854]\n",
      "18642 [Discriminator loss: 0.074044, acc.: 96.88%] [Generator loss: 7.059403]\n",
      "18643 [Discriminator loss: 0.061115, acc.: 98.44%] [Generator loss: 8.209270]\n",
      "18644 [Discriminator loss: 0.104150, acc.: 96.88%] [Generator loss: 8.372610]\n",
      "18645 [Discriminator loss: 0.029973, acc.: 100.00%] [Generator loss: 8.642919]\n",
      "18646 [Discriminator loss: 0.029198, acc.: 98.44%] [Generator loss: 9.020271]\n",
      "18647 [Discriminator loss: 0.053903, acc.: 96.88%] [Generator loss: 7.867416]\n",
      "18648 [Discriminator loss: 0.053851, acc.: 98.44%] [Generator loss: 7.470885]\n",
      "18649 [Discriminator loss: 0.164088, acc.: 93.75%] [Generator loss: 7.738818]\n",
      "18650 [Discriminator loss: 0.050772, acc.: 98.44%] [Generator loss: 8.500736]\n",
      "18651 [Discriminator loss: 0.015578, acc.: 100.00%] [Generator loss: 10.732964]\n",
      "18652 [Discriminator loss: 0.035547, acc.: 96.88%] [Generator loss: 8.336992]\n",
      "18653 [Discriminator loss: 0.141939, acc.: 95.31%] [Generator loss: 7.377238]\n",
      "18654 [Discriminator loss: 0.031555, acc.: 98.44%] [Generator loss: 7.781325]\n",
      "18655 [Discriminator loss: 0.025506, acc.: 100.00%] [Generator loss: 8.333330]\n",
      "18656 [Discriminator loss: 0.017670, acc.: 100.00%] [Generator loss: 8.506466]\n",
      "18657 [Discriminator loss: 0.091457, acc.: 96.88%] [Generator loss: 7.941517]\n",
      "18658 [Discriminator loss: 0.047709, acc.: 98.44%] [Generator loss: 7.788122]\n",
      "18659 [Discriminator loss: 0.122887, acc.: 93.75%] [Generator loss: 7.679509]\n",
      "18660 [Discriminator loss: 0.062104, acc.: 98.44%] [Generator loss: 8.742914]\n",
      "18661 [Discriminator loss: 0.058695, acc.: 96.88%] [Generator loss: 8.131370]\n",
      "18662 [Discriminator loss: 0.075982, acc.: 96.88%] [Generator loss: 8.872217]\n",
      "18663 [Discriminator loss: 0.036904, acc.: 98.44%] [Generator loss: 8.452940]\n",
      "18664 [Discriminator loss: 0.017625, acc.: 100.00%] [Generator loss: 8.477484]\n",
      "18665 [Discriminator loss: 0.062265, acc.: 98.44%] [Generator loss: 9.080938]\n",
      "18666 [Discriminator loss: 0.064097, acc.: 96.88%] [Generator loss: 7.341147]\n",
      "18667 [Discriminator loss: 0.130106, acc.: 96.88%] [Generator loss: 8.973297]\n",
      "18668 [Discriminator loss: 0.019906, acc.: 100.00%] [Generator loss: 8.425337]\n",
      "18669 [Discriminator loss: 0.045859, acc.: 96.88%] [Generator loss: 6.792822]\n",
      "18670 [Discriminator loss: 0.105363, acc.: 96.88%] [Generator loss: 6.283607]\n",
      "18671 [Discriminator loss: 0.176041, acc.: 92.19%] [Generator loss: 9.345836]\n",
      "18672 [Discriminator loss: 0.005673, acc.: 100.00%] [Generator loss: 9.224648]\n",
      "18673 [Discriminator loss: 0.090276, acc.: 96.88%] [Generator loss: 6.673476]\n",
      "18674 [Discriminator loss: 0.050227, acc.: 98.44%] [Generator loss: 6.875735]\n",
      "18675 [Discriminator loss: 0.049964, acc.: 98.44%] [Generator loss: 7.605327]\n",
      "18676 [Discriminator loss: 0.003806, acc.: 100.00%] [Generator loss: 8.560127]\n",
      "18677 [Discriminator loss: 0.062845, acc.: 98.44%] [Generator loss: 9.393423]\n",
      "18678 [Discriminator loss: 0.124325, acc.: 96.88%] [Generator loss: 7.451401]\n",
      "18679 [Discriminator loss: 0.018665, acc.: 100.00%] [Generator loss: 8.910652]\n",
      "18680 [Discriminator loss: 0.091294, acc.: 95.31%] [Generator loss: 6.735307]\n",
      "18681 [Discriminator loss: 0.110591, acc.: 96.88%] [Generator loss: 7.295860]\n",
      "18682 [Discriminator loss: 0.035489, acc.: 98.44%] [Generator loss: 9.432556]\n",
      "18683 [Discriminator loss: 0.064856, acc.: 98.44%] [Generator loss: 10.191967]\n",
      "18684 [Discriminator loss: 0.040886, acc.: 98.44%] [Generator loss: 8.538004]\n",
      "18685 [Discriminator loss: 0.114280, acc.: 93.75%] [Generator loss: 8.157952]\n",
      "18686 [Discriminator loss: 0.082530, acc.: 98.44%] [Generator loss: 8.649290]\n",
      "18687 [Discriminator loss: 0.015389, acc.: 100.00%] [Generator loss: 8.763750]\n",
      "18688 [Discriminator loss: 0.046249, acc.: 96.88%] [Generator loss: 8.677731]\n",
      "18689 [Discriminator loss: 0.033000, acc.: 98.44%] [Generator loss: 7.953970]\n",
      "18690 [Discriminator loss: 0.130379, acc.: 95.31%] [Generator loss: 10.074068]\n",
      "18691 [Discriminator loss: 0.052194, acc.: 96.88%] [Generator loss: 9.988590]\n",
      "18692 [Discriminator loss: 0.011734, acc.: 100.00%] [Generator loss: 9.305128]\n",
      "18693 [Discriminator loss: 0.097084, acc.: 95.31%] [Generator loss: 7.443299]\n",
      "18694 [Discriminator loss: 0.047601, acc.: 98.44%] [Generator loss: 8.383197]\n",
      "18695 [Discriminator loss: 0.041064, acc.: 98.44%] [Generator loss: 8.071911]\n",
      "18696 [Discriminator loss: 0.065020, acc.: 98.44%] [Generator loss: 8.416385]\n",
      "18697 [Discriminator loss: 0.050059, acc.: 96.88%] [Generator loss: 8.207139]\n",
      "18698 [Discriminator loss: 0.008316, acc.: 100.00%] [Generator loss: 8.978028]\n",
      "18699 [Discriminator loss: 0.098249, acc.: 96.88%] [Generator loss: 8.632995]\n",
      "18700 [Discriminator loss: 0.077582, acc.: 95.31%] [Generator loss: 9.582801]\n",
      "18701 [Discriminator loss: 0.036429, acc.: 98.44%] [Generator loss: 8.811659]\n",
      "18702 [Discriminator loss: 0.014135, acc.: 100.00%] [Generator loss: 8.435739]\n",
      "18703 [Discriminator loss: 0.226719, acc.: 89.06%] [Generator loss: 6.583669]\n",
      "18704 [Discriminator loss: 0.044305, acc.: 98.44%] [Generator loss: 7.494417]\n",
      "18705 [Discriminator loss: 0.037317, acc.: 98.44%] [Generator loss: 8.456947]\n",
      "18706 [Discriminator loss: 0.007546, acc.: 100.00%] [Generator loss: 10.479314]\n",
      "18707 [Discriminator loss: 0.024287, acc.: 100.00%] [Generator loss: 8.987370]\n",
      "18708 [Discriminator loss: 0.069148, acc.: 96.88%] [Generator loss: 6.149682]\n",
      "18709 [Discriminator loss: 0.108885, acc.: 95.31%] [Generator loss: 8.167294]\n",
      "18710 [Discriminator loss: 0.045390, acc.: 98.44%] [Generator loss: 8.820075]\n",
      "18711 [Discriminator loss: 0.033503, acc.: 98.44%] [Generator loss: 7.550833]\n",
      "18712 [Discriminator loss: 0.015294, acc.: 100.00%] [Generator loss: 9.819317]\n",
      "18713 [Discriminator loss: 0.138246, acc.: 96.88%] [Generator loss: 8.538078]\n",
      "18714 [Discriminator loss: 0.036901, acc.: 98.44%] [Generator loss: 9.693727]\n",
      "18715 [Discriminator loss: 0.021342, acc.: 100.00%] [Generator loss: 8.890814]\n",
      "18716 [Discriminator loss: 0.029667, acc.: 98.44%] [Generator loss: 8.006357]\n",
      "18717 [Discriminator loss: 0.054128, acc.: 98.44%] [Generator loss: 8.235222]\n",
      "18718 [Discriminator loss: 0.012605, acc.: 100.00%] [Generator loss: 9.869258]\n",
      "18719 [Discriminator loss: 0.058260, acc.: 98.44%] [Generator loss: 7.491845]\n",
      "18720 [Discriminator loss: 0.028908, acc.: 98.44%] [Generator loss: 8.170229]\n",
      "18721 [Discriminator loss: 0.089425, acc.: 98.44%] [Generator loss: 8.256483]\n",
      "18722 [Discriminator loss: 0.039193, acc.: 100.00%] [Generator loss: 8.318851]\n",
      "18723 [Discriminator loss: 0.117571, acc.: 96.88%] [Generator loss: 8.224614]\n",
      "18724 [Discriminator loss: 0.026486, acc.: 100.00%] [Generator loss: 7.289017]\n",
      "18725 [Discriminator loss: 0.048601, acc.: 96.88%] [Generator loss: 8.799983]\n",
      "18726 [Discriminator loss: 0.057362, acc.: 98.44%] [Generator loss: 9.071053]\n",
      "18727 [Discriminator loss: 0.046673, acc.: 98.44%] [Generator loss: 8.084803]\n",
      "18728 [Discriminator loss: 0.034577, acc.: 98.44%] [Generator loss: 6.716053]\n",
      "18729 [Discriminator loss: 0.079960, acc.: 96.88%] [Generator loss: 8.951548]\n",
      "18730 [Discriminator loss: 0.045749, acc.: 98.44%] [Generator loss: 9.130629]\n",
      "18731 [Discriminator loss: 0.125527, acc.: 93.75%] [Generator loss: 6.543314]\n",
      "18732 [Discriminator loss: 0.046874, acc.: 98.44%] [Generator loss: 6.000371]\n",
      "18733 [Discriminator loss: 0.035713, acc.: 100.00%] [Generator loss: 7.091737]\n",
      "18734 [Discriminator loss: 0.123016, acc.: 95.31%] [Generator loss: 7.043396]\n",
      "18735 [Discriminator loss: 0.083586, acc.: 95.31%] [Generator loss: 8.681751]\n",
      "18736 [Discriminator loss: 0.034739, acc.: 100.00%] [Generator loss: 9.955591]\n",
      "18737 [Discriminator loss: 0.043135, acc.: 96.88%] [Generator loss: 7.681953]\n",
      "18738 [Discriminator loss: 0.036840, acc.: 98.44%] [Generator loss: 7.395535]\n",
      "18739 [Discriminator loss: 0.035604, acc.: 100.00%] [Generator loss: 7.804078]\n",
      "18740 [Discriminator loss: 0.050368, acc.: 98.44%] [Generator loss: 6.170761]\n",
      "18741 [Discriminator loss: 0.021694, acc.: 100.00%] [Generator loss: 6.526524]\n",
      "18742 [Discriminator loss: 0.047906, acc.: 98.44%] [Generator loss: 6.692127]\n",
      "18743 [Discriminator loss: 0.059697, acc.: 96.88%] [Generator loss: 8.424642]\n",
      "18744 [Discriminator loss: 0.032137, acc.: 100.00%] [Generator loss: 7.302800]\n",
      "18745 [Discriminator loss: 0.005889, acc.: 100.00%] [Generator loss: 8.506960]\n",
      "18746 [Discriminator loss: 0.131877, acc.: 92.19%] [Generator loss: 8.819736]\n",
      "18747 [Discriminator loss: 0.048718, acc.: 96.88%] [Generator loss: 9.431625]\n",
      "18748 [Discriminator loss: 0.173028, acc.: 92.19%] [Generator loss: 7.662332]\n",
      "18749 [Discriminator loss: 0.021183, acc.: 100.00%] [Generator loss: 8.987941]\n",
      "18750 [Discriminator loss: 0.179915, acc.: 90.62%] [Generator loss: 10.096132]\n",
      "18751 [Discriminator loss: 0.071840, acc.: 96.88%] [Generator loss: 10.067004]\n",
      "18752 [Discriminator loss: 0.057516, acc.: 98.44%] [Generator loss: 8.522943]\n",
      "18753 [Discriminator loss: 0.036611, acc.: 100.00%] [Generator loss: 9.054060]\n",
      "18754 [Discriminator loss: 0.043260, acc.: 98.44%] [Generator loss: 8.289107]\n",
      "18755 [Discriminator loss: 0.063397, acc.: 98.44%] [Generator loss: 8.535023]\n",
      "18756 [Discriminator loss: 0.096552, acc.: 98.44%] [Generator loss: 8.623900]\n",
      "18757 [Discriminator loss: 0.070657, acc.: 96.88%] [Generator loss: 9.323920]\n",
      "18758 [Discriminator loss: 0.039334, acc.: 96.88%] [Generator loss: 8.101618]\n",
      "18759 [Discriminator loss: 0.015889, acc.: 100.00%] [Generator loss: 7.796832]\n",
      "18760 [Discriminator loss: 0.060154, acc.: 98.44%] [Generator loss: 8.593140]\n",
      "18761 [Discriminator loss: 0.147322, acc.: 96.88%] [Generator loss: 8.246620]\n",
      "18762 [Discriminator loss: 0.125917, acc.: 96.88%] [Generator loss: 8.491608]\n",
      "18763 [Discriminator loss: 0.040035, acc.: 100.00%] [Generator loss: 9.412411]\n",
      "18764 [Discriminator loss: 0.130201, acc.: 96.88%] [Generator loss: 7.414886]\n",
      "18765 [Discriminator loss: 0.218661, acc.: 96.88%] [Generator loss: 10.173681]\n",
      "18766 [Discriminator loss: 0.075808, acc.: 95.31%] [Generator loss: 10.442488]\n",
      "18767 [Discriminator loss: 0.087071, acc.: 96.88%] [Generator loss: 8.971931]\n",
      "18768 [Discriminator loss: 0.114928, acc.: 98.44%] [Generator loss: 7.200513]\n",
      "18769 [Discriminator loss: 0.021624, acc.: 98.44%] [Generator loss: 7.648324]\n",
      "18770 [Discriminator loss: 0.031249, acc.: 98.44%] [Generator loss: 8.049356]\n",
      "18771 [Discriminator loss: 0.075250, acc.: 98.44%] [Generator loss: 9.661417]\n",
      "18772 [Discriminator loss: 0.115774, acc.: 95.31%] [Generator loss: 6.999229]\n",
      "18773 [Discriminator loss: 0.096249, acc.: 98.44%] [Generator loss: 8.270788]\n",
      "18774 [Discriminator loss: 0.108743, acc.: 95.31%] [Generator loss: 7.881955]\n",
      "18775 [Discriminator loss: 0.173805, acc.: 90.62%] [Generator loss: 9.335146]\n",
      "18776 [Discriminator loss: 0.043663, acc.: 98.44%] [Generator loss: 9.095304]\n",
      "18777 [Discriminator loss: 0.079190, acc.: 95.31%] [Generator loss: 7.371447]\n",
      "18778 [Discriminator loss: 0.166013, acc.: 95.31%] [Generator loss: 7.761343]\n",
      "18779 [Discriminator loss: 0.136732, acc.: 95.31%] [Generator loss: 9.451992]\n",
      "18780 [Discriminator loss: 0.118828, acc.: 95.31%] [Generator loss: 8.401988]\n",
      "18781 [Discriminator loss: 0.067958, acc.: 98.44%] [Generator loss: 8.078312]\n",
      "18782 [Discriminator loss: 0.022443, acc.: 98.44%] [Generator loss: 8.121112]\n",
      "18783 [Discriminator loss: 0.074182, acc.: 96.88%] [Generator loss: 7.646773]\n",
      "18784 [Discriminator loss: 0.016479, acc.: 100.00%] [Generator loss: 7.757857]\n",
      "18785 [Discriminator loss: 0.120166, acc.: 93.75%] [Generator loss: 9.808825]\n",
      "18786 [Discriminator loss: 0.072624, acc.: 98.44%] [Generator loss: 9.104963]\n",
      "18787 [Discriminator loss: 0.078192, acc.: 96.88%] [Generator loss: 7.537534]\n",
      "18788 [Discriminator loss: 0.033083, acc.: 100.00%] [Generator loss: 5.731547]\n",
      "18789 [Discriminator loss: 0.157304, acc.: 93.75%] [Generator loss: 8.624096]\n",
      "18790 [Discriminator loss: 0.028574, acc.: 98.44%] [Generator loss: 9.804171]\n",
      "18791 [Discriminator loss: 0.183398, acc.: 93.75%] [Generator loss: 7.296924]\n",
      "18792 [Discriminator loss: 0.194546, acc.: 96.88%] [Generator loss: 7.821315]\n",
      "18793 [Discriminator loss: 0.118408, acc.: 98.44%] [Generator loss: 9.838840]\n",
      "18794 [Discriminator loss: 0.034240, acc.: 98.44%] [Generator loss: 8.728248]\n",
      "18795 [Discriminator loss: 0.072666, acc.: 95.31%] [Generator loss: 8.934420]\n",
      "18796 [Discriminator loss: 0.025785, acc.: 100.00%] [Generator loss: 8.836578]\n",
      "18797 [Discriminator loss: 0.111813, acc.: 93.75%] [Generator loss: 7.868113]\n",
      "18798 [Discriminator loss: 0.105667, acc.: 95.31%] [Generator loss: 10.546382]\n",
      "18799 [Discriminator loss: 0.038883, acc.: 100.00%] [Generator loss: 9.594667]\n",
      "18800 [Discriminator loss: 0.144107, acc.: 95.31%] [Generator loss: 8.328567]\n",
      "18801 [Discriminator loss: 0.031736, acc.: 98.44%] [Generator loss: 8.789796]\n",
      "18802 [Discriminator loss: 0.044666, acc.: 98.44%] [Generator loss: 8.597241]\n",
      "18803 [Discriminator loss: 0.093414, acc.: 98.44%] [Generator loss: 9.760088]\n",
      "18804 [Discriminator loss: 0.021716, acc.: 98.44%] [Generator loss: 8.147523]\n",
      "18805 [Discriminator loss: 0.063765, acc.: 96.88%] [Generator loss: 6.983197]\n",
      "18806 [Discriminator loss: 0.013435, acc.: 100.00%] [Generator loss: 7.438939]\n",
      "18807 [Discriminator loss: 0.015114, acc.: 100.00%] [Generator loss: 7.753964]\n",
      "18808 [Discriminator loss: 0.046554, acc.: 98.44%] [Generator loss: 6.858707]\n",
      "18809 [Discriminator loss: 0.058868, acc.: 96.88%] [Generator loss: 5.762985]\n",
      "18810 [Discriminator loss: 0.086288, acc.: 95.31%] [Generator loss: 8.308088]\n",
      "18811 [Discriminator loss: 0.014278, acc.: 100.00%] [Generator loss: 8.107510]\n",
      "18812 [Discriminator loss: 0.058679, acc.: 96.88%] [Generator loss: 9.392452]\n",
      "18813 [Discriminator loss: 0.058632, acc.: 96.88%] [Generator loss: 8.444307]\n",
      "18814 [Discriminator loss: 0.088619, acc.: 95.31%] [Generator loss: 8.329369]\n",
      "18815 [Discriminator loss: 0.060660, acc.: 96.88%] [Generator loss: 11.222118]\n",
      "18816 [Discriminator loss: 0.061484, acc.: 96.88%] [Generator loss: 9.973083]\n",
      "18817 [Discriminator loss: 0.140263, acc.: 93.75%] [Generator loss: 7.280850]\n",
      "18818 [Discriminator loss: 0.081798, acc.: 95.31%] [Generator loss: 7.448772]\n",
      "18819 [Discriminator loss: 0.146135, acc.: 93.75%] [Generator loss: 7.474169]\n",
      "18820 [Discriminator loss: 0.029882, acc.: 100.00%] [Generator loss: 9.602343]\n",
      "18821 [Discriminator loss: 0.046789, acc.: 96.88%] [Generator loss: 9.088295]\n",
      "18822 [Discriminator loss: 0.084992, acc.: 98.44%] [Generator loss: 9.168240]\n",
      "18823 [Discriminator loss: 0.065842, acc.: 95.31%] [Generator loss: 9.345542]\n",
      "18824 [Discriminator loss: 0.065941, acc.: 98.44%] [Generator loss: 8.874428]\n",
      "18825 [Discriminator loss: 0.097992, acc.: 96.88%] [Generator loss: 6.764426]\n",
      "18826 [Discriminator loss: 0.150443, acc.: 98.44%] [Generator loss: 8.409822]\n",
      "18827 [Discriminator loss: 0.095489, acc.: 95.31%] [Generator loss: 11.261545]\n",
      "18828 [Discriminator loss: 0.049117, acc.: 96.88%] [Generator loss: 9.580435]\n",
      "18829 [Discriminator loss: 0.015206, acc.: 100.00%] [Generator loss: 8.749731]\n",
      "18830 [Discriminator loss: 0.194669, acc.: 92.19%] [Generator loss: 8.189970]\n",
      "18831 [Discriminator loss: 0.100713, acc.: 98.44%] [Generator loss: 6.986624]\n",
      "18832 [Discriminator loss: 0.060960, acc.: 98.44%] [Generator loss: 8.974066]\n",
      "18833 [Discriminator loss: 0.119528, acc.: 95.31%] [Generator loss: 7.934991]\n",
      "18834 [Discriminator loss: 0.058787, acc.: 98.44%] [Generator loss: 8.079313]\n",
      "18835 [Discriminator loss: 0.049468, acc.: 100.00%] [Generator loss: 7.507922]\n",
      "18836 [Discriminator loss: 0.085632, acc.: 96.88%] [Generator loss: 8.341450]\n",
      "18837 [Discriminator loss: 0.024228, acc.: 98.44%] [Generator loss: 10.161202]\n",
      "18838 [Discriminator loss: 0.068126, acc.: 96.88%] [Generator loss: 9.487064]\n",
      "18839 [Discriminator loss: 0.011765, acc.: 100.00%] [Generator loss: 9.479792]\n",
      "18840 [Discriminator loss: 0.158039, acc.: 92.19%] [Generator loss: 6.251287]\n",
      "18841 [Discriminator loss: 0.095431, acc.: 95.31%] [Generator loss: 6.457373]\n",
      "18842 [Discriminator loss: 0.007520, acc.: 100.00%] [Generator loss: 9.119854]\n",
      "18843 [Discriminator loss: 0.013949, acc.: 100.00%] [Generator loss: 8.321766]\n",
      "18844 [Discriminator loss: 0.008588, acc.: 100.00%] [Generator loss: 8.226763]\n",
      "18845 [Discriminator loss: 0.073442, acc.: 98.44%] [Generator loss: 8.201228]\n",
      "18846 [Discriminator loss: 0.024973, acc.: 98.44%] [Generator loss: 10.158729]\n",
      "18847 [Discriminator loss: 0.247727, acc.: 92.19%] [Generator loss: 8.206520]\n",
      "18848 [Discriminator loss: 0.086053, acc.: 96.88%] [Generator loss: 9.229013]\n",
      "18849 [Discriminator loss: 0.014112, acc.: 100.00%] [Generator loss: 9.711376]\n",
      "18850 [Discriminator loss: 0.267403, acc.: 84.38%] [Generator loss: 9.121034]\n",
      "18851 [Discriminator loss: 0.139670, acc.: 98.44%] [Generator loss: 7.905318]\n",
      "18852 [Discriminator loss: 0.109756, acc.: 95.31%] [Generator loss: 6.537706]\n",
      "18853 [Discriminator loss: 0.176815, acc.: 93.75%] [Generator loss: 9.978516]\n",
      "18854 [Discriminator loss: 0.051699, acc.: 98.44%] [Generator loss: 9.832182]\n",
      "18855 [Discriminator loss: 0.059839, acc.: 96.88%] [Generator loss: 7.013877]\n",
      "18856 [Discriminator loss: 0.092891, acc.: 95.31%] [Generator loss: 10.186783]\n",
      "18857 [Discriminator loss: 0.093018, acc.: 95.31%] [Generator loss: 10.065066]\n",
      "18858 [Discriminator loss: 0.131917, acc.: 93.75%] [Generator loss: 8.030316]\n",
      "18859 [Discriminator loss: 0.009850, acc.: 100.00%] [Generator loss: 7.256479]\n",
      "18860 [Discriminator loss: 0.062709, acc.: 98.44%] [Generator loss: 9.673818]\n",
      "18861 [Discriminator loss: 0.025622, acc.: 100.00%] [Generator loss: 9.125267]\n",
      "18862 [Discriminator loss: 0.066072, acc.: 96.88%] [Generator loss: 8.528229]\n",
      "18863 [Discriminator loss: 0.033036, acc.: 98.44%] [Generator loss: 8.999555]\n",
      "18864 [Discriminator loss: 0.026993, acc.: 100.00%] [Generator loss: 8.324396]\n",
      "18865 [Discriminator loss: 0.051642, acc.: 98.44%] [Generator loss: 8.636928]\n",
      "18866 [Discriminator loss: 0.083410, acc.: 93.75%] [Generator loss: 8.457642]\n",
      "18867 [Discriminator loss: 0.028421, acc.: 98.44%] [Generator loss: 9.748887]\n",
      "18868 [Discriminator loss: 0.036972, acc.: 100.00%] [Generator loss: 8.720856]\n",
      "18869 [Discriminator loss: 0.087898, acc.: 95.31%] [Generator loss: 9.432142]\n",
      "18870 [Discriminator loss: 0.091686, acc.: 96.88%] [Generator loss: 9.060039]\n",
      "18871 [Discriminator loss: 0.025902, acc.: 98.44%] [Generator loss: 9.748188]\n",
      "18872 [Discriminator loss: 0.261241, acc.: 89.06%] [Generator loss: 7.126601]\n",
      "18873 [Discriminator loss: 0.019777, acc.: 98.44%] [Generator loss: 9.412394]\n",
      "18874 [Discriminator loss: 0.183643, acc.: 92.19%] [Generator loss: 9.298714]\n",
      "18875 [Discriminator loss: 0.046759, acc.: 100.00%] [Generator loss: 8.520803]\n",
      "18876 [Discriminator loss: 0.112810, acc.: 96.88%] [Generator loss: 9.426179]\n",
      "18877 [Discriminator loss: 0.034298, acc.: 100.00%] [Generator loss: 9.103939]\n",
      "18878 [Discriminator loss: 0.185101, acc.: 96.88%] [Generator loss: 8.332162]\n",
      "18879 [Discriminator loss: 0.045861, acc.: 98.44%] [Generator loss: 7.771345]\n",
      "18880 [Discriminator loss: 0.050451, acc.: 98.44%] [Generator loss: 9.057471]\n",
      "18881 [Discriminator loss: 0.024208, acc.: 98.44%] [Generator loss: 8.205820]\n",
      "18882 [Discriminator loss: 0.016314, acc.: 100.00%] [Generator loss: 7.768284]\n",
      "18883 [Discriminator loss: 0.011553, acc.: 100.00%] [Generator loss: 7.258886]\n",
      "18884 [Discriminator loss: 0.007365, acc.: 100.00%] [Generator loss: 6.821538]\n",
      "18885 [Discriminator loss: 0.014740, acc.: 100.00%] [Generator loss: 8.885993]\n",
      "18886 [Discriminator loss: 0.047699, acc.: 98.44%] [Generator loss: 9.001211]\n",
      "18887 [Discriminator loss: 0.137267, acc.: 92.19%] [Generator loss: 8.003704]\n",
      "18888 [Discriminator loss: 0.033806, acc.: 98.44%] [Generator loss: 9.106343]\n",
      "18889 [Discriminator loss: 0.034341, acc.: 98.44%] [Generator loss: 8.316616]\n",
      "18890 [Discriminator loss: 0.100969, acc.: 95.31%] [Generator loss: 9.237720]\n",
      "18891 [Discriminator loss: 0.073113, acc.: 95.31%] [Generator loss: 8.448477]\n",
      "18892 [Discriminator loss: 0.039396, acc.: 100.00%] [Generator loss: 8.000702]\n",
      "18893 [Discriminator loss: 0.009686, acc.: 100.00%] [Generator loss: 8.248002]\n",
      "18894 [Discriminator loss: 0.107226, acc.: 95.31%] [Generator loss: 8.756462]\n",
      "18895 [Discriminator loss: 0.053988, acc.: 98.44%] [Generator loss: 9.065521]\n",
      "18896 [Discriminator loss: 0.008041, acc.: 100.00%] [Generator loss: 10.223269]\n",
      "18897 [Discriminator loss: 0.118809, acc.: 98.44%] [Generator loss: 10.481989]\n",
      "18898 [Discriminator loss: 0.180339, acc.: 95.31%] [Generator loss: 6.951694]\n",
      "18899 [Discriminator loss: 0.075970, acc.: 96.88%] [Generator loss: 7.686681]\n",
      "18900 [Discriminator loss: 0.009621, acc.: 100.00%] [Generator loss: 7.790983]\n",
      "18901 [Discriminator loss: 0.058823, acc.: 96.88%] [Generator loss: 7.854373]\n",
      "18902 [Discriminator loss: 0.137217, acc.: 95.31%] [Generator loss: 7.166827]\n",
      "18903 [Discriminator loss: 0.090095, acc.: 98.44%] [Generator loss: 9.215973]\n",
      "18904 [Discriminator loss: 0.089150, acc.: 96.88%] [Generator loss: 10.175141]\n",
      "18905 [Discriminator loss: 0.077374, acc.: 96.88%] [Generator loss: 8.228109]\n",
      "18906 [Discriminator loss: 0.146519, acc.: 93.75%] [Generator loss: 7.357834]\n",
      "18907 [Discriminator loss: 0.047774, acc.: 100.00%] [Generator loss: 7.050007]\n",
      "18908 [Discriminator loss: 0.052778, acc.: 96.88%] [Generator loss: 7.784159]\n",
      "18909 [Discriminator loss: 0.097329, acc.: 96.88%] [Generator loss: 9.726927]\n",
      "18910 [Discriminator loss: 0.014864, acc.: 100.00%] [Generator loss: 8.044849]\n",
      "18911 [Discriminator loss: 0.030716, acc.: 100.00%] [Generator loss: 8.917642]\n",
      "18912 [Discriminator loss: 0.038558, acc.: 98.44%] [Generator loss: 8.105915]\n",
      "18913 [Discriminator loss: 0.094230, acc.: 96.88%] [Generator loss: 8.087627]\n",
      "18914 [Discriminator loss: 0.158528, acc.: 92.19%] [Generator loss: 9.315788]\n",
      "18915 [Discriminator loss: 0.011766, acc.: 100.00%] [Generator loss: 9.204235]\n",
      "18916 [Discriminator loss: 0.343189, acc.: 87.50%] [Generator loss: 6.601549]\n",
      "18917 [Discriminator loss: 0.073678, acc.: 95.31%] [Generator loss: 6.990963]\n",
      "18918 [Discriminator loss: 0.096889, acc.: 98.44%] [Generator loss: 9.573831]\n",
      "18919 [Discriminator loss: 0.039067, acc.: 98.44%] [Generator loss: 8.621140]\n",
      "18920 [Discriminator loss: 0.048816, acc.: 98.44%] [Generator loss: 8.415211]\n",
      "18921 [Discriminator loss: 0.066439, acc.: 98.44%] [Generator loss: 8.316236]\n",
      "18922 [Discriminator loss: 0.037187, acc.: 100.00%] [Generator loss: 7.812694]\n",
      "18923 [Discriminator loss: 0.129985, acc.: 93.75%] [Generator loss: 9.514238]\n",
      "18924 [Discriminator loss: 0.100762, acc.: 95.31%] [Generator loss: 8.975561]\n",
      "18925 [Discriminator loss: 0.015277, acc.: 100.00%] [Generator loss: 7.919208]\n",
      "18926 [Discriminator loss: 0.076410, acc.: 96.88%] [Generator loss: 5.946982]\n",
      "18927 [Discriminator loss: 0.161912, acc.: 95.31%] [Generator loss: 7.405608]\n",
      "18928 [Discriminator loss: 0.076487, acc.: 96.88%] [Generator loss: 8.848059]\n",
      "18929 [Discriminator loss: 0.235522, acc.: 90.62%] [Generator loss: 8.125463]\n",
      "18930 [Discriminator loss: 0.025148, acc.: 100.00%] [Generator loss: 8.869831]\n",
      "18931 [Discriminator loss: 0.044042, acc.: 98.44%] [Generator loss: 8.363726]\n",
      "18932 [Discriminator loss: 0.040946, acc.: 98.44%] [Generator loss: 7.973160]\n",
      "18933 [Discriminator loss: 0.061448, acc.: 98.44%] [Generator loss: 7.781214]\n",
      "18934 [Discriminator loss: 0.071553, acc.: 98.44%] [Generator loss: 7.595931]\n",
      "18935 [Discriminator loss: 0.133777, acc.: 93.75%] [Generator loss: 9.399746]\n",
      "18936 [Discriminator loss: 0.064298, acc.: 95.31%] [Generator loss: 8.699253]\n",
      "18937 [Discriminator loss: 0.030551, acc.: 100.00%] [Generator loss: 7.944368]\n",
      "18938 [Discriminator loss: 0.088424, acc.: 98.44%] [Generator loss: 7.817358]\n",
      "18939 [Discriminator loss: 0.029262, acc.: 100.00%] [Generator loss: 7.127036]\n",
      "18940 [Discriminator loss: 0.083095, acc.: 96.88%] [Generator loss: 7.697693]\n",
      "18941 [Discriminator loss: 0.011619, acc.: 100.00%] [Generator loss: 8.248018]\n",
      "18942 [Discriminator loss: 0.045864, acc.: 98.44%] [Generator loss: 9.177299]\n",
      "18943 [Discriminator loss: 0.085297, acc.: 96.88%] [Generator loss: 8.319672]\n",
      "18944 [Discriminator loss: 0.079422, acc.: 98.44%] [Generator loss: 7.989962]\n",
      "18945 [Discriminator loss: 0.012333, acc.: 100.00%] [Generator loss: 8.726454]\n",
      "18946 [Discriminator loss: 0.105697, acc.: 96.88%] [Generator loss: 7.962885]\n",
      "18947 [Discriminator loss: 0.230595, acc.: 92.19%] [Generator loss: 8.393318]\n",
      "18948 [Discriminator loss: 0.042481, acc.: 100.00%] [Generator loss: 9.163517]\n",
      "18949 [Discriminator loss: 0.136775, acc.: 96.88%] [Generator loss: 7.895197]\n",
      "18950 [Discriminator loss: 0.060178, acc.: 96.88%] [Generator loss: 8.369977]\n",
      "18951 [Discriminator loss: 0.037261, acc.: 100.00%] [Generator loss: 9.487072]\n",
      "18952 [Discriminator loss: 0.019593, acc.: 100.00%] [Generator loss: 8.428488]\n",
      "18953 [Discriminator loss: 0.116784, acc.: 95.31%] [Generator loss: 7.208580]\n",
      "18954 [Discriminator loss: 0.012977, acc.: 100.00%] [Generator loss: 6.615657]\n",
      "18955 [Discriminator loss: 0.099861, acc.: 95.31%] [Generator loss: 9.512770]\n",
      "18956 [Discriminator loss: 0.006171, acc.: 100.00%] [Generator loss: 8.688838]\n",
      "18957 [Discriminator loss: 0.041762, acc.: 98.44%] [Generator loss: 8.671585]\n",
      "18958 [Discriminator loss: 0.070308, acc.: 96.88%] [Generator loss: 8.292035]\n",
      "18959 [Discriminator loss: 0.038801, acc.: 96.88%] [Generator loss: 8.948603]\n",
      "18960 [Discriminator loss: 0.032924, acc.: 98.44%] [Generator loss: 7.076346]\n",
      "18961 [Discriminator loss: 0.038625, acc.: 100.00%] [Generator loss: 8.101513]\n",
      "18962 [Discriminator loss: 0.021577, acc.: 100.00%] [Generator loss: 8.449043]\n",
      "18963 [Discriminator loss: 0.073430, acc.: 96.88%] [Generator loss: 7.567125]\n",
      "18964 [Discriminator loss: 0.029120, acc.: 100.00%] [Generator loss: 6.696048]\n",
      "18965 [Discriminator loss: 0.065583, acc.: 96.88%] [Generator loss: 9.814087]\n",
      "18966 [Discriminator loss: 0.162279, acc.: 93.75%] [Generator loss: 8.018197]\n",
      "18967 [Discriminator loss: 0.091779, acc.: 95.31%] [Generator loss: 8.082625]\n",
      "18968 [Discriminator loss: 0.012529, acc.: 100.00%] [Generator loss: 8.799973]\n",
      "18969 [Discriminator loss: 0.046735, acc.: 98.44%] [Generator loss: 8.178109]\n",
      "18970 [Discriminator loss: 0.015452, acc.: 100.00%] [Generator loss: 8.397310]\n",
      "18971 [Discriminator loss: 0.108790, acc.: 98.44%] [Generator loss: 7.059145]\n",
      "18972 [Discriminator loss: 0.071509, acc.: 96.88%] [Generator loss: 9.571709]\n",
      "18973 [Discriminator loss: 0.039651, acc.: 98.44%] [Generator loss: 8.054522]\n",
      "18974 [Discriminator loss: 0.112215, acc.: 96.88%] [Generator loss: 7.563039]\n",
      "18975 [Discriminator loss: 0.077304, acc.: 96.88%] [Generator loss: 8.788790]\n",
      "18976 [Discriminator loss: 0.156678, acc.: 95.31%] [Generator loss: 8.994434]\n",
      "18977 [Discriminator loss: 0.067927, acc.: 96.88%] [Generator loss: 7.358001]\n",
      "18978 [Discriminator loss: 0.146580, acc.: 96.88%] [Generator loss: 9.618532]\n",
      "18979 [Discriminator loss: 0.067242, acc.: 96.88%] [Generator loss: 8.851271]\n",
      "18980 [Discriminator loss: 0.060121, acc.: 95.31%] [Generator loss: 8.141403]\n",
      "18981 [Discriminator loss: 0.240460, acc.: 93.75%] [Generator loss: 7.690709]\n",
      "18982 [Discriminator loss: 0.195777, acc.: 95.31%] [Generator loss: 10.898113]\n",
      "18983 [Discriminator loss: 0.014121, acc.: 100.00%] [Generator loss: 12.076828]\n",
      "18984 [Discriminator loss: 0.262042, acc.: 87.50%] [Generator loss: 5.147145]\n",
      "18985 [Discriminator loss: 0.193422, acc.: 95.31%] [Generator loss: 7.681128]\n",
      "18986 [Discriminator loss: 0.040393, acc.: 98.44%] [Generator loss: 7.862673]\n",
      "18987 [Discriminator loss: 0.093817, acc.: 96.88%] [Generator loss: 6.560522]\n",
      "18988 [Discriminator loss: 0.016974, acc.: 100.00%] [Generator loss: 7.877277]\n",
      "18989 [Discriminator loss: 0.043328, acc.: 98.44%] [Generator loss: 8.771381]\n",
      "18990 [Discriminator loss: 0.040788, acc.: 98.44%] [Generator loss: 9.034792]\n",
      "18991 [Discriminator loss: 0.060076, acc.: 96.88%] [Generator loss: 7.451548]\n",
      "18992 [Discriminator loss: 0.209191, acc.: 92.19%] [Generator loss: 9.041509]\n",
      "18993 [Discriminator loss: 0.057839, acc.: 98.44%] [Generator loss: 8.929504]\n",
      "18994 [Discriminator loss: 0.144312, acc.: 93.75%] [Generator loss: 8.595810]\n",
      "18995 [Discriminator loss: 0.105133, acc.: 93.75%] [Generator loss: 9.753297]\n",
      "18996 [Discriminator loss: 0.047876, acc.: 98.44%] [Generator loss: 10.333472]\n",
      "18997 [Discriminator loss: 0.092356, acc.: 95.31%] [Generator loss: 9.657083]\n",
      "18998 [Discriminator loss: 0.071636, acc.: 96.88%] [Generator loss: 8.993820]\n",
      "18999 [Discriminator loss: 0.028307, acc.: 98.44%] [Generator loss: 10.350217]\n",
      "19000 [Discriminator loss: 0.079230, acc.: 96.88%] [Generator loss: 8.257847]\n",
      "19001 [Discriminator loss: 0.068450, acc.: 96.88%] [Generator loss: 7.096583]\n",
      "19002 [Discriminator loss: 0.066630, acc.: 98.44%] [Generator loss: 8.056053]\n",
      "19003 [Discriminator loss: 0.173033, acc.: 92.19%] [Generator loss: 8.672838]\n",
      "19004 [Discriminator loss: 0.041477, acc.: 98.44%] [Generator loss: 10.533369]\n",
      "19005 [Discriminator loss: 0.064575, acc.: 96.88%] [Generator loss: 9.379744]\n",
      "19006 [Discriminator loss: 0.119899, acc.: 95.31%] [Generator loss: 6.890115]\n",
      "19007 [Discriminator loss: 0.118112, acc.: 93.75%] [Generator loss: 9.245509]\n",
      "19008 [Discriminator loss: 0.014064, acc.: 100.00%] [Generator loss: 8.615776]\n",
      "19009 [Discriminator loss: 0.100738, acc.: 96.88%] [Generator loss: 8.151140]\n",
      "19010 [Discriminator loss: 0.064946, acc.: 96.88%] [Generator loss: 8.153770]\n",
      "19011 [Discriminator loss: 0.107430, acc.: 98.44%] [Generator loss: 9.070297]\n",
      "19012 [Discriminator loss: 0.109041, acc.: 95.31%] [Generator loss: 7.618087]\n",
      "19013 [Discriminator loss: 0.077488, acc.: 96.88%] [Generator loss: 6.710597]\n",
      "19014 [Discriminator loss: 0.033330, acc.: 98.44%] [Generator loss: 7.203475]\n",
      "19015 [Discriminator loss: 0.045844, acc.: 98.44%] [Generator loss: 8.316025]\n",
      "19016 [Discriminator loss: 0.021532, acc.: 98.44%] [Generator loss: 9.504873]\n",
      "19017 [Discriminator loss: 0.130782, acc.: 95.31%] [Generator loss: 7.907894]\n",
      "19018 [Discriminator loss: 0.040489, acc.: 98.44%] [Generator loss: 8.552249]\n",
      "19019 [Discriminator loss: 0.145741, acc.: 93.75%] [Generator loss: 8.238668]\n",
      "19020 [Discriminator loss: 0.109744, acc.: 95.31%] [Generator loss: 9.928437]\n",
      "19021 [Discriminator loss: 0.079248, acc.: 96.88%] [Generator loss: 7.398440]\n",
      "19022 [Discriminator loss: 0.023463, acc.: 100.00%] [Generator loss: 8.944357]\n",
      "19023 [Discriminator loss: 0.028584, acc.: 98.44%] [Generator loss: 8.542480]\n",
      "19024 [Discriminator loss: 0.031998, acc.: 100.00%] [Generator loss: 8.962224]\n",
      "19025 [Discriminator loss: 0.108104, acc.: 96.88%] [Generator loss: 6.999305]\n",
      "19026 [Discriminator loss: 0.081145, acc.: 96.88%] [Generator loss: 8.728554]\n",
      "19027 [Discriminator loss: 0.081756, acc.: 98.44%] [Generator loss: 8.995527]\n",
      "19028 [Discriminator loss: 0.225823, acc.: 87.50%] [Generator loss: 11.083569]\n",
      "19029 [Discriminator loss: 0.070073, acc.: 96.88%] [Generator loss: 10.730833]\n",
      "19030 [Discriminator loss: 0.222245, acc.: 92.19%] [Generator loss: 7.502831]\n",
      "19031 [Discriminator loss: 0.093877, acc.: 95.31%] [Generator loss: 9.927457]\n",
      "19032 [Discriminator loss: 0.045476, acc.: 96.88%] [Generator loss: 8.790244]\n",
      "19033 [Discriminator loss: 0.069023, acc.: 98.44%] [Generator loss: 8.806391]\n",
      "19034 [Discriminator loss: 0.010812, acc.: 100.00%] [Generator loss: 8.738867]\n",
      "19035 [Discriminator loss: 0.213911, acc.: 95.31%] [Generator loss: 5.707573]\n",
      "19036 [Discriminator loss: 0.068536, acc.: 96.88%] [Generator loss: 7.418605]\n",
      "19037 [Discriminator loss: 0.072985, acc.: 95.31%] [Generator loss: 8.114628]\n",
      "19038 [Discriminator loss: 0.020168, acc.: 100.00%] [Generator loss: 9.810553]\n",
      "19039 [Discriminator loss: 0.127896, acc.: 93.75%] [Generator loss: 7.852086]\n",
      "19040 [Discriminator loss: 0.015315, acc.: 100.00%] [Generator loss: 7.676066]\n",
      "19041 [Discriminator loss: 0.027672, acc.: 100.00%] [Generator loss: 8.547131]\n",
      "19042 [Discriminator loss: 0.022117, acc.: 100.00%] [Generator loss: 7.917755]\n",
      "19043 [Discriminator loss: 0.093383, acc.: 98.44%] [Generator loss: 8.872889]\n",
      "19044 [Discriminator loss: 0.080435, acc.: 96.88%] [Generator loss: 7.994778]\n",
      "19045 [Discriminator loss: 0.036107, acc.: 98.44%] [Generator loss: 8.003572]\n",
      "19046 [Discriminator loss: 0.053375, acc.: 98.44%] [Generator loss: 8.065273]\n",
      "19047 [Discriminator loss: 0.080016, acc.: 96.88%] [Generator loss: 5.633109]\n",
      "19048 [Discriminator loss: 0.125545, acc.: 98.44%] [Generator loss: 8.984313]\n",
      "19049 [Discriminator loss: 0.047879, acc.: 98.44%] [Generator loss: 9.148146]\n",
      "19050 [Discriminator loss: 0.194316, acc.: 90.62%] [Generator loss: 7.732955]\n",
      "19051 [Discriminator loss: 0.016110, acc.: 100.00%] [Generator loss: 8.066121]\n",
      "19052 [Discriminator loss: 0.093377, acc.: 96.88%] [Generator loss: 9.693415]\n",
      "19053 [Discriminator loss: 0.101005, acc.: 95.31%] [Generator loss: 7.460404]\n",
      "19054 [Discriminator loss: 0.208713, acc.: 93.75%] [Generator loss: 9.863152]\n",
      "19055 [Discriminator loss: 0.182699, acc.: 93.75%] [Generator loss: 10.055283]\n",
      "19056 [Discriminator loss: 0.054459, acc.: 96.88%] [Generator loss: 10.854396]\n",
      "19057 [Discriminator loss: 0.041512, acc.: 98.44%] [Generator loss: 8.763994]\n",
      "19058 [Discriminator loss: 0.244475, acc.: 90.62%] [Generator loss: 7.884925]\n",
      "19059 [Discriminator loss: 0.092539, acc.: 96.88%] [Generator loss: 8.842892]\n",
      "19060 [Discriminator loss: 0.021604, acc.: 100.00%] [Generator loss: 9.422272]\n",
      "19061 [Discriminator loss: 0.070259, acc.: 98.44%] [Generator loss: 8.749201]\n",
      "19062 [Discriminator loss: 0.226949, acc.: 92.19%] [Generator loss: 9.451437]\n",
      "19063 [Discriminator loss: 0.059483, acc.: 96.88%] [Generator loss: 9.194304]\n",
      "19064 [Discriminator loss: 0.019477, acc.: 100.00%] [Generator loss: 8.795321]\n",
      "19065 [Discriminator loss: 0.112053, acc.: 96.88%] [Generator loss: 7.078955]\n",
      "19066 [Discriminator loss: 0.079202, acc.: 96.88%] [Generator loss: 7.422018]\n",
      "19067 [Discriminator loss: 0.070994, acc.: 96.88%] [Generator loss: 7.797662]\n",
      "19068 [Discriminator loss: 0.093254, acc.: 96.88%] [Generator loss: 10.459005]\n",
      "19069 [Discriminator loss: 0.265452, acc.: 90.62%] [Generator loss: 8.159006]\n",
      "19070 [Discriminator loss: 0.155632, acc.: 93.75%] [Generator loss: 9.474424]\n",
      "19071 [Discriminator loss: 0.012299, acc.: 100.00%] [Generator loss: 9.016161]\n",
      "19072 [Discriminator loss: 0.080972, acc.: 95.31%] [Generator loss: 9.510047]\n",
      "19073 [Discriminator loss: 0.020182, acc.: 100.00%] [Generator loss: 9.742420]\n",
      "19074 [Discriminator loss: 0.059531, acc.: 96.88%] [Generator loss: 8.974382]\n",
      "19075 [Discriminator loss: 0.055708, acc.: 96.88%] [Generator loss: 7.666880]\n",
      "19076 [Discriminator loss: 0.016474, acc.: 100.00%] [Generator loss: 8.067666]\n",
      "19077 [Discriminator loss: 0.116618, acc.: 93.75%] [Generator loss: 8.706785]\n",
      "19078 [Discriminator loss: 0.059284, acc.: 98.44%] [Generator loss: 8.687376]\n",
      "19079 [Discriminator loss: 0.208693, acc.: 93.75%] [Generator loss: 8.760924]\n",
      "19080 [Discriminator loss: 0.161249, acc.: 95.31%] [Generator loss: 9.058769]\n",
      "19081 [Discriminator loss: 0.043843, acc.: 98.44%] [Generator loss: 9.718830]\n",
      "19082 [Discriminator loss: 0.074251, acc.: 98.44%] [Generator loss: 7.596083]\n",
      "19083 [Discriminator loss: 0.021584, acc.: 100.00%] [Generator loss: 7.829245]\n",
      "19084 [Discriminator loss: 0.008513, acc.: 100.00%] [Generator loss: 9.069614]\n",
      "19085 [Discriminator loss: 0.049886, acc.: 98.44%] [Generator loss: 7.668325]\n",
      "19086 [Discriminator loss: 0.066645, acc.: 96.88%] [Generator loss: 7.625113]\n",
      "19087 [Discriminator loss: 0.245904, acc.: 90.62%] [Generator loss: 9.567944]\n",
      "19088 [Discriminator loss: 0.059490, acc.: 96.88%] [Generator loss: 8.818232]\n",
      "19089 [Discriminator loss: 0.050295, acc.: 98.44%] [Generator loss: 9.985982]\n",
      "19090 [Discriminator loss: 0.142481, acc.: 95.31%] [Generator loss: 8.129270]\n",
      "19091 [Discriminator loss: 0.116370, acc.: 95.31%] [Generator loss: 9.169680]\n",
      "19092 [Discriminator loss: 0.123742, acc.: 96.88%] [Generator loss: 8.931475]\n",
      "19093 [Discriminator loss: 0.083398, acc.: 96.88%] [Generator loss: 8.350210]\n",
      "19094 [Discriminator loss: 0.078478, acc.: 96.88%] [Generator loss: 8.725975]\n",
      "19095 [Discriminator loss: 0.068619, acc.: 95.31%] [Generator loss: 9.012283]\n",
      "19096 [Discriminator loss: 0.040392, acc.: 98.44%] [Generator loss: 8.280052]\n",
      "19097 [Discriminator loss: 0.049656, acc.: 100.00%] [Generator loss: 7.497112]\n",
      "19098 [Discriminator loss: 0.075154, acc.: 98.44%] [Generator loss: 7.420135]\n",
      "19099 [Discriminator loss: 0.030231, acc.: 100.00%] [Generator loss: 7.410902]\n",
      "19100 [Discriminator loss: 0.088874, acc.: 93.75%] [Generator loss: 6.655766]\n",
      "19101 [Discriminator loss: 0.050448, acc.: 96.88%] [Generator loss: 8.116398]\n",
      "19102 [Discriminator loss: 0.020076, acc.: 100.00%] [Generator loss: 8.742867]\n",
      "19103 [Discriminator loss: 0.058093, acc.: 98.44%] [Generator loss: 10.224100]\n",
      "19104 [Discriminator loss: 0.054543, acc.: 96.88%] [Generator loss: 9.539956]\n",
      "19105 [Discriminator loss: 0.081207, acc.: 96.88%] [Generator loss: 7.606306]\n",
      "19106 [Discriminator loss: 0.134296, acc.: 93.75%] [Generator loss: 9.200729]\n",
      "19107 [Discriminator loss: 0.154990, acc.: 93.75%] [Generator loss: 7.898576]\n",
      "19108 [Discriminator loss: 0.036070, acc.: 98.44%] [Generator loss: 8.428825]\n",
      "19109 [Discriminator loss: 0.082386, acc.: 98.44%] [Generator loss: 8.345669]\n",
      "19110 [Discriminator loss: 0.171276, acc.: 95.31%] [Generator loss: 7.514087]\n",
      "19111 [Discriminator loss: 0.038095, acc.: 98.44%] [Generator loss: 8.032942]\n",
      "19112 [Discriminator loss: 0.106699, acc.: 95.31%] [Generator loss: 7.249172]\n",
      "19113 [Discriminator loss: 0.104891, acc.: 96.88%] [Generator loss: 8.915094]\n",
      "19114 [Discriminator loss: 0.063421, acc.: 96.88%] [Generator loss: 9.763552]\n",
      "19115 [Discriminator loss: 0.028472, acc.: 100.00%] [Generator loss: 9.051940]\n",
      "19116 [Discriminator loss: 0.141479, acc.: 95.31%] [Generator loss: 6.905796]\n",
      "19117 [Discriminator loss: 0.044361, acc.: 98.44%] [Generator loss: 8.010902]\n",
      "19118 [Discriminator loss: 0.005755, acc.: 100.00%] [Generator loss: 8.006389]\n",
      "19119 [Discriminator loss: 0.177787, acc.: 93.75%] [Generator loss: 9.213108]\n",
      "19120 [Discriminator loss: 0.028545, acc.: 98.44%] [Generator loss: 7.253767]\n",
      "19121 [Discriminator loss: 0.105946, acc.: 93.75%] [Generator loss: 8.428616]\n",
      "19122 [Discriminator loss: 0.081221, acc.: 96.88%] [Generator loss: 7.651272]\n",
      "19123 [Discriminator loss: 0.031474, acc.: 100.00%] [Generator loss: 8.393377]\n",
      "19124 [Discriminator loss: 0.112035, acc.: 96.88%] [Generator loss: 6.021051]\n",
      "19125 [Discriminator loss: 0.110123, acc.: 95.31%] [Generator loss: 7.825747]\n",
      "19126 [Discriminator loss: 0.100254, acc.: 95.31%] [Generator loss: 7.445450]\n",
      "19127 [Discriminator loss: 0.078348, acc.: 98.44%] [Generator loss: 8.920585]\n",
      "19128 [Discriminator loss: 0.063183, acc.: 96.88%] [Generator loss: 8.458622]\n",
      "19129 [Discriminator loss: 0.022830, acc.: 98.44%] [Generator loss: 8.895158]\n",
      "19130 [Discriminator loss: 0.224177, acc.: 90.62%] [Generator loss: 7.399404]\n",
      "19131 [Discriminator loss: 0.238478, acc.: 90.62%] [Generator loss: 10.222445]\n",
      "19132 [Discriminator loss: 0.069358, acc.: 96.88%] [Generator loss: 11.265163]\n",
      "19133 [Discriminator loss: 0.020801, acc.: 100.00%] [Generator loss: 9.346632]\n",
      "19134 [Discriminator loss: 0.040135, acc.: 98.44%] [Generator loss: 9.522181]\n",
      "19135 [Discriminator loss: 0.069075, acc.: 96.88%] [Generator loss: 10.474024]\n",
      "19136 [Discriminator loss: 0.040006, acc.: 98.44%] [Generator loss: 9.892094]\n",
      "19137 [Discriminator loss: 0.003357, acc.: 100.00%] [Generator loss: 10.392101]\n",
      "19138 [Discriminator loss: 0.008948, acc.: 100.00%] [Generator loss: 9.734177]\n",
      "19139 [Discriminator loss: 0.052700, acc.: 98.44%] [Generator loss: 8.061255]\n",
      "19140 [Discriminator loss: 0.131232, acc.: 92.19%] [Generator loss: 8.696947]\n",
      "19141 [Discriminator loss: 0.133699, acc.: 95.31%] [Generator loss: 9.812814]\n",
      "19142 [Discriminator loss: 0.107700, acc.: 93.75%] [Generator loss: 8.363283]\n",
      "19143 [Discriminator loss: 0.034087, acc.: 98.44%] [Generator loss: 7.285525]\n",
      "19144 [Discriminator loss: 0.024348, acc.: 100.00%] [Generator loss: 8.732713]\n",
      "19145 [Discriminator loss: 0.155008, acc.: 93.75%] [Generator loss: 8.219517]\n",
      "19146 [Discriminator loss: 0.068352, acc.: 96.88%] [Generator loss: 8.518914]\n",
      "19147 [Discriminator loss: 0.118522, acc.: 92.19%] [Generator loss: 6.515651]\n",
      "19148 [Discriminator loss: 0.060325, acc.: 95.31%] [Generator loss: 8.692174]\n",
      "19149 [Discriminator loss: 0.160047, acc.: 90.62%] [Generator loss: 10.318573]\n",
      "19150 [Discriminator loss: 0.022365, acc.: 100.00%] [Generator loss: 10.463186]\n",
      "19151 [Discriminator loss: 0.093355, acc.: 95.31%] [Generator loss: 8.182209]\n",
      "19152 [Discriminator loss: 0.095475, acc.: 93.75%] [Generator loss: 8.215946]\n",
      "19153 [Discriminator loss: 0.013788, acc.: 100.00%] [Generator loss: 9.392237]\n",
      "19154 [Discriminator loss: 0.194912, acc.: 93.75%] [Generator loss: 8.142754]\n",
      "19155 [Discriminator loss: 0.012834, acc.: 100.00%] [Generator loss: 8.665592]\n",
      "19156 [Discriminator loss: 0.047094, acc.: 98.44%] [Generator loss: 7.777528]\n",
      "19157 [Discriminator loss: 0.092902, acc.: 96.88%] [Generator loss: 7.725013]\n",
      "19158 [Discriminator loss: 0.067991, acc.: 96.88%] [Generator loss: 7.943262]\n",
      "19159 [Discriminator loss: 0.074020, acc.: 95.31%] [Generator loss: 8.817727]\n",
      "19160 [Discriminator loss: 0.109690, acc.: 96.88%] [Generator loss: 6.604523]\n",
      "19161 [Discriminator loss: 0.062806, acc.: 96.88%] [Generator loss: 8.917223]\n",
      "19162 [Discriminator loss: 0.061133, acc.: 96.88%] [Generator loss: 7.933408]\n",
      "19163 [Discriminator loss: 0.023797, acc.: 100.00%] [Generator loss: 7.773927]\n",
      "19164 [Discriminator loss: 0.058761, acc.: 98.44%] [Generator loss: 9.615746]\n",
      "19165 [Discriminator loss: 0.198356, acc.: 92.19%] [Generator loss: 8.303902]\n",
      "19166 [Discriminator loss: 0.055921, acc.: 95.31%] [Generator loss: 6.450599]\n",
      "19167 [Discriminator loss: 0.136050, acc.: 93.75%] [Generator loss: 7.287202]\n",
      "19168 [Discriminator loss: 0.075477, acc.: 98.44%] [Generator loss: 11.165106]\n",
      "19169 [Discriminator loss: 0.098451, acc.: 98.44%] [Generator loss: 7.338794]\n",
      "19170 [Discriminator loss: 0.057892, acc.: 95.31%] [Generator loss: 8.273990]\n",
      "19171 [Discriminator loss: 0.041749, acc.: 96.88%] [Generator loss: 7.312418]\n",
      "19172 [Discriminator loss: 0.053259, acc.: 98.44%] [Generator loss: 8.219894]\n",
      "19173 [Discriminator loss: 0.100452, acc.: 98.44%] [Generator loss: 8.362528]\n",
      "19174 [Discriminator loss: 0.098106, acc.: 96.88%] [Generator loss: 9.464974]\n",
      "19175 [Discriminator loss: 0.047569, acc.: 98.44%] [Generator loss: 6.512646]\n",
      "19176 [Discriminator loss: 0.036138, acc.: 98.44%] [Generator loss: 7.436634]\n",
      "19177 [Discriminator loss: 0.103912, acc.: 93.75%] [Generator loss: 9.833977]\n",
      "19178 [Discriminator loss: 0.011893, acc.: 100.00%] [Generator loss: 9.064947]\n",
      "19179 [Discriminator loss: 0.057794, acc.: 96.88%] [Generator loss: 7.086702]\n",
      "19180 [Discriminator loss: 0.037240, acc.: 98.44%] [Generator loss: 7.837117]\n",
      "19181 [Discriminator loss: 0.092731, acc.: 96.88%] [Generator loss: 7.560380]\n",
      "19182 [Discriminator loss: 0.100168, acc.: 96.88%] [Generator loss: 8.375889]\n",
      "19183 [Discriminator loss: 0.164907, acc.: 93.75%] [Generator loss: 7.312544]\n",
      "19184 [Discriminator loss: 0.051992, acc.: 98.44%] [Generator loss: 8.019802]\n",
      "19185 [Discriminator loss: 0.066837, acc.: 96.88%] [Generator loss: 9.577515]\n",
      "19186 [Discriminator loss: 0.022294, acc.: 98.44%] [Generator loss: 8.513533]\n",
      "19187 [Discriminator loss: 0.057735, acc.: 98.44%] [Generator loss: 6.345052]\n",
      "19188 [Discriminator loss: 0.039156, acc.: 98.44%] [Generator loss: 7.863708]\n",
      "19189 [Discriminator loss: 0.249970, acc.: 89.06%] [Generator loss: 7.467033]\n",
      "19190 [Discriminator loss: 0.058630, acc.: 96.88%] [Generator loss: 9.509926]\n",
      "19191 [Discriminator loss: 0.057921, acc.: 96.88%] [Generator loss: 7.076164]\n",
      "19192 [Discriminator loss: 0.024767, acc.: 100.00%] [Generator loss: 8.083360]\n",
      "19193 [Discriminator loss: 0.064727, acc.: 98.44%] [Generator loss: 8.502605]\n",
      "19194 [Discriminator loss: 0.112524, acc.: 98.44%] [Generator loss: 7.295218]\n",
      "19195 [Discriminator loss: 0.054816, acc.: 100.00%] [Generator loss: 6.841662]\n",
      "19196 [Discriminator loss: 0.175393, acc.: 93.75%] [Generator loss: 9.451718]\n",
      "19197 [Discriminator loss: 0.035803, acc.: 96.88%] [Generator loss: 10.406201]\n",
      "19198 [Discriminator loss: 0.166260, acc.: 96.88%] [Generator loss: 6.968119]\n",
      "19199 [Discriminator loss: 0.083935, acc.: 96.88%] [Generator loss: 10.160575]\n",
      "19200 [Discriminator loss: 0.063301, acc.: 96.88%] [Generator loss: 9.110498]\n",
      "19201 [Discriminator loss: 0.053508, acc.: 98.44%] [Generator loss: 6.649781]\n",
      "19202 [Discriminator loss: 0.072722, acc.: 95.31%] [Generator loss: 9.353486]\n",
      "19203 [Discriminator loss: 0.060608, acc.: 100.00%] [Generator loss: 8.779998]\n",
      "19204 [Discriminator loss: 0.119561, acc.: 96.88%] [Generator loss: 7.544771]\n",
      "19205 [Discriminator loss: 0.260596, acc.: 92.19%] [Generator loss: 9.874637]\n",
      "19206 [Discriminator loss: 0.028052, acc.: 98.44%] [Generator loss: 10.741537]\n",
      "19207 [Discriminator loss: 0.085748, acc.: 98.44%] [Generator loss: 8.805389]\n",
      "19208 [Discriminator loss: 0.038823, acc.: 98.44%] [Generator loss: 7.625670]\n",
      "19209 [Discriminator loss: 0.069086, acc.: 98.44%] [Generator loss: 5.941044]\n",
      "19210 [Discriminator loss: 0.128417, acc.: 95.31%] [Generator loss: 9.106956]\n",
      "19211 [Discriminator loss: 0.022991, acc.: 100.00%] [Generator loss: 10.609070]\n",
      "19212 [Discriminator loss: 0.077636, acc.: 98.44%] [Generator loss: 7.570786]\n",
      "19213 [Discriminator loss: 0.043098, acc.: 98.44%] [Generator loss: 8.139804]\n",
      "19214 [Discriminator loss: 0.138436, acc.: 95.31%] [Generator loss: 7.663716]\n",
      "19215 [Discriminator loss: 0.049788, acc.: 98.44%] [Generator loss: 9.112848]\n",
      "19216 [Discriminator loss: 0.134643, acc.: 96.88%] [Generator loss: 6.911393]\n",
      "19217 [Discriminator loss: 0.142571, acc.: 96.88%] [Generator loss: 7.028686]\n",
      "19218 [Discriminator loss: 0.029918, acc.: 100.00%] [Generator loss: 7.890166]\n",
      "19219 [Discriminator loss: 0.046285, acc.: 96.88%] [Generator loss: 7.526332]\n",
      "19220 [Discriminator loss: 0.049693, acc.: 100.00%] [Generator loss: 8.665340]\n",
      "19221 [Discriminator loss: 0.101221, acc.: 95.31%] [Generator loss: 8.298547]\n",
      "19222 [Discriminator loss: 0.032966, acc.: 98.44%] [Generator loss: 7.788343]\n",
      "19223 [Discriminator loss: 0.047215, acc.: 96.88%] [Generator loss: 9.073858]\n",
      "19224 [Discriminator loss: 0.002113, acc.: 100.00%] [Generator loss: 9.332579]\n",
      "19225 [Discriminator loss: 0.023796, acc.: 100.00%] [Generator loss: 8.231054]\n",
      "19226 [Discriminator loss: 0.240100, acc.: 90.62%] [Generator loss: 7.303474]\n",
      "19227 [Discriminator loss: 0.061488, acc.: 98.44%] [Generator loss: 8.991838]\n",
      "19228 [Discriminator loss: 0.050086, acc.: 96.88%] [Generator loss: 9.094101]\n",
      "19229 [Discriminator loss: 0.010253, acc.: 100.00%] [Generator loss: 7.408964]\n",
      "19230 [Discriminator loss: 0.447519, acc.: 84.38%] [Generator loss: 7.761153]\n",
      "19231 [Discriminator loss: 0.231006, acc.: 87.50%] [Generator loss: 9.105693]\n",
      "19232 [Discriminator loss: 0.036817, acc.: 98.44%] [Generator loss: 8.348530]\n",
      "19233 [Discriminator loss: 0.056607, acc.: 96.88%] [Generator loss: 10.048482]\n",
      "19234 [Discriminator loss: 0.069045, acc.: 98.44%] [Generator loss: 9.374516]\n",
      "19235 [Discriminator loss: 0.041879, acc.: 96.88%] [Generator loss: 9.400995]\n",
      "19236 [Discriminator loss: 0.115786, acc.: 93.75%] [Generator loss: 9.304838]\n",
      "19237 [Discriminator loss: 0.034453, acc.: 98.44%] [Generator loss: 8.180549]\n",
      "19238 [Discriminator loss: 0.029201, acc.: 100.00%] [Generator loss: 7.095636]\n",
      "19239 [Discriminator loss: 0.018409, acc.: 98.44%] [Generator loss: 7.260395]\n",
      "19240 [Discriminator loss: 0.097617, acc.: 96.88%] [Generator loss: 6.849140]\n",
      "19241 [Discriminator loss: 0.011050, acc.: 100.00%] [Generator loss: 8.207714]\n",
      "19242 [Discriminator loss: 0.091833, acc.: 96.88%] [Generator loss: 8.485888]\n",
      "19243 [Discriminator loss: 0.008876, acc.: 100.00%] [Generator loss: 7.176601]\n",
      "19244 [Discriminator loss: 0.075076, acc.: 98.44%] [Generator loss: 6.387941]\n",
      "19245 [Discriminator loss: 0.029064, acc.: 98.44%] [Generator loss: 7.399904]\n",
      "19246 [Discriminator loss: 0.128403, acc.: 96.88%] [Generator loss: 9.380495]\n",
      "19247 [Discriminator loss: 0.074227, acc.: 96.88%] [Generator loss: 9.804640]\n",
      "19248 [Discriminator loss: 0.101363, acc.: 98.44%] [Generator loss: 6.538914]\n",
      "19249 [Discriminator loss: 0.081870, acc.: 96.88%] [Generator loss: 9.185549]\n",
      "19250 [Discriminator loss: 0.144755, acc.: 93.75%] [Generator loss: 10.329995]\n",
      "19251 [Discriminator loss: 0.014903, acc.: 100.00%] [Generator loss: 8.577972]\n",
      "19252 [Discriminator loss: 0.127628, acc.: 95.31%] [Generator loss: 8.242479]\n",
      "19253 [Discriminator loss: 0.160135, acc.: 93.75%] [Generator loss: 8.723835]\n",
      "19254 [Discriminator loss: 0.021701, acc.: 98.44%] [Generator loss: 8.960234]\n",
      "19255 [Discriminator loss: 0.087319, acc.: 96.88%] [Generator loss: 8.322054]\n",
      "19256 [Discriminator loss: 0.024586, acc.: 100.00%] [Generator loss: 8.519625]\n",
      "19257 [Discriminator loss: 0.313363, acc.: 95.31%] [Generator loss: 9.128980]\n",
      "19258 [Discriminator loss: 0.064462, acc.: 96.88%] [Generator loss: 10.234562]\n",
      "19259 [Discriminator loss: 0.146328, acc.: 92.19%] [Generator loss: 9.055168]\n",
      "19260 [Discriminator loss: 0.016034, acc.: 100.00%] [Generator loss: 7.758042]\n",
      "19261 [Discriminator loss: 0.095188, acc.: 95.31%] [Generator loss: 7.320059]\n",
      "19262 [Discriminator loss: 0.072835, acc.: 95.31%] [Generator loss: 8.762268]\n",
      "19263 [Discriminator loss: 0.025184, acc.: 98.44%] [Generator loss: 9.567789]\n",
      "19264 [Discriminator loss: 0.070180, acc.: 96.88%] [Generator loss: 8.495626]\n",
      "19265 [Discriminator loss: 0.072781, acc.: 96.88%] [Generator loss: 9.737969]\n",
      "19266 [Discriminator loss: 0.009237, acc.: 100.00%] [Generator loss: 10.308014]\n",
      "19267 [Discriminator loss: 0.036699, acc.: 98.44%] [Generator loss: 8.143196]\n",
      "19268 [Discriminator loss: 0.240205, acc.: 93.75%] [Generator loss: 7.746125]\n",
      "19269 [Discriminator loss: 0.170440, acc.: 96.88%] [Generator loss: 8.757850]\n",
      "19270 [Discriminator loss: 0.036484, acc.: 98.44%] [Generator loss: 9.456718]\n",
      "19271 [Discriminator loss: 0.198854, acc.: 95.31%] [Generator loss: 9.269446]\n",
      "19272 [Discriminator loss: 0.043898, acc.: 98.44%] [Generator loss: 7.826346]\n",
      "19273 [Discriminator loss: 0.114046, acc.: 95.31%] [Generator loss: 9.386003]\n",
      "19274 [Discriminator loss: 0.058957, acc.: 96.88%] [Generator loss: 8.700655]\n",
      "19275 [Discriminator loss: 0.053061, acc.: 96.88%] [Generator loss: 9.088293]\n",
      "19276 [Discriminator loss: 0.049718, acc.: 96.88%] [Generator loss: 8.568434]\n",
      "19277 [Discriminator loss: 0.050388, acc.: 98.44%] [Generator loss: 6.524484]\n",
      "19278 [Discriminator loss: 0.096001, acc.: 95.31%] [Generator loss: 8.704889]\n",
      "19279 [Discriminator loss: 0.153716, acc.: 93.75%] [Generator loss: 6.676037]\n",
      "19280 [Discriminator loss: 0.080421, acc.: 96.88%] [Generator loss: 9.532812]\n",
      "19281 [Discriminator loss: 0.045476, acc.: 98.44%] [Generator loss: 9.528652]\n",
      "19282 [Discriminator loss: 0.058373, acc.: 98.44%] [Generator loss: 7.505466]\n",
      "19283 [Discriminator loss: 0.136203, acc.: 95.31%] [Generator loss: 8.634453]\n",
      "19284 [Discriminator loss: 0.017316, acc.: 100.00%] [Generator loss: 9.058609]\n",
      "19285 [Discriminator loss: 0.063275, acc.: 96.88%] [Generator loss: 8.117585]\n",
      "19286 [Discriminator loss: 0.068219, acc.: 96.88%] [Generator loss: 8.110643]\n",
      "19287 [Discriminator loss: 0.104541, acc.: 98.44%] [Generator loss: 8.807306]\n",
      "19288 [Discriminator loss: 0.087345, acc.: 96.88%] [Generator loss: 9.355469]\n",
      "19289 [Discriminator loss: 0.051583, acc.: 96.88%] [Generator loss: 7.924821]\n",
      "19290 [Discriminator loss: 0.121458, acc.: 95.31%] [Generator loss: 8.699593]\n",
      "19291 [Discriminator loss: 0.186801, acc.: 92.19%] [Generator loss: 9.499741]\n",
      "19292 [Discriminator loss: 0.020878, acc.: 100.00%] [Generator loss: 8.992853]\n",
      "19293 [Discriminator loss: 0.109095, acc.: 96.88%] [Generator loss: 8.597454]\n",
      "19294 [Discriminator loss: 0.039586, acc.: 98.44%] [Generator loss: 8.032461]\n",
      "19295 [Discriminator loss: 0.025780, acc.: 100.00%] [Generator loss: 7.489496]\n",
      "19296 [Discriminator loss: 0.047913, acc.: 98.44%] [Generator loss: 7.974132]\n",
      "19297 [Discriminator loss: 0.038570, acc.: 98.44%] [Generator loss: 6.857958]\n",
      "19298 [Discriminator loss: 0.100556, acc.: 95.31%] [Generator loss: 8.998449]\n",
      "19299 [Discriminator loss: 0.007821, acc.: 100.00%] [Generator loss: 9.836761]\n",
      "19300 [Discriminator loss: 0.082414, acc.: 98.44%] [Generator loss: 10.229483]\n",
      "19301 [Discriminator loss: 0.207463, acc.: 92.19%] [Generator loss: 8.551751]\n",
      "19302 [Discriminator loss: 0.066768, acc.: 98.44%] [Generator loss: 9.830058]\n",
      "19303 [Discriminator loss: 0.012963, acc.: 100.00%] [Generator loss: 9.484483]\n",
      "19304 [Discriminator loss: 0.034571, acc.: 98.44%] [Generator loss: 9.313900]\n",
      "19305 [Discriminator loss: 0.035695, acc.: 98.44%] [Generator loss: 8.386869]\n",
      "19306 [Discriminator loss: 0.066508, acc.: 98.44%] [Generator loss: 7.685897]\n",
      "19307 [Discriminator loss: 0.069665, acc.: 96.88%] [Generator loss: 8.637236]\n",
      "19308 [Discriminator loss: 0.012983, acc.: 100.00%] [Generator loss: 9.319500]\n",
      "19309 [Discriminator loss: 0.053481, acc.: 98.44%] [Generator loss: 8.008712]\n",
      "19310 [Discriminator loss: 0.098149, acc.: 95.31%] [Generator loss: 9.238614]\n",
      "19311 [Discriminator loss: 0.159141, acc.: 93.75%] [Generator loss: 8.976521]\n",
      "19312 [Discriminator loss: 0.137884, acc.: 95.31%] [Generator loss: 8.414571]\n",
      "19313 [Discriminator loss: 0.031905, acc.: 98.44%] [Generator loss: 7.584490]\n",
      "19314 [Discriminator loss: 0.109639, acc.: 95.31%] [Generator loss: 9.725337]\n",
      "19315 [Discriminator loss: 0.018601, acc.: 100.00%] [Generator loss: 8.722061]\n",
      "19316 [Discriminator loss: 0.093686, acc.: 95.31%] [Generator loss: 8.944513]\n",
      "19317 [Discriminator loss: 0.063533, acc.: 96.88%] [Generator loss: 7.553400]\n",
      "19318 [Discriminator loss: 0.210338, acc.: 95.31%] [Generator loss: 8.586973]\n",
      "19319 [Discriminator loss: 0.080484, acc.: 95.31%] [Generator loss: 8.949680]\n",
      "19320 [Discriminator loss: 0.107103, acc.: 96.88%] [Generator loss: 7.292875]\n",
      "19321 [Discriminator loss: 0.031470, acc.: 100.00%] [Generator loss: 6.955893]\n",
      "19322 [Discriminator loss: 0.074021, acc.: 96.88%] [Generator loss: 9.047550]\n",
      "19323 [Discriminator loss: 0.016726, acc.: 100.00%] [Generator loss: 10.125402]\n",
      "19324 [Discriminator loss: 0.092506, acc.: 96.88%] [Generator loss: 6.269989]\n",
      "19325 [Discriminator loss: 0.064359, acc.: 98.44%] [Generator loss: 8.358724]\n",
      "19326 [Discriminator loss: 0.016693, acc.: 100.00%] [Generator loss: 8.906205]\n",
      "19327 [Discriminator loss: 0.145918, acc.: 92.19%] [Generator loss: 7.831274]\n",
      "19328 [Discriminator loss: 0.128631, acc.: 95.31%] [Generator loss: 7.296748]\n",
      "19329 [Discriminator loss: 0.099320, acc.: 96.88%] [Generator loss: 8.548325]\n",
      "19330 [Discriminator loss: 0.160135, acc.: 93.75%] [Generator loss: 9.018248]\n",
      "19331 [Discriminator loss: 0.010574, acc.: 100.00%] [Generator loss: 9.731705]\n",
      "19332 [Discriminator loss: 0.085909, acc.: 96.88%] [Generator loss: 7.553605]\n",
      "19333 [Discriminator loss: 0.038990, acc.: 98.44%] [Generator loss: 9.427648]\n",
      "19334 [Discriminator loss: 0.029310, acc.: 96.88%] [Generator loss: 8.480122]\n",
      "19335 [Discriminator loss: 0.106768, acc.: 93.75%] [Generator loss: 8.910893]\n",
      "19336 [Discriminator loss: 0.013165, acc.: 100.00%] [Generator loss: 9.816546]\n",
      "19337 [Discriminator loss: 0.103192, acc.: 95.31%] [Generator loss: 8.250610]\n",
      "19338 [Discriminator loss: 0.072317, acc.: 98.44%] [Generator loss: 8.579455]\n",
      "19339 [Discriminator loss: 0.084753, acc.: 96.88%] [Generator loss: 7.556835]\n",
      "19340 [Discriminator loss: 0.076665, acc.: 96.88%] [Generator loss: 8.956547]\n",
      "19341 [Discriminator loss: 0.183367, acc.: 90.62%] [Generator loss: 9.301659]\n",
      "19342 [Discriminator loss: 0.009633, acc.: 100.00%] [Generator loss: 9.654663]\n",
      "19343 [Discriminator loss: 0.039123, acc.: 98.44%] [Generator loss: 8.962399]\n",
      "19344 [Discriminator loss: 0.058276, acc.: 98.44%] [Generator loss: 6.955612]\n",
      "19345 [Discriminator loss: 0.101949, acc.: 95.31%] [Generator loss: 7.887295]\n",
      "19346 [Discriminator loss: 0.013527, acc.: 100.00%] [Generator loss: 9.538986]\n",
      "19347 [Discriminator loss: 0.017224, acc.: 100.00%] [Generator loss: 8.642137]\n",
      "19348 [Discriminator loss: 0.035903, acc.: 100.00%] [Generator loss: 9.342053]\n",
      "19349 [Discriminator loss: 0.084252, acc.: 95.31%] [Generator loss: 7.349940]\n",
      "19350 [Discriminator loss: 0.055480, acc.: 96.88%] [Generator loss: 7.927242]\n",
      "19351 [Discriminator loss: 0.058610, acc.: 95.31%] [Generator loss: 7.959452]\n",
      "19352 [Discriminator loss: 0.121247, acc.: 96.88%] [Generator loss: 6.104256]\n",
      "19353 [Discriminator loss: 0.055446, acc.: 98.44%] [Generator loss: 7.811303]\n",
      "19354 [Discriminator loss: 0.004747, acc.: 100.00%] [Generator loss: 7.138627]\n",
      "19355 [Discriminator loss: 0.078473, acc.: 93.75%] [Generator loss: 9.932820]\n",
      "19356 [Discriminator loss: 0.123064, acc.: 92.19%] [Generator loss: 10.286409]\n",
      "19357 [Discriminator loss: 0.031607, acc.: 98.44%] [Generator loss: 10.247976]\n",
      "19358 [Discriminator loss: 0.205135, acc.: 92.19%] [Generator loss: 7.844967]\n",
      "19359 [Discriminator loss: 0.269743, acc.: 92.19%] [Generator loss: 8.648326]\n",
      "19360 [Discriminator loss: 0.076212, acc.: 96.88%] [Generator loss: 8.850767]\n",
      "19361 [Discriminator loss: 0.098587, acc.: 96.88%] [Generator loss: 8.096745]\n",
      "19362 [Discriminator loss: 0.047974, acc.: 98.44%] [Generator loss: 10.423734]\n",
      "19363 [Discriminator loss: 0.026173, acc.: 100.00%] [Generator loss: 8.993961]\n",
      "19364 [Discriminator loss: 0.053290, acc.: 96.88%] [Generator loss: 8.194033]\n",
      "19365 [Discriminator loss: 0.048504, acc.: 98.44%] [Generator loss: 8.093392]\n",
      "19366 [Discriminator loss: 0.108353, acc.: 96.88%] [Generator loss: 8.541821]\n",
      "19367 [Discriminator loss: 0.023825, acc.: 100.00%] [Generator loss: 8.400238]\n",
      "19368 [Discriminator loss: 0.161900, acc.: 95.31%] [Generator loss: 7.651910]\n",
      "19369 [Discriminator loss: 0.216555, acc.: 95.31%] [Generator loss: 10.570345]\n",
      "19370 [Discriminator loss: 0.074619, acc.: 96.88%] [Generator loss: 9.126637]\n",
      "19371 [Discriminator loss: 0.157525, acc.: 93.75%] [Generator loss: 8.817721]\n",
      "19372 [Discriminator loss: 0.028194, acc.: 100.00%] [Generator loss: 8.935240]\n",
      "19373 [Discriminator loss: 0.037937, acc.: 100.00%] [Generator loss: 7.623688]\n",
      "19374 [Discriminator loss: 0.216556, acc.: 90.62%] [Generator loss: 10.637595]\n",
      "19375 [Discriminator loss: 0.044466, acc.: 98.44%] [Generator loss: 10.972352]\n",
      "19376 [Discriminator loss: 0.224098, acc.: 89.06%] [Generator loss: 7.569208]\n",
      "19377 [Discriminator loss: 0.013404, acc.: 100.00%] [Generator loss: 8.406891]\n",
      "19378 [Discriminator loss: 0.021783, acc.: 100.00%] [Generator loss: 6.940406]\n",
      "19379 [Discriminator loss: 0.020836, acc.: 98.44%] [Generator loss: 9.207554]\n",
      "19380 [Discriminator loss: 0.009899, acc.: 100.00%] [Generator loss: 8.773677]\n",
      "19381 [Discriminator loss: 0.099128, acc.: 96.88%] [Generator loss: 9.258102]\n",
      "19382 [Discriminator loss: 0.037756, acc.: 98.44%] [Generator loss: 9.483543]\n",
      "19383 [Discriminator loss: 0.011693, acc.: 100.00%] [Generator loss: 8.379297]\n",
      "19384 [Discriminator loss: 0.094611, acc.: 95.31%] [Generator loss: 8.953024]\n",
      "19385 [Discriminator loss: 0.029396, acc.: 100.00%] [Generator loss: 8.460579]\n",
      "19386 [Discriminator loss: 0.046261, acc.: 98.44%] [Generator loss: 8.733132]\n",
      "19387 [Discriminator loss: 0.173235, acc.: 92.19%] [Generator loss: 10.080990]\n",
      "19388 [Discriminator loss: 0.055383, acc.: 98.44%] [Generator loss: 9.641555]\n",
      "19389 [Discriminator loss: 0.123556, acc.: 95.31%] [Generator loss: 10.458058]\n",
      "19390 [Discriminator loss: 0.034567, acc.: 96.88%] [Generator loss: 8.726624]\n",
      "19391 [Discriminator loss: 0.020016, acc.: 98.44%] [Generator loss: 7.971771]\n",
      "19392 [Discriminator loss: 0.204916, acc.: 90.62%] [Generator loss: 9.510896]\n",
      "19393 [Discriminator loss: 0.070211, acc.: 98.44%] [Generator loss: 9.045654]\n",
      "19394 [Discriminator loss: 0.097651, acc.: 96.88%] [Generator loss: 8.936932]\n",
      "19395 [Discriminator loss: 0.060436, acc.: 98.44%] [Generator loss: 8.894432]\n",
      "19396 [Discriminator loss: 0.085635, acc.: 96.88%] [Generator loss: 7.712358]\n",
      "19397 [Discriminator loss: 0.067493, acc.: 95.31%] [Generator loss: 8.297910]\n",
      "19398 [Discriminator loss: 0.022291, acc.: 100.00%] [Generator loss: 8.643156]\n",
      "19399 [Discriminator loss: 0.136512, acc.: 96.88%] [Generator loss: 7.504646]\n",
      "19400 [Discriminator loss: 0.032383, acc.: 100.00%] [Generator loss: 7.689082]\n",
      "19401 [Discriminator loss: 0.014600, acc.: 100.00%] [Generator loss: 8.356143]\n",
      "19402 [Discriminator loss: 0.103212, acc.: 95.31%] [Generator loss: 9.399521]\n",
      "19403 [Discriminator loss: 0.104536, acc.: 95.31%] [Generator loss: 7.756687]\n",
      "19404 [Discriminator loss: 0.025064, acc.: 98.44%] [Generator loss: 7.788708]\n",
      "19405 [Discriminator loss: 0.210462, acc.: 92.19%] [Generator loss: 7.791379]\n",
      "19406 [Discriminator loss: 0.015998, acc.: 100.00%] [Generator loss: 8.971876]\n",
      "19407 [Discriminator loss: 0.036073, acc.: 98.44%] [Generator loss: 10.710687]\n",
      "19408 [Discriminator loss: 0.038964, acc.: 98.44%] [Generator loss: 9.715615]\n",
      "19409 [Discriminator loss: 0.073083, acc.: 95.31%] [Generator loss: 8.560020]\n",
      "19410 [Discriminator loss: 0.041868, acc.: 98.44%] [Generator loss: 7.987298]\n",
      "19411 [Discriminator loss: 0.113716, acc.: 95.31%] [Generator loss: 6.620866]\n",
      "19412 [Discriminator loss: 0.109500, acc.: 95.31%] [Generator loss: 8.358056]\n",
      "19413 [Discriminator loss: 0.025361, acc.: 98.44%] [Generator loss: 10.058134]\n",
      "19414 [Discriminator loss: 0.071196, acc.: 96.88%] [Generator loss: 9.101152]\n",
      "19415 [Discriminator loss: 0.033265, acc.: 100.00%] [Generator loss: 7.818796]\n",
      "19416 [Discriminator loss: 0.025298, acc.: 98.44%] [Generator loss: 9.600605]\n",
      "19417 [Discriminator loss: 0.064053, acc.: 98.44%] [Generator loss: 9.602205]\n",
      "19418 [Discriminator loss: 0.040122, acc.: 98.44%] [Generator loss: 9.657785]\n",
      "19419 [Discriminator loss: 0.031070, acc.: 100.00%] [Generator loss: 8.136440]\n",
      "19420 [Discriminator loss: 0.174911, acc.: 95.31%] [Generator loss: 10.182854]\n",
      "19421 [Discriminator loss: 0.013180, acc.: 100.00%] [Generator loss: 10.085053]\n",
      "19422 [Discriminator loss: 0.063476, acc.: 98.44%] [Generator loss: 8.730771]\n",
      "19423 [Discriminator loss: 0.051502, acc.: 96.88%] [Generator loss: 8.095832]\n",
      "19424 [Discriminator loss: 0.023266, acc.: 98.44%] [Generator loss: 8.936230]\n",
      "19425 [Discriminator loss: 0.007901, acc.: 100.00%] [Generator loss: 8.192442]\n",
      "19426 [Discriminator loss: 0.049928, acc.: 98.44%] [Generator loss: 7.347758]\n",
      "19427 [Discriminator loss: 0.114191, acc.: 95.31%] [Generator loss: 8.545950]\n",
      "19428 [Discriminator loss: 0.035460, acc.: 98.44%] [Generator loss: 8.273686]\n",
      "19429 [Discriminator loss: 0.065539, acc.: 96.88%] [Generator loss: 7.890847]\n",
      "19430 [Discriminator loss: 0.151379, acc.: 92.19%] [Generator loss: 9.275869]\n",
      "19431 [Discriminator loss: 0.043986, acc.: 98.44%] [Generator loss: 8.754616]\n",
      "19432 [Discriminator loss: 0.016254, acc.: 100.00%] [Generator loss: 9.804855]\n",
      "19433 [Discriminator loss: 0.072539, acc.: 98.44%] [Generator loss: 9.198612]\n",
      "19434 [Discriminator loss: 0.010551, acc.: 100.00%] [Generator loss: 8.378428]\n",
      "19435 [Discriminator loss: 0.066490, acc.: 96.88%] [Generator loss: 7.663726]\n",
      "19436 [Discriminator loss: 0.080295, acc.: 96.88%] [Generator loss: 8.521863]\n",
      "19437 [Discriminator loss: 0.161430, acc.: 95.31%] [Generator loss: 9.127151]\n",
      "19438 [Discriminator loss: 0.021571, acc.: 98.44%] [Generator loss: 9.171684]\n",
      "19439 [Discriminator loss: 0.136998, acc.: 93.75%] [Generator loss: 7.887619]\n",
      "19440 [Discriminator loss: 0.094748, acc.: 96.88%] [Generator loss: 7.884946]\n",
      "19441 [Discriminator loss: 0.034080, acc.: 98.44%] [Generator loss: 9.442660]\n",
      "19442 [Discriminator loss: 0.134089, acc.: 95.31%] [Generator loss: 10.234498]\n",
      "19443 [Discriminator loss: 0.152376, acc.: 95.31%] [Generator loss: 10.880057]\n",
      "19444 [Discriminator loss: 0.024420, acc.: 98.44%] [Generator loss: 11.329501]\n",
      "19445 [Discriminator loss: 0.133056, acc.: 96.88%] [Generator loss: 8.867479]\n",
      "19446 [Discriminator loss: 0.103534, acc.: 96.88%] [Generator loss: 7.029636]\n",
      "19447 [Discriminator loss: 0.170390, acc.: 93.75%] [Generator loss: 8.588954]\n",
      "19448 [Discriminator loss: 0.012131, acc.: 100.00%] [Generator loss: 9.009627]\n",
      "19449 [Discriminator loss: 0.074412, acc.: 96.88%] [Generator loss: 8.445631]\n",
      "19450 [Discriminator loss: 0.009333, acc.: 100.00%] [Generator loss: 8.410754]\n",
      "19451 [Discriminator loss: 0.055433, acc.: 98.44%] [Generator loss: 6.775363]\n",
      "19452 [Discriminator loss: 0.015505, acc.: 100.00%] [Generator loss: 8.569555]\n",
      "19453 [Discriminator loss: 0.022432, acc.: 98.44%] [Generator loss: 7.831719]\n",
      "19454 [Discriminator loss: 0.115705, acc.: 95.31%] [Generator loss: 8.042686]\n",
      "19455 [Discriminator loss: 0.157161, acc.: 93.75%] [Generator loss: 8.069258]\n",
      "19456 [Discriminator loss: 0.124200, acc.: 90.62%] [Generator loss: 7.264017]\n",
      "19457 [Discriminator loss: 0.145161, acc.: 92.19%] [Generator loss: 8.899152]\n",
      "19458 [Discriminator loss: 0.020811, acc.: 98.44%] [Generator loss: 7.702928]\n",
      "19459 [Discriminator loss: 0.196317, acc.: 92.19%] [Generator loss: 11.251331]\n",
      "19460 [Discriminator loss: 0.019495, acc.: 100.00%] [Generator loss: 11.504463]\n",
      "19461 [Discriminator loss: 0.155933, acc.: 92.19%] [Generator loss: 7.958479]\n",
      "19462 [Discriminator loss: 0.033683, acc.: 98.44%] [Generator loss: 6.806173]\n",
      "19463 [Discriminator loss: 0.151408, acc.: 90.62%] [Generator loss: 8.517554]\n",
      "19464 [Discriminator loss: 0.059994, acc.: 96.88%] [Generator loss: 9.560491]\n",
      "19465 [Discriminator loss: 0.051466, acc.: 100.00%] [Generator loss: 9.766060]\n",
      "19466 [Discriminator loss: 0.116306, acc.: 93.75%] [Generator loss: 7.883246]\n",
      "19467 [Discriminator loss: 0.024464, acc.: 98.44%] [Generator loss: 7.642834]\n",
      "19468 [Discriminator loss: 0.016296, acc.: 100.00%] [Generator loss: 6.901651]\n",
      "19469 [Discriminator loss: 0.025373, acc.: 98.44%] [Generator loss: 8.472805]\n",
      "19470 [Discriminator loss: 0.067007, acc.: 98.44%] [Generator loss: 9.249269]\n",
      "19471 [Discriminator loss: 0.029545, acc.: 98.44%] [Generator loss: 9.746035]\n",
      "19472 [Discriminator loss: 0.107005, acc.: 95.31%] [Generator loss: 8.055746]\n",
      "19473 [Discriminator loss: 0.031862, acc.: 98.44%] [Generator loss: 8.492787]\n",
      "19474 [Discriminator loss: 0.067257, acc.: 95.31%] [Generator loss: 7.641346]\n",
      "19475 [Discriminator loss: 0.067219, acc.: 95.31%] [Generator loss: 9.380417]\n",
      "19476 [Discriminator loss: 0.108058, acc.: 96.88%] [Generator loss: 7.513476]\n",
      "19477 [Discriminator loss: 0.033338, acc.: 100.00%] [Generator loss: 8.263086]\n",
      "19478 [Discriminator loss: 0.021275, acc.: 100.00%] [Generator loss: 9.319078]\n",
      "19479 [Discriminator loss: 0.116704, acc.: 96.88%] [Generator loss: 10.331965]\n",
      "19480 [Discriminator loss: 0.180438, acc.: 93.75%] [Generator loss: 8.942949]\n",
      "19481 [Discriminator loss: 0.009425, acc.: 100.00%] [Generator loss: 10.060926]\n",
      "19482 [Discriminator loss: 0.101604, acc.: 96.88%] [Generator loss: 8.513243]\n",
      "19483 [Discriminator loss: 0.250413, acc.: 89.06%] [Generator loss: 7.275233]\n",
      "19484 [Discriminator loss: 0.088110, acc.: 95.31%] [Generator loss: 7.536525]\n",
      "19485 [Discriminator loss: 0.133907, acc.: 95.31%] [Generator loss: 10.769235]\n",
      "19486 [Discriminator loss: 0.087199, acc.: 98.44%] [Generator loss: 8.949003]\n",
      "19487 [Discriminator loss: 0.082116, acc.: 96.88%] [Generator loss: 6.813212]\n",
      "19488 [Discriminator loss: 0.087861, acc.: 96.88%] [Generator loss: 7.845040]\n",
      "19489 [Discriminator loss: 0.091442, acc.: 98.44%] [Generator loss: 9.399821]\n",
      "19490 [Discriminator loss: 0.083550, acc.: 93.75%] [Generator loss: 7.231015]\n",
      "19491 [Discriminator loss: 0.147786, acc.: 93.75%] [Generator loss: 7.257847]\n",
      "19492 [Discriminator loss: 0.114413, acc.: 98.44%] [Generator loss: 7.553490]\n",
      "19493 [Discriminator loss: 0.005895, acc.: 100.00%] [Generator loss: 9.087554]\n",
      "19494 [Discriminator loss: 0.067644, acc.: 98.44%] [Generator loss: 9.873238]\n",
      "19495 [Discriminator loss: 0.129602, acc.: 96.88%] [Generator loss: 8.748486]\n",
      "19496 [Discriminator loss: 0.097242, acc.: 98.44%] [Generator loss: 7.113484]\n",
      "19497 [Discriminator loss: 0.047439, acc.: 98.44%] [Generator loss: 8.908096]\n",
      "19498 [Discriminator loss: 0.062522, acc.: 96.88%] [Generator loss: 9.658655]\n",
      "19499 [Discriminator loss: 0.049420, acc.: 100.00%] [Generator loss: 9.254192]\n",
      "19500 [Discriminator loss: 0.074996, acc.: 96.88%] [Generator loss: 9.128432]\n",
      "19501 [Discriminator loss: 0.053817, acc.: 96.88%] [Generator loss: 8.387047]\n",
      "19502 [Discriminator loss: 0.120887, acc.: 95.31%] [Generator loss: 8.833446]\n",
      "19503 [Discriminator loss: 0.054147, acc.: 98.44%] [Generator loss: 10.877932]\n",
      "19504 [Discriminator loss: 0.139878, acc.: 93.75%] [Generator loss: 8.147821]\n",
      "19505 [Discriminator loss: 0.036404, acc.: 98.44%] [Generator loss: 9.252541]\n",
      "19506 [Discriminator loss: 0.027921, acc.: 100.00%] [Generator loss: 8.175968]\n",
      "19507 [Discriminator loss: 0.013123, acc.: 100.00%] [Generator loss: 9.521296]\n",
      "19508 [Discriminator loss: 0.018101, acc.: 100.00%] [Generator loss: 8.708456]\n",
      "19509 [Discriminator loss: 0.065036, acc.: 96.88%] [Generator loss: 8.249114]\n",
      "19510 [Discriminator loss: 0.104054, acc.: 95.31%] [Generator loss: 7.673403]\n",
      "19511 [Discriminator loss: 0.086666, acc.: 98.44%] [Generator loss: 9.211414]\n",
      "19512 [Discriminator loss: 0.062467, acc.: 96.88%] [Generator loss: 8.203655]\n",
      "19513 [Discriminator loss: 0.071898, acc.: 98.44%] [Generator loss: 8.124033]\n",
      "19514 [Discriminator loss: 0.103770, acc.: 95.31%] [Generator loss: 10.695883]\n",
      "19515 [Discriminator loss: 0.103532, acc.: 96.88%] [Generator loss: 8.048710]\n",
      "19516 [Discriminator loss: 0.044121, acc.: 100.00%] [Generator loss: 8.739031]\n",
      "19517 [Discriminator loss: 0.035925, acc.: 100.00%] [Generator loss: 8.748739]\n",
      "19518 [Discriminator loss: 0.083822, acc.: 95.31%] [Generator loss: 8.315222]\n",
      "19519 [Discriminator loss: 0.114773, acc.: 95.31%] [Generator loss: 6.328794]\n",
      "19520 [Discriminator loss: 0.094657, acc.: 95.31%] [Generator loss: 6.318641]\n",
      "19521 [Discriminator loss: 0.078175, acc.: 96.88%] [Generator loss: 6.948582]\n",
      "19522 [Discriminator loss: 0.016752, acc.: 100.00%] [Generator loss: 10.931586]\n",
      "19523 [Discriminator loss: 0.083338, acc.: 96.88%] [Generator loss: 8.119776]\n",
      "19524 [Discriminator loss: 0.037711, acc.: 98.44%] [Generator loss: 8.597219]\n",
      "19525 [Discriminator loss: 0.081448, acc.: 96.88%] [Generator loss: 7.898980]\n",
      "19526 [Discriminator loss: 0.023340, acc.: 98.44%] [Generator loss: 7.692510]\n",
      "19527 [Discriminator loss: 0.021856, acc.: 100.00%] [Generator loss: 6.046705]\n",
      "19528 [Discriminator loss: 0.151268, acc.: 95.31%] [Generator loss: 8.071465]\n",
      "19529 [Discriminator loss: 0.012097, acc.: 100.00%] [Generator loss: 9.960999]\n",
      "19530 [Discriminator loss: 0.022435, acc.: 98.44%] [Generator loss: 8.966443]\n",
      "19531 [Discriminator loss: 0.065777, acc.: 98.44%] [Generator loss: 8.032705]\n",
      "19532 [Discriminator loss: 0.035663, acc.: 100.00%] [Generator loss: 8.972327]\n",
      "19533 [Discriminator loss: 0.171386, acc.: 95.31%] [Generator loss: 8.577819]\n",
      "19534 [Discriminator loss: 0.110016, acc.: 95.31%] [Generator loss: 9.909412]\n",
      "19535 [Discriminator loss: 0.097573, acc.: 96.88%] [Generator loss: 8.585619]\n",
      "19536 [Discriminator loss: 0.045693, acc.: 100.00%] [Generator loss: 7.865449]\n",
      "19537 [Discriminator loss: 0.104291, acc.: 96.88%] [Generator loss: 9.626661]\n",
      "19538 [Discriminator loss: 0.026729, acc.: 98.44%] [Generator loss: 9.854347]\n",
      "19539 [Discriminator loss: 0.164239, acc.: 95.31%] [Generator loss: 7.615906]\n",
      "19540 [Discriminator loss: 0.172795, acc.: 95.31%] [Generator loss: 8.370975]\n",
      "19541 [Discriminator loss: 0.026455, acc.: 98.44%] [Generator loss: 7.852600]\n",
      "19542 [Discriminator loss: 0.026684, acc.: 98.44%] [Generator loss: 9.097805]\n",
      "19543 [Discriminator loss: 0.083742, acc.: 95.31%] [Generator loss: 8.707448]\n",
      "19544 [Discriminator loss: 0.020345, acc.: 100.00%] [Generator loss: 9.223150]\n",
      "19545 [Discriminator loss: 0.053603, acc.: 96.88%] [Generator loss: 9.104049]\n",
      "19546 [Discriminator loss: 0.090612, acc.: 98.44%] [Generator loss: 9.294975]\n",
      "19547 [Discriminator loss: 0.045288, acc.: 96.88%] [Generator loss: 8.558855]\n",
      "19548 [Discriminator loss: 0.201256, acc.: 90.62%] [Generator loss: 8.261008]\n",
      "19549 [Discriminator loss: 0.026172, acc.: 100.00%] [Generator loss: 8.392537]\n",
      "19550 [Discriminator loss: 0.024253, acc.: 100.00%] [Generator loss: 8.671640]\n",
      "19551 [Discriminator loss: 0.039622, acc.: 98.44%] [Generator loss: 8.512069]\n",
      "19552 [Discriminator loss: 0.036706, acc.: 100.00%] [Generator loss: 8.716247]\n",
      "19553 [Discriminator loss: 0.117906, acc.: 93.75%] [Generator loss: 7.377981]\n",
      "19554 [Discriminator loss: 0.030291, acc.: 98.44%] [Generator loss: 6.953186]\n",
      "19555 [Discriminator loss: 0.049997, acc.: 98.44%] [Generator loss: 8.819790]\n",
      "19556 [Discriminator loss: 0.033701, acc.: 98.44%] [Generator loss: 9.002789]\n",
      "19557 [Discriminator loss: 0.107527, acc.: 93.75%] [Generator loss: 6.771542]\n",
      "19558 [Discriminator loss: 0.068885, acc.: 96.88%] [Generator loss: 8.223002]\n",
      "19559 [Discriminator loss: 0.087687, acc.: 95.31%] [Generator loss: 7.143334]\n",
      "19560 [Discriminator loss: 0.052957, acc.: 95.31%] [Generator loss: 10.286181]\n",
      "19561 [Discriminator loss: 0.028812, acc.: 98.44%] [Generator loss: 8.881863]\n",
      "19562 [Discriminator loss: 0.038306, acc.: 100.00%] [Generator loss: 8.482619]\n",
      "19563 [Discriminator loss: 0.049485, acc.: 98.44%] [Generator loss: 7.917378]\n",
      "19564 [Discriminator loss: 0.044330, acc.: 98.44%] [Generator loss: 8.092887]\n",
      "19565 [Discriminator loss: 0.044300, acc.: 98.44%] [Generator loss: 8.307276]\n",
      "19566 [Discriminator loss: 0.033016, acc.: 100.00%] [Generator loss: 7.840487]\n",
      "19567 [Discriminator loss: 0.112401, acc.: 95.31%] [Generator loss: 8.307872]\n",
      "19568 [Discriminator loss: 0.033084, acc.: 98.44%] [Generator loss: 7.998064]\n",
      "19569 [Discriminator loss: 0.053629, acc.: 96.88%] [Generator loss: 8.878572]\n",
      "19570 [Discriminator loss: 0.031933, acc.: 98.44%] [Generator loss: 8.520438]\n",
      "19571 [Discriminator loss: 0.105177, acc.: 96.88%] [Generator loss: 9.194009]\n",
      "19572 [Discriminator loss: 0.115958, acc.: 95.31%] [Generator loss: 8.005846]\n",
      "19573 [Discriminator loss: 0.120356, acc.: 96.88%] [Generator loss: 8.952524]\n",
      "19574 [Discriminator loss: 0.027907, acc.: 100.00%] [Generator loss: 8.114642]\n",
      "19575 [Discriminator loss: 0.051649, acc.: 96.88%] [Generator loss: 7.962906]\n",
      "19576 [Discriminator loss: 0.036124, acc.: 100.00%] [Generator loss: 7.804570]\n",
      "19577 [Discriminator loss: 0.032298, acc.: 98.44%] [Generator loss: 9.750700]\n",
      "19578 [Discriminator loss: 0.112359, acc.: 93.75%] [Generator loss: 8.742880]\n",
      "19579 [Discriminator loss: 0.047687, acc.: 96.88%] [Generator loss: 9.842423]\n",
      "19580 [Discriminator loss: 0.276037, acc.: 93.75%] [Generator loss: 9.922816]\n",
      "19581 [Discriminator loss: 0.082724, acc.: 95.31%] [Generator loss: 8.646380]\n",
      "19582 [Discriminator loss: 0.100246, acc.: 93.75%] [Generator loss: 8.702399]\n",
      "19583 [Discriminator loss: 0.093476, acc.: 93.75%] [Generator loss: 8.383606]\n",
      "19584 [Discriminator loss: 0.016176, acc.: 100.00%] [Generator loss: 8.766870]\n",
      "19585 [Discriminator loss: 0.110993, acc.: 95.31%] [Generator loss: 8.109566]\n",
      "19586 [Discriminator loss: 0.075307, acc.: 95.31%] [Generator loss: 8.068197]\n",
      "19587 [Discriminator loss: 0.122810, acc.: 95.31%] [Generator loss: 8.372865]\n",
      "19588 [Discriminator loss: 0.058514, acc.: 98.44%] [Generator loss: 9.573629]\n",
      "19589 [Discriminator loss: 0.074214, acc.: 96.88%] [Generator loss: 6.971105]\n",
      "19590 [Discriminator loss: 0.134763, acc.: 95.31%] [Generator loss: 7.556513]\n",
      "19591 [Discriminator loss: 0.040245, acc.: 98.44%] [Generator loss: 9.207904]\n",
      "19592 [Discriminator loss: 0.096438, acc.: 96.88%] [Generator loss: 8.716273]\n",
      "19593 [Discriminator loss: 0.097804, acc.: 95.31%] [Generator loss: 7.821177]\n",
      "19594 [Discriminator loss: 0.039870, acc.: 100.00%] [Generator loss: 8.757252]\n",
      "19595 [Discriminator loss: 0.006829, acc.: 100.00%] [Generator loss: 8.081228]\n",
      "19596 [Discriminator loss: 0.016132, acc.: 100.00%] [Generator loss: 7.981411]\n",
      "19597 [Discriminator loss: 0.108354, acc.: 95.31%] [Generator loss: 8.951967]\n",
      "19598 [Discriminator loss: 0.021377, acc.: 98.44%] [Generator loss: 9.402102]\n",
      "19599 [Discriminator loss: 0.019277, acc.: 100.00%] [Generator loss: 7.113963]\n",
      "19600 [Discriminator loss: 0.150619, acc.: 93.75%] [Generator loss: 7.735014]\n",
      "19601 [Discriminator loss: 0.161185, acc.: 90.62%] [Generator loss: 7.570252]\n",
      "19602 [Discriminator loss: 0.066403, acc.: 98.44%] [Generator loss: 7.885620]\n",
      "19603 [Discriminator loss: 0.036618, acc.: 100.00%] [Generator loss: 8.498979]\n",
      "19604 [Discriminator loss: 0.081698, acc.: 96.88%] [Generator loss: 8.198367]\n",
      "19605 [Discriminator loss: 0.123725, acc.: 96.88%] [Generator loss: 7.719782]\n",
      "19606 [Discriminator loss: 0.056875, acc.: 96.88%] [Generator loss: 8.540020]\n",
      "19607 [Discriminator loss: 0.079419, acc.: 96.88%] [Generator loss: 10.168802]\n",
      "19608 [Discriminator loss: 0.051722, acc.: 98.44%] [Generator loss: 10.075060]\n",
      "19609 [Discriminator loss: 0.077051, acc.: 95.31%] [Generator loss: 9.859841]\n",
      "19610 [Discriminator loss: 0.021961, acc.: 100.00%] [Generator loss: 9.686576]\n",
      "19611 [Discriminator loss: 0.049594, acc.: 96.88%] [Generator loss: 9.950226]\n",
      "19612 [Discriminator loss: 0.196293, acc.: 92.19%] [Generator loss: 7.633851]\n",
      "19613 [Discriminator loss: 0.043845, acc.: 98.44%] [Generator loss: 7.405190]\n",
      "19614 [Discriminator loss: 0.030922, acc.: 98.44%] [Generator loss: 8.059336]\n",
      "19615 [Discriminator loss: 0.020952, acc.: 100.00%] [Generator loss: 9.275373]\n",
      "19616 [Discriminator loss: 0.099848, acc.: 96.88%] [Generator loss: 6.873392]\n",
      "19617 [Discriminator loss: 0.012667, acc.: 100.00%] [Generator loss: 8.385464]\n",
      "19618 [Discriminator loss: 0.085363, acc.: 96.88%] [Generator loss: 8.222049]\n",
      "19619 [Discriminator loss: 0.083704, acc.: 93.75%] [Generator loss: 8.294138]\n",
      "19620 [Discriminator loss: 0.106327, acc.: 96.88%] [Generator loss: 8.399218]\n",
      "19621 [Discriminator loss: 0.071889, acc.: 96.88%] [Generator loss: 5.927266]\n",
      "19622 [Discriminator loss: 0.015405, acc.: 100.00%] [Generator loss: 7.758327]\n",
      "19623 [Discriminator loss: 0.048252, acc.: 98.44%] [Generator loss: 7.486557]\n",
      "19624 [Discriminator loss: 0.083969, acc.: 98.44%] [Generator loss: 7.592999]\n",
      "19625 [Discriminator loss: 0.086133, acc.: 96.88%] [Generator loss: 7.869622]\n",
      "19626 [Discriminator loss: 0.021696, acc.: 100.00%] [Generator loss: 8.563007]\n",
      "19627 [Discriminator loss: 0.009383, acc.: 100.00%] [Generator loss: 8.701399]\n",
      "19628 [Discriminator loss: 0.046196, acc.: 96.88%] [Generator loss: 9.062662]\n",
      "19629 [Discriminator loss: 0.031778, acc.: 98.44%] [Generator loss: 8.102306]\n",
      "19630 [Discriminator loss: 0.046844, acc.: 98.44%] [Generator loss: 9.474333]\n",
      "19631 [Discriminator loss: 0.038496, acc.: 98.44%] [Generator loss: 8.278166]\n",
      "19632 [Discriminator loss: 0.032512, acc.: 98.44%] [Generator loss: 9.039977]\n",
      "19633 [Discriminator loss: 0.218846, acc.: 92.19%] [Generator loss: 8.486409]\n",
      "19634 [Discriminator loss: 0.026953, acc.: 100.00%] [Generator loss: 8.450777]\n",
      "19635 [Discriminator loss: 0.170106, acc.: 93.75%] [Generator loss: 7.744874]\n",
      "19636 [Discriminator loss: 0.136250, acc.: 95.31%] [Generator loss: 7.361689]\n",
      "19637 [Discriminator loss: 0.022370, acc.: 100.00%] [Generator loss: 9.908405]\n",
      "19638 [Discriminator loss: 0.023226, acc.: 100.00%] [Generator loss: 8.247626]\n",
      "19639 [Discriminator loss: 0.100017, acc.: 96.88%] [Generator loss: 8.915226]\n",
      "19640 [Discriminator loss: 0.050788, acc.: 98.44%] [Generator loss: 6.540239]\n",
      "19641 [Discriminator loss: 0.228503, acc.: 90.62%] [Generator loss: 9.325592]\n",
      "19642 [Discriminator loss: 0.021845, acc.: 100.00%] [Generator loss: 9.492081]\n",
      "19643 [Discriminator loss: 0.076201, acc.: 95.31%] [Generator loss: 9.697610]\n",
      "19644 [Discriminator loss: 0.031895, acc.: 98.44%] [Generator loss: 8.338154]\n",
      "19645 [Discriminator loss: 0.042498, acc.: 98.44%] [Generator loss: 7.996927]\n",
      "19646 [Discriminator loss: 0.018683, acc.: 100.00%] [Generator loss: 7.148042]\n",
      "19647 [Discriminator loss: 0.077950, acc.: 96.88%] [Generator loss: 8.777606]\n",
      "19648 [Discriminator loss: 0.039928, acc.: 98.44%] [Generator loss: 8.739372]\n",
      "19649 [Discriminator loss: 0.027248, acc.: 100.00%] [Generator loss: 9.398199]\n",
      "19650 [Discriminator loss: 0.079757, acc.: 95.31%] [Generator loss: 8.454352]\n",
      "19651 [Discriminator loss: 0.122739, acc.: 95.31%] [Generator loss: 9.957388]\n",
      "19652 [Discriminator loss: 0.038877, acc.: 98.44%] [Generator loss: 8.362859]\n",
      "19653 [Discriminator loss: 0.055093, acc.: 98.44%] [Generator loss: 8.768394]\n",
      "19654 [Discriminator loss: 0.013763, acc.: 100.00%] [Generator loss: 8.913549]\n",
      "19655 [Discriminator loss: 0.035702, acc.: 98.44%] [Generator loss: 8.370510]\n",
      "19656 [Discriminator loss: 0.025697, acc.: 98.44%] [Generator loss: 8.042946]\n",
      "19657 [Discriminator loss: 0.141631, acc.: 95.31%] [Generator loss: 9.211331]\n",
      "19658 [Discriminator loss: 0.084828, acc.: 96.88%] [Generator loss: 8.398009]\n",
      "19659 [Discriminator loss: 0.157695, acc.: 92.19%] [Generator loss: 10.403215]\n",
      "19660 [Discriminator loss: 0.035691, acc.: 98.44%] [Generator loss: 8.702592]\n",
      "19661 [Discriminator loss: 0.023236, acc.: 100.00%] [Generator loss: 8.370662]\n",
      "19662 [Discriminator loss: 0.101517, acc.: 95.31%] [Generator loss: 8.222742]\n",
      "19663 [Discriminator loss: 0.020799, acc.: 100.00%] [Generator loss: 6.990453]\n",
      "19664 [Discriminator loss: 0.029835, acc.: 98.44%] [Generator loss: 8.524285]\n",
      "19665 [Discriminator loss: 0.023735, acc.: 100.00%] [Generator loss: 9.010702]\n",
      "19666 [Discriminator loss: 0.238280, acc.: 92.19%] [Generator loss: 8.229372]\n",
      "19667 [Discriminator loss: 0.021184, acc.: 100.00%] [Generator loss: 8.718361]\n",
      "19668 [Discriminator loss: 0.105140, acc.: 95.31%] [Generator loss: 9.333006]\n",
      "19669 [Discriminator loss: 0.083587, acc.: 96.88%] [Generator loss: 9.586159]\n",
      "19670 [Discriminator loss: 0.080327, acc.: 98.44%] [Generator loss: 7.499597]\n",
      "19671 [Discriminator loss: 0.026561, acc.: 100.00%] [Generator loss: 8.486526]\n",
      "19672 [Discriminator loss: 0.039653, acc.: 98.44%] [Generator loss: 9.414835]\n",
      "19673 [Discriminator loss: 0.143949, acc.: 98.44%] [Generator loss: 10.217957]\n",
      "19674 [Discriminator loss: 0.050091, acc.: 98.44%] [Generator loss: 9.803863]\n",
      "19675 [Discriminator loss: 0.013822, acc.: 100.00%] [Generator loss: 8.241031]\n",
      "19676 [Discriminator loss: 0.108542, acc.: 96.88%] [Generator loss: 8.069647]\n",
      "19677 [Discriminator loss: 0.082797, acc.: 96.88%] [Generator loss: 9.296572]\n",
      "19678 [Discriminator loss: 0.020514, acc.: 98.44%] [Generator loss: 10.237775]\n",
      "19679 [Discriminator loss: 0.003693, acc.: 100.00%] [Generator loss: 9.514647]\n",
      "19680 [Discriminator loss: 0.148448, acc.: 93.75%] [Generator loss: 7.146077]\n",
      "19681 [Discriminator loss: 0.022290, acc.: 100.00%] [Generator loss: 8.471272]\n",
      "19682 [Discriminator loss: 0.012857, acc.: 100.00%] [Generator loss: 7.667232]\n",
      "19683 [Discriminator loss: 0.045995, acc.: 98.44%] [Generator loss: 8.552786]\n",
      "19684 [Discriminator loss: 0.003295, acc.: 100.00%] [Generator loss: 8.848185]\n",
      "19685 [Discriminator loss: 0.144379, acc.: 95.31%] [Generator loss: 7.226255]\n",
      "19686 [Discriminator loss: 0.192455, acc.: 93.75%] [Generator loss: 7.801240]\n",
      "19687 [Discriminator loss: 0.068495, acc.: 98.44%] [Generator loss: 8.873538]\n",
      "19688 [Discriminator loss: 0.019799, acc.: 100.00%] [Generator loss: 10.643244]\n",
      "19689 [Discriminator loss: 0.062040, acc.: 96.88%] [Generator loss: 10.210628]\n",
      "19690 [Discriminator loss: 0.077842, acc.: 98.44%] [Generator loss: 7.957089]\n",
      "19691 [Discriminator loss: 0.101937, acc.: 96.88%] [Generator loss: 7.581901]\n",
      "19692 [Discriminator loss: 0.164624, acc.: 92.19%] [Generator loss: 9.402802]\n",
      "19693 [Discriminator loss: 0.005499, acc.: 100.00%] [Generator loss: 9.410345]\n",
      "19694 [Discriminator loss: 0.043737, acc.: 100.00%] [Generator loss: 9.809587]\n",
      "19695 [Discriminator loss: 0.097479, acc.: 96.88%] [Generator loss: 7.708334]\n",
      "19696 [Discriminator loss: 0.038007, acc.: 98.44%] [Generator loss: 7.724465]\n",
      "19697 [Discriminator loss: 0.029747, acc.: 98.44%] [Generator loss: 7.501015]\n",
      "19698 [Discriminator loss: 0.008444, acc.: 100.00%] [Generator loss: 7.564199]\n",
      "19699 [Discriminator loss: 0.103557, acc.: 96.88%] [Generator loss: 8.054077]\n",
      "19700 [Discriminator loss: 0.049942, acc.: 98.44%] [Generator loss: 6.680096]\n",
      "19701 [Discriminator loss: 0.079638, acc.: 98.44%] [Generator loss: 7.979179]\n",
      "19702 [Discriminator loss: 0.090250, acc.: 93.75%] [Generator loss: 7.789038]\n",
      "19703 [Discriminator loss: 0.038724, acc.: 98.44%] [Generator loss: 8.885672]\n",
      "19704 [Discriminator loss: 0.025859, acc.: 100.00%] [Generator loss: 9.371399]\n",
      "19705 [Discriminator loss: 0.005361, acc.: 100.00%] [Generator loss: 8.091413]\n",
      "19706 [Discriminator loss: 0.083475, acc.: 95.31%] [Generator loss: 7.431059]\n",
      "19707 [Discriminator loss: 0.059226, acc.: 98.44%] [Generator loss: 8.640863]\n",
      "19708 [Discriminator loss: 0.014645, acc.: 100.00%] [Generator loss: 9.000311]\n",
      "19709 [Discriminator loss: 0.232930, acc.: 89.06%] [Generator loss: 7.411450]\n",
      "19710 [Discriminator loss: 0.073342, acc.: 95.31%] [Generator loss: 9.370403]\n",
      "19711 [Discriminator loss: 0.140325, acc.: 96.88%] [Generator loss: 8.433515]\n",
      "19712 [Discriminator loss: 0.024146, acc.: 98.44%] [Generator loss: 8.240166]\n",
      "19713 [Discriminator loss: 0.015168, acc.: 100.00%] [Generator loss: 8.295512]\n",
      "19714 [Discriminator loss: 0.012607, acc.: 100.00%] [Generator loss: 8.661455]\n",
      "19715 [Discriminator loss: 0.036150, acc.: 100.00%] [Generator loss: 6.768945]\n",
      "19716 [Discriminator loss: 0.071786, acc.: 98.44%] [Generator loss: 8.021037]\n",
      "19717 [Discriminator loss: 0.129209, acc.: 93.75%] [Generator loss: 8.559265]\n",
      "19718 [Discriminator loss: 0.070196, acc.: 96.88%] [Generator loss: 8.372385]\n",
      "19719 [Discriminator loss: 0.005626, acc.: 100.00%] [Generator loss: 10.119403]\n",
      "19720 [Discriminator loss: 0.012550, acc.: 100.00%] [Generator loss: 8.720256]\n",
      "19721 [Discriminator loss: 0.112085, acc.: 93.75%] [Generator loss: 9.757748]\n",
      "19722 [Discriminator loss: 0.105144, acc.: 96.88%] [Generator loss: 8.070456]\n",
      "19723 [Discriminator loss: 0.017703, acc.: 100.00%] [Generator loss: 9.584064]\n",
      "19724 [Discriminator loss: 0.114602, acc.: 95.31%] [Generator loss: 9.550834]\n",
      "19725 [Discriminator loss: 0.267039, acc.: 92.19%] [Generator loss: 8.877535]\n",
      "19726 [Discriminator loss: 0.014989, acc.: 100.00%] [Generator loss: 10.165846]\n",
      "19727 [Discriminator loss: 0.146708, acc.: 95.31%] [Generator loss: 7.213150]\n",
      "19728 [Discriminator loss: 0.028998, acc.: 98.44%] [Generator loss: 8.131369]\n",
      "19729 [Discriminator loss: 0.026873, acc.: 98.44%] [Generator loss: 7.734162]\n",
      "19730 [Discriminator loss: 0.021475, acc.: 100.00%] [Generator loss: 8.769660]\n",
      "19731 [Discriminator loss: 0.216194, acc.: 95.31%] [Generator loss: 8.815741]\n",
      "19732 [Discriminator loss: 0.116677, acc.: 95.31%] [Generator loss: 9.484762]\n",
      "19733 [Discriminator loss: 0.081177, acc.: 96.88%] [Generator loss: 9.366243]\n",
      "19734 [Discriminator loss: 0.010540, acc.: 100.00%] [Generator loss: 9.421152]\n",
      "19735 [Discriminator loss: 0.071097, acc.: 98.44%] [Generator loss: 7.956240]\n",
      "19736 [Discriminator loss: 0.057602, acc.: 98.44%] [Generator loss: 8.840112]\n",
      "19737 [Discriminator loss: 0.065447, acc.: 96.88%] [Generator loss: 8.209915]\n",
      "19738 [Discriminator loss: 0.045095, acc.: 98.44%] [Generator loss: 9.051826]\n",
      "19739 [Discriminator loss: 0.161217, acc.: 93.75%] [Generator loss: 9.255632]\n",
      "19740 [Discriminator loss: 0.035696, acc.: 98.44%] [Generator loss: 10.434862]\n",
      "19741 [Discriminator loss: 0.094014, acc.: 96.88%] [Generator loss: 10.283464]\n",
      "19742 [Discriminator loss: 0.091554, acc.: 98.44%] [Generator loss: 8.849075]\n",
      "19743 [Discriminator loss: 0.028862, acc.: 98.44%] [Generator loss: 8.320174]\n",
      "19744 [Discriminator loss: 0.020908, acc.: 100.00%] [Generator loss: 8.244904]\n",
      "19745 [Discriminator loss: 0.103909, acc.: 96.88%] [Generator loss: 7.165286]\n",
      "19746 [Discriminator loss: 0.059987, acc.: 96.88%] [Generator loss: 9.543000]\n",
      "19747 [Discriminator loss: 0.120637, acc.: 93.75%] [Generator loss: 9.815960]\n",
      "19748 [Discriminator loss: 0.074784, acc.: 95.31%] [Generator loss: 9.377728]\n",
      "19749 [Discriminator loss: 0.186086, acc.: 95.31%] [Generator loss: 6.918661]\n",
      "19750 [Discriminator loss: 0.049016, acc.: 98.44%] [Generator loss: 7.431817]\n",
      "19751 [Discriminator loss: 0.095568, acc.: 96.88%] [Generator loss: 9.074160]\n",
      "19752 [Discriminator loss: 0.055071, acc.: 98.44%] [Generator loss: 9.946987]\n",
      "19753 [Discriminator loss: 0.092909, acc.: 95.31%] [Generator loss: 9.222368]\n",
      "19754 [Discriminator loss: 0.030367, acc.: 98.44%] [Generator loss: 9.477219]\n",
      "19755 [Discriminator loss: 0.098080, acc.: 93.75%] [Generator loss: 7.065878]\n",
      "19756 [Discriminator loss: 0.051686, acc.: 98.44%] [Generator loss: 10.001415]\n",
      "19757 [Discriminator loss: 0.041694, acc.: 98.44%] [Generator loss: 9.153214]\n",
      "19758 [Discriminator loss: 0.023193, acc.: 100.00%] [Generator loss: 8.948565]\n",
      "19759 [Discriminator loss: 0.061548, acc.: 98.44%] [Generator loss: 7.926516]\n",
      "19760 [Discriminator loss: 0.091978, acc.: 96.88%] [Generator loss: 6.149809]\n",
      "19761 [Discriminator loss: 0.074641, acc.: 95.31%] [Generator loss: 6.490887]\n",
      "19762 [Discriminator loss: 0.093855, acc.: 96.88%] [Generator loss: 9.739700]\n",
      "19763 [Discriminator loss: 0.005533, acc.: 100.00%] [Generator loss: 10.383879]\n",
      "19764 [Discriminator loss: 0.246177, acc.: 89.06%] [Generator loss: 4.997811]\n",
      "19765 [Discriminator loss: 0.120415, acc.: 93.75%] [Generator loss: 8.273979]\n",
      "19766 [Discriminator loss: 0.046173, acc.: 98.44%] [Generator loss: 9.772177]\n",
      "19767 [Discriminator loss: 0.081018, acc.: 98.44%] [Generator loss: 8.148839]\n",
      "19768 [Discriminator loss: 0.028907, acc.: 100.00%] [Generator loss: 7.526808]\n",
      "19769 [Discriminator loss: 0.037616, acc.: 100.00%] [Generator loss: 8.138935]\n",
      "19770 [Discriminator loss: 0.066826, acc.: 96.88%] [Generator loss: 7.495513]\n",
      "19771 [Discriminator loss: 0.075965, acc.: 95.31%] [Generator loss: 7.698076]\n",
      "19772 [Discriminator loss: 0.174871, acc.: 93.75%] [Generator loss: 11.456953]\n",
      "19773 [Discriminator loss: 0.173678, acc.: 92.19%] [Generator loss: 8.448942]\n",
      "19774 [Discriminator loss: 0.096267, acc.: 98.44%] [Generator loss: 7.587700]\n",
      "19775 [Discriminator loss: 0.100424, acc.: 96.88%] [Generator loss: 7.824417]\n",
      "19776 [Discriminator loss: 0.049741, acc.: 98.44%] [Generator loss: 8.856785]\n",
      "19777 [Discriminator loss: 0.011667, acc.: 100.00%] [Generator loss: 6.368968]\n",
      "19778 [Discriminator loss: 0.276940, acc.: 92.19%] [Generator loss: 9.469258]\n",
      "19779 [Discriminator loss: 0.111864, acc.: 93.75%] [Generator loss: 7.222878]\n",
      "19780 [Discriminator loss: 0.058426, acc.: 98.44%] [Generator loss: 7.977595]\n",
      "19781 [Discriminator loss: 0.032078, acc.: 98.44%] [Generator loss: 9.904707]\n",
      "19782 [Discriminator loss: 0.019429, acc.: 100.00%] [Generator loss: 9.230186]\n",
      "19783 [Discriminator loss: 0.048962, acc.: 96.88%] [Generator loss: 7.361828]\n",
      "19784 [Discriminator loss: 0.107087, acc.: 96.88%] [Generator loss: 7.557777]\n",
      "19785 [Discriminator loss: 0.111177, acc.: 95.31%] [Generator loss: 8.680290]\n",
      "19786 [Discriminator loss: 0.033494, acc.: 96.88%] [Generator loss: 8.954732]\n",
      "19787 [Discriminator loss: 0.199090, acc.: 92.19%] [Generator loss: 7.927862]\n",
      "19788 [Discriminator loss: 0.052761, acc.: 98.44%] [Generator loss: 8.149535]\n",
      "19789 [Discriminator loss: 0.105971, acc.: 95.31%] [Generator loss: 6.649211]\n",
      "19790 [Discriminator loss: 0.069706, acc.: 98.44%] [Generator loss: 8.559155]\n",
      "19791 [Discriminator loss: 0.036627, acc.: 96.88%] [Generator loss: 8.770014]\n",
      "19792 [Discriminator loss: 0.043002, acc.: 98.44%] [Generator loss: 9.961027]\n",
      "19793 [Discriminator loss: 0.095170, acc.: 96.88%] [Generator loss: 7.851981]\n",
      "19794 [Discriminator loss: 0.176419, acc.: 95.31%] [Generator loss: 9.531480]\n",
      "19795 [Discriminator loss: 0.014055, acc.: 100.00%] [Generator loss: 8.789463]\n",
      "19796 [Discriminator loss: 0.160907, acc.: 96.88%] [Generator loss: 9.175882]\n",
      "19797 [Discriminator loss: 0.021033, acc.: 100.00%] [Generator loss: 7.631152]\n",
      "19798 [Discriminator loss: 0.041076, acc.: 96.88%] [Generator loss: 6.798615]\n",
      "19799 [Discriminator loss: 0.016715, acc.: 100.00%] [Generator loss: 7.878741]\n",
      "19800 [Discriminator loss: 0.035979, acc.: 98.44%] [Generator loss: 6.506291]\n",
      "19801 [Discriminator loss: 0.069843, acc.: 96.88%] [Generator loss: 8.017111]\n",
      "19802 [Discriminator loss: 0.034656, acc.: 98.44%] [Generator loss: 8.764315]\n",
      "19803 [Discriminator loss: 0.136017, acc.: 95.31%] [Generator loss: 9.056447]\n",
      "19804 [Discriminator loss: 0.143388, acc.: 92.19%] [Generator loss: 9.335655]\n",
      "19805 [Discriminator loss: 0.134713, acc.: 95.31%] [Generator loss: 9.855541]\n",
      "19806 [Discriminator loss: 0.068778, acc.: 98.44%] [Generator loss: 10.010882]\n",
      "19807 [Discriminator loss: 0.125801, acc.: 95.31%] [Generator loss: 8.534616]\n",
      "19808 [Discriminator loss: 0.081640, acc.: 96.88%] [Generator loss: 8.355141]\n",
      "19809 [Discriminator loss: 0.109273, acc.: 98.44%] [Generator loss: 7.272221]\n",
      "19810 [Discriminator loss: 0.017300, acc.: 100.00%] [Generator loss: 6.555954]\n",
      "19811 [Discriminator loss: 0.066590, acc.: 98.44%] [Generator loss: 7.020337]\n",
      "19812 [Discriminator loss: 0.015823, acc.: 100.00%] [Generator loss: 7.492805]\n",
      "19813 [Discriminator loss: 0.060193, acc.: 98.44%] [Generator loss: 8.074922]\n",
      "19814 [Discriminator loss: 0.186972, acc.: 95.31%] [Generator loss: 8.656608]\n",
      "19815 [Discriminator loss: 0.118019, acc.: 96.88%] [Generator loss: 8.066417]\n",
      "19816 [Discriminator loss: 0.052726, acc.: 98.44%] [Generator loss: 7.727097]\n",
      "19817 [Discriminator loss: 0.020472, acc.: 100.00%] [Generator loss: 8.436269]\n",
      "19818 [Discriminator loss: 0.035289, acc.: 100.00%] [Generator loss: 7.172810]\n",
      "19819 [Discriminator loss: 0.044581, acc.: 98.44%] [Generator loss: 8.892416]\n",
      "19820 [Discriminator loss: 0.057095, acc.: 98.44%] [Generator loss: 8.977427]\n",
      "19821 [Discriminator loss: 0.060233, acc.: 96.88%] [Generator loss: 8.552670]\n",
      "19822 [Discriminator loss: 0.050855, acc.: 96.88%] [Generator loss: 7.530123]\n",
      "19823 [Discriminator loss: 0.167844, acc.: 90.62%] [Generator loss: 8.446840]\n",
      "19824 [Discriminator loss: 0.040431, acc.: 96.88%] [Generator loss: 9.458772]\n",
      "19825 [Discriminator loss: 0.068272, acc.: 98.44%] [Generator loss: 8.782323]\n",
      "19826 [Discriminator loss: 0.429736, acc.: 87.50%] [Generator loss: 6.326412]\n",
      "19827 [Discriminator loss: 0.055831, acc.: 96.88%] [Generator loss: 7.101513]\n",
      "19828 [Discriminator loss: 0.039241, acc.: 98.44%] [Generator loss: 6.926158]\n",
      "19829 [Discriminator loss: 0.094514, acc.: 95.31%] [Generator loss: 7.717037]\n",
      "19830 [Discriminator loss: 0.088578, acc.: 96.88%] [Generator loss: 9.619604]\n",
      "19831 [Discriminator loss: 0.034564, acc.: 98.44%] [Generator loss: 9.968084]\n",
      "19832 [Discriminator loss: 0.094358, acc.: 95.31%] [Generator loss: 6.713373]\n",
      "19833 [Discriminator loss: 0.084132, acc.: 98.44%] [Generator loss: 8.889996]\n",
      "19834 [Discriminator loss: 0.011121, acc.: 100.00%] [Generator loss: 7.705185]\n",
      "19835 [Discriminator loss: 0.172783, acc.: 96.88%] [Generator loss: 8.505225]\n",
      "19836 [Discriminator loss: 0.113442, acc.: 93.75%] [Generator loss: 9.973249]\n",
      "19837 [Discriminator loss: 0.404601, acc.: 87.50%] [Generator loss: 7.341421]\n",
      "19838 [Discriminator loss: 0.099162, acc.: 96.88%] [Generator loss: 9.574949]\n",
      "19839 [Discriminator loss: 0.123665, acc.: 98.44%] [Generator loss: 8.213261]\n",
      "19840 [Discriminator loss: 0.044165, acc.: 98.44%] [Generator loss: 8.105846]\n",
      "19841 [Discriminator loss: 0.096791, acc.: 96.88%] [Generator loss: 7.737993]\n",
      "19842 [Discriminator loss: 0.081588, acc.: 96.88%] [Generator loss: 8.611839]\n",
      "19843 [Discriminator loss: 0.017959, acc.: 98.44%] [Generator loss: 9.478501]\n",
      "19844 [Discriminator loss: 0.155921, acc.: 92.19%] [Generator loss: 6.717232]\n",
      "19845 [Discriminator loss: 0.105368, acc.: 95.31%] [Generator loss: 8.808158]\n",
      "19846 [Discriminator loss: 0.023432, acc.: 98.44%] [Generator loss: 8.861979]\n",
      "19847 [Discriminator loss: 0.100995, acc.: 96.88%] [Generator loss: 9.143393]\n",
      "19848 [Discriminator loss: 0.046400, acc.: 98.44%] [Generator loss: 9.092052]\n",
      "19849 [Discriminator loss: 0.062106, acc.: 96.88%] [Generator loss: 8.398424]\n",
      "19850 [Discriminator loss: 0.087338, acc.: 96.88%] [Generator loss: 7.948267]\n",
      "19851 [Discriminator loss: 0.013972, acc.: 100.00%] [Generator loss: 8.395929]\n",
      "19852 [Discriminator loss: 0.009873, acc.: 100.00%] [Generator loss: 8.430665]\n",
      "19853 [Discriminator loss: 0.116383, acc.: 95.31%] [Generator loss: 6.142325]\n",
      "19854 [Discriminator loss: 0.104118, acc.: 95.31%] [Generator loss: 8.388907]\n",
      "19855 [Discriminator loss: 0.009490, acc.: 100.00%] [Generator loss: 7.753858]\n",
      "19856 [Discriminator loss: 0.037100, acc.: 98.44%] [Generator loss: 8.168617]\n",
      "19857 [Discriminator loss: 0.010283, acc.: 100.00%] [Generator loss: 8.562227]\n",
      "19858 [Discriminator loss: 0.064041, acc.: 98.44%] [Generator loss: 9.488397]\n",
      "19859 [Discriminator loss: 0.021145, acc.: 100.00%] [Generator loss: 8.321321]\n",
      "19860 [Discriminator loss: 0.050627, acc.: 96.88%] [Generator loss: 7.457533]\n",
      "19861 [Discriminator loss: 0.079150, acc.: 98.44%] [Generator loss: 7.819969]\n",
      "19862 [Discriminator loss: 0.180857, acc.: 92.19%] [Generator loss: 8.324144]\n",
      "19863 [Discriminator loss: 0.074259, acc.: 95.31%] [Generator loss: 10.234070]\n",
      "19864 [Discriminator loss: 0.084093, acc.: 96.88%] [Generator loss: 9.430582]\n",
      "19865 [Discriminator loss: 0.059363, acc.: 96.88%] [Generator loss: 8.574044]\n",
      "19866 [Discriminator loss: 0.066479, acc.: 96.88%] [Generator loss: 9.353174]\n",
      "19867 [Discriminator loss: 0.077659, acc.: 96.88%] [Generator loss: 8.227919]\n",
      "19868 [Discriminator loss: 0.094409, acc.: 96.88%] [Generator loss: 7.099444]\n",
      "19869 [Discriminator loss: 0.114673, acc.: 96.88%] [Generator loss: 8.472285]\n",
      "19870 [Discriminator loss: 0.100258, acc.: 98.44%] [Generator loss: 7.502482]\n",
      "19871 [Discriminator loss: 0.045437, acc.: 96.88%] [Generator loss: 7.496842]\n",
      "19872 [Discriminator loss: 0.081034, acc.: 96.88%] [Generator loss: 8.182104]\n",
      "19873 [Discriminator loss: 0.043123, acc.: 98.44%] [Generator loss: 7.984110]\n",
      "19874 [Discriminator loss: 0.058230, acc.: 98.44%] [Generator loss: 8.130613]\n",
      "19875 [Discriminator loss: 0.252459, acc.: 92.19%] [Generator loss: 9.901482]\n",
      "19876 [Discriminator loss: 0.011719, acc.: 100.00%] [Generator loss: 10.245255]\n",
      "19877 [Discriminator loss: 0.003262, acc.: 100.00%] [Generator loss: 10.132256]\n",
      "19878 [Discriminator loss: 0.052193, acc.: 96.88%] [Generator loss: 9.321257]\n",
      "19879 [Discriminator loss: 0.056735, acc.: 96.88%] [Generator loss: 7.693804]\n",
      "19880 [Discriminator loss: 0.142565, acc.: 96.88%] [Generator loss: 8.957556]\n",
      "19881 [Discriminator loss: 0.062264, acc.: 96.88%] [Generator loss: 9.649287]\n",
      "19882 [Discriminator loss: 0.044894, acc.: 98.44%] [Generator loss: 8.274466]\n",
      "19883 [Discriminator loss: 0.128374, acc.: 96.88%] [Generator loss: 7.844113]\n",
      "19884 [Discriminator loss: 0.031565, acc.: 98.44%] [Generator loss: 9.950051]\n",
      "19885 [Discriminator loss: 0.040035, acc.: 100.00%] [Generator loss: 8.250556]\n",
      "19886 [Discriminator loss: 0.091687, acc.: 95.31%] [Generator loss: 7.123063]\n",
      "19887 [Discriminator loss: 0.008144, acc.: 100.00%] [Generator loss: 8.277905]\n",
      "19888 [Discriminator loss: 0.111731, acc.: 95.31%] [Generator loss: 8.526669]\n",
      "19889 [Discriminator loss: 0.028808, acc.: 98.44%] [Generator loss: 9.514149]\n",
      "19890 [Discriminator loss: 0.005498, acc.: 100.00%] [Generator loss: 8.051708]\n",
      "19891 [Discriminator loss: 0.046413, acc.: 98.44%] [Generator loss: 9.204523]\n",
      "19892 [Discriminator loss: 0.084076, acc.: 96.88%] [Generator loss: 7.075875]\n",
      "19893 [Discriminator loss: 0.019285, acc.: 100.00%] [Generator loss: 7.961119]\n",
      "19894 [Discriminator loss: 0.036106, acc.: 100.00%] [Generator loss: 9.392049]\n",
      "19895 [Discriminator loss: 0.052486, acc.: 96.88%] [Generator loss: 8.800245]\n",
      "19896 [Discriminator loss: 0.128875, acc.: 96.88%] [Generator loss: 7.168754]\n",
      "19897 [Discriminator loss: 0.041732, acc.: 98.44%] [Generator loss: 7.788817]\n",
      "19898 [Discriminator loss: 0.075216, acc.: 98.44%] [Generator loss: 7.623465]\n",
      "19899 [Discriminator loss: 0.110741, acc.: 92.19%] [Generator loss: 9.763187]\n",
      "19900 [Discriminator loss: 0.093150, acc.: 96.88%] [Generator loss: 9.772102]\n",
      "19901 [Discriminator loss: 0.014021, acc.: 100.00%] [Generator loss: 9.461254]\n",
      "19902 [Discriminator loss: 0.128563, acc.: 93.75%] [Generator loss: 8.335880]\n",
      "19903 [Discriminator loss: 0.021553, acc.: 100.00%] [Generator loss: 7.140704]\n",
      "19904 [Discriminator loss: 0.063663, acc.: 96.88%] [Generator loss: 8.767525]\n",
      "19905 [Discriminator loss: 0.037928, acc.: 98.44%] [Generator loss: 8.838863]\n",
      "19906 [Discriminator loss: 0.047566, acc.: 100.00%] [Generator loss: 9.421520]\n",
      "19907 [Discriminator loss: 0.023157, acc.: 100.00%] [Generator loss: 9.137491]\n",
      "19908 [Discriminator loss: 0.176914, acc.: 92.19%] [Generator loss: 7.990746]\n",
      "19909 [Discriminator loss: 0.138301, acc.: 96.88%] [Generator loss: 8.409178]\n",
      "19910 [Discriminator loss: 0.018537, acc.: 98.44%] [Generator loss: 10.447947]\n",
      "19911 [Discriminator loss: 0.335325, acc.: 92.19%] [Generator loss: 7.813707]\n",
      "19912 [Discriminator loss: 0.048133, acc.: 98.44%] [Generator loss: 9.290871]\n",
      "19913 [Discriminator loss: 0.046958, acc.: 98.44%] [Generator loss: 8.348513]\n",
      "19914 [Discriminator loss: 0.030810, acc.: 98.44%] [Generator loss: 10.142284]\n",
      "19915 [Discriminator loss: 0.037289, acc.: 98.44%] [Generator loss: 8.632793]\n",
      "19916 [Discriminator loss: 0.067807, acc.: 96.88%] [Generator loss: 9.105923]\n",
      "19917 [Discriminator loss: 0.157994, acc.: 92.19%] [Generator loss: 8.565927]\n",
      "19918 [Discriminator loss: 0.057188, acc.: 96.88%] [Generator loss: 7.269065]\n",
      "19919 [Discriminator loss: 0.092792, acc.: 95.31%] [Generator loss: 8.138199]\n",
      "19920 [Discriminator loss: 0.104835, acc.: 95.31%] [Generator loss: 8.265799]\n",
      "19921 [Discriminator loss: 0.106256, acc.: 93.75%] [Generator loss: 8.602219]\n",
      "19922 [Discriminator loss: 0.003416, acc.: 100.00%] [Generator loss: 10.791912]\n",
      "19923 [Discriminator loss: 0.055715, acc.: 96.88%] [Generator loss: 9.638683]\n",
      "19924 [Discriminator loss: 0.078198, acc.: 96.88%] [Generator loss: 8.644927]\n",
      "19925 [Discriminator loss: 0.027263, acc.: 98.44%] [Generator loss: 9.009785]\n",
      "19926 [Discriminator loss: 0.073044, acc.: 98.44%] [Generator loss: 8.513648]\n",
      "19927 [Discriminator loss: 0.037921, acc.: 100.00%] [Generator loss: 7.719897]\n",
      "19928 [Discriminator loss: 0.022816, acc.: 98.44%] [Generator loss: 7.652252]\n",
      "19929 [Discriminator loss: 0.022847, acc.: 98.44%] [Generator loss: 9.005000]\n",
      "19930 [Discriminator loss: 0.070916, acc.: 95.31%] [Generator loss: 7.861238]\n",
      "19931 [Discriminator loss: 0.056734, acc.: 98.44%] [Generator loss: 8.662685]\n",
      "19932 [Discriminator loss: 0.029397, acc.: 100.00%] [Generator loss: 8.170804]\n",
      "19933 [Discriminator loss: 0.051544, acc.: 98.44%] [Generator loss: 8.039984]\n",
      "19934 [Discriminator loss: 0.091420, acc.: 96.88%] [Generator loss: 7.091394]\n",
      "19935 [Discriminator loss: 0.159621, acc.: 95.31%] [Generator loss: 7.160875]\n",
      "19936 [Discriminator loss: 0.039604, acc.: 98.44%] [Generator loss: 7.672319]\n",
      "19937 [Discriminator loss: 0.115606, acc.: 96.88%] [Generator loss: 9.657854]\n",
      "19938 [Discriminator loss: 0.092806, acc.: 93.75%] [Generator loss: 7.666664]\n",
      "19939 [Discriminator loss: 0.078025, acc.: 96.88%] [Generator loss: 8.418793]\n",
      "19940 [Discriminator loss: 0.085653, acc.: 96.88%] [Generator loss: 8.031221]\n",
      "19941 [Discriminator loss: 0.166192, acc.: 92.19%] [Generator loss: 9.600914]\n",
      "19942 [Discriminator loss: 0.029939, acc.: 98.44%] [Generator loss: 11.135929]\n",
      "19943 [Discriminator loss: 0.131683, acc.: 95.31%] [Generator loss: 8.718145]\n",
      "19944 [Discriminator loss: 0.104363, acc.: 96.88%] [Generator loss: 7.735459]\n",
      "19945 [Discriminator loss: 0.072005, acc.: 96.88%] [Generator loss: 8.832635]\n",
      "19946 [Discriminator loss: 0.073140, acc.: 95.31%] [Generator loss: 8.820969]\n",
      "19947 [Discriminator loss: 0.026949, acc.: 98.44%] [Generator loss: 7.074564]\n",
      "19948 [Discriminator loss: 0.025482, acc.: 98.44%] [Generator loss: 7.812360]\n",
      "19949 [Discriminator loss: 0.170492, acc.: 93.75%] [Generator loss: 8.666597]\n",
      "19950 [Discriminator loss: 0.044537, acc.: 98.44%] [Generator loss: 10.572270]\n",
      "19951 [Discriminator loss: 0.078880, acc.: 96.88%] [Generator loss: 7.848470]\n",
      "19952 [Discriminator loss: 0.089669, acc.: 96.88%] [Generator loss: 8.854881]\n",
      "19953 [Discriminator loss: 0.011361, acc.: 100.00%] [Generator loss: 8.151594]\n",
      "19954 [Discriminator loss: 0.180032, acc.: 92.19%] [Generator loss: 8.018955]\n",
      "19955 [Discriminator loss: 0.098487, acc.: 95.31%] [Generator loss: 8.857151]\n",
      "19956 [Discriminator loss: 0.026155, acc.: 100.00%] [Generator loss: 8.396841]\n",
      "19957 [Discriminator loss: 0.068477, acc.: 96.88%] [Generator loss: 8.270020]\n",
      "19958 [Discriminator loss: 0.026813, acc.: 98.44%] [Generator loss: 9.126612]\n",
      "19959 [Discriminator loss: 0.023050, acc.: 100.00%] [Generator loss: 7.359029]\n",
      "19960 [Discriminator loss: 0.115369, acc.: 98.44%] [Generator loss: 8.329990]\n",
      "19961 [Discriminator loss: 0.031193, acc.: 98.44%] [Generator loss: 8.980239]\n",
      "19962 [Discriminator loss: 0.042402, acc.: 100.00%] [Generator loss: 9.676702]\n",
      "19963 [Discriminator loss: 0.099578, acc.: 93.75%] [Generator loss: 8.743995]\n",
      "19964 [Discriminator loss: 0.018287, acc.: 100.00%] [Generator loss: 10.929037]\n",
      "19965 [Discriminator loss: 0.067454, acc.: 98.44%] [Generator loss: 8.768398]\n",
      "19966 [Discriminator loss: 0.091701, acc.: 95.31%] [Generator loss: 6.623491]\n",
      "19967 [Discriminator loss: 0.168487, acc.: 89.06%] [Generator loss: 7.968543]\n",
      "19968 [Discriminator loss: 0.059430, acc.: 96.88%] [Generator loss: 8.430706]\n",
      "19969 [Discriminator loss: 0.084707, acc.: 96.88%] [Generator loss: 9.808969]\n",
      "19970 [Discriminator loss: 0.219135, acc.: 92.19%] [Generator loss: 9.436718]\n",
      "19971 [Discriminator loss: 0.064952, acc.: 98.44%] [Generator loss: 9.198345]\n",
      "19972 [Discriminator loss: 0.101821, acc.: 95.31%] [Generator loss: 9.179620]\n",
      "19973 [Discriminator loss: 0.004605, acc.: 100.00%] [Generator loss: 9.532230]\n",
      "19974 [Discriminator loss: 0.046327, acc.: 100.00%] [Generator loss: 8.864643]\n",
      "19975 [Discriminator loss: 0.088679, acc.: 95.31%] [Generator loss: 7.560882]\n",
      "19976 [Discriminator loss: 0.069943, acc.: 96.88%] [Generator loss: 9.552736]\n",
      "19977 [Discriminator loss: 0.047133, acc.: 98.44%] [Generator loss: 9.668247]\n",
      "19978 [Discriminator loss: 0.079753, acc.: 96.88%] [Generator loss: 8.516979]\n",
      "19979 [Discriminator loss: 0.010958, acc.: 100.00%] [Generator loss: 8.508879]\n",
      "19980 [Discriminator loss: 0.045483, acc.: 98.44%] [Generator loss: 7.872990]\n",
      "19981 [Discriminator loss: 0.258575, acc.: 90.62%] [Generator loss: 8.271994]\n",
      "19982 [Discriminator loss: 0.065105, acc.: 98.44%] [Generator loss: 8.728413]\n",
      "19983 [Discriminator loss: 0.067594, acc.: 96.88%] [Generator loss: 8.167154]\n",
      "19984 [Discriminator loss: 0.030402, acc.: 100.00%] [Generator loss: 8.759289]\n",
      "19985 [Discriminator loss: 0.050212, acc.: 98.44%] [Generator loss: 9.166917]\n",
      "19986 [Discriminator loss: 0.108850, acc.: 95.31%] [Generator loss: 9.736883]\n",
      "19987 [Discriminator loss: 0.269588, acc.: 93.75%] [Generator loss: 8.660335]\n",
      "19988 [Discriminator loss: 0.176179, acc.: 96.88%] [Generator loss: 8.399185]\n",
      "19989 [Discriminator loss: 0.148262, acc.: 93.75%] [Generator loss: 11.059700]\n",
      "19990 [Discriminator loss: 0.061286, acc.: 98.44%] [Generator loss: 10.607723]\n",
      "19991 [Discriminator loss: 0.136986, acc.: 95.31%] [Generator loss: 7.407974]\n",
      "19992 [Discriminator loss: 0.056273, acc.: 96.88%] [Generator loss: 7.458715]\n",
      "19993 [Discriminator loss: 0.018320, acc.: 100.00%] [Generator loss: 9.177696]\n",
      "19994 [Discriminator loss: 0.044321, acc.: 98.44%] [Generator loss: 8.036239]\n",
      "19995 [Discriminator loss: 0.053013, acc.: 96.88%] [Generator loss: 10.498848]\n",
      "19996 [Discriminator loss: 0.094658, acc.: 95.31%] [Generator loss: 8.342687]\n",
      "19997 [Discriminator loss: 0.017324, acc.: 100.00%] [Generator loss: 7.465623]\n",
      "19998 [Discriminator loss: 0.033997, acc.: 100.00%] [Generator loss: 8.476632]\n",
      "19999 [Discriminator loss: 0.094520, acc.: 98.44%] [Generator loss: 8.574951]\n",
      "20000 [Discriminator loss: 0.088193, acc.: 96.88%] [Generator loss: 8.257025]\n",
      "20001 [Discriminator loss: 0.123683, acc.: 96.88%] [Generator loss: 7.879897]\n",
      "20002 [Discriminator loss: 0.009317, acc.: 100.00%] [Generator loss: 10.332579]\n",
      "20003 [Discriminator loss: 0.025983, acc.: 100.00%] [Generator loss: 8.263831]\n",
      "20004 [Discriminator loss: 0.014335, acc.: 100.00%] [Generator loss: 9.711334]\n",
      "20005 [Discriminator loss: 0.234791, acc.: 92.19%] [Generator loss: 7.900415]\n",
      "20006 [Discriminator loss: 0.013497, acc.: 100.00%] [Generator loss: 9.742261]\n",
      "20007 [Discriminator loss: 0.103984, acc.: 95.31%] [Generator loss: 8.668324]\n",
      "20008 [Discriminator loss: 0.119744, acc.: 95.31%] [Generator loss: 7.624137]\n",
      "20009 [Discriminator loss: 0.247699, acc.: 92.19%] [Generator loss: 8.885206]\n",
      "20010 [Discriminator loss: 0.064536, acc.: 96.88%] [Generator loss: 8.740780]\n",
      "20011 [Discriminator loss: 0.110833, acc.: 95.31%] [Generator loss: 7.978813]\n",
      "20012 [Discriminator loss: 0.034482, acc.: 100.00%] [Generator loss: 8.199196]\n",
      "20013 [Discriminator loss: 0.065115, acc.: 95.31%] [Generator loss: 9.272611]\n",
      "20014 [Discriminator loss: 0.060859, acc.: 96.88%] [Generator loss: 7.010972]\n",
      "20015 [Discriminator loss: 0.103104, acc.: 96.88%] [Generator loss: 7.544713]\n",
      "20016 [Discriminator loss: 0.013660, acc.: 100.00%] [Generator loss: 9.756817]\n",
      "20017 [Discriminator loss: 0.014877, acc.: 100.00%] [Generator loss: 7.903157]\n",
      "20018 [Discriminator loss: 0.020154, acc.: 100.00%] [Generator loss: 7.174778]\n",
      "20019 [Discriminator loss: 0.088818, acc.: 95.31%] [Generator loss: 8.509371]\n",
      "20020 [Discriminator loss: 0.067857, acc.: 98.44%] [Generator loss: 7.253435]\n",
      "20021 [Discriminator loss: 0.051480, acc.: 96.88%] [Generator loss: 6.595525]\n",
      "20022 [Discriminator loss: 0.043394, acc.: 96.88%] [Generator loss: 7.673752]\n",
      "20023 [Discriminator loss: 0.096835, acc.: 96.88%] [Generator loss: 9.057358]\n",
      "20024 [Discriminator loss: 0.039805, acc.: 98.44%] [Generator loss: 8.769900]\n",
      "20025 [Discriminator loss: 0.043032, acc.: 98.44%] [Generator loss: 9.160133]\n",
      "20026 [Discriminator loss: 0.047522, acc.: 96.88%] [Generator loss: 9.096270]\n",
      "20027 [Discriminator loss: 0.081168, acc.: 96.88%] [Generator loss: 7.607223]\n",
      "20028 [Discriminator loss: 0.042529, acc.: 98.44%] [Generator loss: 8.183695]\n",
      "20029 [Discriminator loss: 0.065252, acc.: 95.31%] [Generator loss: 10.777309]\n",
      "20030 [Discriminator loss: 0.093591, acc.: 96.88%] [Generator loss: 9.740959]\n",
      "20031 [Discriminator loss: 0.088765, acc.: 98.44%] [Generator loss: 8.241620]\n",
      "20032 [Discriminator loss: 0.141015, acc.: 95.31%] [Generator loss: 8.384621]\n",
      "20033 [Discriminator loss: 0.142057, acc.: 95.31%] [Generator loss: 6.529006]\n",
      "20034 [Discriminator loss: 0.021014, acc.: 98.44%] [Generator loss: 7.342358]\n",
      "20035 [Discriminator loss: 0.063273, acc.: 98.44%] [Generator loss: 7.253831]\n",
      "20036 [Discriminator loss: 0.132069, acc.: 95.31%] [Generator loss: 8.365812]\n",
      "20037 [Discriminator loss: 0.052764, acc.: 96.88%] [Generator loss: 9.582208]\n",
      "20038 [Discriminator loss: 0.065820, acc.: 95.31%] [Generator loss: 10.695728]\n",
      "20039 [Discriminator loss: 0.031260, acc.: 98.44%] [Generator loss: 9.430288]\n",
      "20040 [Discriminator loss: 0.087367, acc.: 95.31%] [Generator loss: 8.023188]\n",
      "20041 [Discriminator loss: 0.009059, acc.: 100.00%] [Generator loss: 8.929647]\n",
      "20042 [Discriminator loss: 0.046104, acc.: 98.44%] [Generator loss: 8.001120]\n",
      "20043 [Discriminator loss: 0.048723, acc.: 98.44%] [Generator loss: 7.230366]\n",
      "20044 [Discriminator loss: 0.060168, acc.: 98.44%] [Generator loss: 9.259724]\n",
      "20045 [Discriminator loss: 0.011615, acc.: 100.00%] [Generator loss: 10.801989]\n",
      "20046 [Discriminator loss: 0.187337, acc.: 90.62%] [Generator loss: 7.609626]\n",
      "20047 [Discriminator loss: 0.053129, acc.: 98.44%] [Generator loss: 8.385878]\n",
      "20048 [Discriminator loss: 0.085137, acc.: 96.88%] [Generator loss: 7.754716]\n",
      "20049 [Discriminator loss: 0.020473, acc.: 100.00%] [Generator loss: 9.326107]\n",
      "20050 [Discriminator loss: 0.113687, acc.: 93.75%] [Generator loss: 7.653696]\n",
      "20051 [Discriminator loss: 0.029152, acc.: 98.44%] [Generator loss: 9.532199]\n",
      "20052 [Discriminator loss: 0.089805, acc.: 95.31%] [Generator loss: 7.916357]\n",
      "20053 [Discriminator loss: 0.026525, acc.: 100.00%] [Generator loss: 8.268475]\n",
      "20054 [Discriminator loss: 0.117344, acc.: 95.31%] [Generator loss: 8.565981]\n",
      "20055 [Discriminator loss: 0.053397, acc.: 98.44%] [Generator loss: 10.366733]\n",
      "20056 [Discriminator loss: 0.082431, acc.: 95.31%] [Generator loss: 9.685527]\n",
      "20057 [Discriminator loss: 0.048228, acc.: 98.44%] [Generator loss: 7.214726]\n",
      "20058 [Discriminator loss: 0.084857, acc.: 95.31%] [Generator loss: 9.947861]\n",
      "20059 [Discriminator loss: 0.134139, acc.: 96.88%] [Generator loss: 9.220728]\n",
      "20060 [Discriminator loss: 0.007881, acc.: 100.00%] [Generator loss: 9.059913]\n",
      "20061 [Discriminator loss: 0.110263, acc.: 93.75%] [Generator loss: 7.275723]\n",
      "20062 [Discriminator loss: 0.034210, acc.: 98.44%] [Generator loss: 7.770976]\n",
      "20063 [Discriminator loss: 0.038068, acc.: 98.44%] [Generator loss: 9.285902]\n",
      "20064 [Discriminator loss: 0.076454, acc.: 98.44%] [Generator loss: 7.937754]\n",
      "20065 [Discriminator loss: 0.058591, acc.: 98.44%] [Generator loss: 8.093936]\n",
      "20066 [Discriminator loss: 0.023471, acc.: 98.44%] [Generator loss: 8.117636]\n",
      "20067 [Discriminator loss: 0.012741, acc.: 100.00%] [Generator loss: 8.023638]\n",
      "20068 [Discriminator loss: 0.035714, acc.: 98.44%] [Generator loss: 7.397499]\n",
      "20069 [Discriminator loss: 0.086874, acc.: 96.88%] [Generator loss: 8.215508]\n",
      "20070 [Discriminator loss: 0.079713, acc.: 96.88%] [Generator loss: 9.087519]\n",
      "20071 [Discriminator loss: 0.106135, acc.: 93.75%] [Generator loss: 11.312283]\n",
      "20072 [Discriminator loss: 0.091252, acc.: 95.31%] [Generator loss: 8.729235]\n",
      "20073 [Discriminator loss: 0.103682, acc.: 92.19%] [Generator loss: 7.163789]\n",
      "20074 [Discriminator loss: 0.021462, acc.: 100.00%] [Generator loss: 8.723961]\n",
      "20075 [Discriminator loss: 0.043997, acc.: 98.44%] [Generator loss: 8.705198]\n",
      "20076 [Discriminator loss: 0.024612, acc.: 100.00%] [Generator loss: 8.442683]\n",
      "20077 [Discriminator loss: 0.031656, acc.: 98.44%] [Generator loss: 9.483805]\n",
      "20078 [Discriminator loss: 0.102630, acc.: 93.75%] [Generator loss: 9.340954]\n",
      "20079 [Discriminator loss: 0.022100, acc.: 98.44%] [Generator loss: 10.675824]\n",
      "20080 [Discriminator loss: 0.045055, acc.: 96.88%] [Generator loss: 8.257624]\n",
      "20081 [Discriminator loss: 0.036615, acc.: 98.44%] [Generator loss: 8.044926]\n",
      "20082 [Discriminator loss: 0.087664, acc.: 96.88%] [Generator loss: 7.338430]\n",
      "20083 [Discriminator loss: 0.309037, acc.: 87.50%] [Generator loss: 8.080281]\n",
      "20084 [Discriminator loss: 0.084545, acc.: 95.31%] [Generator loss: 9.817362]\n",
      "20085 [Discriminator loss: 0.051103, acc.: 96.88%] [Generator loss: 8.892020]\n",
      "20086 [Discriminator loss: 0.050128, acc.: 96.88%] [Generator loss: 8.333286]\n",
      "20087 [Discriminator loss: 0.043268, acc.: 98.44%] [Generator loss: 9.865202]\n",
      "20088 [Discriminator loss: 0.015058, acc.: 100.00%] [Generator loss: 9.086691]\n",
      "20089 [Discriminator loss: 0.071189, acc.: 98.44%] [Generator loss: 9.336434]\n",
      "20090 [Discriminator loss: 0.120466, acc.: 96.88%] [Generator loss: 9.867101]\n",
      "20091 [Discriminator loss: 0.015288, acc.: 100.00%] [Generator loss: 10.649679]\n",
      "20092 [Discriminator loss: 0.335572, acc.: 90.62%] [Generator loss: 11.199839]\n",
      "20093 [Discriminator loss: 0.036069, acc.: 96.88%] [Generator loss: 11.966226]\n",
      "20094 [Discriminator loss: 0.323192, acc.: 87.50%] [Generator loss: 6.951161]\n",
      "20095 [Discriminator loss: 0.049223, acc.: 98.44%] [Generator loss: 6.764732]\n",
      "20096 [Discriminator loss: 0.018083, acc.: 100.00%] [Generator loss: 8.084238]\n",
      "20097 [Discriminator loss: 0.072750, acc.: 95.31%] [Generator loss: 8.503695]\n",
      "20098 [Discriminator loss: 0.069115, acc.: 96.88%] [Generator loss: 8.942528]\n",
      "20099 [Discriminator loss: 0.025350, acc.: 98.44%] [Generator loss: 6.559998]\n",
      "20100 [Discriminator loss: 0.019045, acc.: 100.00%] [Generator loss: 9.884203]\n",
      "20101 [Discriminator loss: 0.007499, acc.: 100.00%] [Generator loss: 9.736074]\n",
      "20102 [Discriminator loss: 0.038940, acc.: 98.44%] [Generator loss: 9.282056]\n",
      "20103 [Discriminator loss: 0.130357, acc.: 95.31%] [Generator loss: 8.910750]\n",
      "20104 [Discriminator loss: 0.014574, acc.: 100.00%] [Generator loss: 8.661343]\n",
      "20105 [Discriminator loss: 0.027505, acc.: 100.00%] [Generator loss: 7.896761]\n",
      "20106 [Discriminator loss: 0.103439, acc.: 96.88%] [Generator loss: 9.499295]\n",
      "20107 [Discriminator loss: 0.169499, acc.: 90.62%] [Generator loss: 11.258034]\n",
      "20108 [Discriminator loss: 0.100309, acc.: 96.88%] [Generator loss: 7.867879]\n",
      "20109 [Discriminator loss: 0.133148, acc.: 93.75%] [Generator loss: 7.990679]\n",
      "20110 [Discriminator loss: 0.043990, acc.: 96.88%] [Generator loss: 8.531344]\n",
      "20111 [Discriminator loss: 0.025091, acc.: 100.00%] [Generator loss: 8.067833]\n",
      "20112 [Discriminator loss: 0.035254, acc.: 98.44%] [Generator loss: 7.592005]\n",
      "20113 [Discriminator loss: 0.049563, acc.: 98.44%] [Generator loss: 8.397847]\n",
      "20114 [Discriminator loss: 0.038316, acc.: 98.44%] [Generator loss: 8.371081]\n",
      "20115 [Discriminator loss: 0.025065, acc.: 100.00%] [Generator loss: 8.159847]\n",
      "20116 [Discriminator loss: 0.172837, acc.: 93.75%] [Generator loss: 8.409002]\n",
      "20117 [Discriminator loss: 0.017374, acc.: 100.00%] [Generator loss: 7.241867]\n",
      "20118 [Discriminator loss: 0.136960, acc.: 93.75%] [Generator loss: 7.577606]\n",
      "20119 [Discriminator loss: 0.066359, acc.: 96.88%] [Generator loss: 7.173850]\n",
      "20120 [Discriminator loss: 0.036934, acc.: 98.44%] [Generator loss: 7.451759]\n",
      "20121 [Discriminator loss: 0.102439, acc.: 98.44%] [Generator loss: 7.052377]\n",
      "20122 [Discriminator loss: 0.029024, acc.: 98.44%] [Generator loss: 9.850135]\n",
      "20123 [Discriminator loss: 0.021250, acc.: 100.00%] [Generator loss: 8.542473]\n",
      "20124 [Discriminator loss: 0.055551, acc.: 96.88%] [Generator loss: 7.170505]\n",
      "20125 [Discriminator loss: 0.071006, acc.: 95.31%] [Generator loss: 9.063659]\n",
      "20126 [Discriminator loss: 0.019015, acc.: 100.00%] [Generator loss: 9.242477]\n",
      "20127 [Discriminator loss: 0.053276, acc.: 98.44%] [Generator loss: 7.357192]\n",
      "20128 [Discriminator loss: 0.077513, acc.: 95.31%] [Generator loss: 10.589726]\n",
      "20129 [Discriminator loss: 0.011122, acc.: 100.00%] [Generator loss: 11.464958]\n",
      "20130 [Discriminator loss: 0.035324, acc.: 98.44%] [Generator loss: 9.469193]\n",
      "20131 [Discriminator loss: 0.050944, acc.: 98.44%] [Generator loss: 8.041018]\n",
      "20132 [Discriminator loss: 0.093746, acc.: 95.31%] [Generator loss: 7.795284]\n",
      "20133 [Discriminator loss: 0.005923, acc.: 100.00%] [Generator loss: 9.736522]\n",
      "20134 [Discriminator loss: 0.039940, acc.: 96.88%] [Generator loss: 10.053375]\n",
      "20135 [Discriminator loss: 0.081850, acc.: 95.31%] [Generator loss: 7.835719]\n",
      "20136 [Discriminator loss: 0.086980, acc.: 95.31%] [Generator loss: 10.226266]\n",
      "20137 [Discriminator loss: 0.011101, acc.: 100.00%] [Generator loss: 10.445656]\n",
      "20138 [Discriminator loss: 0.109112, acc.: 93.75%] [Generator loss: 9.765683]\n",
      "20139 [Discriminator loss: 0.084423, acc.: 98.44%] [Generator loss: 9.064022]\n",
      "20140 [Discriminator loss: 0.007238, acc.: 100.00%] [Generator loss: 8.819289]\n",
      "20141 [Discriminator loss: 0.130259, acc.: 95.31%] [Generator loss: 6.030520]\n",
      "20142 [Discriminator loss: 0.123673, acc.: 95.31%] [Generator loss: 9.632147]\n",
      "20143 [Discriminator loss: 0.057012, acc.: 96.88%] [Generator loss: 10.217958]\n",
      "20144 [Discriminator loss: 0.019163, acc.: 100.00%] [Generator loss: 10.796665]\n",
      "20145 [Discriminator loss: 0.037338, acc.: 100.00%] [Generator loss: 8.400933]\n",
      "20146 [Discriminator loss: 0.052374, acc.: 98.44%] [Generator loss: 7.705307]\n",
      "20147 [Discriminator loss: 0.096753, acc.: 95.31%] [Generator loss: 8.693457]\n",
      "20148 [Discriminator loss: 0.019637, acc.: 100.00%] [Generator loss: 7.560021]\n",
      "20149 [Discriminator loss: 0.216660, acc.: 90.62%] [Generator loss: 7.807453]\n",
      "20150 [Discriminator loss: 0.097940, acc.: 95.31%] [Generator loss: 8.108330]\n",
      "20151 [Discriminator loss: 0.051033, acc.: 96.88%] [Generator loss: 9.167821]\n",
      "20152 [Discriminator loss: 0.011369, acc.: 100.00%] [Generator loss: 8.634549]\n",
      "20153 [Discriminator loss: 0.095699, acc.: 96.88%] [Generator loss: 7.154500]\n",
      "20154 [Discriminator loss: 0.150825, acc.: 92.19%] [Generator loss: 10.501608]\n",
      "20155 [Discriminator loss: 0.031382, acc.: 98.44%] [Generator loss: 9.959520]\n",
      "20156 [Discriminator loss: 0.052022, acc.: 96.88%] [Generator loss: 8.006141]\n",
      "20157 [Discriminator loss: 0.024814, acc.: 100.00%] [Generator loss: 7.856524]\n",
      "20158 [Discriminator loss: 0.025567, acc.: 100.00%] [Generator loss: 7.616075]\n",
      "20159 [Discriminator loss: 0.122378, acc.: 96.88%] [Generator loss: 7.734203]\n",
      "20160 [Discriminator loss: 0.114503, acc.: 95.31%] [Generator loss: 8.421429]\n",
      "20161 [Discriminator loss: 0.148870, acc.: 93.75%] [Generator loss: 8.765289]\n",
      "20162 [Discriminator loss: 0.103970, acc.: 96.88%] [Generator loss: 10.470551]\n",
      "20163 [Discriminator loss: 0.032952, acc.: 98.44%] [Generator loss: 9.668562]\n",
      "20164 [Discriminator loss: 0.019999, acc.: 100.00%] [Generator loss: 9.693430]\n",
      "20165 [Discriminator loss: 0.017808, acc.: 100.00%] [Generator loss: 8.611037]\n",
      "20166 [Discriminator loss: 0.012940, acc.: 100.00%] [Generator loss: 7.207701]\n",
      "20167 [Discriminator loss: 0.185202, acc.: 92.19%] [Generator loss: 7.857623]\n",
      "20168 [Discriminator loss: 0.057590, acc.: 96.88%] [Generator loss: 9.860642]\n",
      "20169 [Discriminator loss: 0.030486, acc.: 98.44%] [Generator loss: 7.187479]\n",
      "20170 [Discriminator loss: 0.113455, acc.: 96.88%] [Generator loss: 8.979771]\n",
      "20171 [Discriminator loss: 0.048839, acc.: 98.44%] [Generator loss: 8.644054]\n",
      "20172 [Discriminator loss: 0.012979, acc.: 100.00%] [Generator loss: 7.719100]\n",
      "20173 [Discriminator loss: 0.194885, acc.: 95.31%] [Generator loss: 9.441905]\n",
      "20174 [Discriminator loss: 0.074584, acc.: 95.31%] [Generator loss: 10.502604]\n",
      "20175 [Discriminator loss: 0.127204, acc.: 95.31%] [Generator loss: 7.540252]\n",
      "20176 [Discriminator loss: 0.023287, acc.: 100.00%] [Generator loss: 8.272402]\n",
      "20177 [Discriminator loss: 0.092665, acc.: 96.88%] [Generator loss: 9.208702]\n",
      "20178 [Discriminator loss: 0.022522, acc.: 100.00%] [Generator loss: 9.233650]\n",
      "20179 [Discriminator loss: 0.031459, acc.: 100.00%] [Generator loss: 8.582836]\n",
      "20180 [Discriminator loss: 0.149718, acc.: 96.88%] [Generator loss: 9.852878]\n",
      "20181 [Discriminator loss: 0.052647, acc.: 96.88%] [Generator loss: 8.003423]\n",
      "20182 [Discriminator loss: 0.145002, acc.: 96.88%] [Generator loss: 8.317098]\n",
      "20183 [Discriminator loss: 0.137359, acc.: 93.75%] [Generator loss: 9.494183]\n",
      "20184 [Discriminator loss: 0.049055, acc.: 98.44%] [Generator loss: 10.044216]\n",
      "20185 [Discriminator loss: 0.107171, acc.: 95.31%] [Generator loss: 8.803366]\n",
      "20186 [Discriminator loss: 0.011497, acc.: 100.00%] [Generator loss: 7.879205]\n",
      "20187 [Discriminator loss: 0.010978, acc.: 100.00%] [Generator loss: 8.404566]\n",
      "20188 [Discriminator loss: 0.043643, acc.: 98.44%] [Generator loss: 8.391308]\n",
      "20189 [Discriminator loss: 0.067163, acc.: 98.44%] [Generator loss: 9.771214]\n",
      "20190 [Discriminator loss: 0.010977, acc.: 100.00%] [Generator loss: 10.562680]\n",
      "20191 [Discriminator loss: 0.042368, acc.: 98.44%] [Generator loss: 8.165630]\n",
      "20192 [Discriminator loss: 0.043458, acc.: 100.00%] [Generator loss: 7.966162]\n",
      "20193 [Discriminator loss: 0.045230, acc.: 98.44%] [Generator loss: 6.483990]\n",
      "20194 [Discriminator loss: 0.018614, acc.: 98.44%] [Generator loss: 6.390105]\n",
      "20195 [Discriminator loss: 0.012854, acc.: 100.00%] [Generator loss: 6.708644]\n",
      "20196 [Discriminator loss: 0.066834, acc.: 95.31%] [Generator loss: 8.322371]\n",
      "20197 [Discriminator loss: 0.060864, acc.: 98.44%] [Generator loss: 8.492821]\n",
      "20198 [Discriminator loss: 0.034709, acc.: 100.00%] [Generator loss: 8.742901]\n",
      "20199 [Discriminator loss: 0.043787, acc.: 98.44%] [Generator loss: 8.348829]\n",
      "20200 [Discriminator loss: 0.021651, acc.: 98.44%] [Generator loss: 9.327201]\n",
      "20201 [Discriminator loss: 0.174218, acc.: 92.19%] [Generator loss: 11.061892]\n",
      "20202 [Discriminator loss: 0.068790, acc.: 96.88%] [Generator loss: 9.151626]\n",
      "20203 [Discriminator loss: 0.143917, acc.: 96.88%] [Generator loss: 9.559220]\n",
      "20204 [Discriminator loss: 0.054638, acc.: 98.44%] [Generator loss: 8.160641]\n",
      "20205 [Discriminator loss: 0.052171, acc.: 98.44%] [Generator loss: 7.677418]\n",
      "20206 [Discriminator loss: 0.159158, acc.: 95.31%] [Generator loss: 8.954916]\n",
      "20207 [Discriminator loss: 0.010489, acc.: 100.00%] [Generator loss: 9.453335]\n",
      "20208 [Discriminator loss: 0.069686, acc.: 98.44%] [Generator loss: 8.143326]\n",
      "20209 [Discriminator loss: 0.044804, acc.: 98.44%] [Generator loss: 8.413430]\n",
      "20210 [Discriminator loss: 0.161411, acc.: 95.31%] [Generator loss: 8.629221]\n",
      "20211 [Discriminator loss: 0.034757, acc.: 98.44%] [Generator loss: 7.856956]\n",
      "20212 [Discriminator loss: 0.112515, acc.: 96.88%] [Generator loss: 9.116709]\n",
      "20213 [Discriminator loss: 0.032309, acc.: 98.44%] [Generator loss: 7.948665]\n",
      "20214 [Discriminator loss: 0.127557, acc.: 93.75%] [Generator loss: 8.295363]\n",
      "20215 [Discriminator loss: 0.051134, acc.: 96.88%] [Generator loss: 8.301218]\n",
      "20216 [Discriminator loss: 0.030367, acc.: 98.44%] [Generator loss: 7.938179]\n",
      "20217 [Discriminator loss: 0.116050, acc.: 93.75%] [Generator loss: 7.734005]\n",
      "20218 [Discriminator loss: 0.057562, acc.: 96.88%] [Generator loss: 9.435163]\n",
      "20219 [Discriminator loss: 0.090247, acc.: 95.31%] [Generator loss: 9.108906]\n",
      "20220 [Discriminator loss: 0.065826, acc.: 96.88%] [Generator loss: 7.254948]\n",
      "20221 [Discriminator loss: 0.099430, acc.: 96.88%] [Generator loss: 8.835476]\n",
      "20222 [Discriminator loss: 0.055287, acc.: 98.44%] [Generator loss: 8.179537]\n",
      "20223 [Discriminator loss: 0.056989, acc.: 98.44%] [Generator loss: 8.636381]\n",
      "20224 [Discriminator loss: 0.007714, acc.: 100.00%] [Generator loss: 8.244619]\n",
      "20225 [Discriminator loss: 0.077856, acc.: 96.88%] [Generator loss: 7.921943]\n",
      "20226 [Discriminator loss: 0.005917, acc.: 100.00%] [Generator loss: 9.023330]\n",
      "20227 [Discriminator loss: 0.086964, acc.: 96.88%] [Generator loss: 8.526059]\n",
      "20228 [Discriminator loss: 0.023389, acc.: 100.00%] [Generator loss: 8.832551]\n",
      "20229 [Discriminator loss: 0.146620, acc.: 95.31%] [Generator loss: 8.214914]\n",
      "20230 [Discriminator loss: 0.012565, acc.: 100.00%] [Generator loss: 9.448580]\n",
      "20231 [Discriminator loss: 0.115302, acc.: 93.75%] [Generator loss: 7.709927]\n",
      "20232 [Discriminator loss: 0.078805, acc.: 98.44%] [Generator loss: 9.393891]\n",
      "20233 [Discriminator loss: 0.084848, acc.: 96.88%] [Generator loss: 10.578866]\n",
      "20234 [Discriminator loss: 0.029866, acc.: 98.44%] [Generator loss: 9.969336]\n",
      "20235 [Discriminator loss: 0.151747, acc.: 95.31%] [Generator loss: 6.078424]\n",
      "20236 [Discriminator loss: 0.039520, acc.: 98.44%] [Generator loss: 7.758979]\n",
      "20237 [Discriminator loss: 0.193699, acc.: 93.75%] [Generator loss: 9.573450]\n",
      "20238 [Discriminator loss: 0.017604, acc.: 100.00%] [Generator loss: 12.071802]\n",
      "20239 [Discriminator loss: 0.380587, acc.: 85.94%] [Generator loss: 6.507762]\n",
      "20240 [Discriminator loss: 0.079216, acc.: 95.31%] [Generator loss: 8.501285]\n",
      "20241 [Discriminator loss: 0.025988, acc.: 98.44%] [Generator loss: 9.293958]\n",
      "20242 [Discriminator loss: 0.069032, acc.: 96.88%] [Generator loss: 9.693994]\n",
      "20243 [Discriminator loss: 0.097275, acc.: 96.88%] [Generator loss: 10.430624]\n",
      "20244 [Discriminator loss: 0.158621, acc.: 93.75%] [Generator loss: 8.471152]\n",
      "20245 [Discriminator loss: 0.030748, acc.: 98.44%] [Generator loss: 9.974451]\n",
      "20246 [Discriminator loss: 0.139731, acc.: 93.75%] [Generator loss: 7.879962]\n",
      "20247 [Discriminator loss: 0.042780, acc.: 96.88%] [Generator loss: 8.110188]\n",
      "20248 [Discriminator loss: 0.045535, acc.: 98.44%] [Generator loss: 9.425314]\n",
      "20249 [Discriminator loss: 0.078962, acc.: 96.88%] [Generator loss: 9.583876]\n",
      "20250 [Discriminator loss: 0.035440, acc.: 98.44%] [Generator loss: 9.488066]\n",
      "20251 [Discriminator loss: 0.048044, acc.: 96.88%] [Generator loss: 8.898564]\n",
      "20252 [Discriminator loss: 0.108989, acc.: 95.31%] [Generator loss: 9.757772]\n",
      "20253 [Discriminator loss: 0.071097, acc.: 96.88%] [Generator loss: 10.190016]\n",
      "20254 [Discriminator loss: 0.191842, acc.: 92.19%] [Generator loss: 9.337849]\n",
      "20255 [Discriminator loss: 0.072861, acc.: 96.88%] [Generator loss: 7.886502]\n",
      "20256 [Discriminator loss: 0.290728, acc.: 87.50%] [Generator loss: 9.295865]\n",
      "20257 [Discriminator loss: 0.025569, acc.: 100.00%] [Generator loss: 10.063037]\n",
      "20258 [Discriminator loss: 0.259313, acc.: 92.19%] [Generator loss: 7.985340]\n",
      "20259 [Discriminator loss: 0.131109, acc.: 95.31%] [Generator loss: 7.683372]\n",
      "20260 [Discriminator loss: 0.021225, acc.: 100.00%] [Generator loss: 8.902687]\n",
      "20261 [Discriminator loss: 0.027171, acc.: 100.00%] [Generator loss: 8.434229]\n",
      "20262 [Discriminator loss: 0.024315, acc.: 98.44%] [Generator loss: 9.120935]\n",
      "20263 [Discriminator loss: 0.027491, acc.: 100.00%] [Generator loss: 9.276098]\n",
      "20264 [Discriminator loss: 0.070591, acc.: 96.88%] [Generator loss: 8.877696]\n",
      "20265 [Discriminator loss: 0.027284, acc.: 100.00%] [Generator loss: 8.492903]\n",
      "20266 [Discriminator loss: 0.022587, acc.: 100.00%] [Generator loss: 8.543327]\n",
      "20267 [Discriminator loss: 0.117419, acc.: 93.75%] [Generator loss: 7.319991]\n",
      "20268 [Discriminator loss: 0.086082, acc.: 96.88%] [Generator loss: 7.789101]\n",
      "20269 [Discriminator loss: 0.043514, acc.: 98.44%] [Generator loss: 7.135988]\n",
      "20270 [Discriminator loss: 0.122738, acc.: 96.88%] [Generator loss: 7.679424]\n",
      "20271 [Discriminator loss: 0.023847, acc.: 100.00%] [Generator loss: 7.702871]\n",
      "20272 [Discriminator loss: 0.125976, acc.: 92.19%] [Generator loss: 8.382706]\n",
      "20273 [Discriminator loss: 0.031047, acc.: 96.88%] [Generator loss: 8.400465]\n",
      "20274 [Discriminator loss: 0.087321, acc.: 96.88%] [Generator loss: 8.264353]\n",
      "20275 [Discriminator loss: 0.079952, acc.: 96.88%] [Generator loss: 9.423939]\n",
      "20276 [Discriminator loss: 0.030076, acc.: 98.44%] [Generator loss: 10.680065]\n",
      "20277 [Discriminator loss: 0.043451, acc.: 100.00%] [Generator loss: 8.062183]\n",
      "20278 [Discriminator loss: 0.016843, acc.: 100.00%] [Generator loss: 7.901122]\n",
      "20279 [Discriminator loss: 0.121222, acc.: 96.88%] [Generator loss: 8.872402]\n",
      "20280 [Discriminator loss: 0.047275, acc.: 98.44%] [Generator loss: 7.535340]\n",
      "20281 [Discriminator loss: 0.195137, acc.: 92.19%] [Generator loss: 8.045912]\n",
      "20282 [Discriminator loss: 0.134410, acc.: 95.31%] [Generator loss: 9.512005]\n",
      "20283 [Discriminator loss: 0.086794, acc.: 98.44%] [Generator loss: 9.586654]\n",
      "20284 [Discriminator loss: 0.058782, acc.: 98.44%] [Generator loss: 8.545118]\n",
      "20285 [Discriminator loss: 0.072781, acc.: 95.31%] [Generator loss: 8.963226]\n",
      "20286 [Discriminator loss: 0.028795, acc.: 100.00%] [Generator loss: 6.750568]\n",
      "20287 [Discriminator loss: 0.034555, acc.: 98.44%] [Generator loss: 8.104899]\n",
      "20288 [Discriminator loss: 0.157390, acc.: 92.19%] [Generator loss: 7.103284]\n",
      "20289 [Discriminator loss: 0.093911, acc.: 96.88%] [Generator loss: 8.941671]\n",
      "20290 [Discriminator loss: 0.033829, acc.: 98.44%] [Generator loss: 9.425452]\n",
      "20291 [Discriminator loss: 0.025166, acc.: 100.00%] [Generator loss: 7.912560]\n",
      "20292 [Discriminator loss: 0.083330, acc.: 98.44%] [Generator loss: 8.314030]\n",
      "20293 [Discriminator loss: 0.013106, acc.: 100.00%] [Generator loss: 9.575453]\n",
      "20294 [Discriminator loss: 0.052174, acc.: 98.44%] [Generator loss: 8.657341]\n",
      "20295 [Discriminator loss: 0.032278, acc.: 98.44%] [Generator loss: 7.674029]\n",
      "20296 [Discriminator loss: 0.153402, acc.: 92.19%] [Generator loss: 7.703607]\n",
      "20297 [Discriminator loss: 0.115208, acc.: 96.88%] [Generator loss: 8.868138]\n",
      "20298 [Discriminator loss: 0.023402, acc.: 100.00%] [Generator loss: 8.647709]\n",
      "20299 [Discriminator loss: 0.059551, acc.: 96.88%] [Generator loss: 7.483929]\n",
      "20300 [Discriminator loss: 0.098102, acc.: 96.88%] [Generator loss: 6.698347]\n",
      "20301 [Discriminator loss: 0.010848, acc.: 100.00%] [Generator loss: 7.144608]\n",
      "20302 [Discriminator loss: 0.045856, acc.: 98.44%] [Generator loss: 8.148842]\n",
      "20303 [Discriminator loss: 0.037746, acc.: 98.44%] [Generator loss: 8.212223]\n",
      "20304 [Discriminator loss: 0.085562, acc.: 98.44%] [Generator loss: 7.969334]\n",
      "20305 [Discriminator loss: 0.062136, acc.: 95.31%] [Generator loss: 8.066515]\n",
      "20306 [Discriminator loss: 0.046294, acc.: 98.44%] [Generator loss: 9.360435]\n",
      "20307 [Discriminator loss: 0.097933, acc.: 93.75%] [Generator loss: 8.760914]\n",
      "20308 [Discriminator loss: 0.207431, acc.: 92.19%] [Generator loss: 8.836003]\n",
      "20309 [Discriminator loss: 0.016334, acc.: 100.00%] [Generator loss: 10.046461]\n",
      "20310 [Discriminator loss: 0.028222, acc.: 100.00%] [Generator loss: 9.996565]\n",
      "20311 [Discriminator loss: 0.134548, acc.: 95.31%] [Generator loss: 10.062180]\n",
      "20312 [Discriminator loss: 0.073756, acc.: 98.44%] [Generator loss: 10.043339]\n",
      "20313 [Discriminator loss: 0.071448, acc.: 96.88%] [Generator loss: 7.932399]\n",
      "20314 [Discriminator loss: 0.171230, acc.: 96.88%] [Generator loss: 8.187408]\n",
      "20315 [Discriminator loss: 0.105545, acc.: 95.31%] [Generator loss: 9.242696]\n",
      "20316 [Discriminator loss: 0.055821, acc.: 96.88%] [Generator loss: 8.016365]\n",
      "20317 [Discriminator loss: 0.104619, acc.: 95.31%] [Generator loss: 9.029821]\n",
      "20318 [Discriminator loss: 0.193082, acc.: 93.75%] [Generator loss: 8.640275]\n",
      "20319 [Discriminator loss: 0.016520, acc.: 100.00%] [Generator loss: 9.057244]\n",
      "20320 [Discriminator loss: 0.012151, acc.: 100.00%] [Generator loss: 9.007761]\n",
      "20321 [Discriminator loss: 0.037393, acc.: 98.44%] [Generator loss: 8.348433]\n",
      "20322 [Discriminator loss: 0.017876, acc.: 100.00%] [Generator loss: 8.841059]\n",
      "20323 [Discriminator loss: 0.022182, acc.: 100.00%] [Generator loss: 8.197925]\n",
      "20324 [Discriminator loss: 0.085078, acc.: 96.88%] [Generator loss: 8.867388]\n",
      "20325 [Discriminator loss: 0.027315, acc.: 98.44%] [Generator loss: 8.445955]\n",
      "20326 [Discriminator loss: 0.248289, acc.: 92.19%] [Generator loss: 10.640352]\n",
      "20327 [Discriminator loss: 0.090705, acc.: 98.44%] [Generator loss: 10.733976]\n",
      "20328 [Discriminator loss: 0.117854, acc.: 96.88%] [Generator loss: 9.311504]\n",
      "20329 [Discriminator loss: 0.066694, acc.: 96.88%] [Generator loss: 8.650288]\n",
      "20330 [Discriminator loss: 0.137245, acc.: 95.31%] [Generator loss: 7.633771]\n",
      "20331 [Discriminator loss: 0.031519, acc.: 98.44%] [Generator loss: 8.006256]\n",
      "20332 [Discriminator loss: 0.117062, acc.: 96.88%] [Generator loss: 8.096630]\n",
      "20333 [Discriminator loss: 0.007317, acc.: 100.00%] [Generator loss: 6.901702]\n",
      "20334 [Discriminator loss: 0.133417, acc.: 95.31%] [Generator loss: 9.688765]\n",
      "20335 [Discriminator loss: 0.026462, acc.: 98.44%] [Generator loss: 10.722776]\n",
      "20336 [Discriminator loss: 0.248295, acc.: 92.19%] [Generator loss: 7.262798]\n",
      "20337 [Discriminator loss: 0.045952, acc.: 98.44%] [Generator loss: 8.353518]\n",
      "20338 [Discriminator loss: 0.012441, acc.: 100.00%] [Generator loss: 8.019703]\n",
      "20339 [Discriminator loss: 0.083689, acc.: 95.31%] [Generator loss: 10.113848]\n",
      "20340 [Discriminator loss: 0.051241, acc.: 98.44%] [Generator loss: 9.335488]\n",
      "20341 [Discriminator loss: 0.014698, acc.: 100.00%] [Generator loss: 9.007730]\n",
      "20342 [Discriminator loss: 0.028422, acc.: 100.00%] [Generator loss: 8.415490]\n",
      "20343 [Discriminator loss: 0.062942, acc.: 98.44%] [Generator loss: 9.807041]\n",
      "20344 [Discriminator loss: 0.012712, acc.: 100.00%] [Generator loss: 10.620209]\n",
      "20345 [Discriminator loss: 0.086331, acc.: 96.88%] [Generator loss: 6.966584]\n",
      "20346 [Discriminator loss: 0.099445, acc.: 93.75%] [Generator loss: 8.328886]\n",
      "20347 [Discriminator loss: 0.203141, acc.: 92.19%] [Generator loss: 9.393379]\n",
      "20348 [Discriminator loss: 0.073323, acc.: 96.88%] [Generator loss: 9.663589]\n",
      "20349 [Discriminator loss: 0.081820, acc.: 96.88%] [Generator loss: 9.658185]\n",
      "20350 [Discriminator loss: 0.162451, acc.: 93.75%] [Generator loss: 11.022146]\n",
      "20351 [Discriminator loss: 0.036874, acc.: 98.44%] [Generator loss: 9.835623]\n",
      "20352 [Discriminator loss: 0.083930, acc.: 95.31%] [Generator loss: 7.139756]\n",
      "20353 [Discriminator loss: 0.105288, acc.: 96.88%] [Generator loss: 6.838938]\n",
      "20354 [Discriminator loss: 0.087935, acc.: 93.75%] [Generator loss: 9.133626]\n",
      "20355 [Discriminator loss: 0.092784, acc.: 96.88%] [Generator loss: 7.310005]\n",
      "20356 [Discriminator loss: 0.044689, acc.: 96.88%] [Generator loss: 9.069029]\n",
      "20357 [Discriminator loss: 0.024708, acc.: 100.00%] [Generator loss: 8.361855]\n",
      "20358 [Discriminator loss: 0.218204, acc.: 90.62%] [Generator loss: 7.417861]\n",
      "20359 [Discriminator loss: 0.099790, acc.: 93.75%] [Generator loss: 8.329168]\n",
      "20360 [Discriminator loss: 0.092854, acc.: 96.88%] [Generator loss: 9.383307]\n",
      "20361 [Discriminator loss: 0.054018, acc.: 96.88%] [Generator loss: 7.461583]\n",
      "20362 [Discriminator loss: 0.014469, acc.: 100.00%] [Generator loss: 7.541115]\n",
      "20363 [Discriminator loss: 0.055316, acc.: 96.88%] [Generator loss: 6.451694]\n",
      "20364 [Discriminator loss: 0.101498, acc.: 96.88%] [Generator loss: 7.554877]\n",
      "20365 [Discriminator loss: 0.052620, acc.: 96.88%] [Generator loss: 8.036786]\n",
      "20366 [Discriminator loss: 0.033265, acc.: 100.00%] [Generator loss: 9.017289]\n",
      "20367 [Discriminator loss: 0.063655, acc.: 98.44%] [Generator loss: 9.103800]\n",
      "20368 [Discriminator loss: 0.080024, acc.: 95.31%] [Generator loss: 6.779574]\n",
      "20369 [Discriminator loss: 0.071796, acc.: 98.44%] [Generator loss: 7.668586]\n",
      "20370 [Discriminator loss: 0.088286, acc.: 96.88%] [Generator loss: 7.556068]\n",
      "20371 [Discriminator loss: 0.042873, acc.: 98.44%] [Generator loss: 6.957018]\n",
      "20372 [Discriminator loss: 0.218461, acc.: 92.19%] [Generator loss: 7.756929]\n",
      "20373 [Discriminator loss: 0.071393, acc.: 96.88%] [Generator loss: 9.745789]\n",
      "20374 [Discriminator loss: 0.047393, acc.: 98.44%] [Generator loss: 9.173338]\n",
      "20375 [Discriminator loss: 0.032242, acc.: 98.44%] [Generator loss: 8.249367]\n",
      "20376 [Discriminator loss: 0.055189, acc.: 96.88%] [Generator loss: 8.599824]\n",
      "20377 [Discriminator loss: 0.053676, acc.: 96.88%] [Generator loss: 7.640432]\n",
      "20378 [Discriminator loss: 0.047981, acc.: 98.44%] [Generator loss: 9.448485]\n",
      "20379 [Discriminator loss: 0.013068, acc.: 100.00%] [Generator loss: 8.966695]\n",
      "20380 [Discriminator loss: 0.048126, acc.: 98.44%] [Generator loss: 7.709957]\n",
      "20381 [Discriminator loss: 0.075647, acc.: 96.88%] [Generator loss: 7.767301]\n",
      "20382 [Discriminator loss: 0.013266, acc.: 100.00%] [Generator loss: 9.570973]\n",
      "20383 [Discriminator loss: 0.065379, acc.: 98.44%] [Generator loss: 9.577482]\n",
      "20384 [Discriminator loss: 0.145046, acc.: 93.75%] [Generator loss: 6.855977]\n",
      "20385 [Discriminator loss: 0.093108, acc.: 96.88%] [Generator loss: 9.536409]\n",
      "20386 [Discriminator loss: 0.007986, acc.: 100.00%] [Generator loss: 9.997933]\n",
      "20387 [Discriminator loss: 0.065093, acc.: 95.31%] [Generator loss: 8.773749]\n",
      "20388 [Discriminator loss: 0.077929, acc.: 95.31%] [Generator loss: 9.402644]\n",
      "20389 [Discriminator loss: 0.105737, acc.: 95.31%] [Generator loss: 10.300573]\n",
      "20390 [Discriminator loss: 0.021142, acc.: 98.44%] [Generator loss: 8.527888]\n",
      "20391 [Discriminator loss: 0.060637, acc.: 98.44%] [Generator loss: 8.925684]\n",
      "20392 [Discriminator loss: 0.029638, acc.: 98.44%] [Generator loss: 8.685344]\n",
      "20393 [Discriminator loss: 0.053844, acc.: 98.44%] [Generator loss: 8.636801]\n",
      "20394 [Discriminator loss: 0.286645, acc.: 89.06%] [Generator loss: 9.073945]\n",
      "20395 [Discriminator loss: 0.029733, acc.: 100.00%] [Generator loss: 8.766722]\n",
      "20396 [Discriminator loss: 0.013819, acc.: 100.00%] [Generator loss: 9.708138]\n",
      "20397 [Discriminator loss: 0.119946, acc.: 95.31%] [Generator loss: 7.483688]\n",
      "20398 [Discriminator loss: 0.065394, acc.: 98.44%] [Generator loss: 9.756520]\n",
      "20399 [Discriminator loss: 0.017515, acc.: 100.00%] [Generator loss: 9.732666]\n",
      "20400 [Discriminator loss: 0.058739, acc.: 96.88%] [Generator loss: 8.534903]\n",
      "20401 [Discriminator loss: 0.088762, acc.: 95.31%] [Generator loss: 10.196131]\n",
      "20402 [Discriminator loss: 0.095463, acc.: 95.31%] [Generator loss: 6.131223]\n",
      "20403 [Discriminator loss: 0.038440, acc.: 96.88%] [Generator loss: 8.321251]\n",
      "20404 [Discriminator loss: 0.201279, acc.: 95.31%] [Generator loss: 10.236540]\n",
      "20405 [Discriminator loss: 0.008988, acc.: 100.00%] [Generator loss: 10.998785]\n",
      "20406 [Discriminator loss: 0.212575, acc.: 92.19%] [Generator loss: 7.327572]\n",
      "20407 [Discriminator loss: 0.090492, acc.: 96.88%] [Generator loss: 7.222790]\n",
      "20408 [Discriminator loss: 0.013906, acc.: 100.00%] [Generator loss: 7.894496]\n",
      "20409 [Discriminator loss: 0.048929, acc.: 98.44%] [Generator loss: 7.183580]\n",
      "20410 [Discriminator loss: 0.040024, acc.: 100.00%] [Generator loss: 9.378088]\n",
      "20411 [Discriminator loss: 0.007803, acc.: 100.00%] [Generator loss: 8.885885]\n",
      "20412 [Discriminator loss: 0.057635, acc.: 98.44%] [Generator loss: 8.445234]\n",
      "20413 [Discriminator loss: 0.016319, acc.: 100.00%] [Generator loss: 9.663883]\n",
      "20414 [Discriminator loss: 0.279538, acc.: 90.62%] [Generator loss: 6.784326]\n",
      "20415 [Discriminator loss: 0.060404, acc.: 98.44%] [Generator loss: 8.563052]\n",
      "20416 [Discriminator loss: 0.002520, acc.: 100.00%] [Generator loss: 8.810413]\n",
      "20417 [Discriminator loss: 0.129247, acc.: 93.75%] [Generator loss: 5.654607]\n",
      "20418 [Discriminator loss: 0.140930, acc.: 93.75%] [Generator loss: 10.568975]\n",
      "20419 [Discriminator loss: 0.019348, acc.: 100.00%] [Generator loss: 11.267328]\n",
      "20420 [Discriminator loss: 0.135866, acc.: 93.75%] [Generator loss: 8.495045]\n",
      "20421 [Discriminator loss: 0.060023, acc.: 98.44%] [Generator loss: 9.117827]\n",
      "20422 [Discriminator loss: 0.028705, acc.: 98.44%] [Generator loss: 7.951927]\n",
      "20423 [Discriminator loss: 0.053314, acc.: 96.88%] [Generator loss: 8.373652]\n",
      "20424 [Discriminator loss: 0.065939, acc.: 96.88%] [Generator loss: 9.020154]\n",
      "20425 [Discriminator loss: 0.122466, acc.: 95.31%] [Generator loss: 9.443217]\n",
      "20426 [Discriminator loss: 0.049150, acc.: 96.88%] [Generator loss: 9.234858]\n",
      "20427 [Discriminator loss: 0.098063, acc.: 95.31%] [Generator loss: 9.107832]\n",
      "20428 [Discriminator loss: 0.082600, acc.: 95.31%] [Generator loss: 7.882788]\n",
      "20429 [Discriminator loss: 0.024755, acc.: 100.00%] [Generator loss: 9.283526]\n",
      "20430 [Discriminator loss: 0.138431, acc.: 96.88%] [Generator loss: 8.642710]\n",
      "20431 [Discriminator loss: 0.039814, acc.: 98.44%] [Generator loss: 8.450886]\n",
      "20432 [Discriminator loss: 0.169993, acc.: 92.19%] [Generator loss: 9.820992]\n",
      "20433 [Discriminator loss: 0.047630, acc.: 96.88%] [Generator loss: 8.288078]\n",
      "20434 [Discriminator loss: 0.025298, acc.: 100.00%] [Generator loss: 7.616005]\n",
      "20435 [Discriminator loss: 0.057434, acc.: 96.88%] [Generator loss: 8.656559]\n",
      "20436 [Discriminator loss: 0.062532, acc.: 95.31%] [Generator loss: 9.418539]\n",
      "20437 [Discriminator loss: 0.092450, acc.: 98.44%] [Generator loss: 6.817507]\n",
      "20438 [Discriminator loss: 0.061285, acc.: 98.44%] [Generator loss: 8.400066]\n",
      "20439 [Discriminator loss: 0.056074, acc.: 98.44%] [Generator loss: 7.189621]\n",
      "20440 [Discriminator loss: 0.050603, acc.: 98.44%] [Generator loss: 8.118389]\n",
      "20441 [Discriminator loss: 0.011308, acc.: 100.00%] [Generator loss: 8.625589]\n",
      "20442 [Discriminator loss: 0.165353, acc.: 93.75%] [Generator loss: 8.463598]\n",
      "20443 [Discriminator loss: 0.277089, acc.: 85.94%] [Generator loss: 8.959736]\n",
      "20444 [Discriminator loss: 0.028499, acc.: 98.44%] [Generator loss: 10.502096]\n",
      "20445 [Discriminator loss: 0.104591, acc.: 93.75%] [Generator loss: 9.159727]\n",
      "20446 [Discriminator loss: 0.088392, acc.: 93.75%] [Generator loss: 6.552402]\n",
      "20447 [Discriminator loss: 0.111249, acc.: 93.75%] [Generator loss: 9.822172]\n",
      "20448 [Discriminator loss: 0.013256, acc.: 100.00%] [Generator loss: 10.667738]\n",
      "20449 [Discriminator loss: 0.159226, acc.: 92.19%] [Generator loss: 8.684593]\n",
      "20450 [Discriminator loss: 0.087377, acc.: 98.44%] [Generator loss: 8.742629]\n",
      "20451 [Discriminator loss: 0.038070, acc.: 98.44%] [Generator loss: 8.042909]\n",
      "20452 [Discriminator loss: 0.235735, acc.: 90.62%] [Generator loss: 8.294983]\n",
      "20453 [Discriminator loss: 0.023298, acc.: 100.00%] [Generator loss: 9.875067]\n",
      "20454 [Discriminator loss: 0.115830, acc.: 95.31%] [Generator loss: 7.819903]\n",
      "20455 [Discriminator loss: 0.245036, acc.: 93.75%] [Generator loss: 8.685251]\n",
      "20456 [Discriminator loss: 0.046851, acc.: 98.44%] [Generator loss: 10.319374]\n",
      "20457 [Discriminator loss: 0.186261, acc.: 92.19%] [Generator loss: 11.310920]\n",
      "20458 [Discriminator loss: 0.191863, acc.: 89.06%] [Generator loss: 8.836802]\n",
      "20459 [Discriminator loss: 0.008631, acc.: 100.00%] [Generator loss: 9.472527]\n",
      "20460 [Discriminator loss: 0.132059, acc.: 95.31%] [Generator loss: 8.786622]\n",
      "20461 [Discriminator loss: 0.033313, acc.: 98.44%] [Generator loss: 8.746422]\n",
      "20462 [Discriminator loss: 0.101932, acc.: 93.75%] [Generator loss: 8.593266]\n",
      "20463 [Discriminator loss: 0.174330, acc.: 93.75%] [Generator loss: 7.738473]\n",
      "20464 [Discriminator loss: 0.078391, acc.: 93.75%] [Generator loss: 9.725940]\n",
      "20465 [Discriminator loss: 0.075709, acc.: 96.88%] [Generator loss: 9.220033]\n",
      "20466 [Discriminator loss: 0.018358, acc.: 100.00%] [Generator loss: 8.871561]\n",
      "20467 [Discriminator loss: 0.110862, acc.: 93.75%] [Generator loss: 7.941091]\n",
      "20468 [Discriminator loss: 0.067102, acc.: 95.31%] [Generator loss: 6.808973]\n",
      "20469 [Discriminator loss: 0.032790, acc.: 100.00%] [Generator loss: 7.470143]\n",
      "20470 [Discriminator loss: 0.084734, acc.: 95.31%] [Generator loss: 9.131006]\n",
      "20471 [Discriminator loss: 0.023766, acc.: 100.00%] [Generator loss: 8.406120]\n",
      "20472 [Discriminator loss: 0.054337, acc.: 98.44%] [Generator loss: 9.774264]\n",
      "20473 [Discriminator loss: 0.161056, acc.: 95.31%] [Generator loss: 8.448420]\n",
      "20474 [Discriminator loss: 0.086240, acc.: 98.44%] [Generator loss: 9.351559]\n",
      "20475 [Discriminator loss: 0.072964, acc.: 96.88%] [Generator loss: 9.302196]\n",
      "20476 [Discriminator loss: 0.119942, acc.: 93.75%] [Generator loss: 8.608944]\n",
      "20477 [Discriminator loss: 0.044322, acc.: 98.44%] [Generator loss: 9.135806]\n",
      "20478 [Discriminator loss: 0.093265, acc.: 95.31%] [Generator loss: 7.677604]\n",
      "20479 [Discriminator loss: 0.248663, acc.: 92.19%] [Generator loss: 8.265670]\n",
      "20480 [Discriminator loss: 0.034227, acc.: 100.00%] [Generator loss: 7.559242]\n",
      "20481 [Discriminator loss: 0.016844, acc.: 100.00%] [Generator loss: 9.332653]\n",
      "20482 [Discriminator loss: 0.021287, acc.: 98.44%] [Generator loss: 9.065927]\n",
      "20483 [Discriminator loss: 0.022060, acc.: 100.00%] [Generator loss: 8.261587]\n",
      "20484 [Discriminator loss: 0.066197, acc.: 96.88%] [Generator loss: 6.787564]\n",
      "20485 [Discriminator loss: 0.060703, acc.: 98.44%] [Generator loss: 7.863002]\n",
      "20486 [Discriminator loss: 0.230567, acc.: 95.31%] [Generator loss: 9.653995]\n",
      "20487 [Discriminator loss: 0.058920, acc.: 98.44%] [Generator loss: 10.662546]\n",
      "20488 [Discriminator loss: 0.241740, acc.: 95.31%] [Generator loss: 7.373873]\n",
      "20489 [Discriminator loss: 0.039816, acc.: 98.44%] [Generator loss: 8.005587]\n",
      "20490 [Discriminator loss: 0.088067, acc.: 95.31%] [Generator loss: 7.079206]\n",
      "20491 [Discriminator loss: 0.064162, acc.: 98.44%] [Generator loss: 9.658728]\n",
      "20492 [Discriminator loss: 0.108437, acc.: 92.19%] [Generator loss: 6.850698]\n",
      "20493 [Discriminator loss: 0.043310, acc.: 96.88%] [Generator loss: 7.909493]\n",
      "20494 [Discriminator loss: 0.026186, acc.: 98.44%] [Generator loss: 8.172651]\n",
      "20495 [Discriminator loss: 0.144305, acc.: 96.88%] [Generator loss: 8.118651]\n",
      "20496 [Discriminator loss: 0.017400, acc.: 100.00%] [Generator loss: 9.156989]\n",
      "20497 [Discriminator loss: 0.016176, acc.: 100.00%] [Generator loss: 8.272528]\n",
      "20498 [Discriminator loss: 0.025940, acc.: 98.44%] [Generator loss: 7.889714]\n",
      "20499 [Discriminator loss: 0.021512, acc.: 100.00%] [Generator loss: 7.990073]\n",
      "20500 [Discriminator loss: 0.096914, acc.: 98.44%] [Generator loss: 6.599228]\n",
      "20501 [Discriminator loss: 0.016809, acc.: 100.00%] [Generator loss: 8.994095]\n",
      "20502 [Discriminator loss: 0.027997, acc.: 100.00%] [Generator loss: 9.171019]\n",
      "20503 [Discriminator loss: 0.202061, acc.: 93.75%] [Generator loss: 8.437978]\n",
      "20504 [Discriminator loss: 0.025763, acc.: 100.00%] [Generator loss: 9.336561]\n",
      "20505 [Discriminator loss: 0.146767, acc.: 95.31%] [Generator loss: 9.751989]\n",
      "20506 [Discriminator loss: 0.034510, acc.: 100.00%] [Generator loss: 8.374378]\n",
      "20507 [Discriminator loss: 0.195621, acc.: 92.19%] [Generator loss: 5.710735]\n",
      "20508 [Discriminator loss: 0.097718, acc.: 95.31%] [Generator loss: 8.325480]\n",
      "20509 [Discriminator loss: 0.046021, acc.: 98.44%] [Generator loss: 7.993378]\n",
      "20510 [Discriminator loss: 0.024387, acc.: 98.44%] [Generator loss: 8.929222]\n",
      "20511 [Discriminator loss: 0.040138, acc.: 98.44%] [Generator loss: 7.554916]\n",
      "20512 [Discriminator loss: 0.020485, acc.: 98.44%] [Generator loss: 7.578005]\n",
      "20513 [Discriminator loss: 0.015238, acc.: 100.00%] [Generator loss: 8.757999]\n",
      "20514 [Discriminator loss: 0.016875, acc.: 100.00%] [Generator loss: 8.502188]\n",
      "20515 [Discriminator loss: 0.124070, acc.: 96.88%] [Generator loss: 7.657330]\n",
      "20516 [Discriminator loss: 0.154168, acc.: 96.88%] [Generator loss: 8.311889]\n",
      "20517 [Discriminator loss: 0.015541, acc.: 100.00%] [Generator loss: 8.703793]\n",
      "20518 [Discriminator loss: 0.042490, acc.: 96.88%] [Generator loss: 8.288434]\n",
      "20519 [Discriminator loss: 0.132362, acc.: 95.31%] [Generator loss: 7.330710]\n",
      "20520 [Discriminator loss: 0.049008, acc.: 96.88%] [Generator loss: 7.259206]\n",
      "20521 [Discriminator loss: 0.022483, acc.: 100.00%] [Generator loss: 7.908277]\n",
      "20522 [Discriminator loss: 0.044581, acc.: 98.44%] [Generator loss: 8.250360]\n",
      "20523 [Discriminator loss: 0.011112, acc.: 100.00%] [Generator loss: 7.858623]\n",
      "20524 [Discriminator loss: 0.051170, acc.: 96.88%] [Generator loss: 9.755171]\n",
      "20525 [Discriminator loss: 0.063413, acc.: 98.44%] [Generator loss: 8.093473]\n",
      "20526 [Discriminator loss: 0.183682, acc.: 92.19%] [Generator loss: 7.906328]\n",
      "20527 [Discriminator loss: 0.055457, acc.: 96.88%] [Generator loss: 8.927575]\n",
      "20528 [Discriminator loss: 0.039973, acc.: 98.44%] [Generator loss: 8.976326]\n",
      "20529 [Discriminator loss: 0.098652, acc.: 96.88%] [Generator loss: 10.112698]\n",
      "20530 [Discriminator loss: 0.031243, acc.: 98.44%] [Generator loss: 10.305915]\n",
      "20531 [Discriminator loss: 0.136588, acc.: 93.75%] [Generator loss: 8.739573]\n",
      "20532 [Discriminator loss: 0.018432, acc.: 98.44%] [Generator loss: 9.423893]\n",
      "20533 [Discriminator loss: 0.082957, acc.: 95.31%] [Generator loss: 7.821197]\n",
      "20534 [Discriminator loss: 0.122752, acc.: 96.88%] [Generator loss: 7.158345]\n",
      "20535 [Discriminator loss: 0.054890, acc.: 96.88%] [Generator loss: 7.619132]\n",
      "20536 [Discriminator loss: 0.048426, acc.: 98.44%] [Generator loss: 8.430408]\n",
      "20537 [Discriminator loss: 0.150855, acc.: 96.88%] [Generator loss: 10.060182]\n",
      "20538 [Discriminator loss: 0.017516, acc.: 100.00%] [Generator loss: 9.696452]\n",
      "20539 [Discriminator loss: 0.129645, acc.: 96.88%] [Generator loss: 7.915751]\n",
      "20540 [Discriminator loss: 0.008715, acc.: 100.00%] [Generator loss: 6.983939]\n",
      "20541 [Discriminator loss: 0.079492, acc.: 95.31%] [Generator loss: 7.117306]\n",
      "20542 [Discriminator loss: 0.028716, acc.: 98.44%] [Generator loss: 9.539149]\n",
      "20543 [Discriminator loss: 0.019537, acc.: 98.44%] [Generator loss: 9.812269]\n",
      "20544 [Discriminator loss: 0.025305, acc.: 98.44%] [Generator loss: 8.754645]\n",
      "20545 [Discriminator loss: 0.040474, acc.: 98.44%] [Generator loss: 7.097946]\n",
      "20546 [Discriminator loss: 0.137569, acc.: 93.75%] [Generator loss: 7.078854]\n",
      "20547 [Discriminator loss: 0.171168, acc.: 92.19%] [Generator loss: 7.542958]\n",
      "20548 [Discriminator loss: 0.103347, acc.: 95.31%] [Generator loss: 10.708123]\n",
      "20549 [Discriminator loss: 0.064230, acc.: 98.44%] [Generator loss: 9.272982]\n",
      "20550 [Discriminator loss: 0.062904, acc.: 98.44%] [Generator loss: 8.040506]\n",
      "20551 [Discriminator loss: 0.074757, acc.: 96.88%] [Generator loss: 8.296548]\n",
      "20552 [Discriminator loss: 0.021919, acc.: 100.00%] [Generator loss: 7.006398]\n",
      "20553 [Discriminator loss: 0.089458, acc.: 96.88%] [Generator loss: 7.962453]\n",
      "20554 [Discriminator loss: 0.133821, acc.: 95.31%] [Generator loss: 8.962323]\n",
      "20555 [Discriminator loss: 0.074458, acc.: 95.31%] [Generator loss: 8.202166]\n",
      "20556 [Discriminator loss: 0.080722, acc.: 96.88%] [Generator loss: 8.730556]\n",
      "20557 [Discriminator loss: 0.080676, acc.: 96.88%] [Generator loss: 7.092399]\n",
      "20558 [Discriminator loss: 0.044979, acc.: 98.44%] [Generator loss: 8.024886]\n",
      "20559 [Discriminator loss: 0.077521, acc.: 96.88%] [Generator loss: 9.186381]\n",
      "20560 [Discriminator loss: 0.043543, acc.: 98.44%] [Generator loss: 8.398294]\n",
      "20561 [Discriminator loss: 0.026922, acc.: 98.44%] [Generator loss: 8.515553]\n",
      "20562 [Discriminator loss: 0.026126, acc.: 98.44%] [Generator loss: 8.706524]\n",
      "20563 [Discriminator loss: 0.023568, acc.: 100.00%] [Generator loss: 7.517452]\n",
      "20564 [Discriminator loss: 0.034674, acc.: 98.44%] [Generator loss: 8.980882]\n",
      "20565 [Discriminator loss: 0.041467, acc.: 98.44%] [Generator loss: 8.048490]\n",
      "20566 [Discriminator loss: 0.119584, acc.: 93.75%] [Generator loss: 8.861530]\n",
      "20567 [Discriminator loss: 0.013825, acc.: 100.00%] [Generator loss: 9.452394]\n",
      "20568 [Discriminator loss: 0.062965, acc.: 95.31%] [Generator loss: 7.514283]\n",
      "20569 [Discriminator loss: 0.086677, acc.: 96.88%] [Generator loss: 8.335519]\n",
      "20570 [Discriminator loss: 0.070008, acc.: 96.88%] [Generator loss: 8.685613]\n",
      "20571 [Discriminator loss: 0.094314, acc.: 93.75%] [Generator loss: 8.524184]\n",
      "20572 [Discriminator loss: 0.047492, acc.: 98.44%] [Generator loss: 7.693073]\n",
      "20573 [Discriminator loss: 0.023607, acc.: 100.00%] [Generator loss: 6.753597]\n",
      "20574 [Discriminator loss: 0.047844, acc.: 98.44%] [Generator loss: 7.828453]\n",
      "20575 [Discriminator loss: 0.019023, acc.: 98.44%] [Generator loss: 8.912603]\n",
      "20576 [Discriminator loss: 0.115586, acc.: 95.31%] [Generator loss: 7.914533]\n",
      "20577 [Discriminator loss: 0.024981, acc.: 100.00%] [Generator loss: 8.965732]\n",
      "20578 [Discriminator loss: 0.042051, acc.: 98.44%] [Generator loss: 8.811029]\n",
      "20579 [Discriminator loss: 0.054256, acc.: 98.44%] [Generator loss: 9.202626]\n",
      "20580 [Discriminator loss: 0.046686, acc.: 96.88%] [Generator loss: 8.635866]\n",
      "20581 [Discriminator loss: 0.022173, acc.: 98.44%] [Generator loss: 7.721937]\n",
      "20582 [Discriminator loss: 0.011427, acc.: 100.00%] [Generator loss: 6.810972]\n",
      "20583 [Discriminator loss: 0.049313, acc.: 100.00%] [Generator loss: 7.598081]\n",
      "20584 [Discriminator loss: 0.088051, acc.: 98.44%] [Generator loss: 9.986108]\n",
      "20585 [Discriminator loss: 0.035707, acc.: 98.44%] [Generator loss: 9.612896]\n",
      "20586 [Discriminator loss: 0.085276, acc.: 95.31%] [Generator loss: 8.972984]\n",
      "20587 [Discriminator loss: 0.063878, acc.: 96.88%] [Generator loss: 8.625793]\n",
      "20588 [Discriminator loss: 0.044803, acc.: 98.44%] [Generator loss: 6.995273]\n",
      "20589 [Discriminator loss: 0.075868, acc.: 95.31%] [Generator loss: 9.118267]\n",
      "20590 [Discriminator loss: 0.016181, acc.: 100.00%] [Generator loss: 10.498568]\n",
      "20591 [Discriminator loss: 0.027456, acc.: 98.44%] [Generator loss: 9.343812]\n",
      "20592 [Discriminator loss: 0.068847, acc.: 96.88%] [Generator loss: 8.453101]\n",
      "20593 [Discriminator loss: 0.010870, acc.: 100.00%] [Generator loss: 7.664376]\n",
      "20594 [Discriminator loss: 0.036566, acc.: 98.44%] [Generator loss: 8.877592]\n",
      "20595 [Discriminator loss: 0.016149, acc.: 100.00%] [Generator loss: 8.863588]\n",
      "20596 [Discriminator loss: 0.037527, acc.: 98.44%] [Generator loss: 6.675223]\n",
      "20597 [Discriminator loss: 0.042692, acc.: 98.44%] [Generator loss: 7.937059]\n",
      "20598 [Discriminator loss: 0.027099, acc.: 100.00%] [Generator loss: 6.996895]\n",
      "20599 [Discriminator loss: 0.053483, acc.: 98.44%] [Generator loss: 8.931500]\n",
      "20600 [Discriminator loss: 0.091004, acc.: 96.88%] [Generator loss: 10.898219]\n",
      "20601 [Discriminator loss: 0.003681, acc.: 100.00%] [Generator loss: 8.275221]\n",
      "20602 [Discriminator loss: 0.101390, acc.: 96.88%] [Generator loss: 8.605444]\n",
      "20603 [Discriminator loss: 0.048846, acc.: 98.44%] [Generator loss: 10.103231]\n",
      "20604 [Discriminator loss: 0.101484, acc.: 95.31%] [Generator loss: 9.308140]\n",
      "20605 [Discriminator loss: 0.039985, acc.: 98.44%] [Generator loss: 7.967168]\n",
      "20606 [Discriminator loss: 0.039347, acc.: 98.44%] [Generator loss: 8.372662]\n",
      "20607 [Discriminator loss: 0.063320, acc.: 98.44%] [Generator loss: 9.867285]\n",
      "20608 [Discriminator loss: 0.016542, acc.: 100.00%] [Generator loss: 7.665325]\n",
      "20609 [Discriminator loss: 0.174854, acc.: 95.31%] [Generator loss: 7.174212]\n",
      "20610 [Discriminator loss: 0.063074, acc.: 96.88%] [Generator loss: 7.662640]\n",
      "20611 [Discriminator loss: 0.036164, acc.: 98.44%] [Generator loss: 8.048280]\n",
      "20612 [Discriminator loss: 0.016217, acc.: 98.44%] [Generator loss: 8.059160]\n",
      "20613 [Discriminator loss: 0.028113, acc.: 100.00%] [Generator loss: 8.597801]\n",
      "20614 [Discriminator loss: 0.009666, acc.: 100.00%] [Generator loss: 8.995865]\n",
      "20615 [Discriminator loss: 0.057341, acc.: 98.44%] [Generator loss: 8.449165]\n",
      "20616 [Discriminator loss: 0.106446, acc.: 98.44%] [Generator loss: 8.338007]\n",
      "20617 [Discriminator loss: 0.082091, acc.: 96.88%] [Generator loss: 9.682867]\n",
      "20618 [Discriminator loss: 0.029874, acc.: 100.00%] [Generator loss: 8.064751]\n",
      "20619 [Discriminator loss: 0.022928, acc.: 98.44%] [Generator loss: 8.559660]\n",
      "20620 [Discriminator loss: 0.164719, acc.: 90.62%] [Generator loss: 8.445539]\n",
      "20621 [Discriminator loss: 0.037924, acc.: 96.88%] [Generator loss: 10.415949]\n",
      "20622 [Discriminator loss: 0.054040, acc.: 98.44%] [Generator loss: 8.489182]\n",
      "20623 [Discriminator loss: 0.165751, acc.: 95.31%] [Generator loss: 7.724884]\n",
      "20624 [Discriminator loss: 0.018394, acc.: 100.00%] [Generator loss: 8.055256]\n",
      "20625 [Discriminator loss: 0.015152, acc.: 100.00%] [Generator loss: 8.050557]\n",
      "20626 [Discriminator loss: 0.033638, acc.: 100.00%] [Generator loss: 7.869223]\n",
      "20627 [Discriminator loss: 0.014891, acc.: 100.00%] [Generator loss: 6.846085]\n",
      "20628 [Discriminator loss: 0.072450, acc.: 96.88%] [Generator loss: 9.472305]\n",
      "20629 [Discriminator loss: 0.065439, acc.: 96.88%] [Generator loss: 8.345695]\n",
      "20630 [Discriminator loss: 0.011711, acc.: 100.00%] [Generator loss: 8.501602]\n",
      "20631 [Discriminator loss: 0.059746, acc.: 96.88%] [Generator loss: 7.705441]\n",
      "20632 [Discriminator loss: 0.082068, acc.: 95.31%] [Generator loss: 9.143945]\n",
      "20633 [Discriminator loss: 0.032445, acc.: 98.44%] [Generator loss: 10.749723]\n",
      "20634 [Discriminator loss: 0.064796, acc.: 96.88%] [Generator loss: 10.799460]\n",
      "20635 [Discriminator loss: 0.161035, acc.: 96.88%] [Generator loss: 9.119885]\n",
      "20636 [Discriminator loss: 0.050453, acc.: 98.44%] [Generator loss: 8.281178]\n",
      "20637 [Discriminator loss: 0.082774, acc.: 96.88%] [Generator loss: 9.349400]\n",
      "20638 [Discriminator loss: 0.094693, acc.: 95.31%] [Generator loss: 8.425943]\n",
      "20639 [Discriminator loss: 0.074138, acc.: 98.44%] [Generator loss: 9.522819]\n",
      "20640 [Discriminator loss: 0.061567, acc.: 98.44%] [Generator loss: 9.666342]\n",
      "20641 [Discriminator loss: 0.069212, acc.: 96.88%] [Generator loss: 9.614207]\n",
      "20642 [Discriminator loss: 0.161877, acc.: 93.75%] [Generator loss: 10.182137]\n",
      "20643 [Discriminator loss: 0.049879, acc.: 98.44%] [Generator loss: 9.970451]\n",
      "20644 [Discriminator loss: 0.017205, acc.: 100.00%] [Generator loss: 8.569923]\n",
      "20645 [Discriminator loss: 0.022644, acc.: 100.00%] [Generator loss: 8.166404]\n",
      "20646 [Discriminator loss: 0.041651, acc.: 96.88%] [Generator loss: 7.490095]\n",
      "20647 [Discriminator loss: 0.073387, acc.: 96.88%] [Generator loss: 9.327620]\n",
      "20648 [Discriminator loss: 0.118329, acc.: 96.88%] [Generator loss: 9.526499]\n",
      "20649 [Discriminator loss: 0.117623, acc.: 98.44%] [Generator loss: 8.859598]\n",
      "20650 [Discriminator loss: 0.042576, acc.: 96.88%] [Generator loss: 9.155580]\n",
      "20651 [Discriminator loss: 0.136494, acc.: 95.31%] [Generator loss: 10.307920]\n",
      "20652 [Discriminator loss: 0.065958, acc.: 98.44%] [Generator loss: 10.252898]\n",
      "20653 [Discriminator loss: 0.068336, acc.: 96.88%] [Generator loss: 10.578789]\n",
      "20654 [Discriminator loss: 0.206548, acc.: 98.44%] [Generator loss: 9.697559]\n",
      "20655 [Discriminator loss: 0.062585, acc.: 96.88%] [Generator loss: 7.781869]\n",
      "20656 [Discriminator loss: 0.046878, acc.: 98.44%] [Generator loss: 7.281263]\n",
      "20657 [Discriminator loss: 0.124670, acc.: 98.44%] [Generator loss: 8.987591]\n",
      "20658 [Discriminator loss: 0.134285, acc.: 93.75%] [Generator loss: 9.650745]\n",
      "20659 [Discriminator loss: 0.036010, acc.: 98.44%] [Generator loss: 8.197929]\n",
      "20660 [Discriminator loss: 0.228232, acc.: 93.75%] [Generator loss: 6.811397]\n",
      "20661 [Discriminator loss: 0.176154, acc.: 92.19%] [Generator loss: 9.098106]\n",
      "20662 [Discriminator loss: 0.016988, acc.: 100.00%] [Generator loss: 11.156039]\n",
      "20663 [Discriminator loss: 0.065626, acc.: 96.88%] [Generator loss: 8.999645]\n",
      "20664 [Discriminator loss: 0.089315, acc.: 96.88%] [Generator loss: 7.283235]\n",
      "20665 [Discriminator loss: 0.006958, acc.: 100.00%] [Generator loss: 7.917606]\n",
      "20666 [Discriminator loss: 0.107264, acc.: 92.19%] [Generator loss: 9.680519]\n",
      "20667 [Discriminator loss: 0.161584, acc.: 92.19%] [Generator loss: 8.925301]\n",
      "20668 [Discriminator loss: 0.057714, acc.: 98.44%] [Generator loss: 9.484505]\n",
      "20669 [Discriminator loss: 0.104161, acc.: 95.31%] [Generator loss: 8.180046]\n",
      "20670 [Discriminator loss: 0.153266, acc.: 95.31%] [Generator loss: 8.438833]\n",
      "20671 [Discriminator loss: 0.094274, acc.: 96.88%] [Generator loss: 8.893754]\n",
      "20672 [Discriminator loss: 0.070572, acc.: 96.88%] [Generator loss: 9.235769]\n",
      "20673 [Discriminator loss: 0.027593, acc.: 100.00%] [Generator loss: 7.494097]\n",
      "20674 [Discriminator loss: 0.154952, acc.: 93.75%] [Generator loss: 8.661623]\n",
      "20675 [Discriminator loss: 0.087979, acc.: 95.31%] [Generator loss: 8.634876]\n",
      "20676 [Discriminator loss: 0.089178, acc.: 96.88%] [Generator loss: 8.444931]\n",
      "20677 [Discriminator loss: 0.045676, acc.: 100.00%] [Generator loss: 9.437071]\n",
      "20678 [Discriminator loss: 0.033513, acc.: 98.44%] [Generator loss: 7.515712]\n",
      "20679 [Discriminator loss: 0.031281, acc.: 100.00%] [Generator loss: 8.610088]\n",
      "20680 [Discriminator loss: 0.046379, acc.: 98.44%] [Generator loss: 9.043927]\n",
      "20681 [Discriminator loss: 0.064803, acc.: 96.88%] [Generator loss: 8.161128]\n",
      "20682 [Discriminator loss: 0.041423, acc.: 98.44%] [Generator loss: 8.512978]\n",
      "20683 [Discriminator loss: 0.043513, acc.: 98.44%] [Generator loss: 9.443824]\n",
      "20684 [Discriminator loss: 0.059485, acc.: 96.88%] [Generator loss: 8.684402]\n",
      "20685 [Discriminator loss: 0.193151, acc.: 92.19%] [Generator loss: 7.771969]\n",
      "20686 [Discriminator loss: 0.008686, acc.: 100.00%] [Generator loss: 7.537647]\n",
      "20687 [Discriminator loss: 0.082657, acc.: 95.31%] [Generator loss: 8.496944]\n",
      "20688 [Discriminator loss: 0.024058, acc.: 100.00%] [Generator loss: 9.725275]\n",
      "20689 [Discriminator loss: 0.175413, acc.: 93.75%] [Generator loss: 9.747910]\n",
      "20690 [Discriminator loss: 0.075706, acc.: 96.88%] [Generator loss: 8.869057]\n",
      "20691 [Discriminator loss: 0.010433, acc.: 100.00%] [Generator loss: 8.540748]\n",
      "20692 [Discriminator loss: 0.085760, acc.: 98.44%] [Generator loss: 7.769844]\n",
      "20693 [Discriminator loss: 0.168430, acc.: 92.19%] [Generator loss: 10.994640]\n",
      "20694 [Discriminator loss: 0.183471, acc.: 92.19%] [Generator loss: 9.516716]\n",
      "20695 [Discriminator loss: 0.177736, acc.: 95.31%] [Generator loss: 8.222834]\n",
      "20696 [Discriminator loss: 0.015725, acc.: 100.00%] [Generator loss: 8.389334]\n",
      "20697 [Discriminator loss: 0.047144, acc.: 98.44%] [Generator loss: 9.698093]\n",
      "20698 [Discriminator loss: 0.108956, acc.: 95.31%] [Generator loss: 8.499601]\n",
      "20699 [Discriminator loss: 0.023264, acc.: 100.00%] [Generator loss: 7.136803]\n",
      "20700 [Discriminator loss: 0.084960, acc.: 95.31%] [Generator loss: 7.580437]\n",
      "20701 [Discriminator loss: 0.069643, acc.: 96.88%] [Generator loss: 8.720230]\n",
      "20702 [Discriminator loss: 0.015760, acc.: 100.00%] [Generator loss: 7.494783]\n",
      "20703 [Discriminator loss: 0.069886, acc.: 96.88%] [Generator loss: 7.264822]\n",
      "20704 [Discriminator loss: 0.091072, acc.: 98.44%] [Generator loss: 6.975533]\n",
      "20705 [Discriminator loss: 0.039149, acc.: 98.44%] [Generator loss: 7.322783]\n",
      "20706 [Discriminator loss: 0.046564, acc.: 100.00%] [Generator loss: 7.026837]\n",
      "20707 [Discriminator loss: 0.034035, acc.: 98.44%] [Generator loss: 8.656893]\n",
      "20708 [Discriminator loss: 0.069532, acc.: 98.44%] [Generator loss: 6.773905]\n",
      "20709 [Discriminator loss: 0.145671, acc.: 92.19%] [Generator loss: 10.856045]\n",
      "20710 [Discriminator loss: 0.026787, acc.: 100.00%] [Generator loss: 9.498625]\n",
      "20711 [Discriminator loss: 0.143125, acc.: 92.19%] [Generator loss: 9.619857]\n",
      "20712 [Discriminator loss: 0.066158, acc.: 96.88%] [Generator loss: 8.037430]\n",
      "20713 [Discriminator loss: 0.072697, acc.: 95.31%] [Generator loss: 8.625196]\n",
      "20714 [Discriminator loss: 0.042424, acc.: 98.44%] [Generator loss: 9.548181]\n",
      "20715 [Discriminator loss: 0.012205, acc.: 100.00%] [Generator loss: 8.660979]\n",
      "20716 [Discriminator loss: 0.043329, acc.: 98.44%] [Generator loss: 9.776151]\n",
      "20717 [Discriminator loss: 0.149342, acc.: 90.62%] [Generator loss: 9.604603]\n",
      "20718 [Discriminator loss: 0.090025, acc.: 98.44%] [Generator loss: 11.219398]\n",
      "20719 [Discriminator loss: 0.374395, acc.: 85.94%] [Generator loss: 8.829463]\n",
      "20720 [Discriminator loss: 0.022739, acc.: 98.44%] [Generator loss: 8.846948]\n",
      "20721 [Discriminator loss: 0.217813, acc.: 92.19%] [Generator loss: 8.926525]\n",
      "20722 [Discriminator loss: 0.044117, acc.: 98.44%] [Generator loss: 9.401451]\n",
      "20723 [Discriminator loss: 0.133691, acc.: 96.88%] [Generator loss: 9.487694]\n",
      "20724 [Discriminator loss: 0.020265, acc.: 100.00%] [Generator loss: 8.691665]\n",
      "20725 [Discriminator loss: 0.012723, acc.: 100.00%] [Generator loss: 7.212563]\n",
      "20726 [Discriminator loss: 0.015537, acc.: 100.00%] [Generator loss: 7.992536]\n",
      "20727 [Discriminator loss: 0.149999, acc.: 93.75%] [Generator loss: 9.053294]\n",
      "20728 [Discriminator loss: 0.016348, acc.: 100.00%] [Generator loss: 9.881459]\n",
      "20729 [Discriminator loss: 0.005930, acc.: 100.00%] [Generator loss: 9.129007]\n",
      "20730 [Discriminator loss: 0.171804, acc.: 95.31%] [Generator loss: 7.491531]\n",
      "20731 [Discriminator loss: 0.018788, acc.: 100.00%] [Generator loss: 8.264973]\n",
      "20732 [Discriminator loss: 0.037507, acc.: 98.44%] [Generator loss: 8.688202]\n",
      "20733 [Discriminator loss: 0.126640, acc.: 95.31%] [Generator loss: 8.833691]\n",
      "20734 [Discriminator loss: 0.078556, acc.: 95.31%] [Generator loss: 9.635674]\n",
      "20735 [Discriminator loss: 0.095491, acc.: 96.88%] [Generator loss: 9.015057]\n",
      "20736 [Discriminator loss: 0.082704, acc.: 95.31%] [Generator loss: 9.678368]\n",
      "20737 [Discriminator loss: 0.147466, acc.: 95.31%] [Generator loss: 8.320635]\n",
      "20738 [Discriminator loss: 0.095667, acc.: 96.88%] [Generator loss: 8.234696]\n",
      "20739 [Discriminator loss: 0.079668, acc.: 96.88%] [Generator loss: 9.493767]\n",
      "20740 [Discriminator loss: 0.008389, acc.: 100.00%] [Generator loss: 9.917013]\n",
      "20741 [Discriminator loss: 0.154220, acc.: 92.19%] [Generator loss: 10.287684]\n",
      "20742 [Discriminator loss: 0.085763, acc.: 95.31%] [Generator loss: 10.422610]\n",
      "20743 [Discriminator loss: 0.037495, acc.: 98.44%] [Generator loss: 9.598844]\n",
      "20744 [Discriminator loss: 0.040962, acc.: 100.00%] [Generator loss: 8.828007]\n",
      "20745 [Discriminator loss: 0.055392, acc.: 96.88%] [Generator loss: 7.785962]\n",
      "20746 [Discriminator loss: 0.042195, acc.: 98.44%] [Generator loss: 8.396627]\n",
      "20747 [Discriminator loss: 0.050834, acc.: 98.44%] [Generator loss: 7.563227]\n",
      "20748 [Discriminator loss: 0.056784, acc.: 98.44%] [Generator loss: 8.176092]\n",
      "20749 [Discriminator loss: 0.113722, acc.: 95.31%] [Generator loss: 10.053460]\n",
      "20750 [Discriminator loss: 0.012470, acc.: 100.00%] [Generator loss: 10.907348]\n",
      "20751 [Discriminator loss: 0.057868, acc.: 98.44%] [Generator loss: 8.163657]\n",
      "20752 [Discriminator loss: 0.082594, acc.: 95.31%] [Generator loss: 9.479732]\n",
      "20753 [Discriminator loss: 0.129266, acc.: 96.88%] [Generator loss: 9.341146]\n",
      "20754 [Discriminator loss: 0.027042, acc.: 98.44%] [Generator loss: 9.628942]\n",
      "20755 [Discriminator loss: 0.094311, acc.: 96.88%] [Generator loss: 7.069490]\n",
      "20756 [Discriminator loss: 0.071967, acc.: 96.88%] [Generator loss: 9.800687]\n",
      "20757 [Discriminator loss: 0.155189, acc.: 93.75%] [Generator loss: 8.256659]\n",
      "20758 [Discriminator loss: 0.015740, acc.: 100.00%] [Generator loss: 7.371264]\n",
      "20759 [Discriminator loss: 0.114740, acc.: 95.31%] [Generator loss: 8.563080]\n",
      "20760 [Discriminator loss: 0.119175, acc.: 93.75%] [Generator loss: 10.573357]\n",
      "20761 [Discriminator loss: 0.056606, acc.: 98.44%] [Generator loss: 8.447330]\n",
      "20762 [Discriminator loss: 0.078419, acc.: 96.88%] [Generator loss: 9.248995]\n",
      "20763 [Discriminator loss: 0.120501, acc.: 95.31%] [Generator loss: 8.163157]\n",
      "20764 [Discriminator loss: 0.131226, acc.: 90.62%] [Generator loss: 8.987541]\n",
      "20765 [Discriminator loss: 0.009490, acc.: 100.00%] [Generator loss: 8.401953]\n",
      "20766 [Discriminator loss: 0.042247, acc.: 100.00%] [Generator loss: 9.826580]\n",
      "20767 [Discriminator loss: 0.027500, acc.: 100.00%] [Generator loss: 9.919285]\n",
      "20768 [Discriminator loss: 0.098877, acc.: 95.31%] [Generator loss: 9.044397]\n",
      "20769 [Discriminator loss: 0.063773, acc.: 98.44%] [Generator loss: 8.491539]\n",
      "20770 [Discriminator loss: 0.012635, acc.: 100.00%] [Generator loss: 8.022602]\n",
      "20771 [Discriminator loss: 0.009210, acc.: 100.00%] [Generator loss: 9.185506]\n",
      "20772 [Discriminator loss: 0.120905, acc.: 95.31%] [Generator loss: 9.087541]\n",
      "20773 [Discriminator loss: 0.067978, acc.: 95.31%] [Generator loss: 8.932999]\n",
      "20774 [Discriminator loss: 0.012983, acc.: 100.00%] [Generator loss: 8.552630]\n",
      "20775 [Discriminator loss: 0.041531, acc.: 98.44%] [Generator loss: 7.842187]\n",
      "20776 [Discriminator loss: 0.088144, acc.: 96.88%] [Generator loss: 8.109654]\n",
      "20777 [Discriminator loss: 0.049416, acc.: 98.44%] [Generator loss: 10.078412]\n",
      "20778 [Discriminator loss: 0.062598, acc.: 95.31%] [Generator loss: 7.195221]\n",
      "20779 [Discriminator loss: 0.225228, acc.: 96.88%] [Generator loss: 7.038947]\n",
      "20780 [Discriminator loss: 0.007096, acc.: 100.00%] [Generator loss: 8.274886]\n",
      "20781 [Discriminator loss: 0.094015, acc.: 95.31%] [Generator loss: 6.190940]\n",
      "20782 [Discriminator loss: 0.030543, acc.: 98.44%] [Generator loss: 8.572042]\n",
      "20783 [Discriminator loss: 0.055479, acc.: 98.44%] [Generator loss: 9.472530]\n",
      "20784 [Discriminator loss: 0.107622, acc.: 96.88%] [Generator loss: 8.222914]\n",
      "20785 [Discriminator loss: 0.041939, acc.: 98.44%] [Generator loss: 8.608675]\n",
      "20786 [Discriminator loss: 0.116931, acc.: 93.75%] [Generator loss: 10.944765]\n",
      "20787 [Discriminator loss: 0.060614, acc.: 96.88%] [Generator loss: 8.206135]\n",
      "20788 [Discriminator loss: 0.057664, acc.: 98.44%] [Generator loss: 9.024076]\n",
      "20789 [Discriminator loss: 0.043449, acc.: 98.44%] [Generator loss: 8.652727]\n",
      "20790 [Discriminator loss: 0.106535, acc.: 93.75%] [Generator loss: 8.693148]\n",
      "20791 [Discriminator loss: 0.029988, acc.: 98.44%] [Generator loss: 8.641039]\n",
      "20792 [Discriminator loss: 0.200961, acc.: 95.31%] [Generator loss: 7.953687]\n",
      "20793 [Discriminator loss: 0.015507, acc.: 100.00%] [Generator loss: 8.292586]\n",
      "20794 [Discriminator loss: 0.074820, acc.: 96.88%] [Generator loss: 8.942693]\n",
      "20795 [Discriminator loss: 0.051153, acc.: 98.44%] [Generator loss: 9.778661]\n",
      "20796 [Discriminator loss: 0.023308, acc.: 100.00%] [Generator loss: 7.856778]\n",
      "20797 [Discriminator loss: 0.065437, acc.: 96.88%] [Generator loss: 8.373765]\n",
      "20798 [Discriminator loss: 0.195396, acc.: 90.62%] [Generator loss: 10.028866]\n",
      "20799 [Discriminator loss: 0.063108, acc.: 98.44%] [Generator loss: 9.802034]\n",
      "20800 [Discriminator loss: 0.079342, acc.: 96.88%] [Generator loss: 8.098694]\n",
      "20801 [Discriminator loss: 0.050073, acc.: 98.44%] [Generator loss: 8.042274]\n",
      "20802 [Discriminator loss: 0.036487, acc.: 98.44%] [Generator loss: 8.287153]\n",
      "20803 [Discriminator loss: 0.064991, acc.: 98.44%] [Generator loss: 6.851933]\n",
      "20804 [Discriminator loss: 0.101852, acc.: 92.19%] [Generator loss: 8.960360]\n",
      "20805 [Discriminator loss: 0.008939, acc.: 100.00%] [Generator loss: 8.440699]\n",
      "20806 [Discriminator loss: 0.158943, acc.: 92.19%] [Generator loss: 7.505079]\n",
      "20807 [Discriminator loss: 0.094354, acc.: 95.31%] [Generator loss: 7.936020]\n",
      "20808 [Discriminator loss: 0.051506, acc.: 96.88%] [Generator loss: 10.625060]\n",
      "20809 [Discriminator loss: 0.035764, acc.: 98.44%] [Generator loss: 9.263290]\n",
      "20810 [Discriminator loss: 0.098209, acc.: 96.88%] [Generator loss: 8.918776]\n",
      "20811 [Discriminator loss: 0.103347, acc.: 95.31%] [Generator loss: 9.106357]\n",
      "20812 [Discriminator loss: 0.013649, acc.: 100.00%] [Generator loss: 8.017780]\n",
      "20813 [Discriminator loss: 0.072457, acc.: 96.88%] [Generator loss: 8.374761]\n",
      "20814 [Discriminator loss: 0.100961, acc.: 93.75%] [Generator loss: 7.496857]\n",
      "20815 [Discriminator loss: 0.154464, acc.: 93.75%] [Generator loss: 10.281185]\n",
      "20816 [Discriminator loss: 0.072859, acc.: 95.31%] [Generator loss: 9.134937]\n",
      "20817 [Discriminator loss: 0.024981, acc.: 98.44%] [Generator loss: 8.715763]\n",
      "20818 [Discriminator loss: 0.046670, acc.: 96.88%] [Generator loss: 8.834171]\n",
      "20819 [Discriminator loss: 0.063140, acc.: 96.88%] [Generator loss: 8.642632]\n",
      "20820 [Discriminator loss: 0.043471, acc.: 100.00%] [Generator loss: 7.726808]\n",
      "20821 [Discriminator loss: 0.073309, acc.: 95.31%] [Generator loss: 6.622738]\n",
      "20822 [Discriminator loss: 0.120576, acc.: 95.31%] [Generator loss: 7.238212]\n",
      "20823 [Discriminator loss: 0.025880, acc.: 98.44%] [Generator loss: 9.935606]\n",
      "20824 [Discriminator loss: 0.090704, acc.: 96.88%] [Generator loss: 9.770916]\n",
      "20825 [Discriminator loss: 0.128499, acc.: 93.75%] [Generator loss: 8.879322]\n",
      "20826 [Discriminator loss: 0.009900, acc.: 100.00%] [Generator loss: 9.428251]\n",
      "20827 [Discriminator loss: 0.064654, acc.: 96.88%] [Generator loss: 9.334619]\n",
      "20828 [Discriminator loss: 0.014828, acc.: 100.00%] [Generator loss: 8.367347]\n",
      "20829 [Discriminator loss: 0.133752, acc.: 93.75%] [Generator loss: 9.542442]\n",
      "20830 [Discriminator loss: 0.037533, acc.: 98.44%] [Generator loss: 7.399922]\n",
      "20831 [Discriminator loss: 0.142243, acc.: 95.31%] [Generator loss: 9.083664]\n",
      "20832 [Discriminator loss: 0.038133, acc.: 98.44%] [Generator loss: 9.430790]\n",
      "20833 [Discriminator loss: 0.172382, acc.: 92.19%] [Generator loss: 9.887877]\n",
      "20834 [Discriminator loss: 0.080455, acc.: 93.75%] [Generator loss: 10.344841]\n",
      "20835 [Discriminator loss: 0.054707, acc.: 98.44%] [Generator loss: 8.887924]\n",
      "20836 [Discriminator loss: 0.036762, acc.: 98.44%] [Generator loss: 9.040109]\n",
      "20837 [Discriminator loss: 0.066107, acc.: 96.88%] [Generator loss: 7.785534]\n",
      "20838 [Discriminator loss: 0.082228, acc.: 98.44%] [Generator loss: 6.809916]\n",
      "20839 [Discriminator loss: 0.090538, acc.: 96.88%] [Generator loss: 8.422062]\n",
      "20840 [Discriminator loss: 0.014033, acc.: 100.00%] [Generator loss: 9.052527]\n",
      "20841 [Discriminator loss: 0.050648, acc.: 96.88%] [Generator loss: 7.903028]\n",
      "20842 [Discriminator loss: 0.018905, acc.: 100.00%] [Generator loss: 8.307468]\n",
      "20843 [Discriminator loss: 0.022135, acc.: 98.44%] [Generator loss: 8.795593]\n",
      "20844 [Discriminator loss: 0.060104, acc.: 96.88%] [Generator loss: 10.818646]\n",
      "20845 [Discriminator loss: 0.056905, acc.: 95.31%] [Generator loss: 8.274244]\n",
      "20846 [Discriminator loss: 0.012954, acc.: 100.00%] [Generator loss: 9.089874]\n",
      "20847 [Discriminator loss: 0.103242, acc.: 95.31%] [Generator loss: 9.012178]\n",
      "20848 [Discriminator loss: 0.072244, acc.: 96.88%] [Generator loss: 8.024437]\n",
      "20849 [Discriminator loss: 0.143435, acc.: 98.44%] [Generator loss: 10.272844]\n",
      "20850 [Discriminator loss: 0.042503, acc.: 98.44%] [Generator loss: 9.943352]\n",
      "20851 [Discriminator loss: 0.075485, acc.: 96.88%] [Generator loss: 8.624387]\n",
      "20852 [Discriminator loss: 0.064863, acc.: 96.88%] [Generator loss: 6.380284]\n",
      "20853 [Discriminator loss: 0.118191, acc.: 98.44%] [Generator loss: 8.424738]\n",
      "20854 [Discriminator loss: 0.035976, acc.: 98.44%] [Generator loss: 8.748932]\n",
      "20855 [Discriminator loss: 0.197509, acc.: 96.88%] [Generator loss: 6.030409]\n",
      "20856 [Discriminator loss: 0.094898, acc.: 92.19%] [Generator loss: 8.620188]\n",
      "20857 [Discriminator loss: 0.046771, acc.: 98.44%] [Generator loss: 9.051978]\n",
      "20858 [Discriminator loss: 0.119541, acc.: 96.88%] [Generator loss: 9.084021]\n",
      "20859 [Discriminator loss: 0.081698, acc.: 96.88%] [Generator loss: 8.370877]\n",
      "20860 [Discriminator loss: 0.010639, acc.: 100.00%] [Generator loss: 10.222757]\n",
      "20861 [Discriminator loss: 0.198029, acc.: 93.75%] [Generator loss: 7.399746]\n",
      "20862 [Discriminator loss: 0.138677, acc.: 92.19%] [Generator loss: 7.658232]\n",
      "20863 [Discriminator loss: 0.047492, acc.: 98.44%] [Generator loss: 8.442562]\n",
      "20864 [Discriminator loss: 0.076419, acc.: 95.31%] [Generator loss: 9.794195]\n",
      "20865 [Discriminator loss: 0.052762, acc.: 98.44%] [Generator loss: 10.378323]\n",
      "20866 [Discriminator loss: 0.074355, acc.: 98.44%] [Generator loss: 9.694838]\n",
      "20867 [Discriminator loss: 0.031796, acc.: 100.00%] [Generator loss: 8.103909]\n",
      "20868 [Discriminator loss: 0.026505, acc.: 98.44%] [Generator loss: 8.766939]\n",
      "20869 [Discriminator loss: 0.030732, acc.: 98.44%] [Generator loss: 8.006183]\n",
      "20870 [Discriminator loss: 0.078107, acc.: 95.31%] [Generator loss: 9.766668]\n",
      "20871 [Discriminator loss: 0.017773, acc.: 98.44%] [Generator loss: 9.538240]\n",
      "20872 [Discriminator loss: 0.058495, acc.: 95.31%] [Generator loss: 6.951907]\n",
      "20873 [Discriminator loss: 0.081853, acc.: 95.31%] [Generator loss: 7.602731]\n",
      "20874 [Discriminator loss: 0.067196, acc.: 96.88%] [Generator loss: 9.424393]\n",
      "20875 [Discriminator loss: 0.018047, acc.: 100.00%] [Generator loss: 8.553129]\n",
      "20876 [Discriminator loss: 0.012832, acc.: 100.00%] [Generator loss: 8.980341]\n",
      "20877 [Discriminator loss: 0.174213, acc.: 92.19%] [Generator loss: 9.064294]\n",
      "20878 [Discriminator loss: 0.168655, acc.: 93.75%] [Generator loss: 9.291759]\n",
      "20879 [Discriminator loss: 0.006839, acc.: 100.00%] [Generator loss: 8.661678]\n",
      "20880 [Discriminator loss: 0.250868, acc.: 90.62%] [Generator loss: 10.708553]\n",
      "20881 [Discriminator loss: 0.127521, acc.: 96.88%] [Generator loss: 9.764976]\n",
      "20882 [Discriminator loss: 0.009661, acc.: 100.00%] [Generator loss: 9.877314]\n",
      "20883 [Discriminator loss: 0.101905, acc.: 96.88%] [Generator loss: 9.810901]\n",
      "20884 [Discriminator loss: 0.143056, acc.: 96.88%] [Generator loss: 9.257584]\n",
      "20885 [Discriminator loss: 0.069462, acc.: 96.88%] [Generator loss: 11.190094]\n",
      "20886 [Discriminator loss: 0.069179, acc.: 98.44%] [Generator loss: 8.334249]\n",
      "20887 [Discriminator loss: 0.015177, acc.: 100.00%] [Generator loss: 7.405389]\n",
      "20888 [Discriminator loss: 0.012949, acc.: 100.00%] [Generator loss: 7.614594]\n",
      "20889 [Discriminator loss: 0.128718, acc.: 95.31%] [Generator loss: 8.419645]\n",
      "20890 [Discriminator loss: 0.032001, acc.: 98.44%] [Generator loss: 7.835678]\n",
      "20891 [Discriminator loss: 0.031127, acc.: 98.44%] [Generator loss: 9.687119]\n",
      "20892 [Discriminator loss: 0.040544, acc.: 98.44%] [Generator loss: 9.363398]\n",
      "20893 [Discriminator loss: 0.063369, acc.: 95.31%] [Generator loss: 7.971504]\n",
      "20894 [Discriminator loss: 0.020558, acc.: 100.00%] [Generator loss: 8.179293]\n",
      "20895 [Discriminator loss: 0.076178, acc.: 96.88%] [Generator loss: 8.369452]\n",
      "20896 [Discriminator loss: 0.080305, acc.: 95.31%] [Generator loss: 7.524618]\n",
      "20897 [Discriminator loss: 0.051765, acc.: 96.88%] [Generator loss: 7.511000]\n",
      "20898 [Discriminator loss: 0.163710, acc.: 95.31%] [Generator loss: 10.482080]\n",
      "20899 [Discriminator loss: 0.037892, acc.: 100.00%] [Generator loss: 10.283419]\n",
      "20900 [Discriminator loss: 0.132842, acc.: 93.75%] [Generator loss: 6.114434]\n",
      "20901 [Discriminator loss: 0.016059, acc.: 100.00%] [Generator loss: 6.249371]\n",
      "20902 [Discriminator loss: 0.026548, acc.: 100.00%] [Generator loss: 7.847495]\n",
      "20903 [Discriminator loss: 0.107511, acc.: 95.31%] [Generator loss: 9.639904]\n",
      "20904 [Discriminator loss: 0.013456, acc.: 100.00%] [Generator loss: 8.627250]\n",
      "20905 [Discriminator loss: 0.031837, acc.: 98.44%] [Generator loss: 10.346417]\n",
      "20906 [Discriminator loss: 0.078851, acc.: 95.31%] [Generator loss: 6.706194]\n",
      "20907 [Discriminator loss: 0.034357, acc.: 98.44%] [Generator loss: 7.051966]\n",
      "20908 [Discriminator loss: 0.077767, acc.: 96.88%] [Generator loss: 6.194794]\n",
      "20909 [Discriminator loss: 0.106988, acc.: 96.88%] [Generator loss: 7.724285]\n",
      "20910 [Discriminator loss: 0.054970, acc.: 96.88%] [Generator loss: 8.111829]\n",
      "20911 [Discriminator loss: 0.019006, acc.: 100.00%] [Generator loss: 9.204823]\n",
      "20912 [Discriminator loss: 0.024631, acc.: 100.00%] [Generator loss: 8.747530]\n",
      "20913 [Discriminator loss: 0.073794, acc.: 95.31%] [Generator loss: 8.163445]\n",
      "20914 [Discriminator loss: 0.081674, acc.: 96.88%] [Generator loss: 9.784875]\n",
      "20915 [Discriminator loss: 0.013129, acc.: 100.00%] [Generator loss: 9.918971]\n",
      "20916 [Discriminator loss: 0.062716, acc.: 98.44%] [Generator loss: 8.754259]\n",
      "20917 [Discriminator loss: 0.106477, acc.: 95.31%] [Generator loss: 6.621810]\n",
      "20918 [Discriminator loss: 0.164121, acc.: 95.31%] [Generator loss: 9.432320]\n",
      "20919 [Discriminator loss: 0.078292, acc.: 93.75%] [Generator loss: 8.468990]\n",
      "20920 [Discriminator loss: 0.135884, acc.: 96.88%] [Generator loss: 8.715229]\n",
      "20921 [Discriminator loss: 0.094347, acc.: 98.44%] [Generator loss: 10.713098]\n",
      "20922 [Discriminator loss: 0.332205, acc.: 93.75%] [Generator loss: 9.077200]\n",
      "20923 [Discriminator loss: 0.021200, acc.: 100.00%] [Generator loss: 9.270827]\n",
      "20924 [Discriminator loss: 0.110338, acc.: 95.31%] [Generator loss: 8.604326]\n",
      "20925 [Discriminator loss: 0.044077, acc.: 96.88%] [Generator loss: 9.243133]\n",
      "20926 [Discriminator loss: 0.051390, acc.: 98.44%] [Generator loss: 8.573330]\n",
      "20927 [Discriminator loss: 0.025214, acc.: 98.44%] [Generator loss: 10.218664]\n",
      "20928 [Discriminator loss: 0.048499, acc.: 98.44%] [Generator loss: 8.199872]\n",
      "20929 [Discriminator loss: 0.039400, acc.: 98.44%] [Generator loss: 7.916222]\n",
      "20930 [Discriminator loss: 0.044577, acc.: 96.88%] [Generator loss: 7.507539]\n",
      "20931 [Discriminator loss: 0.017340, acc.: 100.00%] [Generator loss: 8.366952]\n",
      "20932 [Discriminator loss: 0.010184, acc.: 100.00%] [Generator loss: 8.197927]\n",
      "20933 [Discriminator loss: 0.023131, acc.: 100.00%] [Generator loss: 8.300198]\n",
      "20934 [Discriminator loss: 0.190608, acc.: 92.19%] [Generator loss: 9.318384]\n",
      "20935 [Discriminator loss: 0.032605, acc.: 98.44%] [Generator loss: 11.592875]\n",
      "20936 [Discriminator loss: 0.398278, acc.: 87.50%] [Generator loss: 6.559340]\n",
      "20937 [Discriminator loss: 0.200660, acc.: 92.19%] [Generator loss: 8.387595]\n",
      "20938 [Discriminator loss: 0.031078, acc.: 100.00%] [Generator loss: 9.969820]\n",
      "20939 [Discriminator loss: 0.096406, acc.: 93.75%] [Generator loss: 9.105753]\n",
      "20940 [Discriminator loss: 0.109694, acc.: 95.31%] [Generator loss: 10.309586]\n",
      "20941 [Discriminator loss: 0.037445, acc.: 98.44%] [Generator loss: 10.215284]\n",
      "20942 [Discriminator loss: 0.110285, acc.: 95.31%] [Generator loss: 10.300689]\n",
      "20943 [Discriminator loss: 0.055369, acc.: 96.88%] [Generator loss: 9.904284]\n",
      "20944 [Discriminator loss: 0.061911, acc.: 98.44%] [Generator loss: 8.015263]\n",
      "20945 [Discriminator loss: 0.106354, acc.: 96.88%] [Generator loss: 9.326044]\n",
      "20946 [Discriminator loss: 0.023140, acc.: 100.00%] [Generator loss: 10.360512]\n",
      "20947 [Discriminator loss: 0.224848, acc.: 95.31%] [Generator loss: 7.769857]\n",
      "20948 [Discriminator loss: 0.093677, acc.: 96.88%] [Generator loss: 7.115127]\n",
      "20949 [Discriminator loss: 0.049752, acc.: 98.44%] [Generator loss: 8.135228]\n",
      "20950 [Discriminator loss: 0.035148, acc.: 98.44%] [Generator loss: 8.131148]\n",
      "20951 [Discriminator loss: 0.082452, acc.: 95.31%] [Generator loss: 8.082518]\n",
      "20952 [Discriminator loss: 0.099400, acc.: 96.88%] [Generator loss: 8.797073]\n",
      "20953 [Discriminator loss: 0.024474, acc.: 100.00%] [Generator loss: 9.158728]\n",
      "20954 [Discriminator loss: 0.133340, acc.: 95.31%] [Generator loss: 9.385160]\n",
      "20955 [Discriminator loss: 0.191806, acc.: 93.75%] [Generator loss: 7.426497]\n",
      "20956 [Discriminator loss: 0.194642, acc.: 92.19%] [Generator loss: 10.060155]\n",
      "20957 [Discriminator loss: 0.016726, acc.: 100.00%] [Generator loss: 11.031120]\n",
      "20958 [Discriminator loss: 0.109408, acc.: 93.75%] [Generator loss: 8.749045]\n",
      "20959 [Discriminator loss: 0.028168, acc.: 98.44%] [Generator loss: 8.412399]\n",
      "20960 [Discriminator loss: 0.077345, acc.: 96.88%] [Generator loss: 7.815969]\n",
      "20961 [Discriminator loss: 0.018356, acc.: 100.00%] [Generator loss: 8.240287]\n",
      "20962 [Discriminator loss: 0.130130, acc.: 95.31%] [Generator loss: 7.275810]\n",
      "20963 [Discriminator loss: 0.174940, acc.: 93.75%] [Generator loss: 8.571156]\n",
      "20964 [Discriminator loss: 0.065948, acc.: 96.88%] [Generator loss: 9.852880]\n",
      "20965 [Discriminator loss: 0.011069, acc.: 100.00%] [Generator loss: 10.259092]\n",
      "20966 [Discriminator loss: 0.218316, acc.: 92.19%] [Generator loss: 5.547190]\n",
      "20967 [Discriminator loss: 0.140593, acc.: 93.75%] [Generator loss: 8.704216]\n",
      "20968 [Discriminator loss: 0.075476, acc.: 98.44%] [Generator loss: 9.871912]\n",
      "20969 [Discriminator loss: 0.210787, acc.: 96.88%] [Generator loss: 7.567801]\n",
      "20970 [Discriminator loss: 0.150220, acc.: 92.19%] [Generator loss: 8.826807]\n",
      "20971 [Discriminator loss: 0.029744, acc.: 100.00%] [Generator loss: 10.938534]\n",
      "20972 [Discriminator loss: 0.165330, acc.: 90.62%] [Generator loss: 6.406012]\n",
      "20973 [Discriminator loss: 0.107199, acc.: 95.31%] [Generator loss: 6.728314]\n",
      "20974 [Discriminator loss: 0.117445, acc.: 93.75%] [Generator loss: 9.656080]\n",
      "20975 [Discriminator loss: 0.018401, acc.: 100.00%] [Generator loss: 10.648628]\n",
      "20976 [Discriminator loss: 0.020720, acc.: 100.00%] [Generator loss: 9.686800]\n",
      "20977 [Discriminator loss: 0.010443, acc.: 100.00%] [Generator loss: 11.084124]\n",
      "20978 [Discriminator loss: 0.282402, acc.: 92.19%] [Generator loss: 8.291584]\n",
      "20979 [Discriminator loss: 0.052005, acc.: 98.44%] [Generator loss: 8.433325]\n",
      "20980 [Discriminator loss: 0.049413, acc.: 98.44%] [Generator loss: 9.106275]\n",
      "20981 [Discriminator loss: 0.047796, acc.: 96.88%] [Generator loss: 9.748884]\n",
      "20982 [Discriminator loss: 0.036963, acc.: 98.44%] [Generator loss: 11.530126]\n",
      "20983 [Discriminator loss: 0.034008, acc.: 98.44%] [Generator loss: 10.099829]\n",
      "20984 [Discriminator loss: 0.083361, acc.: 98.44%] [Generator loss: 9.107439]\n",
      "20985 [Discriminator loss: 0.160067, acc.: 95.31%] [Generator loss: 8.713007]\n",
      "20986 [Discriminator loss: 0.042138, acc.: 100.00%] [Generator loss: 7.280512]\n",
      "20987 [Discriminator loss: 0.028446, acc.: 100.00%] [Generator loss: 7.315500]\n",
      "20988 [Discriminator loss: 0.056912, acc.: 96.88%] [Generator loss: 9.011336]\n",
      "20989 [Discriminator loss: 0.027394, acc.: 98.44%] [Generator loss: 10.215614]\n",
      "20990 [Discriminator loss: 0.002787, acc.: 100.00%] [Generator loss: 9.036854]\n",
      "20991 [Discriminator loss: 0.108756, acc.: 95.31%] [Generator loss: 6.841544]\n",
      "20992 [Discriminator loss: 0.053729, acc.: 98.44%] [Generator loss: 8.918797]\n",
      "20993 [Discriminator loss: 0.037949, acc.: 100.00%] [Generator loss: 8.593937]\n",
      "20994 [Discriminator loss: 0.007216, acc.: 100.00%] [Generator loss: 7.216255]\n",
      "20995 [Discriminator loss: 0.017139, acc.: 100.00%] [Generator loss: 8.178053]\n",
      "20996 [Discriminator loss: 0.042037, acc.: 98.44%] [Generator loss: 7.415618]\n",
      "20997 [Discriminator loss: 0.122263, acc.: 98.44%] [Generator loss: 8.062599]\n",
      "20998 [Discriminator loss: 0.041976, acc.: 98.44%] [Generator loss: 6.505455]\n",
      "20999 [Discriminator loss: 0.122147, acc.: 93.75%] [Generator loss: 9.130418]\n",
      "21000 [Discriminator loss: 0.024781, acc.: 100.00%] [Generator loss: 8.845969]\n",
      "21001 [Discriminator loss: 0.057223, acc.: 98.44%] [Generator loss: 8.855377]\n",
      "21002 [Discriminator loss: 0.203216, acc.: 93.75%] [Generator loss: 8.802778]\n",
      "21003 [Discriminator loss: 0.012510, acc.: 100.00%] [Generator loss: 7.764209]\n",
      "21004 [Discriminator loss: 0.056526, acc.: 98.44%] [Generator loss: 7.489334]\n",
      "21005 [Discriminator loss: 0.066180, acc.: 96.88%] [Generator loss: 7.018185]\n",
      "21006 [Discriminator loss: 0.103877, acc.: 93.75%] [Generator loss: 9.707557]\n",
      "21007 [Discriminator loss: 0.056076, acc.: 98.44%] [Generator loss: 9.024099]\n",
      "21008 [Discriminator loss: 0.080063, acc.: 96.88%] [Generator loss: 9.428016]\n",
      "21009 [Discriminator loss: 0.033841, acc.: 98.44%] [Generator loss: 8.541078]\n",
      "21010 [Discriminator loss: 0.020527, acc.: 100.00%] [Generator loss: 9.345382]\n",
      "21011 [Discriminator loss: 0.027108, acc.: 98.44%] [Generator loss: 8.618145]\n",
      "21012 [Discriminator loss: 0.057130, acc.: 98.44%] [Generator loss: 9.634274]\n",
      "21013 [Discriminator loss: 0.013608, acc.: 100.00%] [Generator loss: 9.792862]\n",
      "21014 [Discriminator loss: 0.006833, acc.: 100.00%] [Generator loss: 9.395869]\n",
      "21015 [Discriminator loss: 0.049278, acc.: 98.44%] [Generator loss: 7.208667]\n",
      "21016 [Discriminator loss: 0.081226, acc.: 95.31%] [Generator loss: 9.618338]\n",
      "21017 [Discriminator loss: 0.215268, acc.: 92.19%] [Generator loss: 8.141996]\n",
      "21018 [Discriminator loss: 0.019439, acc.: 100.00%] [Generator loss: 6.259666]\n",
      "21019 [Discriminator loss: 0.051603, acc.: 96.88%] [Generator loss: 9.617696]\n",
      "21020 [Discriminator loss: 0.101885, acc.: 96.88%] [Generator loss: 8.763637]\n",
      "21021 [Discriminator loss: 0.077076, acc.: 95.31%] [Generator loss: 8.434805]\n",
      "21022 [Discriminator loss: 0.091650, acc.: 95.31%] [Generator loss: 7.506021]\n",
      "21023 [Discriminator loss: 0.110014, acc.: 96.88%] [Generator loss: 10.279007]\n",
      "21024 [Discriminator loss: 0.123687, acc.: 93.75%] [Generator loss: 9.626676]\n",
      "21025 [Discriminator loss: 0.088963, acc.: 96.88%] [Generator loss: 9.056982]\n",
      "21026 [Discriminator loss: 0.028801, acc.: 100.00%] [Generator loss: 8.604666]\n",
      "21027 [Discriminator loss: 0.034601, acc.: 98.44%] [Generator loss: 8.135726]\n",
      "21028 [Discriminator loss: 0.059668, acc.: 98.44%] [Generator loss: 8.489961]\n",
      "21029 [Discriminator loss: 0.017710, acc.: 100.00%] [Generator loss: 8.490150]\n",
      "21030 [Discriminator loss: 0.048585, acc.: 96.88%] [Generator loss: 9.435017]\n",
      "21031 [Discriminator loss: 0.083868, acc.: 98.44%] [Generator loss: 9.501266]\n",
      "21032 [Discriminator loss: 0.121144, acc.: 93.75%] [Generator loss: 8.898479]\n",
      "21033 [Discriminator loss: 0.032987, acc.: 100.00%] [Generator loss: 7.830545]\n",
      "21034 [Discriminator loss: 0.078146, acc.: 96.88%] [Generator loss: 8.024721]\n",
      "21035 [Discriminator loss: 0.113937, acc.: 95.31%] [Generator loss: 8.969200]\n",
      "21036 [Discriminator loss: 0.030157, acc.: 100.00%] [Generator loss: 8.934532]\n",
      "21037 [Discriminator loss: 0.019817, acc.: 100.00%] [Generator loss: 8.857292]\n",
      "21038 [Discriminator loss: 0.010088, acc.: 100.00%] [Generator loss: 8.147074]\n",
      "21039 [Discriminator loss: 0.022690, acc.: 100.00%] [Generator loss: 7.836584]\n",
      "21040 [Discriminator loss: 0.138205, acc.: 95.31%] [Generator loss: 9.498456]\n",
      "21041 [Discriminator loss: 0.048528, acc.: 98.44%] [Generator loss: 9.239082]\n",
      "21042 [Discriminator loss: 0.249102, acc.: 90.62%] [Generator loss: 8.431996]\n",
      "21043 [Discriminator loss: 0.005977, acc.: 100.00%] [Generator loss: 9.484516]\n",
      "21044 [Discriminator loss: 0.102907, acc.: 96.88%] [Generator loss: 9.776035]\n",
      "21045 [Discriminator loss: 0.038061, acc.: 98.44%] [Generator loss: 10.033965]\n",
      "21046 [Discriminator loss: 0.012076, acc.: 100.00%] [Generator loss: 9.232981]\n",
      "21047 [Discriminator loss: 0.041350, acc.: 96.88%] [Generator loss: 9.600872]\n",
      "21048 [Discriminator loss: 0.026050, acc.: 100.00%] [Generator loss: 9.752390]\n",
      "21049 [Discriminator loss: 0.111491, acc.: 98.44%] [Generator loss: 7.669970]\n",
      "21050 [Discriminator loss: 0.064657, acc.: 96.88%] [Generator loss: 7.096674]\n",
      "21051 [Discriminator loss: 0.027119, acc.: 100.00%] [Generator loss: 8.023677]\n",
      "21052 [Discriminator loss: 0.057569, acc.: 98.44%] [Generator loss: 9.592427]\n",
      "21053 [Discriminator loss: 0.080882, acc.: 95.31%] [Generator loss: 7.736181]\n",
      "21054 [Discriminator loss: 0.051688, acc.: 98.44%] [Generator loss: 9.184827]\n",
      "21055 [Discriminator loss: 0.071770, acc.: 96.88%] [Generator loss: 9.519428]\n",
      "21056 [Discriminator loss: 0.083264, acc.: 95.31%] [Generator loss: 7.564094]\n",
      "21057 [Discriminator loss: 0.231043, acc.: 92.19%] [Generator loss: 8.160610]\n",
      "21058 [Discriminator loss: 0.004538, acc.: 100.00%] [Generator loss: 7.962575]\n",
      "21059 [Discriminator loss: 0.154359, acc.: 96.88%] [Generator loss: 7.009387]\n",
      "21060 [Discriminator loss: 0.089357, acc.: 96.88%] [Generator loss: 8.322710]\n",
      "21061 [Discriminator loss: 0.048540, acc.: 98.44%] [Generator loss: 8.067062]\n",
      "21062 [Discriminator loss: 0.051365, acc.: 96.88%] [Generator loss: 8.004929]\n",
      "21063 [Discriminator loss: 0.028604, acc.: 100.00%] [Generator loss: 8.212545]\n",
      "21064 [Discriminator loss: 0.016306, acc.: 100.00%] [Generator loss: 9.687110]\n",
      "21065 [Discriminator loss: 0.072687, acc.: 96.88%] [Generator loss: 9.093022]\n",
      "21066 [Discriminator loss: 0.101925, acc.: 96.88%] [Generator loss: 8.550981]\n",
      "21067 [Discriminator loss: 0.045709, acc.: 98.44%] [Generator loss: 9.310441]\n",
      "21068 [Discriminator loss: 0.071120, acc.: 96.88%] [Generator loss: 8.241101]\n",
      "21069 [Discriminator loss: 0.030964, acc.: 98.44%] [Generator loss: 8.752756]\n",
      "21070 [Discriminator loss: 0.107308, acc.: 96.88%] [Generator loss: 8.661213]\n",
      "21071 [Discriminator loss: 0.061956, acc.: 96.88%] [Generator loss: 9.309404]\n",
      "21072 [Discriminator loss: 0.025916, acc.: 100.00%] [Generator loss: 9.594351]\n",
      "21073 [Discriminator loss: 0.028375, acc.: 98.44%] [Generator loss: 8.811079]\n",
      "21074 [Discriminator loss: 0.049831, acc.: 96.88%] [Generator loss: 7.577505]\n",
      "21075 [Discriminator loss: 0.029047, acc.: 98.44%] [Generator loss: 8.896158]\n",
      "21076 [Discriminator loss: 0.109517, acc.: 95.31%] [Generator loss: 9.572901]\n",
      "21077 [Discriminator loss: 0.030510, acc.: 98.44%] [Generator loss: 8.595652]\n",
      "21078 [Discriminator loss: 0.066178, acc.: 96.88%] [Generator loss: 9.157676]\n",
      "21079 [Discriminator loss: 0.019161, acc.: 100.00%] [Generator loss: 9.216990]\n",
      "21080 [Discriminator loss: 0.058310, acc.: 96.88%] [Generator loss: 8.684956]\n",
      "21081 [Discriminator loss: 0.051794, acc.: 98.44%] [Generator loss: 8.519663]\n",
      "21082 [Discriminator loss: 0.066279, acc.: 96.88%] [Generator loss: 8.578794]\n",
      "21083 [Discriminator loss: 0.041308, acc.: 98.44%] [Generator loss: 9.033115]\n",
      "21084 [Discriminator loss: 0.066932, acc.: 95.31%] [Generator loss: 8.534577]\n",
      "21085 [Discriminator loss: 0.017308, acc.: 98.44%] [Generator loss: 9.459780]\n",
      "21086 [Discriminator loss: 0.082438, acc.: 98.44%] [Generator loss: 9.621058]\n",
      "21087 [Discriminator loss: 0.059518, acc.: 98.44%] [Generator loss: 8.571880]\n",
      "21088 [Discriminator loss: 0.088788, acc.: 98.44%] [Generator loss: 8.120275]\n",
      "21089 [Discriminator loss: 0.164607, acc.: 96.88%] [Generator loss: 7.546290]\n",
      "21090 [Discriminator loss: 0.045758, acc.: 98.44%] [Generator loss: 9.730854]\n",
      "21091 [Discriminator loss: 0.099561, acc.: 98.44%] [Generator loss: 8.532954]\n",
      "21092 [Discriminator loss: 0.011237, acc.: 100.00%] [Generator loss: 8.546888]\n",
      "21093 [Discriminator loss: 0.038192, acc.: 98.44%] [Generator loss: 9.283480]\n",
      "21094 [Discriminator loss: 0.053115, acc.: 98.44%] [Generator loss: 9.005558]\n",
      "21095 [Discriminator loss: 0.170592, acc.: 95.31%] [Generator loss: 7.952990]\n",
      "21096 [Discriminator loss: 0.050980, acc.: 98.44%] [Generator loss: 7.816348]\n",
      "21097 [Discriminator loss: 0.029182, acc.: 100.00%] [Generator loss: 9.375521]\n",
      "21098 [Discriminator loss: 0.023759, acc.: 98.44%] [Generator loss: 8.613354]\n",
      "21099 [Discriminator loss: 0.025860, acc.: 98.44%] [Generator loss: 7.913970]\n",
      "21100 [Discriminator loss: 0.031200, acc.: 98.44%] [Generator loss: 7.596954]\n",
      "21101 [Discriminator loss: 0.084568, acc.: 98.44%] [Generator loss: 8.868574]\n",
      "21102 [Discriminator loss: 0.023027, acc.: 100.00%] [Generator loss: 7.960179]\n",
      "21103 [Discriminator loss: 0.177555, acc.: 93.75%] [Generator loss: 10.157391]\n",
      "21104 [Discriminator loss: 0.143565, acc.: 95.31%] [Generator loss: 10.291584]\n",
      "21105 [Discriminator loss: 0.051854, acc.: 98.44%] [Generator loss: 11.571314]\n",
      "21106 [Discriminator loss: 0.122553, acc.: 96.88%] [Generator loss: 9.104686]\n",
      "21107 [Discriminator loss: 0.037068, acc.: 98.44%] [Generator loss: 8.255797]\n",
      "21108 [Discriminator loss: 0.052590, acc.: 98.44%] [Generator loss: 9.261026]\n",
      "21109 [Discriminator loss: 0.068664, acc.: 96.88%] [Generator loss: 8.040378]\n",
      "21110 [Discriminator loss: 0.128446, acc.: 96.88%] [Generator loss: 10.234603]\n",
      "21111 [Discriminator loss: 0.008233, acc.: 100.00%] [Generator loss: 11.048201]\n",
      "21112 [Discriminator loss: 0.013864, acc.: 100.00%] [Generator loss: 11.060722]\n",
      "21113 [Discriminator loss: 0.075704, acc.: 96.88%] [Generator loss: 8.181622]\n",
      "21114 [Discriminator loss: 0.021936, acc.: 100.00%] [Generator loss: 8.017641]\n",
      "21115 [Discriminator loss: 0.055966, acc.: 98.44%] [Generator loss: 8.642014]\n",
      "21116 [Discriminator loss: 0.128450, acc.: 92.19%] [Generator loss: 9.354784]\n",
      "21117 [Discriminator loss: 0.025162, acc.: 100.00%] [Generator loss: 9.422316]\n",
      "21118 [Discriminator loss: 0.091680, acc.: 95.31%] [Generator loss: 7.763802]\n",
      "21119 [Discriminator loss: 0.045070, acc.: 98.44%] [Generator loss: 8.704769]\n",
      "21120 [Discriminator loss: 0.010322, acc.: 100.00%] [Generator loss: 8.392208]\n",
      "21121 [Discriminator loss: 0.008787, acc.: 100.00%] [Generator loss: 8.287889]\n",
      "21122 [Discriminator loss: 0.055904, acc.: 98.44%] [Generator loss: 8.370651]\n",
      "21123 [Discriminator loss: 0.051573, acc.: 96.88%] [Generator loss: 9.259458]\n",
      "21124 [Discriminator loss: 0.063529, acc.: 98.44%] [Generator loss: 9.336246]\n",
      "21125 [Discriminator loss: 0.048600, acc.: 98.44%] [Generator loss: 8.714660]\n",
      "21126 [Discriminator loss: 0.019943, acc.: 100.00%] [Generator loss: 8.546817]\n",
      "21127 [Discriminator loss: 0.065741, acc.: 96.88%] [Generator loss: 9.775037]\n",
      "21128 [Discriminator loss: 0.040390, acc.: 98.44%] [Generator loss: 8.070559]\n",
      "21129 [Discriminator loss: 0.095230, acc.: 95.31%] [Generator loss: 9.978470]\n",
      "21130 [Discriminator loss: 0.106993, acc.: 95.31%] [Generator loss: 7.827073]\n",
      "21131 [Discriminator loss: 0.038513, acc.: 98.44%] [Generator loss: 8.971670]\n",
      "21132 [Discriminator loss: 0.021982, acc.: 100.00%] [Generator loss: 7.776717]\n",
      "21133 [Discriminator loss: 0.056952, acc.: 96.88%] [Generator loss: 8.033493]\n",
      "21134 [Discriminator loss: 0.023222, acc.: 100.00%] [Generator loss: 7.865865]\n",
      "21135 [Discriminator loss: 0.078035, acc.: 95.31%] [Generator loss: 8.776386]\n",
      "21136 [Discriminator loss: 0.015792, acc.: 100.00%] [Generator loss: 8.309217]\n",
      "21137 [Discriminator loss: 0.027471, acc.: 98.44%] [Generator loss: 9.159945]\n",
      "21138 [Discriminator loss: 0.013513, acc.: 100.00%] [Generator loss: 8.567241]\n",
      "21139 [Discriminator loss: 0.031385, acc.: 98.44%] [Generator loss: 9.267759]\n",
      "21140 [Discriminator loss: 0.019420, acc.: 100.00%] [Generator loss: 8.929070]\n",
      "21141 [Discriminator loss: 0.085148, acc.: 96.88%] [Generator loss: 6.961299]\n",
      "21142 [Discriminator loss: 0.042047, acc.: 98.44%] [Generator loss: 8.624304]\n",
      "21143 [Discriminator loss: 0.043893, acc.: 98.44%] [Generator loss: 7.042181]\n",
      "21144 [Discriminator loss: 0.024166, acc.: 100.00%] [Generator loss: 7.890429]\n",
      "21145 [Discriminator loss: 0.052767, acc.: 96.88%] [Generator loss: 7.756754]\n",
      "21146 [Discriminator loss: 0.043507, acc.: 98.44%] [Generator loss: 8.234098]\n",
      "21147 [Discriminator loss: 0.062703, acc.: 98.44%] [Generator loss: 9.771866]\n",
      "21148 [Discriminator loss: 0.055968, acc.: 98.44%] [Generator loss: 10.371109]\n",
      "21149 [Discriminator loss: 0.017859, acc.: 98.44%] [Generator loss: 8.336708]\n",
      "21150 [Discriminator loss: 0.059117, acc.: 96.88%] [Generator loss: 9.435158]\n",
      "21151 [Discriminator loss: 0.036796, acc.: 98.44%] [Generator loss: 9.874516]\n",
      "21152 [Discriminator loss: 0.026433, acc.: 100.00%] [Generator loss: 10.367643]\n",
      "21153 [Discriminator loss: 0.039615, acc.: 98.44%] [Generator loss: 9.107999]\n",
      "21154 [Discriminator loss: 0.008255, acc.: 100.00%] [Generator loss: 10.453280]\n",
      "21155 [Discriminator loss: 0.100776, acc.: 98.44%] [Generator loss: 8.599429]\n",
      "21156 [Discriminator loss: 0.102256, acc.: 96.88%] [Generator loss: 9.628769]\n",
      "21157 [Discriminator loss: 0.034126, acc.: 98.44%] [Generator loss: 9.475208]\n",
      "21158 [Discriminator loss: 0.141427, acc.: 90.62%] [Generator loss: 9.638876]\n",
      "21159 [Discriminator loss: 0.030387, acc.: 98.44%] [Generator loss: 10.526821]\n",
      "21160 [Discriminator loss: 0.011161, acc.: 100.00%] [Generator loss: 10.434218]\n",
      "21161 [Discriminator loss: 0.319741, acc.: 92.19%] [Generator loss: 6.661266]\n",
      "21162 [Discriminator loss: 0.139700, acc.: 93.75%] [Generator loss: 8.827274]\n",
      "21163 [Discriminator loss: 0.015873, acc.: 100.00%] [Generator loss: 8.923336]\n",
      "21164 [Discriminator loss: 0.200675, acc.: 95.31%] [Generator loss: 9.916016]\n",
      "21165 [Discriminator loss: 0.137428, acc.: 93.75%] [Generator loss: 8.170769]\n",
      "21166 [Discriminator loss: 0.014072, acc.: 100.00%] [Generator loss: 8.406787]\n",
      "21167 [Discriminator loss: 0.040855, acc.: 98.44%] [Generator loss: 8.867818]\n",
      "21168 [Discriminator loss: 0.088533, acc.: 96.88%] [Generator loss: 8.740719]\n",
      "21169 [Discriminator loss: 0.032413, acc.: 98.44%] [Generator loss: 8.021944]\n",
      "21170 [Discriminator loss: 0.050030, acc.: 96.88%] [Generator loss: 9.941834]\n",
      "21171 [Discriminator loss: 0.024906, acc.: 98.44%] [Generator loss: 7.489056]\n",
      "21172 [Discriminator loss: 0.048325, acc.: 98.44%] [Generator loss: 7.888850]\n",
      "21173 [Discriminator loss: 0.045498, acc.: 98.44%] [Generator loss: 7.851607]\n",
      "21174 [Discriminator loss: 0.058905, acc.: 96.88%] [Generator loss: 8.725161]\n",
      "21175 [Discriminator loss: 0.042121, acc.: 98.44%] [Generator loss: 7.315917]\n",
      "21176 [Discriminator loss: 0.063911, acc.: 95.31%] [Generator loss: 10.506969]\n",
      "21177 [Discriminator loss: 0.090357, acc.: 95.31%] [Generator loss: 9.194771]\n",
      "21178 [Discriminator loss: 0.018356, acc.: 100.00%] [Generator loss: 8.476894]\n",
      "21179 [Discriminator loss: 0.268664, acc.: 87.50%] [Generator loss: 8.377866]\n",
      "21180 [Discriminator loss: 0.007100, acc.: 100.00%] [Generator loss: 10.181648]\n",
      "21181 [Discriminator loss: 0.146323, acc.: 95.31%] [Generator loss: 10.058315]\n",
      "21182 [Discriminator loss: 0.180060, acc.: 90.62%] [Generator loss: 8.850317]\n",
      "21183 [Discriminator loss: 0.031540, acc.: 100.00%] [Generator loss: 11.061358]\n",
      "21184 [Discriminator loss: 0.071867, acc.: 96.88%] [Generator loss: 8.921001]\n",
      "21185 [Discriminator loss: 0.172820, acc.: 93.75%] [Generator loss: 10.504839]\n",
      "21186 [Discriminator loss: 0.072369, acc.: 95.31%] [Generator loss: 8.340985]\n",
      "21187 [Discriminator loss: 0.010632, acc.: 100.00%] [Generator loss: 7.657742]\n",
      "21188 [Discriminator loss: 0.029955, acc.: 98.44%] [Generator loss: 7.554108]\n",
      "21189 [Discriminator loss: 0.054653, acc.: 96.88%] [Generator loss: 9.067274]\n",
      "21190 [Discriminator loss: 0.050965, acc.: 98.44%] [Generator loss: 9.877505]\n",
      "21191 [Discriminator loss: 0.353681, acc.: 87.50%] [Generator loss: 10.925938]\n",
      "21192 [Discriminator loss: 0.160496, acc.: 96.88%] [Generator loss: 10.081291]\n",
      "21193 [Discriminator loss: 0.141053, acc.: 96.88%] [Generator loss: 8.778471]\n",
      "21194 [Discriminator loss: 0.057347, acc.: 98.44%] [Generator loss: 7.808807]\n",
      "21195 [Discriminator loss: 0.064609, acc.: 96.88%] [Generator loss: 8.125645]\n",
      "21196 [Discriminator loss: 0.015513, acc.: 100.00%] [Generator loss: 10.363450]\n",
      "21197 [Discriminator loss: 0.014044, acc.: 100.00%] [Generator loss: 9.098796]\n",
      "21198 [Discriminator loss: 0.017715, acc.: 100.00%] [Generator loss: 7.094669]\n",
      "21199 [Discriminator loss: 0.092276, acc.: 96.88%] [Generator loss: 7.278601]\n",
      "21200 [Discriminator loss: 0.203345, acc.: 92.19%] [Generator loss: 9.058764]\n",
      "21201 [Discriminator loss: 0.016147, acc.: 100.00%] [Generator loss: 8.780319]\n",
      "21202 [Discriminator loss: 0.009520, acc.: 100.00%] [Generator loss: 8.812832]\n",
      "21203 [Discriminator loss: 0.071870, acc.: 98.44%] [Generator loss: 7.330786]\n",
      "21204 [Discriminator loss: 0.014368, acc.: 100.00%] [Generator loss: 8.237997]\n",
      "21205 [Discriminator loss: 0.179935, acc.: 92.19%] [Generator loss: 7.908587]\n",
      "21206 [Discriminator loss: 0.032239, acc.: 100.00%] [Generator loss: 7.458949]\n",
      "21207 [Discriminator loss: 0.026480, acc.: 100.00%] [Generator loss: 8.767420]\n",
      "21208 [Discriminator loss: 0.107352, acc.: 96.88%] [Generator loss: 7.290065]\n",
      "21209 [Discriminator loss: 0.053153, acc.: 98.44%] [Generator loss: 8.410775]\n",
      "21210 [Discriminator loss: 0.014273, acc.: 100.00%] [Generator loss: 8.738016]\n",
      "21211 [Discriminator loss: 0.164928, acc.: 93.75%] [Generator loss: 6.965956]\n",
      "21212 [Discriminator loss: 0.068524, acc.: 95.31%] [Generator loss: 9.241352]\n",
      "21213 [Discriminator loss: 0.014019, acc.: 100.00%] [Generator loss: 8.734301]\n",
      "21214 [Discriminator loss: 0.076701, acc.: 96.88%] [Generator loss: 7.855094]\n",
      "21215 [Discriminator loss: 0.097582, acc.: 95.31%] [Generator loss: 7.368100]\n",
      "21216 [Discriminator loss: 0.048562, acc.: 96.88%] [Generator loss: 8.028112]\n",
      "21217 [Discriminator loss: 0.066402, acc.: 96.88%] [Generator loss: 7.682876]\n",
      "21218 [Discriminator loss: 0.050727, acc.: 96.88%] [Generator loss: 8.102741]\n",
      "21219 [Discriminator loss: 0.038195, acc.: 100.00%] [Generator loss: 6.905571]\n",
      "21220 [Discriminator loss: 0.096255, acc.: 96.88%] [Generator loss: 6.692614]\n",
      "21221 [Discriminator loss: 0.085291, acc.: 96.88%] [Generator loss: 8.927864]\n",
      "21222 [Discriminator loss: 0.054241, acc.: 95.31%] [Generator loss: 9.647532]\n",
      "21223 [Discriminator loss: 0.004395, acc.: 100.00%] [Generator loss: 9.318045]\n",
      "21224 [Discriminator loss: 0.039455, acc.: 98.44%] [Generator loss: 8.580799]\n",
      "21225 [Discriminator loss: 0.105778, acc.: 95.31%] [Generator loss: 7.928929]\n",
      "21226 [Discriminator loss: 0.031141, acc.: 98.44%] [Generator loss: 7.612194]\n",
      "21227 [Discriminator loss: 0.088080, acc.: 98.44%] [Generator loss: 7.683297]\n",
      "21228 [Discriminator loss: 0.010236, acc.: 100.00%] [Generator loss: 9.482754]\n",
      "21229 [Discriminator loss: 0.084124, acc.: 96.88%] [Generator loss: 8.316511]\n",
      "21230 [Discriminator loss: 0.121015, acc.: 95.31%] [Generator loss: 10.240646]\n",
      "21231 [Discriminator loss: 0.017436, acc.: 100.00%] [Generator loss: 8.127988]\n",
      "21232 [Discriminator loss: 0.066018, acc.: 96.88%] [Generator loss: 7.629858]\n",
      "21233 [Discriminator loss: 0.059152, acc.: 98.44%] [Generator loss: 9.590031]\n",
      "21234 [Discriminator loss: 0.037759, acc.: 98.44%] [Generator loss: 9.882696]\n",
      "21235 [Discriminator loss: 0.049052, acc.: 98.44%] [Generator loss: 8.412439]\n",
      "21236 [Discriminator loss: 0.057388, acc.: 96.88%] [Generator loss: 10.322662]\n",
      "21237 [Discriminator loss: 0.016373, acc.: 100.00%] [Generator loss: 9.588491]\n",
      "21238 [Discriminator loss: 0.033036, acc.: 98.44%] [Generator loss: 8.936071]\n",
      "21239 [Discriminator loss: 0.093164, acc.: 96.88%] [Generator loss: 9.294607]\n",
      "21240 [Discriminator loss: 0.051330, acc.: 98.44%] [Generator loss: 8.172650]\n",
      "21241 [Discriminator loss: 0.025266, acc.: 100.00%] [Generator loss: 9.234555]\n",
      "21242 [Discriminator loss: 0.039334, acc.: 98.44%] [Generator loss: 7.787598]\n",
      "21243 [Discriminator loss: 0.111846, acc.: 96.88%] [Generator loss: 8.504423]\n",
      "21244 [Discriminator loss: 0.014735, acc.: 100.00%] [Generator loss: 9.068253]\n",
      "21245 [Discriminator loss: 0.006076, acc.: 100.00%] [Generator loss: 8.287594]\n",
      "21246 [Discriminator loss: 0.023027, acc.: 100.00%] [Generator loss: 7.253622]\n",
      "21247 [Discriminator loss: 0.150663, acc.: 93.75%] [Generator loss: 6.370830]\n",
      "21248 [Discriminator loss: 0.087560, acc.: 98.44%] [Generator loss: 7.629367]\n",
      "21249 [Discriminator loss: 0.033030, acc.: 96.88%] [Generator loss: 8.902729]\n",
      "21250 [Discriminator loss: 0.120511, acc.: 93.75%] [Generator loss: 8.734932]\n",
      "21251 [Discriminator loss: 0.048934, acc.: 98.44%] [Generator loss: 9.153507]\n",
      "21252 [Discriminator loss: 0.024374, acc.: 100.00%] [Generator loss: 8.587715]\n",
      "21253 [Discriminator loss: 0.188483, acc.: 95.31%] [Generator loss: 8.509445]\n",
      "21254 [Discriminator loss: 0.046903, acc.: 100.00%] [Generator loss: 9.659369]\n",
      "21255 [Discriminator loss: 0.099818, acc.: 96.88%] [Generator loss: 8.224220]\n",
      "21256 [Discriminator loss: 0.020328, acc.: 100.00%] [Generator loss: 8.629490]\n",
      "21257 [Discriminator loss: 0.014714, acc.: 100.00%] [Generator loss: 6.285584]\n",
      "21258 [Discriminator loss: 0.029645, acc.: 100.00%] [Generator loss: 7.748510]\n",
      "21259 [Discriminator loss: 0.023526, acc.: 100.00%] [Generator loss: 8.970531]\n",
      "21260 [Discriminator loss: 0.057324, acc.: 96.88%] [Generator loss: 8.426559]\n",
      "21261 [Discriminator loss: 0.006663, acc.: 100.00%] [Generator loss: 7.047658]\n",
      "21262 [Discriminator loss: 0.091707, acc.: 95.31%] [Generator loss: 8.644255]\n",
      "21263 [Discriminator loss: 0.066473, acc.: 96.88%] [Generator loss: 9.411364]\n",
      "21264 [Discriminator loss: 0.019259, acc.: 98.44%] [Generator loss: 10.388495]\n",
      "21265 [Discriminator loss: 0.016607, acc.: 100.00%] [Generator loss: 7.053604]\n",
      "21266 [Discriminator loss: 0.045243, acc.: 100.00%] [Generator loss: 9.544075]\n",
      "21267 [Discriminator loss: 0.022129, acc.: 100.00%] [Generator loss: 9.434586]\n",
      "21268 [Discriminator loss: 0.022247, acc.: 100.00%] [Generator loss: 7.870692]\n",
      "21269 [Discriminator loss: 0.032833, acc.: 98.44%] [Generator loss: 8.805411]\n",
      "21270 [Discriminator loss: 0.254114, acc.: 92.19%] [Generator loss: 9.025881]\n",
      "21271 [Discriminator loss: 0.219186, acc.: 95.31%] [Generator loss: 7.599432]\n",
      "21272 [Discriminator loss: 0.017079, acc.: 100.00%] [Generator loss: 9.307671]\n",
      "21273 [Discriminator loss: 0.052879, acc.: 98.44%] [Generator loss: 8.557264]\n",
      "21274 [Discriminator loss: 0.053158, acc.: 98.44%] [Generator loss: 8.596981]\n",
      "21275 [Discriminator loss: 0.074510, acc.: 96.88%] [Generator loss: 8.207625]\n",
      "21276 [Discriminator loss: 0.068038, acc.: 96.88%] [Generator loss: 8.224055]\n",
      "21277 [Discriminator loss: 0.044885, acc.: 98.44%] [Generator loss: 8.092673]\n",
      "21278 [Discriminator loss: 0.127679, acc.: 96.88%] [Generator loss: 8.664541]\n",
      "21279 [Discriminator loss: 0.042287, acc.: 98.44%] [Generator loss: 9.185030]\n",
      "21280 [Discriminator loss: 0.016096, acc.: 100.00%] [Generator loss: 9.866024]\n",
      "21281 [Discriminator loss: 0.036217, acc.: 98.44%] [Generator loss: 9.050274]\n",
      "21282 [Discriminator loss: 0.055167, acc.: 98.44%] [Generator loss: 8.413560]\n",
      "21283 [Discriminator loss: 0.055212, acc.: 98.44%] [Generator loss: 8.579493]\n",
      "21284 [Discriminator loss: 0.065835, acc.: 96.88%] [Generator loss: 7.293999]\n",
      "21285 [Discriminator loss: 0.088738, acc.: 96.88%] [Generator loss: 8.338194]\n",
      "21286 [Discriminator loss: 0.022509, acc.: 100.00%] [Generator loss: 8.881721]\n",
      "21287 [Discriminator loss: 0.124391, acc.: 93.75%] [Generator loss: 8.741295]\n",
      "21288 [Discriminator loss: 0.083686, acc.: 96.88%] [Generator loss: 6.978826]\n",
      "21289 [Discriminator loss: 0.033396, acc.: 100.00%] [Generator loss: 7.512954]\n",
      "21290 [Discriminator loss: 0.057769, acc.: 98.44%] [Generator loss: 8.529743]\n",
      "21291 [Discriminator loss: 0.029046, acc.: 98.44%] [Generator loss: 8.507592]\n",
      "21292 [Discriminator loss: 0.121559, acc.: 92.19%] [Generator loss: 8.511241]\n",
      "21293 [Discriminator loss: 0.268264, acc.: 95.31%] [Generator loss: 8.068148]\n",
      "21294 [Discriminator loss: 0.035380, acc.: 100.00%] [Generator loss: 9.358353]\n",
      "21295 [Discriminator loss: 0.010223, acc.: 100.00%] [Generator loss: 9.445087]\n",
      "21296 [Discriminator loss: 0.091647, acc.: 96.88%] [Generator loss: 8.234203]\n",
      "21297 [Discriminator loss: 0.064576, acc.: 98.44%] [Generator loss: 8.132473]\n",
      "21298 [Discriminator loss: 0.018989, acc.: 100.00%] [Generator loss: 8.488993]\n",
      "21299 [Discriminator loss: 0.023445, acc.: 98.44%] [Generator loss: 9.412267]\n",
      "21300 [Discriminator loss: 0.022896, acc.: 100.00%] [Generator loss: 8.485566]\n",
      "21301 [Discriminator loss: 0.013151, acc.: 100.00%] [Generator loss: 9.686566]\n",
      "21302 [Discriminator loss: 0.010684, acc.: 100.00%] [Generator loss: 9.296501]\n",
      "21303 [Discriminator loss: 0.106496, acc.: 95.31%] [Generator loss: 8.111494]\n",
      "21304 [Discriminator loss: 0.028505, acc.: 100.00%] [Generator loss: 8.659439]\n",
      "21305 [Discriminator loss: 0.080424, acc.: 96.88%] [Generator loss: 7.087546]\n",
      "21306 [Discriminator loss: 0.090006, acc.: 96.88%] [Generator loss: 8.896951]\n",
      "21307 [Discriminator loss: 0.105221, acc.: 96.88%] [Generator loss: 8.388163]\n",
      "21308 [Discriminator loss: 0.121848, acc.: 96.88%] [Generator loss: 8.742239]\n",
      "21309 [Discriminator loss: 0.019684, acc.: 100.00%] [Generator loss: 8.910971]\n",
      "21310 [Discriminator loss: 0.048679, acc.: 98.44%] [Generator loss: 7.480638]\n",
      "21311 [Discriminator loss: 0.013383, acc.: 100.00%] [Generator loss: 9.183722]\n",
      "21312 [Discriminator loss: 0.029577, acc.: 98.44%] [Generator loss: 8.512581]\n",
      "21313 [Discriminator loss: 0.049263, acc.: 96.88%] [Generator loss: 9.495014]\n",
      "21314 [Discriminator loss: 0.029150, acc.: 98.44%] [Generator loss: 9.368769]\n",
      "21315 [Discriminator loss: 0.045137, acc.: 98.44%] [Generator loss: 8.444920]\n",
      "21316 [Discriminator loss: 0.037272, acc.: 98.44%] [Generator loss: 7.667942]\n",
      "21317 [Discriminator loss: 0.052750, acc.: 98.44%] [Generator loss: 8.967045]\n",
      "21318 [Discriminator loss: 0.025599, acc.: 100.00%] [Generator loss: 9.467649]\n",
      "21319 [Discriminator loss: 0.012549, acc.: 100.00%] [Generator loss: 9.738237]\n",
      "21320 [Discriminator loss: 0.012325, acc.: 100.00%] [Generator loss: 9.391556]\n",
      "21321 [Discriminator loss: 0.056058, acc.: 98.44%] [Generator loss: 7.359693]\n",
      "21322 [Discriminator loss: 0.049706, acc.: 98.44%] [Generator loss: 7.878980]\n",
      "21323 [Discriminator loss: 0.014001, acc.: 100.00%] [Generator loss: 8.617026]\n",
      "21324 [Discriminator loss: 0.060118, acc.: 96.88%] [Generator loss: 9.847063]\n",
      "21325 [Discriminator loss: 0.031029, acc.: 98.44%] [Generator loss: 9.671213]\n",
      "21326 [Discriminator loss: 0.026736, acc.: 100.00%] [Generator loss: 8.425920]\n",
      "21327 [Discriminator loss: 0.066079, acc.: 96.88%] [Generator loss: 10.055336]\n",
      "21328 [Discriminator loss: 0.067246, acc.: 96.88%] [Generator loss: 8.367987]\n",
      "21329 [Discriminator loss: 0.039801, acc.: 98.44%] [Generator loss: 9.005806]\n",
      "21330 [Discriminator loss: 0.102404, acc.: 95.31%] [Generator loss: 8.834386]\n",
      "21331 [Discriminator loss: 0.015435, acc.: 98.44%] [Generator loss: 9.684286]\n",
      "21332 [Discriminator loss: 0.014704, acc.: 100.00%] [Generator loss: 8.608714]\n",
      "21333 [Discriminator loss: 0.020012, acc.: 100.00%] [Generator loss: 8.547318]\n",
      "21334 [Discriminator loss: 0.145940, acc.: 96.88%] [Generator loss: 7.278414]\n",
      "21335 [Discriminator loss: 0.015201, acc.: 100.00%] [Generator loss: 7.191158]\n",
      "21336 [Discriminator loss: 0.149896, acc.: 93.75%] [Generator loss: 8.113686]\n",
      "21337 [Discriminator loss: 0.053853, acc.: 96.88%] [Generator loss: 9.052195]\n",
      "21338 [Discriminator loss: 0.096299, acc.: 96.88%] [Generator loss: 8.804811]\n",
      "21339 [Discriminator loss: 0.021411, acc.: 98.44%] [Generator loss: 10.016733]\n",
      "21340 [Discriminator loss: 0.094300, acc.: 95.31%] [Generator loss: 7.535022]\n",
      "21341 [Discriminator loss: 0.054967, acc.: 98.44%] [Generator loss: 8.636430]\n",
      "21342 [Discriminator loss: 0.062426, acc.: 96.88%] [Generator loss: 8.436643]\n",
      "21343 [Discriminator loss: 0.038674, acc.: 98.44%] [Generator loss: 9.472721]\n",
      "21344 [Discriminator loss: 0.009139, acc.: 100.00%] [Generator loss: 9.615989]\n",
      "21345 [Discriminator loss: 0.008615, acc.: 100.00%] [Generator loss: 10.510493]\n",
      "21346 [Discriminator loss: 0.122919, acc.: 96.88%] [Generator loss: 8.417120]\n",
      "21347 [Discriminator loss: 0.041023, acc.: 98.44%] [Generator loss: 8.618932]\n",
      "21348 [Discriminator loss: 0.061220, acc.: 98.44%] [Generator loss: 9.493220]\n",
      "21349 [Discriminator loss: 0.031720, acc.: 98.44%] [Generator loss: 9.689654]\n",
      "21350 [Discriminator loss: 0.010237, acc.: 100.00%] [Generator loss: 9.437588]\n",
      "21351 [Discriminator loss: 0.020242, acc.: 100.00%] [Generator loss: 9.178781]\n",
      "21352 [Discriminator loss: 0.054663, acc.: 98.44%] [Generator loss: 8.429850]\n",
      "21353 [Discriminator loss: 0.063589, acc.: 96.88%] [Generator loss: 9.271142]\n",
      "21354 [Discriminator loss: 0.016504, acc.: 100.00%] [Generator loss: 8.408384]\n",
      "21355 [Discriminator loss: 0.008150, acc.: 100.00%] [Generator loss: 9.627100]\n",
      "21356 [Discriminator loss: 0.101297, acc.: 98.44%] [Generator loss: 6.953946]\n",
      "21357 [Discriminator loss: 0.045913, acc.: 98.44%] [Generator loss: 8.307529]\n",
      "21358 [Discriminator loss: 0.023137, acc.: 100.00%] [Generator loss: 8.133204]\n",
      "21359 [Discriminator loss: 0.063103, acc.: 98.44%] [Generator loss: 9.163652]\n",
      "21360 [Discriminator loss: 0.019194, acc.: 100.00%] [Generator loss: 9.869041]\n",
      "21361 [Discriminator loss: 0.027965, acc.: 100.00%] [Generator loss: 9.411471]\n",
      "21362 [Discriminator loss: 0.022356, acc.: 100.00%] [Generator loss: 7.561318]\n",
      "21363 [Discriminator loss: 0.259143, acc.: 93.75%] [Generator loss: 8.023578]\n",
      "21364 [Discriminator loss: 0.015338, acc.: 100.00%] [Generator loss: 8.648376]\n",
      "21365 [Discriminator loss: 0.034822, acc.: 98.44%] [Generator loss: 9.498260]\n",
      "21366 [Discriminator loss: 0.062687, acc.: 98.44%] [Generator loss: 10.185625]\n",
      "21367 [Discriminator loss: 0.102876, acc.: 95.31%] [Generator loss: 10.038319]\n",
      "21368 [Discriminator loss: 0.054613, acc.: 98.44%] [Generator loss: 10.198812]\n",
      "21369 [Discriminator loss: 0.056796, acc.: 96.88%] [Generator loss: 8.550159]\n",
      "21370 [Discriminator loss: 0.046889, acc.: 96.88%] [Generator loss: 7.684958]\n",
      "21371 [Discriminator loss: 0.028093, acc.: 100.00%] [Generator loss: 8.710896]\n",
      "21372 [Discriminator loss: 0.065360, acc.: 95.31%] [Generator loss: 9.759592]\n",
      "21373 [Discriminator loss: 0.212984, acc.: 92.19%] [Generator loss: 6.497071]\n",
      "21374 [Discriminator loss: 0.034467, acc.: 100.00%] [Generator loss: 7.064377]\n",
      "21375 [Discriminator loss: 0.030750, acc.: 98.44%] [Generator loss: 8.185291]\n",
      "21376 [Discriminator loss: 0.062856, acc.: 98.44%] [Generator loss: 8.884271]\n",
      "21377 [Discriminator loss: 0.055787, acc.: 96.88%] [Generator loss: 9.217701]\n",
      "21378 [Discriminator loss: 0.038358, acc.: 98.44%] [Generator loss: 10.419397]\n",
      "21379 [Discriminator loss: 0.016724, acc.: 100.00%] [Generator loss: 10.236726]\n",
      "21380 [Discriminator loss: 0.087793, acc.: 96.88%] [Generator loss: 9.035334]\n",
      "21381 [Discriminator loss: 0.215024, acc.: 95.31%] [Generator loss: 9.647039]\n",
      "21382 [Discriminator loss: 0.062627, acc.: 96.88%] [Generator loss: 9.260746]\n",
      "21383 [Discriminator loss: 0.031450, acc.: 98.44%] [Generator loss: 9.881428]\n",
      "21384 [Discriminator loss: 0.076810, acc.: 96.88%] [Generator loss: 8.444908]\n",
      "21385 [Discriminator loss: 0.031592, acc.: 98.44%] [Generator loss: 8.730570]\n",
      "21386 [Discriminator loss: 0.162426, acc.: 96.88%] [Generator loss: 8.996144]\n",
      "21387 [Discriminator loss: 0.035529, acc.: 100.00%] [Generator loss: 11.290643]\n",
      "21388 [Discriminator loss: 0.139929, acc.: 95.31%] [Generator loss: 6.704587]\n",
      "21389 [Discriminator loss: 0.119683, acc.: 98.44%] [Generator loss: 7.981075]\n",
      "21390 [Discriminator loss: 0.060129, acc.: 98.44%] [Generator loss: 8.056679]\n",
      "21391 [Discriminator loss: 0.055978, acc.: 98.44%] [Generator loss: 10.448733]\n",
      "21392 [Discriminator loss: 0.009111, acc.: 100.00%] [Generator loss: 10.303976]\n",
      "21393 [Discriminator loss: 0.124360, acc.: 95.31%] [Generator loss: 7.834567]\n",
      "21394 [Discriminator loss: 0.062517, acc.: 100.00%] [Generator loss: 7.583305]\n",
      "21395 [Discriminator loss: 0.013974, acc.: 100.00%] [Generator loss: 7.406283]\n",
      "21396 [Discriminator loss: 0.143086, acc.: 96.88%] [Generator loss: 8.240736]\n",
      "21397 [Discriminator loss: 0.015852, acc.: 100.00%] [Generator loss: 8.489454]\n",
      "21398 [Discriminator loss: 0.101721, acc.: 96.88%] [Generator loss: 7.335465]\n",
      "21399 [Discriminator loss: 0.079552, acc.: 96.88%] [Generator loss: 9.090719]\n",
      "21400 [Discriminator loss: 0.035985, acc.: 98.44%] [Generator loss: 8.287350]\n",
      "21401 [Discriminator loss: 0.079412, acc.: 98.44%] [Generator loss: 8.981595]\n",
      "21402 [Discriminator loss: 0.036322, acc.: 98.44%] [Generator loss: 9.500315]\n",
      "21403 [Discriminator loss: 0.148700, acc.: 93.75%] [Generator loss: 7.937259]\n",
      "21404 [Discriminator loss: 0.064999, acc.: 96.88%] [Generator loss: 7.372423]\n",
      "21405 [Discriminator loss: 0.050194, acc.: 98.44%] [Generator loss: 7.404527]\n",
      "21406 [Discriminator loss: 0.020851, acc.: 100.00%] [Generator loss: 8.793882]\n",
      "21407 [Discriminator loss: 0.015959, acc.: 100.00%] [Generator loss: 7.693453]\n",
      "21408 [Discriminator loss: 0.055848, acc.: 95.31%] [Generator loss: 8.436710]\n",
      "21409 [Discriminator loss: 0.073156, acc.: 98.44%] [Generator loss: 9.284920]\n",
      "21410 [Discriminator loss: 0.034718, acc.: 98.44%] [Generator loss: 9.973539]\n",
      "21411 [Discriminator loss: 0.074662, acc.: 96.88%] [Generator loss: 7.854188]\n",
      "21412 [Discriminator loss: 0.077289, acc.: 95.31%] [Generator loss: 9.980416]\n",
      "21413 [Discriminator loss: 0.033111, acc.: 98.44%] [Generator loss: 9.029004]\n",
      "21414 [Discriminator loss: 0.154442, acc.: 95.31%] [Generator loss: 9.671164]\n",
      "21415 [Discriminator loss: 0.011327, acc.: 100.00%] [Generator loss: 8.821203]\n",
      "21416 [Discriminator loss: 0.136760, acc.: 95.31%] [Generator loss: 10.077759]\n",
      "21417 [Discriminator loss: 0.077394, acc.: 98.44%] [Generator loss: 10.080311]\n",
      "21418 [Discriminator loss: 0.046599, acc.: 96.88%] [Generator loss: 9.582337]\n",
      "21419 [Discriminator loss: 0.113114, acc.: 92.19%] [Generator loss: 7.802495]\n",
      "21420 [Discriminator loss: 0.048913, acc.: 96.88%] [Generator loss: 8.456152]\n",
      "21421 [Discriminator loss: 0.066451, acc.: 96.88%] [Generator loss: 8.057079]\n",
      "21422 [Discriminator loss: 0.142429, acc.: 93.75%] [Generator loss: 10.173863]\n",
      "21423 [Discriminator loss: 0.065963, acc.: 96.88%] [Generator loss: 9.528561]\n",
      "21424 [Discriminator loss: 0.013152, acc.: 100.00%] [Generator loss: 7.972701]\n",
      "21425 [Discriminator loss: 0.014036, acc.: 100.00%] [Generator loss: 8.454220]\n",
      "21426 [Discriminator loss: 0.051982, acc.: 96.88%] [Generator loss: 8.550798]\n",
      "21427 [Discriminator loss: 0.048851, acc.: 98.44%] [Generator loss: 9.830692]\n",
      "21428 [Discriminator loss: 0.022655, acc.: 100.00%] [Generator loss: 9.365080]\n",
      "21429 [Discriminator loss: 0.143941, acc.: 95.31%] [Generator loss: 7.615606]\n",
      "21430 [Discriminator loss: 0.028101, acc.: 100.00%] [Generator loss: 7.491414]\n",
      "21431 [Discriminator loss: 0.025094, acc.: 100.00%] [Generator loss: 8.204811]\n",
      "21432 [Discriminator loss: 0.210217, acc.: 92.19%] [Generator loss: 7.265838]\n",
      "21433 [Discriminator loss: 0.037498, acc.: 98.44%] [Generator loss: 8.881798]\n",
      "21434 [Discriminator loss: 0.085406, acc.: 96.88%] [Generator loss: 9.816984]\n",
      "21435 [Discriminator loss: 0.065936, acc.: 95.31%] [Generator loss: 9.842799]\n",
      "21436 [Discriminator loss: 0.099645, acc.: 96.88%] [Generator loss: 6.773157]\n",
      "21437 [Discriminator loss: 0.098572, acc.: 93.75%] [Generator loss: 8.283885]\n",
      "21438 [Discriminator loss: 0.046788, acc.: 98.44%] [Generator loss: 10.154948]\n",
      "21439 [Discriminator loss: 0.038937, acc.: 98.44%] [Generator loss: 9.682661]\n",
      "21440 [Discriminator loss: 0.015249, acc.: 100.00%] [Generator loss: 9.468237]\n",
      "21441 [Discriminator loss: 0.025347, acc.: 100.00%] [Generator loss: 8.932010]\n",
      "21442 [Discriminator loss: 0.093854, acc.: 96.88%] [Generator loss: 9.771137]\n",
      "21443 [Discriminator loss: 0.025783, acc.: 100.00%] [Generator loss: 8.193292]\n",
      "21444 [Discriminator loss: 0.007701, acc.: 100.00%] [Generator loss: 8.262403]\n",
      "21445 [Discriminator loss: 0.029198, acc.: 98.44%] [Generator loss: 8.364811]\n",
      "21446 [Discriminator loss: 0.063918, acc.: 96.88%] [Generator loss: 9.679303]\n",
      "21447 [Discriminator loss: 0.019458, acc.: 98.44%] [Generator loss: 10.727642]\n",
      "21448 [Discriminator loss: 0.077994, acc.: 95.31%] [Generator loss: 8.848700]\n",
      "21449 [Discriminator loss: 0.033335, acc.: 100.00%] [Generator loss: 9.897914]\n",
      "21450 [Discriminator loss: 0.011253, acc.: 100.00%] [Generator loss: 11.303352]\n",
      "21451 [Discriminator loss: 0.058830, acc.: 98.44%] [Generator loss: 7.908465]\n",
      "21452 [Discriminator loss: 0.080031, acc.: 96.88%] [Generator loss: 8.794401]\n",
      "21453 [Discriminator loss: 0.045694, acc.: 98.44%] [Generator loss: 8.322315]\n",
      "21454 [Discriminator loss: 0.061209, acc.: 98.44%] [Generator loss: 8.215324]\n",
      "21455 [Discriminator loss: 0.017280, acc.: 98.44%] [Generator loss: 7.597206]\n",
      "21456 [Discriminator loss: 0.142094, acc.: 95.31%] [Generator loss: 9.927210]\n",
      "21457 [Discriminator loss: 0.039015, acc.: 100.00%] [Generator loss: 10.306488]\n",
      "21458 [Discriminator loss: 0.071595, acc.: 98.44%] [Generator loss: 10.170815]\n",
      "21459 [Discriminator loss: 0.042254, acc.: 98.44%] [Generator loss: 8.728752]\n",
      "21460 [Discriminator loss: 0.096892, acc.: 95.31%] [Generator loss: 6.921071]\n",
      "21461 [Discriminator loss: 0.025642, acc.: 98.44%] [Generator loss: 9.033142]\n",
      "21462 [Discriminator loss: 0.049051, acc.: 98.44%] [Generator loss: 10.896358]\n",
      "21463 [Discriminator loss: 0.036710, acc.: 100.00%] [Generator loss: 9.858137]\n",
      "21464 [Discriminator loss: 0.008993, acc.: 100.00%] [Generator loss: 10.406485]\n",
      "21465 [Discriminator loss: 0.052871, acc.: 96.88%] [Generator loss: 7.742918]\n",
      "21466 [Discriminator loss: 0.021358, acc.: 100.00%] [Generator loss: 6.741334]\n",
      "21467 [Discriminator loss: 0.043495, acc.: 98.44%] [Generator loss: 7.132956]\n",
      "21468 [Discriminator loss: 0.027231, acc.: 100.00%] [Generator loss: 8.333592]\n",
      "21469 [Discriminator loss: 0.023183, acc.: 98.44%] [Generator loss: 8.303001]\n",
      "21470 [Discriminator loss: 0.130464, acc.: 93.75%] [Generator loss: 6.674064]\n",
      "21471 [Discriminator loss: 0.078652, acc.: 98.44%] [Generator loss: 8.746248]\n",
      "21472 [Discriminator loss: 0.054419, acc.: 98.44%] [Generator loss: 9.795994]\n",
      "21473 [Discriminator loss: 0.049494, acc.: 98.44%] [Generator loss: 9.820237]\n",
      "21474 [Discriminator loss: 0.002979, acc.: 100.00%] [Generator loss: 8.675211]\n",
      "21475 [Discriminator loss: 0.319652, acc.: 89.06%] [Generator loss: 11.131623]\n",
      "21476 [Discriminator loss: 0.111792, acc.: 95.31%] [Generator loss: 10.990686]\n",
      "21477 [Discriminator loss: 0.050676, acc.: 98.44%] [Generator loss: 10.285464]\n",
      "21478 [Discriminator loss: 0.132809, acc.: 95.31%] [Generator loss: 9.096073]\n",
      "21479 [Discriminator loss: 0.029606, acc.: 100.00%] [Generator loss: 8.207787]\n",
      "21480 [Discriminator loss: 0.030689, acc.: 98.44%] [Generator loss: 9.653354]\n",
      "21481 [Discriminator loss: 0.074152, acc.: 98.44%] [Generator loss: 9.034605]\n",
      "21482 [Discriminator loss: 0.039251, acc.: 98.44%] [Generator loss: 8.407327]\n",
      "21483 [Discriminator loss: 0.072164, acc.: 98.44%] [Generator loss: 9.927320]\n",
      "21484 [Discriminator loss: 0.006485, acc.: 100.00%] [Generator loss: 9.878571]\n",
      "21485 [Discriminator loss: 0.044705, acc.: 96.88%] [Generator loss: 9.859863]\n",
      "21486 [Discriminator loss: 0.107653, acc.: 93.75%] [Generator loss: 6.866252]\n",
      "21487 [Discriminator loss: 0.152230, acc.: 93.75%] [Generator loss: 9.802562]\n",
      "21488 [Discriminator loss: 0.008989, acc.: 100.00%] [Generator loss: 9.872967]\n",
      "21489 [Discriminator loss: 0.096845, acc.: 95.31%] [Generator loss: 9.660912]\n",
      "21490 [Discriminator loss: 0.078218, acc.: 96.88%] [Generator loss: 8.923296]\n",
      "21491 [Discriminator loss: 0.028405, acc.: 100.00%] [Generator loss: 9.425495]\n",
      "21492 [Discriminator loss: 0.072151, acc.: 95.31%] [Generator loss: 9.608656]\n",
      "21493 [Discriminator loss: 0.049160, acc.: 98.44%] [Generator loss: 9.242124]\n",
      "21494 [Discriminator loss: 0.145541, acc.: 96.88%] [Generator loss: 9.036142]\n",
      "21495 [Discriminator loss: 0.091488, acc.: 96.88%] [Generator loss: 10.240978]\n",
      "21496 [Discriminator loss: 0.041703, acc.: 98.44%] [Generator loss: 8.893199]\n",
      "21497 [Discriminator loss: 0.027517, acc.: 100.00%] [Generator loss: 9.121750]\n",
      "21498 [Discriminator loss: 0.225764, acc.: 92.19%] [Generator loss: 10.405203]\n",
      "21499 [Discriminator loss: 0.044042, acc.: 98.44%] [Generator loss: 10.126216]\n",
      "21500 [Discriminator loss: 0.236597, acc.: 85.94%] [Generator loss: 9.121429]\n",
      "21501 [Discriminator loss: 0.064165, acc.: 98.44%] [Generator loss: 10.145221]\n",
      "21502 [Discriminator loss: 0.041943, acc.: 98.44%] [Generator loss: 9.784270]\n",
      "21503 [Discriminator loss: 0.011008, acc.: 100.00%] [Generator loss: 10.616367]\n",
      "21504 [Discriminator loss: 0.056580, acc.: 98.44%] [Generator loss: 9.644625]\n",
      "21505 [Discriminator loss: 0.086536, acc.: 96.88%] [Generator loss: 9.031523]\n",
      "21506 [Discriminator loss: 0.149506, acc.: 92.19%] [Generator loss: 9.836524]\n",
      "21507 [Discriminator loss: 0.028246, acc.: 100.00%] [Generator loss: 10.445263]\n",
      "21508 [Discriminator loss: 0.136501, acc.: 93.75%] [Generator loss: 7.908407]\n",
      "21509 [Discriminator loss: 0.058218, acc.: 95.31%] [Generator loss: 8.542538]\n",
      "21510 [Discriminator loss: 0.015324, acc.: 100.00%] [Generator loss: 8.789591]\n",
      "21511 [Discriminator loss: 0.156113, acc.: 95.31%] [Generator loss: 9.294220]\n",
      "21512 [Discriminator loss: 0.073406, acc.: 96.88%] [Generator loss: 9.544522]\n",
      "21513 [Discriminator loss: 0.015414, acc.: 100.00%] [Generator loss: 10.011211]\n",
      "21514 [Discriminator loss: 0.035879, acc.: 98.44%] [Generator loss: 8.961366]\n",
      "21515 [Discriminator loss: 0.055331, acc.: 98.44%] [Generator loss: 9.881658]\n",
      "21516 [Discriminator loss: 0.144702, acc.: 92.19%] [Generator loss: 8.404385]\n",
      "21517 [Discriminator loss: 0.046608, acc.: 98.44%] [Generator loss: 9.473429]\n",
      "21518 [Discriminator loss: 0.007174, acc.: 100.00%] [Generator loss: 7.213774]\n",
      "21519 [Discriminator loss: 0.025126, acc.: 100.00%] [Generator loss: 8.766704]\n",
      "21520 [Discriminator loss: 0.076590, acc.: 96.88%] [Generator loss: 7.730510]\n",
      "21521 [Discriminator loss: 0.005107, acc.: 100.00%] [Generator loss: 8.544963]\n",
      "21522 [Discriminator loss: 0.140845, acc.: 92.19%] [Generator loss: 10.155414]\n",
      "21523 [Discriminator loss: 0.021721, acc.: 98.44%] [Generator loss: 11.321617]\n",
      "21524 [Discriminator loss: 0.083808, acc.: 96.88%] [Generator loss: 8.618869]\n",
      "21525 [Discriminator loss: 0.128762, acc.: 93.75%] [Generator loss: 8.410898]\n",
      "21526 [Discriminator loss: 0.048485, acc.: 98.44%] [Generator loss: 8.657525]\n",
      "21527 [Discriminator loss: 0.015609, acc.: 100.00%] [Generator loss: 8.376532]\n",
      "21528 [Discriminator loss: 0.081481, acc.: 96.88%] [Generator loss: 7.774759]\n",
      "21529 [Discriminator loss: 0.032296, acc.: 96.88%] [Generator loss: 9.364418]\n",
      "21530 [Discriminator loss: 0.032972, acc.: 98.44%] [Generator loss: 9.067347]\n",
      "21531 [Discriminator loss: 0.055418, acc.: 98.44%] [Generator loss: 7.312761]\n",
      "21532 [Discriminator loss: 0.094129, acc.: 95.31%] [Generator loss: 9.378280]\n",
      "21533 [Discriminator loss: 0.048008, acc.: 98.44%] [Generator loss: 10.044867]\n",
      "21534 [Discriminator loss: 0.229442, acc.: 89.06%] [Generator loss: 7.499999]\n",
      "21535 [Discriminator loss: 0.033166, acc.: 98.44%] [Generator loss: 9.036576]\n",
      "21536 [Discriminator loss: 0.015162, acc.: 100.00%] [Generator loss: 8.593412]\n",
      "21537 [Discriminator loss: 0.047336, acc.: 98.44%] [Generator loss: 10.831564]\n",
      "21538 [Discriminator loss: 0.032506, acc.: 98.44%] [Generator loss: 10.000793]\n",
      "21539 [Discriminator loss: 0.036615, acc.: 100.00%] [Generator loss: 8.886553]\n",
      "21540 [Discriminator loss: 0.105630, acc.: 95.31%] [Generator loss: 8.157588]\n",
      "21541 [Discriminator loss: 0.015306, acc.: 100.00%] [Generator loss: 6.976125]\n",
      "21542 [Discriminator loss: 0.148592, acc.: 95.31%] [Generator loss: 7.621628]\n",
      "21543 [Discriminator loss: 0.070210, acc.: 98.44%] [Generator loss: 7.148288]\n",
      "21544 [Discriminator loss: 0.031802, acc.: 98.44%] [Generator loss: 8.971746]\n",
      "21545 [Discriminator loss: 0.070183, acc.: 96.88%] [Generator loss: 6.911521]\n",
      "21546 [Discriminator loss: 0.024179, acc.: 98.44%] [Generator loss: 7.919071]\n",
      "21547 [Discriminator loss: 0.027596, acc.: 100.00%] [Generator loss: 7.904753]\n",
      "21548 [Discriminator loss: 0.114014, acc.: 93.75%] [Generator loss: 8.411096]\n",
      "21549 [Discriminator loss: 0.126075, acc.: 93.75%] [Generator loss: 9.201149]\n",
      "21550 [Discriminator loss: 0.057259, acc.: 98.44%] [Generator loss: 9.088176]\n",
      "21551 [Discriminator loss: 0.138880, acc.: 95.31%] [Generator loss: 8.473038]\n",
      "21552 [Discriminator loss: 0.190955, acc.: 92.19%] [Generator loss: 8.275180]\n",
      "21553 [Discriminator loss: 0.019348, acc.: 100.00%] [Generator loss: 9.618030]\n",
      "21554 [Discriminator loss: 0.089980, acc.: 93.75%] [Generator loss: 10.008235]\n",
      "21555 [Discriminator loss: 0.152173, acc.: 92.19%] [Generator loss: 7.820405]\n",
      "21556 [Discriminator loss: 0.065807, acc.: 96.88%] [Generator loss: 10.003693]\n",
      "21557 [Discriminator loss: 0.015678, acc.: 100.00%] [Generator loss: 8.493388]\n",
      "21558 [Discriminator loss: 0.062507, acc.: 98.44%] [Generator loss: 10.110561]\n",
      "21559 [Discriminator loss: 0.047716, acc.: 98.44%] [Generator loss: 10.686340]\n",
      "21560 [Discriminator loss: 0.062007, acc.: 96.88%] [Generator loss: 8.678959]\n",
      "21561 [Discriminator loss: 0.091284, acc.: 96.88%] [Generator loss: 9.446733]\n",
      "21562 [Discriminator loss: 0.010209, acc.: 100.00%] [Generator loss: 9.377546]\n",
      "21563 [Discriminator loss: 0.029369, acc.: 98.44%] [Generator loss: 9.337309]\n",
      "21564 [Discriminator loss: 0.030502, acc.: 98.44%] [Generator loss: 8.442765]\n",
      "21565 [Discriminator loss: 0.082382, acc.: 98.44%] [Generator loss: 7.326697]\n",
      "21566 [Discriminator loss: 0.138871, acc.: 96.88%] [Generator loss: 10.267391]\n",
      "21567 [Discriminator loss: 0.053904, acc.: 95.31%] [Generator loss: 10.134850]\n",
      "21568 [Discriminator loss: 0.018517, acc.: 98.44%] [Generator loss: 8.760673]\n",
      "21569 [Discriminator loss: 0.043191, acc.: 98.44%] [Generator loss: 9.008032]\n",
      "21570 [Discriminator loss: 0.120869, acc.: 95.31%] [Generator loss: 9.641608]\n",
      "21571 [Discriminator loss: 0.059754, acc.: 96.88%] [Generator loss: 9.909330]\n",
      "21572 [Discriminator loss: 0.048030, acc.: 96.88%] [Generator loss: 9.185143]\n",
      "21573 [Discriminator loss: 0.034483, acc.: 100.00%] [Generator loss: 9.250769]\n",
      "21574 [Discriminator loss: 0.011609, acc.: 100.00%] [Generator loss: 8.393898]\n",
      "21575 [Discriminator loss: 0.035365, acc.: 98.44%] [Generator loss: 9.865551]\n",
      "21576 [Discriminator loss: 0.037178, acc.: 98.44%] [Generator loss: 9.049458]\n",
      "21577 [Discriminator loss: 0.065994, acc.: 96.88%] [Generator loss: 7.721603]\n",
      "21578 [Discriminator loss: 0.060396, acc.: 96.88%] [Generator loss: 7.743649]\n",
      "21579 [Discriminator loss: 0.022383, acc.: 100.00%] [Generator loss: 6.140064]\n",
      "21580 [Discriminator loss: 0.022781, acc.: 98.44%] [Generator loss: 8.721436]\n",
      "21581 [Discriminator loss: 0.058253, acc.: 96.88%] [Generator loss: 8.970251]\n",
      "21582 [Discriminator loss: 0.167889, acc.: 95.31%] [Generator loss: 9.042770]\n",
      "21583 [Discriminator loss: 0.110425, acc.: 96.88%] [Generator loss: 8.414364]\n",
      "21584 [Discriminator loss: 0.111846, acc.: 92.19%] [Generator loss: 8.142585]\n",
      "21585 [Discriminator loss: 0.142559, acc.: 95.31%] [Generator loss: 11.050108]\n",
      "21586 [Discriminator loss: 0.037832, acc.: 100.00%] [Generator loss: 9.238512]\n",
      "21587 [Discriminator loss: 0.105728, acc.: 93.75%] [Generator loss: 7.473225]\n",
      "21588 [Discriminator loss: 0.031825, acc.: 98.44%] [Generator loss: 9.074259]\n",
      "21589 [Discriminator loss: 0.110809, acc.: 92.19%] [Generator loss: 8.120354]\n",
      "21590 [Discriminator loss: 0.038223, acc.: 98.44%] [Generator loss: 6.887529]\n",
      "21591 [Discriminator loss: 0.329109, acc.: 89.06%] [Generator loss: 10.508322]\n",
      "21592 [Discriminator loss: 0.028375, acc.: 98.44%] [Generator loss: 12.731371]\n",
      "21593 [Discriminator loss: 0.138518, acc.: 93.75%] [Generator loss: 9.742752]\n",
      "21594 [Discriminator loss: 0.078808, acc.: 96.88%] [Generator loss: 8.044811]\n",
      "21595 [Discriminator loss: 0.274740, acc.: 92.19%] [Generator loss: 9.561643]\n",
      "21596 [Discriminator loss: 0.008085, acc.: 100.00%] [Generator loss: 11.332476]\n",
      "21597 [Discriminator loss: 0.086728, acc.: 98.44%] [Generator loss: 9.466234]\n",
      "21598 [Discriminator loss: 0.105832, acc.: 96.88%] [Generator loss: 9.933469]\n",
      "21599 [Discriminator loss: 0.016758, acc.: 100.00%] [Generator loss: 9.482021]\n",
      "21600 [Discriminator loss: 0.023749, acc.: 100.00%] [Generator loss: 8.596599]\n",
      "21601 [Discriminator loss: 0.031001, acc.: 98.44%] [Generator loss: 9.089171]\n",
      "21602 [Discriminator loss: 0.029363, acc.: 100.00%] [Generator loss: 8.391827]\n",
      "21603 [Discriminator loss: 0.056031, acc.: 96.88%] [Generator loss: 9.279819]\n",
      "21604 [Discriminator loss: 0.108444, acc.: 95.31%] [Generator loss: 10.396665]\n",
      "21605 [Discriminator loss: 0.016652, acc.: 100.00%] [Generator loss: 9.948086]\n",
      "21606 [Discriminator loss: 0.031974, acc.: 98.44%] [Generator loss: 9.094907]\n",
      "21607 [Discriminator loss: 0.140106, acc.: 95.31%] [Generator loss: 9.379434]\n",
      "21608 [Discriminator loss: 0.301951, acc.: 90.62%] [Generator loss: 10.489317]\n",
      "21609 [Discriminator loss: 0.014748, acc.: 100.00%] [Generator loss: 9.724386]\n",
      "21610 [Discriminator loss: 0.155029, acc.: 95.31%] [Generator loss: 8.514323]\n",
      "21611 [Discriminator loss: 0.054914, acc.: 96.88%] [Generator loss: 9.329931]\n",
      "21612 [Discriminator loss: 0.056435, acc.: 98.44%] [Generator loss: 10.004162]\n",
      "21613 [Discriminator loss: 0.051285, acc.: 96.88%] [Generator loss: 9.223366]\n",
      "21614 [Discriminator loss: 0.176856, acc.: 93.75%] [Generator loss: 7.956491]\n",
      "21615 [Discriminator loss: 0.015421, acc.: 100.00%] [Generator loss: 11.293427]\n",
      "21616 [Discriminator loss: 0.103163, acc.: 96.88%] [Generator loss: 8.124348]\n",
      "21617 [Discriminator loss: 0.246954, acc.: 92.19%] [Generator loss: 10.213997]\n",
      "21618 [Discriminator loss: 0.037847, acc.: 98.44%] [Generator loss: 11.928104]\n",
      "21619 [Discriminator loss: 0.014837, acc.: 100.00%] [Generator loss: 11.062952]\n",
      "21620 [Discriminator loss: 0.091669, acc.: 95.31%] [Generator loss: 8.675039]\n",
      "21621 [Discriminator loss: 0.014783, acc.: 100.00%] [Generator loss: 7.120625]\n",
      "21622 [Discriminator loss: 0.166407, acc.: 93.75%] [Generator loss: 7.787288]\n",
      "21623 [Discriminator loss: 0.139219, acc.: 95.31%] [Generator loss: 8.604654]\n",
      "21624 [Discriminator loss: 0.112322, acc.: 95.31%] [Generator loss: 8.419697]\n",
      "21625 [Discriminator loss: 0.082887, acc.: 96.88%] [Generator loss: 9.256511]\n",
      "21626 [Discriminator loss: 0.018610, acc.: 100.00%] [Generator loss: 11.061461]\n",
      "21627 [Discriminator loss: 0.117849, acc.: 95.31%] [Generator loss: 7.339628]\n",
      "21628 [Discriminator loss: 0.061801, acc.: 98.44%] [Generator loss: 7.119551]\n",
      "21629 [Discriminator loss: 0.020309, acc.: 100.00%] [Generator loss: 7.558290]\n",
      "21630 [Discriminator loss: 0.033138, acc.: 96.88%] [Generator loss: 7.608187]\n",
      "21631 [Discriminator loss: 0.020767, acc.: 100.00%] [Generator loss: 8.247150]\n",
      "21632 [Discriminator loss: 0.089170, acc.: 95.31%] [Generator loss: 7.777978]\n",
      "21633 [Discriminator loss: 0.103917, acc.: 93.75%] [Generator loss: 8.180227]\n",
      "21634 [Discriminator loss: 0.051877, acc.: 95.31%] [Generator loss: 12.108537]\n",
      "21635 [Discriminator loss: 0.049989, acc.: 98.44%] [Generator loss: 10.319597]\n",
      "21636 [Discriminator loss: 0.059967, acc.: 98.44%] [Generator loss: 8.807976]\n",
      "21637 [Discriminator loss: 0.077247, acc.: 98.44%] [Generator loss: 9.043406]\n",
      "21638 [Discriminator loss: 0.026435, acc.: 98.44%] [Generator loss: 11.309896]\n",
      "21639 [Discriminator loss: 0.078778, acc.: 96.88%] [Generator loss: 9.022370]\n",
      "21640 [Discriminator loss: 0.049964, acc.: 96.88%] [Generator loss: 10.026103]\n",
      "21641 [Discriminator loss: 0.081785, acc.: 96.88%] [Generator loss: 8.217276]\n",
      "21642 [Discriminator loss: 0.069999, acc.: 96.88%] [Generator loss: 7.405140]\n",
      "21643 [Discriminator loss: 0.184358, acc.: 93.75%] [Generator loss: 9.483942]\n",
      "21644 [Discriminator loss: 0.025996, acc.: 98.44%] [Generator loss: 10.148867]\n",
      "21645 [Discriminator loss: 0.166174, acc.: 93.75%] [Generator loss: 8.056341]\n",
      "21646 [Discriminator loss: 0.088259, acc.: 95.31%] [Generator loss: 8.512081]\n",
      "21647 [Discriminator loss: 0.079473, acc.: 96.88%] [Generator loss: 8.654094]\n",
      "21648 [Discriminator loss: 0.127746, acc.: 93.75%] [Generator loss: 8.110575]\n",
      "21649 [Discriminator loss: 0.017184, acc.: 100.00%] [Generator loss: 10.888762]\n",
      "21650 [Discriminator loss: 0.018454, acc.: 100.00%] [Generator loss: 10.855525]\n",
      "21651 [Discriminator loss: 0.046222, acc.: 98.44%] [Generator loss: 9.284206]\n",
      "21652 [Discriminator loss: 0.062613, acc.: 96.88%] [Generator loss: 7.902779]\n",
      "21653 [Discriminator loss: 0.109443, acc.: 95.31%] [Generator loss: 7.639149]\n",
      "21654 [Discriminator loss: 0.201519, acc.: 90.62%] [Generator loss: 10.368365]\n",
      "21655 [Discriminator loss: 0.115573, acc.: 98.44%] [Generator loss: 10.699322]\n",
      "21656 [Discriminator loss: 0.065918, acc.: 96.88%] [Generator loss: 9.605967]\n",
      "21657 [Discriminator loss: 0.098639, acc.: 96.88%] [Generator loss: 10.448884]\n",
      "21658 [Discriminator loss: 0.032335, acc.: 100.00%] [Generator loss: 8.535772]\n",
      "21659 [Discriminator loss: 0.090184, acc.: 96.88%] [Generator loss: 10.224661]\n",
      "21660 [Discriminator loss: 0.039271, acc.: 98.44%] [Generator loss: 9.883113]\n",
      "21661 [Discriminator loss: 0.155051, acc.: 92.19%] [Generator loss: 5.969275]\n",
      "21662 [Discriminator loss: 0.110328, acc.: 93.75%] [Generator loss: 8.757587]\n",
      "21663 [Discriminator loss: 0.019679, acc.: 98.44%] [Generator loss: 8.426634]\n",
      "21664 [Discriminator loss: 0.017011, acc.: 100.00%] [Generator loss: 9.205689]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    facegenerator = FaceGenerator(64,64,3)\n",
    "    facegenerator.train(datafolder='../input/onlyiiitbproperdataset/onlyiiitbproperdataset/OnlyIIITBProperDataSet/',\n",
    "                        epochs=100001, batch_size=32,save_images_interval=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.random.randint(0,859,32)\n",
    "# np.random.normal(0,1,(32,100)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x720 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = 8, 10\n",
    "\n",
    "img = Image.open('new_generated_80000.png')\n",
    "plt.imshow(img)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
